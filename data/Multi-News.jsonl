{"id":"2949225035","dialogue":"\"We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior.\"","summary":"\"Within the MAS community, some work @cite_15 has focused on how artificial AI-based learning agents would fare in communities of similar agents. For example, @cite_6 and @cite_8 show how agents can learn the capabilities of others via repeated interactions, but these agents do not learn to predict what actions other might take. Most of the work in MAS also fails to recognize the possible gains from using explicit agent models to predict agent actions. @cite_9 is an exception and gives another approach for using nested agent models. However, they do not go so far as to try to quantify the advantages of their nested models or show how these could be learned via observations. We believe that our research will bring to the foreground some of the common observations seen in these research areas and help to clarify the implications and utility of learning and using nested agent models.\"","":""}
{"id":"2963943458","dialogue":"\"Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.\"","summary":"\"Grasping action is the most basic component of any interaction and it is composed of three major components @cite_21 . The first one is related to the process of approaching the arm and hand to the target object, considering the overall body movement. The second component focuses on the hand and body pre-shaping before the grasping action. Finally, the last component fits the hand to the geometry of the object by closing each of the fingers until contact is established.\"","":""}
{"id":"2963943458","dialogue":"\"Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.\"","summary":"Grasping data-driven approaches have existed since a long time ago @cite_21 . These methods are based on large databases of predefined hand poses selected using user criteria or based on grasp taxonomies (i.e. final grasp poses when an object was successfully grasped) which provide us the ability to discriminate between different grasp types.","":""}
{"id":"2963943458","dialogue":"\"Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.\"","summary":"\"The selection process is also constrained by the hand high degree of freedom (DOF). In order to deal with dimensionality and redundancy many researchers have used techniques such as principal component analysis (PCA) @cite_1 @cite_28 . For the same purpose, @cite_22 studied the correlations between hand DOFs aiming to simplify hand models reducing DOF number. The results suggest to simplify hand models by reducing DOFs from 50 to 15 for both hands in conjunction without loosing relevant features.\"","":""}
{"id":"2963943458","dialogue":"\"Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.\"","summary":"\"In order to achieve realistic object interactions, physical simulations on the objects should also be considered @cite_29 @cite_11 @cite_26 . Moreover, hand and finger movement trajectories need to be both, kinematically and dynamically valid @cite_19 . @cite_29 simulate hand interaction, such as two hands grasping each other in the handshake gesture. @cite_26 simulate grasping an object, drop it on a specific spot on the palm and let it roll on the hand palm. A limitation of this approach is that information about the object must be known in advance, which disable robot to interact with unknown objects. By using an initial grasp pose and a desired object trajectory, the algorithm proposed by Liu @cite_15 can generate physically-based hand manipulation poses varying the contact points with the object, grasping forces and also joint configurations. This approach works well for complex manipulations such as twist-opening a bottle. Ye and Liu @cite_19 reconstruct a realistic hand motion and grasping generating feasible contact point trajectories. Selection of valid motions is defined as a randomized depth-first tree traversal, where nodes are recursively expanded if they are kinematically and dynamically feasible. Otherwise, backtracking is performed in order to explore other possibilities.\"","":""}
{"id":"1742257591","dialogue":"\"Graph Interpolation Grammars are a declarative formalism with an operational semantics. Their goal is to emulate salient features of the human parser, and notably incrementality. The parsing process defined by GIGs incrementally builds a syntactic representation of a sentence as each successive lexeme is read. A GIG rule specifies a set of parse configurations that trigger its application and an operation to perform on a matching configuration. Rules are partly context-sensitive; furthermore, they are reversible, meaning that their operations can be undone, which allows the parsing process to be nondeterministic. These two factors confer enough expressive power to the formalism for parsing natural languages.\"","summary":"\"Graph interpolation can be viewed as an extension of tree adjunction to parse graphs. And, indeed, TAGs @cite_2 , by introducing a 2-dimensional formalism into computational linguistics, have made a decisive step towards designing a syntactic theory that is both computationally tractable and linguistically realistic. In this respect, it is an obligatory reference for any syntactic theory intent on satisfying these criteria.\"","":""}
{"id":"1742257591","dialogue":"\"Graph Interpolation Grammars are a declarative formalism with an operational semantics. Their goal is to emulate salient features of the human parser, and notably incrementality. The parsing process defined by GIGs incrementally builds a syntactic representation of a sentence as each successive lexeme is read. A GIG rule specifies a set of parse configurations that trigger its application and an operation to perform on a matching configuration. Rules are partly context-sensitive; furthermore, they are reversible, meaning that their operations can be undone, which allows the parsing process to be nondeterministic. These two factors confer enough expressive power to the formalism for parsing natural languages.\"","summary":"\"In Lexical Functional Grammars @cite_4 , grammatical functions are loosely coupled with phrase structure, which seems to be just the opposite of what is done in a GIG, in which functional edges are part of the phrase structure. Nonetheless, these two approaches share the concern of bringing out a functional structure, even if much of what enters into an f-structure (i.e. a functional structure) in LFG is to be addressed by the semantic component ---a topic for further research--- in GIG.\"","":""}
{"id":"1575569168","dialogue":"\"Automatic text categorization is a complex and useful task for many natural language processing applications. Recent approaches to text categorization focus more on algorithms than on resources involved in this operation. In contrast to this trend, we present an approach based on the integration of widely available resources as lexical databases and training collections to overcome current limitations of the task. Our approach makes use of WordNet synonymy information to increase evidence for bad trained categories. When testing a direct categorization, a WordNet based one, a training algorithm, and our integrated approach, the latter exhibits a better perfomance than any of the others. Incidentally, WordNet based approach perfomance is comparable with the training approach one.\"","summary":"\"To our knowledge, lexical databases have been used only once in TC. Hearst @cite_10 adapted a disambiguation algorithm by Yarowsky using WordNet to recognize category occurrences. Categories are made of WordNet terms, which is not the general case of standard or user-defined categories. It is a hard task to adapt WordNet subsets to pre-existing categories, especially when they are domain dependent. Hearst's approach shows promising results confirmed by the fact that our WordNet -based approach performs at least equally to a simple training approach.\"","":""}
{"id":"1575569168","dialogue":"\"Automatic text categorization is a complex and useful task for many natural language processing applications. Recent approaches to text categorization focus more on algorithms than on resources involved in this operation. In contrast to this trend, we present an approach based on the integration of widely available resources as lexical databases and training collections to overcome current limitations of the task. Our approach makes use of WordNet synonymy information to increase evidence for bad trained categories. When testing a direct categorization, a WordNet based one, a training algorithm, and our integrated approach, the latter exhibits a better perfomance than any of the others. Incidentally, WordNet based approach perfomance is comparable with the training approach one.\"","summary":"\"Lexical databases have been employed recently in word sense disambiguation. For example, Agirre and Rigau @cite_3 make use of a semantic distance that takes into account structural factors in WordNet for achieving good results for this task. Additionally, Resnik @cite_2 combines the use of WordNet and a text collection for a definition of a distance for disambiguating noun groupings. Although the text collection is not a training collection (in the sense of a collection of manually labelled texts for a pre-defined text processing task), his approach can be regarded as the most similar to ours in the disambiguation task. Finally, Ng and Lee @cite_11 make use of several sources of information inside a training collection (neighborhood, part of speech, morfological form, etc.) to get good results in disambiguating unrestricted text.\"","":""}
{"id":"2951421399","dialogue":"\"This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.\"","summary":"\"Word--sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., @cite_13 , @cite_2 , @cite_24 , @cite_6 , @cite_14 , @cite_5 , @cite_3 , @cite_16 , @cite_37 ). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create. It seems more reasonable to assume that such text will not usually be available and attempt to pursue unsupervised approaches that rely only on the features in a text that can be automatically identified.\"","":""}
{"id":"2951421399","dialogue":"\"This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.\"","summary":"\"A more recent bootstrapping approach is described in @cite_23 . This algorithm requires a small number of training examples to serve as a seed. There are a variety of options discussed for automatically selecting seeds; one is to identify collocations that uniquely distinguish between senses. For plant , the collocations manufacturing plant and living plant make such a distinction. Based on 106 examples of manufacturing plant and 82 examples of living plant this algorithm is able to distinguish between two senses of plant for 7,350 examples with 97 percent accuracy. Experiments with 11 other words using collocation seeds result in an average accuracy of 96 percent.\"","":""}
{"id":"2951421399","dialogue":"\"This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.\"","summary":"\"While @cite_23 does not discuss distinguishing more than 2 senses of a word, there is no immediate reason to doubt that the one sense per collocation'' rule @cite_24 would still hold for a larger number of senses. In future work we will evaluate using the one sense per collocation'' rule to seed our various methods. This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency.\"","":""}
{"id":"2951421399","dialogue":"\"This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.\"","summary":"\"Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., @cite_19 , @cite_26 , @cite_25 , @cite_34 , @cite_1 , @cite_35 ).\"","":""}
{"id":"2951421399","dialogue":"\"This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.\"","summary":"\"An early application of clustering to word--sense disambiguation is described in @cite_29 . There words are represented in terms of the co-occurrence statistics of four letter sequences. This representation uses 97 features to characterize a word, where each feature is a linear combination of letter four-grams formulated by a singular value decomposition of a 5000 by 5000 matrix of letter four-gram co-occurrence frequencies. The weight associated with each feature reflects all usages of the word in the sample. A context vector is formed for each occurrence of an ambiguous word by summing the vectors of the contextual words (the number of contextual words considered in the sum is unspecified). The set of context vectors for the word to be disambiguated are then clustered, and the clusters are manually sense tagged.\"","":""}
{"id":"2951421399","dialogue":"\"This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set.\"","summary":"The features used in this work are complex and difficult to interpret and it isn't clear that this complexity is required. @cite_23 compares his method to @cite_29 and shows that for four words the former performs significantly better in distinguishing between two senses.","":""}
{"id":"2950225692","dialogue":"\"This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).\"","summary":"\"The literature on corpus-based determination of word similarity has recently been growing by leaps and bounds, and is too extensive to discuss in detail here (for a review, see @cite_1 ), but most approaches to the problem share a common assumption: semantically similar words have similar distributional behavior in a corpus. Using this assumption, it is common to treat the words that co-occur near a word as constituting features, and to compute word similarity in terms of how similar their feature sets are. As in information retrieval, the feature'' representation of a word often takes the form of a vector, with the similarity computation amounting to a computation of distance in a highly multidimensional space. Given a distance measure, it is not uncommon to derive word classes by hierarchical clustering. A difficulty with most distributional methods, however, is how the measure of similarity (or distance) is to be interpreted. Although word classes resulting from distributional clustering are often described as semantic,'' they often capture syntactic, pragmatic, or stylistic factors as well.\"","":""}
{"id":"2950202165","dialogue":"\"Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation.\"","summary":"\"Statistical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part--of--speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word--sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word--sense disambiguation both here and in a variety of other works (e.g., @cite_18 , @cite_10 , @cite_2 , and @cite_1 ).\"","":""}
{"id":"2950202165","dialogue":"\"Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation.\"","summary":"\"In order to utilize models with more complicated interactions among feature variables, @cite_8 introduce the use of sequential model selection and decomposable models for word--sense disambiguation. They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for model predictive power. In their procedure, the exact conditional test was used to guide the generation of new models and the test of model predictive power was used to select the final model from among those generated during the search.\"","":""}
{"id":"2950202165","dialogue":"\"Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation.\"","summary":"\"Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., @cite_17 , @cite_20 , and @cite_14 present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., @cite_6 ), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. @cite_7 and @cite_1 ) but, while it is a type of model selection, the models are not parametric.\"","":""}
{"id":"2950224005","dialogue":"\"In this paper, we define the notion of a preventative expression and discuss a corpus study of such expressions in instructional text. We discuss our coding schema, which takes into account both form and function features, and present measures of inter-coder reliability for those features. We then discuss the correlations that exist between the function and the form features.\"","summary":"\"In computational linguistics, on the other hand, positive imperatives have been extensively investigated, both from the point of view of interpretation @cite_13 @cite_8 @cite_6 @cite_1 and generation @cite_9 @cite_10 @cite_4 @cite_7 . Little work, however, has been directed at negative imperatives. (for exceptions see the work of in interpretation and of in generation).\"","":""}
{"id":"2971016516","dialogue":"\"Hashing is promising for large-scale information retrieval tasks thanks to the efficiency of distance evaluation between binary codes. Generative hashing is often used to generate hashing codes in an unsupervised way. However, existing generative hashing methods only considered the use of simple priors, like Gaussian and Bernoulli priors, which limits these methods to further improve their performance. In this paper, two mixture-prior generative models are proposed, under the objective to produce high-quality hashing codes for documents. Specifically, a Gaussian mixture prior is first imposed onto the variational auto-encoder (VAE), followed by a separate step to cast the continuous latent representation of VAE into binary code. To avoid the performance loss caused by the separate casting, a model using a Bernoulli mixture prior is further developed, in which an end-to-end training is admitted by resorting to the straight-through (ST) discrete gradient estimator. Experimental results on several benchmark datasets demonstrate that the proposed methods, especially the one using Bernoulli mixture priors, consistently outperform existing ones by a substantial margin.\"","summary":"\"Recently, VDSH @cite_28 proposed to use a VAE to learn the latent representations of documents and then use a separate stage to cast the continuous representations into binary codes. While fairly successful, this generative hashing model requires a two-stage training. NASH @cite_11 proposed to substitute the Gaussian prior in VDSH with a Bernoulli prior to tackle this problem, by using a straight-through estimator @cite_3 to estimate the gradient of neural network involving the binary variables. This model can be trained in an end-to-end manner. Our models differ from VDSH and NASH in that mixture priors are employed to yield better hashing codes, whereas only the simplest priors are used in both VDSH and NASH.\"","":""}
{"id":"2970733215","dialogue":"\"Blind image denoising is an important yet very challenging problem in computer vision due to the complicated acquisition process of real images. In this work we propose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.\"","summary":"\"Most classical image denoising methods belong to this category, through designing a MAP model with a fidelity loss term and a regularization one delivering the pre-known image prior. Along this line, total variation denoising @cite_30 , anisotropic diffusion @cite_40 and wavelet coring @cite_25 use the statistical regularities of images to remove the image noise. Later, the nonlocal similarity prior, meaning many small patches in a non-local image area possess similar configurations, was widely used in image denoising. Typical ones include CBM3D @cite_13 and non-local means @cite_10 . Some dictionary learning methods @cite_32 @cite_6 @cite_9 and Field-of-Experts (FoE) @cite_23 , also revealing certain prior knowledge of image patches, had also been attempted for the task. Several other approaches focusing on the fidelity term, which are mainly determined by the noise assumption on data. E.g., Mulitscale @cite_18 assumed the noise of each patch and its similar patches in the same image to be correlated Gaussian distribution, and LR-MoG @cite_19 , DP-GMM @cite_21 and DDPT @cite_8 fitted the image noise by using Mixture of Gaussian (MoG) as an approximator for noises.\"","":""}
{"id":"2970733215","dialogue":"\"Blind image denoising is an important yet very challenging problem in computer vision due to the complicated acquisition process of real images. In this work we propose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.\"","summary":"\"Instead of pre-setting image prior, deep learning methods directly learn a denoiser (formed as a deep neural network) from noisy to clean ones on a large collection of noisy-clean image pairs. Jain and Seung @cite_1 firstly adopted a five layer convolution neural network (CNN) for the task. Then some auto-encoder based methods @cite_22 @cite_34 were applied. Meantime, @cite_11 achieved the comparable performance with BM3D using plain multi-layer perceptron (MLP). @cite_27 further proposed the denoising convolution network (DnCNN) and achieved state-of-the-art performance on Gaussian denoising tasks. @cite_33 proposed a deep fully convolution encoding-decoding network with symmetric skip connection. In order to boost the flexibility against spatial variant noise, FFDNet @cite_26 was proposed by pre-evaluating the noise level and inputting it to the network together with the noisy image. @cite_16 and @cite_28 both attempted to simulate the generation process of the images in camera.\"","":""}
{"id":"2970096436","dialogue":"\"Textual network embeddings aim to learn a low-dimensional representation for every node in the network so that both the structural and textual information from the networks can be well preserved in the representations. Traditionally, the structural and textual embeddings were learned by models that rarely take the mutual influences between them into account. In this paper, a deep neural architecture is proposed to effectively fuse the two kinds of informations into one representation. The novelties of the proposed architecture are manifested in the aspects of a newly defined objective function, the complementary information fusion method for structural and textual features, and the mutual gate mechanism for textual feature extraction. Experimental results show that the proposed model outperforms the comparing methods on all three datasets.\"","summary":"\"Text Embedding There has been various methods to embed textual information into vector representations for NLP tasks. The classical method for embedding textual information could be one-hot vector, term frequency inverse document frequency (TF-IDF), etc. Due to the high-dimension and sparsity problems in here, @cite_18 proposed a novel neural network based skip-gram model to learn distributed word embeddings via word co-occurrences in a local window of textual content. To exploit the internal structure of text, convolutional neural networks (CNNs) @cite_4 @cite_10 is applied to obtain latent features of local textual content. Then, by following a pooling layer, fixed-length representations are generated. To have the embeddings better reflect the correlations among texts, soft attention mechanisms @cite_3 @cite_11 is proposed to calculate the relative importances of words in a sentence by evaluating their relevances to the content of comparing sentences. Alternatively, gating mechanism is applied to strengthen the relevant textual information, while weakening the irrelevant one by controlling the information-flow path of a network in @cite_17 @cite_20 .\"","":""}
{"id":"2971306187","dialogue":"\"Recurrent Neural Network (RNN) has been deployed as the de facto model to tackle a wide variety of language generation problems and achieved state-of-the-art (SOTA) performance. However despite its impressive results, the large number of parameters in the RNN model makes deployment in mobile and embedded devices infeasible. Driven by this problem, many works have proposed a number of pruning methods to reduce the sizes of the RNN model. In this work, we propose an end-to-end pruning method for image captioning models equipped with visual attention. Our proposed method is able to achieve sparsity levels up to 97.5 without significant performance loss relative to the baseline (around 1 loss at 40x compression of GRU model). Our method is also simple to use and tune, facilitating faster development times for neural network practitioners. We perform extensive experiments on the popular MS-COCO dataset in order to empirically validate the efficacy of our proposed method.\"","summary":"\"Modern neural networks that provide good performance tend to be large and overparameterised, fuelled by observations that larger @cite_38 @cite_6 @cite_46 networks tend to be easier to train. This in turn drives numerous efforts to reduce model size using techniques such as weight pruning and quantisation @cite_34 @cite_5 @cite_31 .\"","":""}
{"id":"2971306187","dialogue":"\"Recurrent Neural Network (RNN) has been deployed as the de facto model to tackle a wide variety of language generation problems and achieved state-of-the-art (SOTA) performance. However despite its impressive results, the large number of parameters in the RNN model makes deployment in mobile and embedded devices infeasible. Driven by this problem, many works have proposed a number of pruning methods to reduce the sizes of the RNN model. In this work, we propose an end-to-end pruning method for image captioning models equipped with visual attention. Our proposed method is able to achieve sparsity levels up to 97.5 without significant performance loss relative to the baseline (around 1 loss at 40x compression of GRU model). Our method is also simple to use and tune, facilitating faster development times for neural network practitioners. We perform extensive experiments on the popular MS-COCO dataset in order to empirically validate the efficacy of our proposed method.\"","summary":"\"Early works like @cite_48 and @cite_2 explored pruning by computing the Hessian of the loss with respect to the parameters in order to assess the saliency of each parameter. Other works involving saliency computation include @cite_0 and @cite_33 where sensitivity of the loss with respect to neurons and weights are used respectively. On the other hand, works such as @cite_13 @cite_55 directly induce network sparsity by incorporating sparsity-enforcing penalty terms into the loss function.\"","":""}
{"id":"2971306187","dialogue":"\"Recurrent Neural Network (RNN) has been deployed as the de facto model to tackle a wide variety of language generation problems and achieved state-of-the-art (SOTA) performance. However despite its impressive results, the large number of parameters in the RNN model makes deployment in mobile and embedded devices infeasible. Driven by this problem, many works have proposed a number of pruning methods to reduce the sizes of the RNN model. In this work, we propose an end-to-end pruning method for image captioning models equipped with visual attention. Our proposed method is able to achieve sparsity levels up to 97.5 without significant performance loss relative to the baseline (around 1 loss at 40x compression of GRU model). Our method is also simple to use and tune, facilitating faster development times for neural network practitioners. We perform extensive experiments on the popular MS-COCO dataset in order to empirically validate the efficacy of our proposed method.\"","summary":"\"Most of the recent works in network pruning focused on vision-centric classification tasks using Convolutional Neural Networks (CNNs) and occasionally RNNs. Techniques proposed include magnitude-based pruning @cite_60 @cite_9 @cite_52 and variational pruning @cite_21 @cite_19 @cite_53 . Among these, magnitude-based weight pruning have become popular due to their effectiveness and simplicity. Most notably, @cite_60 employed a combination of pruning, quantization and Huffman encoding resulting in massive reductions in model size without affecting accuracy. While unstructured sparse connectivity provides reduction in storage size, it requires sparse General Matrix-Matrix Multiply (GEMM) libraries such as cuSPARSE and SPBLAS in order to achieve accelerated inference. Motivated by existing hardware architectures optimised for dense linear algebra, many works propose techniques to prune and induce sparsity in a structured way in which entire filters are removed @cite_24 @cite_61 @cite_49 .\"","":""}
{"id":"2971306187","dialogue":"\"Recurrent Neural Network (RNN) has been deployed as the de facto model to tackle a wide variety of language generation problems and achieved state-of-the-art (SOTA) performance. However despite its impressive results, the large number of parameters in the RNN model makes deployment in mobile and embedded devices infeasible. Driven by this problem, many works have proposed a number of pruning methods to reduce the sizes of the RNN model. In this work, we propose an end-to-end pruning method for image captioning models equipped with visual attention. Our proposed method is able to achieve sparsity levels up to 97.5 without significant performance loss relative to the baseline (around 1 loss at 40x compression of GRU model). Our method is also simple to use and tune, facilitating faster development times for neural network practitioners. We perform extensive experiments on the popular MS-COCO dataset in order to empirically validate the efficacy of our proposed method.\"","summary":"\"[label= *)] Simple and fast. Our approach enables easy pruning of the RNN decoder equipped with visual attention, whereby the best number of weights to prune in each layer is automatically determined. Compared to works such as @cite_45 @cite_51 , our approach is simpler with a single hyperparameter versus @math - @math hyperparameters. Our method also does not rely on reinforcement learning techniques such as in the work of @cite_4 . Moreover, our method applies pruning to all the weights in the RNN decoder and does not require special considerations to exclude pruning from certain weight classes. Lastly our method completes pruning in a single-shot process rather than requiring iterative train-and-prune process as in @cite_17 @cite_18 @cite_47 @cite_50 . Good performance-to-sparsity ratio enabling extreme sparsity. Our approach achieves good performance across sparsity levels from @math l_2 @math l_1 @math l_0$ regulariser are used to encourage network sparsity. Their work also only focuses on image classification using CNNs.\"","":""}
{"id":"2971306187","dialogue":"\"Recurrent Neural Network (RNN) has been deployed as the de facto model to tackle a wide variety of language generation problems and achieved state-of-the-art (SOTA) performance. However despite its impressive results, the large number of parameters in the RNN model makes deployment in mobile and embedded devices infeasible. Driven by this problem, many works have proposed a number of pruning methods to reduce the sizes of the RNN model. In this work, we propose an end-to-end pruning method for image captioning models equipped with visual attention. Our proposed method is able to achieve sparsity levels up to 97.5 without significant performance loss relative to the baseline (around 1 loss at 40x compression of GRU model). Our method is also simple to use and tune, facilitating faster development times for neural network practitioners. We perform extensive experiments on the popular MS-COCO dataset in order to empirically validate the efficacy of our proposed method.\"","summary":"\"While there are other works on compressing RNNs, most of the methods proposed either comes with structural constraints or are complementary to model pruning in principle. Examples include using low-rank matrix factorisations @cite_28 @cite_40 , product quantisation on embeddings @cite_14 , factorising word predictions into multiple time steps @cite_36 @cite_15 @cite_25 , and grouping RNNs @cite_56 . Lastly, another closely related work by @cite_18 also incorporated model pruning into image captioning. However we note three notable differences: 1) their work is focused on proposing a new LSTM cell structure named the ; 2) their work utilises the grow-and-prune (GP) method @cite_47 which necessitates compute and time expensive iterative pruning; and 3) the compression figures stated are calculated based on the size of the LSTM cells instead of the entire decoder.\"","":""}
{"id":"2971193649","dialogue":"\"BERT (, 2018) and RoBERTa (, 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ( 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\"","summary":"\"BERT @cite_19 is a pre-trained transformer network @cite_11 , which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a new state-of-the-art performance on the Semantic Textual Semilarity (STS) benchmark @cite_18 . RoBERTa @cite_29 showed, that the performance of BERT can further improved by small adaptations to the pre-training process. We also tested XLNet @cite_22 , but it led in general to worse results than BERT.\"","":""}
{"id":"2970282336","dialogue":"\"Video action recognition, which is topical in computer vision and video analysis, aims to allocate a short video clip to a pre-defined category such as brushing hair or climbing stairs. Recent works focus on action recognition with deep neural networks that achieve state-of-the-art results in need of high-performance platforms. Despite the fast development of mobile computing, video action recognition on mobile devices has not been fully discussed. In this paper, we focus on the novel mobile video action recognition task, where only the computational capabilities of mobile devices are accessible. Instead of raw videos with huge storage, we choose to extract multiple modalities (including I-frames, motion vectors, and residuals) directly from compressed videos. By employing MobileNetV2 as backbone, we propose a novel Temporal Trilinear Pooling (TTP) module to fuse the multiple modalities for mobile video action recognition. In addition to motion vectors, we also provide a temporal fusion method to explicitly induce the temporal context. The efficiency test on a mobile device indicates that our model can perform mobile video action recognition at about 40FPS. The comparative results on two benchmarks show that our model outperforms existing action recognition methods in model size and time consuming, but with competitive accuracy.\"","summary":"\"Pooling methods are requisite either in two-stream networks @cite_32 @cite_33 or in other feature fusion models. @cite_9 simply uses average pooling and outperforms others. @cite_28 proposes bilinear pooling to model local parts of object: two feature representations are learned separately and then multiplied using the outer product to obtain the holistic representation. @cite_23 combines two-stream network with a compact bilinear representation @cite_4 . @cite_30 defines a general kernel-based pooling framework which captures higher-order interactions of features. However, most existing bilinear pooling models are capable to combine only two features, and none of their variants could cope with more than two features, which is needed in video action recognition.\"","":""}
{"id":"2970282336","dialogue":"\"Video action recognition, which is topical in computer vision and video analysis, aims to allocate a short video clip to a pre-defined category such as brushing hair or climbing stairs. Recent works focus on action recognition with deep neural networks that achieve state-of-the-art results in need of high-performance platforms. Despite the fast development of mobile computing, video action recognition on mobile devices has not been fully discussed. In this paper, we focus on the novel mobile video action recognition task, where only the computational capabilities of mobile devices are accessible. Instead of raw videos with huge storage, we choose to extract multiple modalities (including I-frames, motion vectors, and residuals) directly from compressed videos. By employing MobileNetV2 as backbone, we propose a novel Temporal Trilinear Pooling (TTP) module to fuse the multiple modalities for mobile video action recognition. In addition to motion vectors, we also provide a temporal fusion method to explicitly induce the temporal context. The efficiency test on a mobile device indicates that our model can perform mobile video action recognition at about 40FPS. The comparative results on two benchmarks show that our model outperforms existing action recognition methods in model size and time consuming, but with competitive accuracy.\"","summary":"\"Recently, lightweight neural networks including SqeezeNet @cite_15 , Xception @cite_8 , ShuffleNet @cite_31 , ShuffleNetV2 @cite_7 , MobileNet @cite_12 , and MobileNetV2 @cite_10 have been proposed to run on mobile devices with the parameters and computation reduced significantly. Since we focus on mobile video action recognition, all these lightweight models could be use as backbone.\"","":""}
{"id":"2970931569","dialogue":"\"In this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. This problem includes difference of convex (DC) functions and a family of bi-convex functions as special cases. We develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. To the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. Our algorithms enable efficient stochastic optimization of a family of non-decomposable DC functions and a family of bi-convex functions. To demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization, and experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.\"","summary":"\"Another important result is following the Bennett's inequality. Corollary 5 in @cite_7 shows that: where @math is the sample variance. It is notable that @math is equivalent (with a constant scaling) to the empirical variance @math . Similarly, the above uniform estimate can be extended to infinite loss classes using different complexity measures .\"","":""}
{"id":"2970931569","dialogue":"\"In this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. This problem includes difference of convex (DC) functions and a family of bi-convex functions as special cases. We develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. To the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. Our algorithms enable efficient stochastic optimization of a family of non-decomposable DC functions and a family of bi-convex functions. To demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization, and experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.\"","summary":"\"An intuitive approach to considering the variance-based regularization is to include the first two terms on the right hand side into the objective, which is the formulation proposed in @cite_7 , i.e., sample variance penalty (SVP): An excess risk bound of @math may be achieved by solving the SVP. However, @cite_7 does not consider solution methods for solving the above variance-regularized empirical risk minimization problem.\"","":""}
{"id":"2970931569","dialogue":"\"In this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. This problem includes difference of convex (DC) functions and a family of bi-convex functions as special cases. We develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. To the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. Our algorithms enable efficient stochastic optimization of a family of non-decomposable DC functions and a family of bi-convex functions. To demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization, and experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.\"","summary":"\"Recently, @cite_17 proposed a min-max formulation based on distributionally robust optimization for variance-based regularization as following: where @math is a hyper-parameter, @math , @math , and @math is called the @math -divergence based on @math . The above problem is convex-concave when the loss function @math is convex in terms of @math . It is was shown in that the above min-max formulation is equivalent to the problem ) with a proper value of @math with high probability under the assumption that the number of training examples @math is sufficiently large (see Theorem 1 and Theorem 2 in @cite_17 ).\"","":""}
{"id":"2970931569","dialogue":"\"In this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. This problem includes difference of convex (DC) functions and a family of bi-convex functions as special cases. We develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. To the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. Our algorithms enable efficient stochastic optimization of a family of non-decomposable DC functions and a family of bi-convex functions. To demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization, and experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.\"","summary":"\"To solve the above min-max formulation, @cite_6 proposed stochastic primal-dual algorithms based on the stochastic mirror prox methods proposed in for addressing convex-concave problems. When the loss function @math is non-convex (e.g., the hypothesis class is defined by deep neural networks), the resulting min-max problem is non-convex in terms of @math and but is concave in terms of @math . Recently, @cite_12 proposed new stochastic algorithms for solving the non-convex concave min-max problem when the objective function is weakly convex with respect to the minimization variable given the maximization variable. They proved the convergence to a nearly stationary point of the minimization objective function. However, the stochastic algorithms proposed in are not scalable due to updating and maintaining of the dual variable @math .\"","":""}
{"id":"2969721933","dialogue":"\"When deploying autonomous agents in unstructured environments over sustained periods of time, adaptability and robustness oftentimes outweigh optimality as a primary consideration. In other words, safety and survivability constraints play a key role and in this paper, we present a novel, constraint-learning framework for control tasks built on the idea of constraints-driven control. However, since control policies that keep a dynamical agent within state constraints over infinite horizons are not always available, this work instead considers constraints that can be satisfied over a sufficiently long time horizon T > 0, which we refer to as limited-duration safety. Consequently, value function learning can be used as a tool to help us find limited-duration safe policies. We show that, in some applications, the existence of limited-duration safe policies is actually sufficient for long-duration autonomy. This idea is illustrated on a swarm of simulated robots that are tasked with covering a given area, but that sporadically need to abandon this task to charge batteries. We show how the battery-charging behavior naturally emerges as a result of the constraints. Additionally, using a cart-pole simulation environment, we show how a control policy can be efficiently transferred from the source task, balancing the pole, to the target task, moving the cart to one direction without letting the pole fall down.\"","summary":"\"Finding feasible control constraints that can be translated to a set of state constraints has been of particular interest both in the controls and machine learning communities. Early work includes the study of artificial potential functions in the context of obstacle avoidance, and the construction of so-called navigation functions was studied in @cite_3 . Alternatively, if there exists a control Lyapunov function @cite_39 , one can stabilize the agent while keeping it inside a level set of the function. Control Lyapunov functions can be learned through demonstrations @cite_56 , for example, and Lyapunov stability was also used in the safe reinforcement learning (see @cite_28 @cite_31 @cite_55 for example). As inverse optimality @cite_23 dictates that finding a stabilizing policy is equivalent to finding an optimal policy in terms of some cost function, these approaches can also be viewed as optimization-based techniques.\"","":""}
{"id":"2969721933","dialogue":"\"When deploying autonomous agents in unstructured environments over sustained periods of time, adaptability and robustness oftentimes outweigh optimality as a primary consideration. In other words, safety and survivability constraints play a key role and in this paper, we present a novel, constraint-learning framework for control tasks built on the idea of constraints-driven control. However, since control policies that keep a dynamical agent within state constraints over infinite horizons are not always available, this work instead considers constraints that can be satisfied over a sufficiently long time horizon T > 0, which we refer to as limited-duration safety. Consequently, value function learning can be used as a tool to help us find limited-duration safe policies. We show that, in some applications, the existence of limited-duration safe policies is actually sufficient for long-duration autonomy. This idea is illustrated on a swarm of simulated robots that are tasked with covering a given area, but that sporadically need to abandon this task to charge batteries. We show how the battery-charging behavior naturally emerges as a result of the constraints. Additionally, using a cart-pole simulation environment, we show how a control policy can be efficiently transferred from the source task, balancing the pole, to the target task, moving the cart to one direction without letting the pole fall down.\"","summary":"\"On the other hand, control barrier functions (CBFs) @cite_45 @cite_17 @cite_1 @cite_8 @cite_46 @cite_43 @cite_12 @cite_15 @cite_32 @cite_7 were proposed to guarantee that an agent remains in a certain region of the state space (i.e., forward invariance @cite_42 ) by using a locally accurate model of the agent dynamics (i.e., a model that accurately predicts a time derivative of the state at the current state and control input). When the system is linearizable and has a high relative degree, an exponential control barrier function @cite_41 was proposed and was applied to control of quadrotors @cite_49 . When a Lyapunov function is available, the work @cite_14 proposed a sum-of-squares approach to compute a valid barrier function. The idea of constraints-driven controls is in stark contrast to solving the task-specific problem that basically aims at singling out one optimal trajectory. However, although there exist converse theorems for safety and barrier functions which claim that a forward invariant set has a barrier function under certain conditions @cite_29 @cite_21 @cite_46 , finding such a set without assuming stability of the system is difficult in general (see @cite_45 for the conditions that a candidate barrier function can be a valid one).\"","":""}
{"id":"2969721933","dialogue":"\"When deploying autonomous agents in unstructured environments over sustained periods of time, adaptability and robustness oftentimes outweigh optimality as a primary consideration. In other words, safety and survivability constraints play a key role and in this paper, we present a novel, constraint-learning framework for control tasks built on the idea of constraints-driven control. However, since control policies that keep a dynamical agent within state constraints over infinite horizons are not always available, this work instead considers constraints that can be satisfied over a sufficiently long time horizon T > 0, which we refer to as limited-duration safety. Consequently, value function learning can be used as a tool to help us find limited-duration safe policies. We show that, in some applications, the existence of limited-duration safe policies is actually sufficient for long-duration autonomy. This idea is illustrated on a swarm of simulated robots that are tasked with covering a given area, but that sporadically need to abandon this task to charge batteries. We show how the battery-charging behavior naturally emerges as a result of the constraints. Additionally, using a cart-pole simulation environment, we show how a control policy can be efficiently transferred from the source task, balancing the pole, to the target task, moving the cart to one direction without letting the pole fall down.\"","summary":"\"Moreover, our work is also related to safe reinforcement learning, such as Lyapunov-based safe learning (cf. @cite_40 @cite_28 ) and constrained Markov decision processes (CMDPs) (cf. @cite_31 @cite_50 ). The former is based on the fact that sublevel sets of a control Lyapunov function are forward invariant, and considers stability as safety. The latter is aimed at selecting an optimal policy that satisfies constraints. Note these approaches are designed for one specific task. Our work, on the other hand, does not require stability, and can consider an arbitrarily shaped set of safe states.\"","":""}
{"id":"2969721933","dialogue":"\"When deploying autonomous agents in unstructured environments over sustained periods of time, adaptability and robustness oftentimes outweigh optimality as a primary consideration. In other words, safety and survivability constraints play a key role and in this paper, we present a novel, constraint-learning framework for control tasks built on the idea of constraints-driven control. However, since control policies that keep a dynamical agent within state constraints over infinite horizons are not always available, this work instead considers constraints that can be satisfied over a sufficiently long time horizon T > 0, which we refer to as limited-duration safety. Consequently, value function learning can be used as a tool to help us find limited-duration safe policies. We show that, in some applications, the existence of limited-duration safe policies is actually sufficient for long-duration autonomy. This idea is illustrated on a swarm of simulated robots that are tasked with covering a given area, but that sporadically need to abandon this task to charge batteries. We show how the battery-charging behavior naturally emerges as a result of the constraints. Additionally, using a cart-pole simulation environment, we show how a control policy can be efficiently transferred from the source task, balancing the pole, to the target task, moving the cart to one direction without letting the pole fall down.\"","summary":"\"Besides","":""}
{"id":"2969244993","dialogue":"\"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.\"","summary":"\"Humans are capable of perceiving 3D environment and inferring ego-motion in a short time, but it is hard for an agent to be equipped with similar capabilities. VO SLAM has been considered as a multi-view geometric problem for decades. It is traditionally solved by minimizing photometric @cite_13 or geometric @cite_2 reprojection errors and works well in regular environments, but fails in challenging conditions like dynamic objects and abrupt motions. In light of these limitations, VO has been studied with learning techniques in recent years and many approaches with promising performance have been proposed.\"","":""}
{"id":"2969244993","dialogue":"\"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.\"","summary":"\"Supervised methods formulate VO as a supervised learning problem and many methods with good results have been proposed. DeMoN @cite_39 jointly estimates pose and depth in an end-to-end manner. Inspired by the practice of parallel tracking and mapping in classic VO SLAM, DeepTAM @cite_29 utilizes two networks for pose and depth estimation. DeepVO @cite_19 treats VO as a sequence-to-sequence learning problem by estimating poses recurrently. The limitation of supervised learning is that it requires a large amount of labeled data. The acquisition of ground truth often requires expensive equipment or highly manual labeling, and some gathered data are inaccurate. Depth obtained by LIDAR is sparse, and the output depth of Kinect contains a lot of noise. Furthermore, some ground truth is unable to obtain ( optical flow). Previous works have tried to address these problems with synthetic datasets @cite_30 , but there is always a gap between synthetic and real-world data.\"","":""}
{"id":"2969244993","dialogue":"\"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.\"","summary":"\"Self-supervised methods In order to alleviate the reliance on ground truth, recently many self-supervised methods have been proposed for VO. The key to self-supervised learning is to find the internal correlations and constraints in the training data. SfMLearner @cite_20 leverages the geometric correlation of depth and pose to learn both of them in a coupled way, with a learned mask to mask out regions that don't meet static scene assumption. As the first self-supervised approach for VO, SfMLearner couples depth and pose estimations with image warping, which becomes the problem of minimizing photometric loss. Inherited from this idea, many self-supervised VO have been proposed, including modifications on loss functions @cite_15 @cite_16 , network architectures @cite_5 @cite_0 @cite_15 @cite_4 @cite_25 , predicted contents @cite_22 , and combination with classic VO SLAM @cite_32 @cite_38 . For example, GeoNet @cite_22 extends the framework to jointly estimate optical flow with forward-backward consistency to infer unstable regions and achieves state-of-the-art performance among self-supervised VO methods.\"","":""}
{"id":"2969244993","dialogue":"\"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.\"","summary":"\"Despite its feasibility, self-supervised VO still underperforms supervised ones. Apart from the effectiveness of direct supervision, a key reason is that they focus mainly on geometric properties @cite_20 but pay little attention to the sequential nature of the problem. In these methods, only a few frames (no more than 5) are processed in the network, while previous estimations are discarded and the current estimation is made from scratch. Instead, the performance can be enhanced by taking geometric relations of sequential observations into account.\"","":""}
{"id":"2969493583","dialogue":"\"In this paper, we study the problem of short sentence ranking for question answering. In order to get best score for all the sentences when given a query. We compute the representation for all the sentences in advance and leverage k-d tree to accelerate the speed. The experimental results shows that our methods beat the strong baseline of BM25 on large information retrieval corpus. We will compare our experiment results to other representation-based neural rankers in the future. And we will do the experiment of speed comparison between BM25-based and our tree-based retrieval approach.\"","summary":"\"In recent years, neural information retrieval and neural question answering research has developed several effective ways to improve ranking accuracy. Interaction-based neural rankers match query and document pair using attention-based deep model; representation-based neural rankers output sentence representations and using cosine distance to score the sentence pairs. There are many effective representation-based model include DSSM @cite_0 , CLSM @cite_10 and LSTM-RNN @cite_4 and many effective interaction-based model include DRMM @cite_8 Match-SRNN @cite_15 and BERT @cite_12 . Our deep model belongs to the representation-based models which could output the final semantic representation vector for each sentence.\"","":""}
{"id":"2969493583","dialogue":"\"In this paper, we study the problem of short sentence ranking for question answering. In order to get best score for all the sentences when given a query. We compute the representation for all the sentences in advance and leverage k-d tree to accelerate the speed. The experimental results shows that our methods beat the strong baseline of BM25 on large information retrieval corpus. We will compare our experiment results to other representation-based neural rankers in the future. And we will do the experiment of speed comparison between BM25-based and our tree-based retrieval approach.\"","summary":"Sentence embeddings is an important topic in this research area. Skip-Thought @cite_5 input one sentence to predict its previous and next sentence. InferSent @cite_9 outperforms Skip-Thought. @cite_16 is the methods that use unsupervised word vectors @cite_17 to construct the sentence vectors which is a strong baseline. Universal Sentence Encoder @cite_7 present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks.","":""}
{"id":"2969758725","dialogue":"\"In competitive parallel computing, the identical copies of a code in a phase of a sequential program are assigned to processor cores and the result of the fastest core is adopted. In the literature, it is reported that a superlinear speedup can be achieved if there is an enough fluctuation among the execution times consumed by the cores. Competitive parallel computing is a promising approach to use a huge amount of cores effectively. However, there is few theoretical studies on speedups which can be achieved by competitive parallel computing at present. In this paper, we present a behavioral model of competitive parallel computing and provide a means to predict a speedup which competitive parallel computing yields through theoretical analyses and simulations. We also found a sufficient condition to provide a linear speedup which competitive parallel computing yields. More specifically, it is sufficient for the execution times which consumed by the cores to follow an exponential distribution. In addition, we found that the different distributions which have the identical coefficient of variation (CV) do not always provide the identical speedup. While CV is a convenient measure to predict a speedup, it is not enough to provide an exact prediction.\"","summary":"\"Wolfgang @cite_10 proposes random competition, in which the computations compete using the randomness in search algorithm. Although he analyzes speedups based on the variance of the measured execution times, there is no mention of CV.\"","":""}
{"id":"2969758725","dialogue":"\"In competitive parallel computing, the identical copies of a code in a phase of a sequential program are assigned to processor cores and the result of the fastest core is adopted. In the literature, it is reported that a superlinear speedup can be achieved if there is an enough fluctuation among the execution times consumed by the cores. Competitive parallel computing is a promising approach to use a huge amount of cores effectively. However, there is few theoretical studies on speedups which can be achieved by competitive parallel computing at present. In this paper, we present a behavioral model of competitive parallel computing and provide a means to predict a speedup which competitive parallel computing yields through theoretical analyses and simulations. We also found a sufficient condition to provide a linear speedup which competitive parallel computing yields. More specifically, it is sufficient for the execution times which consumed by the cores to follow an exponential distribution. In addition, we found that the different distributions which have the identical coefficient of variation (CV) do not always provide the identical speedup. While CV is a convenient measure to predict a speedup, it is not enough to provide an exact prediction.\"","summary":"\"Without enough attention to the degree of the variance of the execution time among processors, using naively wastes computing resources. To overcome this problem, Cledat @cite_12 @cite_9 @cite_2 proposes the methods called and . The CV of WalkSAT, one of the application they adopted for evaluation, is less than one and the speedup is worse than a linear speedup. Meanwhile, the CV of another application, namely, MSL motion planning is greater than one and a superlinear speedup is achieved. These results are consistent with our result. Therefore, it is proper to claim that our results reinforce and extend their work.\"","":""}
{"id":"2967036800","dialogue":"\"The interconnectivity of cyber and physical systems and Internet of things has created ubiquitous concerns of cyber threats for enterprise system managers. It is common that the asset owners and enterprise network operators need to work with cybersecurity professionals to manage the risk by remunerating them for their efforts that are not directly observable. In this paper, we use a principal-agent framework to capture the service relationships between the two parties, i.e., the asset owner (principal) and the cyber risk manager (agent). Specifically, we consider a dynamic systemic risk management problem with asymmetric information where the principal can only observe cyber risk outcomes of the enterprise network rather than directly the efforts that the manager expends on protecting the resources. Under this information pattern, the principal aims to minimize the systemic cyber risks by designing a dynamic contract that specifies the compensation flows and the anticipated efforts of the manager by taking into account his incentives and rational behaviors. We formulate a bi-level mechanism design problem for dynamic contract design within the framework of a class of stochastic differential games. We show that the principal has rational controllability of the systemic risk by designing an incentive compatible estimator of the agent's hidden efforts. We characterize the optimal solution by reformulating the problem as a stochastic optimal control program which can be solved using dynamic programming. We further investigate a benchmark scenario with complete information and identify conditions that yield zero information rent and lead to a new certainty equivalence principle for principal-agent problems. Finally, case studies over networked systems are carried out to illustrate the theoretical results obtained.\"","summary":"\"Cybersecurity becomes a critical issue due to the large-scale deployment of smart devices and their integration with information and communication techologies (ICTs) @cite_6 @cite_54 . Hence, security risk management is an important task which has been investigated in different research fields, such as communications and infrastructures @cite_29 @cite_43 , cloud computing @cite_33 and IoT @cite_16 . The interconnections between nodes and devices make the risk management a challenge problem as the cyber risk can propogate and escalate into systemic risk @cite_35 , and hence the interdependent security risk analysis is necessary @cite_37 . Managing systemic risk is nontrivial as demonstrated in financial systems @cite_39 , critical infrastructures @cite_44 , and communication networks @cite_42 . In a network with a small number of agents, graph-theoretic methods have been widely adopted to model the strategic interactions and risk interdependencies between agents @cite_39 @cite_24 . When the number of nodes becomes large, @cite_7 has proposed a mean-field game approach where a representative agent captures the system dynamics. Different from @cite_51 @cite_17 in minimizing the static systemic risk at equilibrium, we focus in this paper on a mechanism design problem that can reduce the systemic risks by understanding the system dynamics.\"","":""}
{"id":"2967126358","dialogue":"\"With an increasing number of malicious attacks, the number of people and organizations falling prey to social engineering attacks is proliferating. Despite considerable research in mitigation systems, attackers continually improve their modus operandi by using sophisticated machine learning, natural language processing techniques with an intent to launch successful targeted attacks aimed at deceiving detection mechanisms as well as the victims. We propose a system for advanced email masquerading attacks using Natural Language Generation (NLG) techniques. Using legitimate as well as an influx of varying malicious content, the proposed deep learning system generates emails with malicious content, customized depending on the attacker's intent. The system leverages Recurrent Neural Networks (RNNs) for automated text generation. We also focus on the performance of the generated emails in defeating statistical detectors, and compare and analyze the emails using a proposed baseline.\"","summary":"\"Natural language generation techniques have been widely popular for synthesizing unique pieces of textual content. NLG techniques proposed by @cite_25 @cite_28 rely on templates pre-constructed for specific purposes. The fake email generation system in @cite_6 uses a set of manually constructed rules to pre-define the structure of the fake emails. Recent advancements in deep learning networks have paved the pathway for generating creative as well as objective textual content with the right amount of text data for training. RNN-based language models have been widely used to generate a wide range of genres like poetry @cite_22 @cite_4 , fake reviews @cite_18 , tweets @cite_13 , geographical information @cite_28 and many more.\"","":""}
{"id":"2967126358","dialogue":"\"With an increasing number of malicious attacks, the number of people and organizations falling prey to social engineering attacks is proliferating. Despite considerable research in mitigation systems, attackers continually improve their modus operandi by using sophisticated machine learning, natural language processing techniques with an intent to launch successful targeted attacks aimed at deceiving detection mechanisms as well as the victims. We propose a system for advanced email masquerading attacks using Natural Language Generation (NLG) techniques. Using legitimate as well as an influx of varying malicious content, the proposed deep learning system generates emails with malicious content, customized depending on the attacker's intent. The system leverages Recurrent Neural Networks (RNNs) for automated text generation. We also focus on the performance of the generated emails in defeating statistical detectors, and compare and analyze the emails using a proposed baseline.\"","summary":"\"The system used for synthesizing emails in this work is somewhat aligned along the lines of the methodology described in @cite_14 @cite_9 . However, our proposed system has no manual labor involved and with some level of post processing has been shown to deceive an automated supervised classification system.\"","":""}
{"id":"2967126358","dialogue":"\"With an increasing number of malicious attacks, the number of people and organizations falling prey to social engineering attacks is proliferating. Despite considerable research in mitigation systems, attackers continually improve their modus operandi by using sophisticated machine learning, natural language processing techniques with an intent to launch successful targeted attacks aimed at deceiving detection mechanisms as well as the victims. We propose a system for advanced email masquerading attacks using Natural Language Generation (NLG) techniques. Using legitimate as well as an influx of varying malicious content, the proposed deep learning system generates emails with malicious content, customized depending on the attacker's intent. The system leverages Recurrent Neural Networks (RNNs) for automated text generation. We also focus on the performance of the generated emails in defeating statistical detectors, and compare and analyze the emails using a proposed baseline.\"","summary":"\"In this paper, we focus primarily on generation of fake emails specifically engineered for phishing and scamming victims. Additionally, we also look at some state-of-the-art phishing email detection systems. Researchers in @cite_24 extract a large number of text body, URL and HTML features from emails, which are then fed into supervised (SVMs, Neural Networks) as well as unsupervised (K-Means clustering) algorithms for the final verdict on the email nature. The system proposed in @cite_5 extracts 25 stylistic and structural features from emails, which are given to a supervised SVM for analysis of email nature. Newer techniques for phishing email detection based on textual content analysis have been proposed in @cite_26 @cite_20 @cite_12 @cite_3 . Masquerade attacks are generated by the system proposed in @cite_6 , which tunes the generated emails based on legitimate content and style of a famous personality. Moreover, this technique can be exploited by phishers for launching email masquerade attacks, therefore making such a system extremely dangerous.\"","":""}
{"id":"2968588162","dialogue":"\"Fingerprinting techniques, which are a common method for indoor localization, have been recently applied with success into outdoor settings. Particularly, the communication signals of Low Power Wide Area Networks (LPWAN) such as Sigfox, have been used for localization. In this rather recent field of study, not many publicly available datasets, which would facilitate the consistent comparison of different positioning systems, exist so far. In the current study, a published dataset of RSSI measurements on a Sigfox network deployed in Antwerp, Belgium is used to analyse the appropriate selection of preprocessing steps and to tune the hyperparameters of a kNN fingerprinting method. Initially, the tuning of hyperparameter k for a variety of distance metrics, and the selection of efficient data transformation schemes, proposed by relevant works, is presented. In addition, accuracy improvements are achieved in this study, by a detailed examination of the appropriate adjustment of the parameters of the data transformation schemes tested, and of the handling of out of range values. With the appropriate tuning of these factors, the achieved mean localization error was 298 meters, and the median error was 109 meters. To facilitate the reproducibility of tests and comparability of results, the code and train validation test split used in this study are available.\"","summary":"\"The proliferation of Low Power Wide Area Networks (LPWAN), such as Sigfox and LoRaWAN, has brought a new domain of application of the fingerprinting methods. A recent study @cite_10 has experimentally verified the intuitive assumption that fingerprinting methods outperform, in terms of accuracy, proximity or ranging positioning methods, in a Sigfox setting.\"","":""}
{"id":"2968455557","dialogue":"\"In this paper","summary":"we design a drug release mechanism for dynamic time division multiple access (TDMA)-based molecular communication via diffusion (MCvD). In the proposed scheme","":""}
{"id":"2969050910","dialogue":"\"In this paper, we report our method for the Information Extraction task in 2019 Language and Intelligence Challenge. We incorporate BERT into the multi-head selection framework for joint entity-relation extraction. This model extends existing approaches from three perspectives. First, BERT is adopted as a feature extraction layer at the bottom of the multi-head selection framework. We further optimize BERT by introducing a semantic-enhanced task during BERT pre-training. Second, we introduce a large-scale Baidu Baike corpus for entity recognition pre-training, which is of weekly supervised learning since there is no actual named entity label. Third, soft label embedding is proposed to effectively transmit information between entity recognition and relation extraction. Combining these three contributions, we enhance the information extracting ability of the multi-head selection model and achieve F1-score 0.876 on testset-1 with a single model. By ensembling four variants of our model, we finally achieve F1 score 0.892 (1st place) on testset-1 and F1 score 0.8924 (2nd place) on testset-2.\"","summary":"\"Recent years, great efforts have been made on extracting relational fact from unstructured raw texts to build large structural knowledge bases. A relational fact is often represented as a triplet which consists of two entities (subject and object) and semantic relation between them. Early works @cite_13 @cite_0 @cite_14 mainly focused on the task of relation classification which assumes the entity pair are identified beforehand. This limits their practical application since they neglect the extraction of entities. To extract both entities and their relation, existing methods can be divided into two categories : the pipelined framework, which first uses sequence labeling models to extract entities, and then uses relation classification models to identify the relation between each entity pair; and the joint approach, which combines the entity model and the relation model through different strategies, such as constraints or parameters sharing. * 2mm\"","":""}
{"id":"2967081197","dialogue":"\"Distributed computing frameworks such as MapReduce are often used to process large computational jobs. They operate by partitioning each job into smaller tasks executed on different servers. The servers also need to exchange intermediate values to complete the computation. Experimental evidence suggests that this so-called Shuffle phase can be a significant part of the overall execution time for several classes of jobs. Prior work has demonstrated a natural tradeoff between computation and communication whereby running redundant copies of jobs can reduce the Shuffle traffic load, thereby leading to reduced overall execution times. For a single job, the main drawback of this approach is that it requires the original job to be split into a number of files that grows exponentially in the system parameters. When extended to multiple jobs (with specific function types), these techniques suffer from a limitation of a similar flavor, i.e., they require an exponentially large number of jobs to be executed. In practical scenarios, these requirements can significantly reduce the promised gains of the method. In this work, we show that a class of combinatorial structures called resolvable designs can be used to develop efficient coded distributed computing schemes for both the single and multiple job scenarios considered in prior work. We present both theoretical analysis and exhaustive experimental results (on Amazon EC2 clusters) that demonstrate the performance advantages of our method. For the single and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over prior work) and 4.31x over the baseline approach, respectively.\"","summary":"\"The work of @cite_22 introduced ShuffleWatcher\"\"","":""}
{"id":"2967081197","dialogue":"\"Distributed computing frameworks such as MapReduce are often used to process large computational jobs. They operate by partitioning each job into smaller tasks executed on different servers. The servers also need to exchange intermediate values to complete the computation. Experimental evidence suggests that this so-called Shuffle phase can be a significant part of the overall execution time for several classes of jobs. Prior work has demonstrated a natural tradeoff between computation and communication whereby running redundant copies of jobs can reduce the Shuffle traffic load, thereby leading to reduced overall execution times. For a single job, the main drawback of this approach is that it requires the original job to be split into a number of files that grows exponentially in the system parameters. When extended to multiple jobs (with specific function types), these techniques suffer from a limitation of a similar flavor, i.e., they require an exponentially large number of jobs to be executed. In practical scenarios, these requirements can significantly reduce the promised gains of the method. In this work, we show that a class of combinatorial structures called resolvable designs can be used to develop efficient coded distributed computing schemes for both the single and multiple job scenarios considered in prior work. We present both theoretical analysis and exhaustive experimental results (on Amazon EC2 clusters) that demonstrate the performance advantages of our method. For the single and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over prior work) and 4.31x over the baseline approach, respectively.\"","summary":"\"The recent work of @cite_7 introduces a scheme to handle the case when each Reduce function is computed by @math workers by utilizing a hybercube structure which controls the allocation of Map and Reduce tasks. Their work is motivated by distributed applications that require multiple rounds of Map and Reduce computations, where the Reduce results of the previous round serve as the inputs to the Map functions of the next one.\"","":""}
{"id":"2967081197","dialogue":"\"Distributed computing frameworks such as MapReduce are often used to process large computational jobs. They operate by partitioning each job into smaller tasks executed on different servers. The servers also need to exchange intermediate values to complete the computation. Experimental evidence suggests that this so-called Shuffle phase can be a significant part of the overall execution time for several classes of jobs. Prior work has demonstrated a natural tradeoff between computation and communication whereby running redundant copies of jobs can reduce the Shuffle traffic load, thereby leading to reduced overall execution times. For a single job, the main drawback of this approach is that it requires the original job to be split into a number of files that grows exponentially in the system parameters. When extended to multiple jobs (with specific function types), these techniques suffer from a limitation of a similar flavor, i.e., they require an exponentially large number of jobs to be executed. In practical scenarios, these requirements can significantly reduce the promised gains of the method. In this work, we show that a class of combinatorial structures called resolvable designs can be used to develop efficient coded distributed computing schemes for both the single and multiple job scenarios considered in prior work. We present both theoretical analysis and exhaustive experimental results (on Amazon EC2 clusters) that demonstrate the performance advantages of our method. For the single and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over prior work) and 4.31x over the baseline approach, respectively.\"","summary":"\"Another approach that re-examines the computation - communication tradeoff from an alternate viewpoint has been investigated in @cite_9 . In this case, the assumption is that a server does not need to process all locally available files and storage constraints do not necessarily imply computation constraints. A lower bound on the was derived along with a heuristic scheme to achieve it in some cases.\"","":""}
{"id":"2967081197","dialogue":"\"Distributed computing frameworks such as MapReduce are often used to process large computational jobs. They operate by partitioning each job into smaller tasks executed on different servers. The servers also need to exchange intermediate values to complete the computation. Experimental evidence suggests that this so-called Shuffle phase can be a significant part of the overall execution time for several classes of jobs. Prior work has demonstrated a natural tradeoff between computation and communication whereby running redundant copies of jobs can reduce the Shuffle traffic load, thereby leading to reduced overall execution times. For a single job, the main drawback of this approach is that it requires the original job to be split into a number of files that grows exponentially in the system parameters. When extended to multiple jobs (with specific function types), these techniques suffer from a limitation of a similar flavor, i.e., they require an exponentially large number of jobs to be executed. In practical scenarios, these requirements can significantly reduce the promised gains of the method. In this work, we show that a class of combinatorial structures called resolvable designs can be used to develop efficient coded distributed computing schemes for both the single and multiple job scenarios considered in prior work. We present both theoretical analysis and exhaustive experimental results (on Amazon EC2 clusters) that demonstrate the performance advantages of our method. For the single and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over prior work) and 4.31x over the baseline approach, respectively.\"","summary":"\"In @cite_24 , the authors propose a scheme which gives each server access to a random subset of the input files and not all Reduce functions depend on the entire data set. This fact changes the policy according to which we decide which server computes which function.\"","":""}
{"id":"2967081197","dialogue":"\"Distributed computing frameworks such as MapReduce are often used to process large computational jobs. They operate by partitioning each job into smaller tasks executed on different servers. The servers also need to exchange intermediate values to complete the computation. Experimental evidence suggests that this so-called Shuffle phase can be a significant part of the overall execution time for several classes of jobs. Prior work has demonstrated a natural tradeoff between computation and communication whereby running redundant copies of jobs can reduce the Shuffle traffic load, thereby leading to reduced overall execution times. For a single job, the main drawback of this approach is that it requires the original job to be split into a number of files that grows exponentially in the system parameters. When extended to multiple jobs (with specific function types), these techniques suffer from a limitation of a similar flavor, i.e., they require an exponentially large number of jobs to be executed. In practical scenarios, these requirements can significantly reduce the promised gains of the method. In this work, we show that a class of combinatorial structures called resolvable designs can be used to develop efficient coded distributed computing schemes for both the single and multiple job scenarios considered in prior work. We present both theoretical analysis and exhaustive experimental results (on Amazon EC2 clusters) that demonstrate the performance advantages of our method. For the single and multiple job cases, we obtain speed-ups of 4.69x (and 2.6x over prior work) and 4.31x over the baseline approach, respectively.\"","summary":"\"As discussed above both @cite_16 and @cite_5 require a certain problem dimension to be very large. In particular, @cite_5 considers a single job and requires it to be split into a number of tasks that grows exponentially in the problem parameters. On the other hand @cite_16 considers functions that can be aggregated but requires the number of jobs being processed simultaneously to grow exponentially. Our work builds on the initial work in @cite_21 and @cite_2 and makes the following contributions.\"","":""}
{"id":"2967155990","dialogue":"\"Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0 on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.\"","summary":"\"Scene text is regarded as a special type of object, several methods @cite_10 @cite_43 @cite_57 @cite_54 @cite_20 @cite_45 are based on Faster R-CNN @cite_4 , SSD @cite_32 and DenseBox @cite_16 , which generates text bounding boxes by regressing coordinates of boxes directly. TextBoxes @cite_43 and RRD @cite_44 adopt SSD as a base detector and adjust the anchor ratios and convolution kernel size to handle variation of aspect ratios of text instances. @cite_10 and EAST @cite_3 perform direct regression to determine vertex coordinates of quadrilateral text boundaries in a per-pixel manner without using anchors and proposals, and conduct the Non-Max Suppression (NMS) to get the final detection results. RRPN @cite_23 generates inclined proposals with text orientation angle information and propose Rotation Region-of-Interest (RRoI) pooling layer to detect arbitrary-oriented text. Limited by the receptive field of CNNs and the relatively simple representations like rectangle bounding box or quadrangle adopted to describe text, detection-based methods may fall short when dealing with more challenging text instances, such as extremely long text and arbitrarily-shaped text.\"","":""}
{"id":"2967155990","dialogue":"\"Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0 on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.\"","summary":"\"Instance segmentation is a challenging task, which involves both segmentation and classification tasks. The most recent and successful two-stage representative is Mask R-CNN @cite_0 , which achieves amazing results on public benchmarks, but requires relatively long execution time due to the per-proposal computation and its deep stem network. Other frameworks rely mostly on pixel-features generated by a single FCN forward pass, and employ post-processing like graphical models, template matching, or pixel embedding to cluster pixels belonging to the same instance. More specifically, Non-local Networks @cite_36 utilizes a self-attention @cite_18 mechanism to enable a pixel-feature to perceive features from all the other positions, while the CCNet @cite_37 harvests the contextual information from all pixels more efficiently by stacking two criss-cross attention modules, which augments the feature representation a lot. In post-processing step, @cite_55 present a pixel affinity scheme and cluster pixels into instances with a simple yet effective graph merge algorithm. Instance-Cut @cite_19 and the work of @cite_28 predict object boundaries intentionally to facilitate the separation of object instances.\"","":""}
{"id":"2968869838","dialogue":"\"Abstract We introduce a game-theoretic model to investigate the strategic interaction between a cyber insurance policyholder whose premium depends on her self-reported security level and an insurer with the power to audit the security level upon receiving an indemnity claim. Audits can reveal fraudulent (or simply careless) policyholders not following reported security procedures, in which case the insurer can refuse to indemnify the policyholder. However, the insurer has to bear an audit cost even when the policyholders have followed the prescribed security procedures. As audits can be expensive, a key problem insurers face is to devise an auditing strategy to deter policyholders from misrepresenting their security levels to gain a premium discount. This decision-making problem was motivated by conducting interviews with underwriters and reviewing regulatory filings in the US; we discovered that premiums are determined by security posture, yet this is often self-reported and insurers are concerned by whether security procedures are practised as reported by the policyholders. To address this problem, we model this interaction as a Bayesian game of incomplete information and devise optimal auditing strategies for the insurers considering the possibility that the policyholder may misrepresent her security level. To the best of our knowledge, this work is the first theoretical consideration of post-incident claims management in cyber security. Our model captures the trade-off between the incentive to exaggerate security posture during the application process and the possibility of punishment for non-compliance with reported security policies. Simulations demonstrate that common sense techniques are not as efficient at providing effective cyber insurance audit decisions as the ones computed using game theory.\"","summary":"\"This paper continues the trend towards rectifying the substantial discrepancy'' @cite_19 between early cyber insurance models and informal claims about the insurance market. Early research considered factors relevant to the viability of a market. Interdependent security occurs when the risk depends on the actions of others'' @cite_17 @cite_29 . Optimists argued that insurers could coordinate the resulting collective action problem @cite_1 @cite_23 , leading to a net social welfare gain and a viable market. Skeptics instead focused on the high correlation in failure of information systems'' @cite_18 @cite_25 @cite_5 , citing it as a major impediment to the supply of cyber insurance. Recent empirical work @cite_28 analyzing 180 cyber insurance filings shows that the cyber insurance market is viable.\"","":""}
{"id":"2968869838","dialogue":"\"Abstract We introduce a game-theoretic model to investigate the strategic interaction between a cyber insurance policyholder whose premium depends on her self-reported security level and an insurer with the power to audit the security level upon receiving an indemnity claim. Audits can reveal fraudulent (or simply careless) policyholders not following reported security procedures, in which case the insurer can refuse to indemnify the policyholder. However, the insurer has to bear an audit cost even when the policyholders have followed the prescribed security procedures. As audits can be expensive, a key problem insurers face is to devise an auditing strategy to deter policyholders from misrepresenting their security levels to gain a premium discount. This decision-making problem was motivated by conducting interviews with underwriters and reviewing regulatory filings in the US; we discovered that premiums are determined by security posture, yet this is often self-reported and insurers are concerned by whether security procedures are practised as reported by the policyholders. To address this problem, we model this interaction as a Bayesian game of incomplete information and devise optimal auditing strategies for the insurers considering the possibility that the policyholder may misrepresent her security level. To the best of our knowledge, this work is the first theoretical consideration of post-incident claims management in cyber security. Our model captures the trade-off between the incentive to exaggerate security posture during the application process and the possibility of punishment for non-compliance with reported security policies. Simulations demonstrate that common sense techniques are not as efficient at providing effective cyber insurance audit decisions as the ones computed using game theory.\"","summary":"\"The timing of the insurer's intervention plays is an important strategic aspect. Ex-ante interventions for the insurer include risk assessments and security investments before the policy term begins. @cite_6 investigated an insurer who could assess security levels perfectly or not at all, concluding that the latter cannot support a functioning market. @cite_7 showed that ex-ante assessments in combination with discounts for adopting security controls can lead to an increase in social welfare. A more recent model introduces stochastic uncertainty about the policyholder's security level @cite_16 .\"","":""}
{"id":"2968869838","dialogue":"\"Abstract We introduce a game-theoretic model to investigate the strategic interaction between a cyber insurance policyholder whose premium depends on her self-reported security level and an insurer with the power to audit the security level upon receiving an indemnity claim. Audits can reveal fraudulent (or simply careless) policyholders not following reported security procedures, in which case the insurer can refuse to indemnify the policyholder. However, the insurer has to bear an audit cost even when the policyholders have followed the prescribed security procedures. As audits can be expensive, a key problem insurers face is to devise an auditing strategy to deter policyholders from misrepresenting their security levels to gain a premium discount. This decision-making problem was motivated by conducting interviews with underwriters and reviewing regulatory filings in the US; we discovered that premiums are determined by security posture, yet this is often self-reported and insurers are concerned by whether security procedures are practised as reported by the policyholders. To address this problem, we model this interaction as a Bayesian game of incomplete information and devise optimal auditing strategies for the insurers considering the possibility that the policyholder may misrepresent her security level. To the best of our knowledge, this work is the first theoretical consideration of post-incident claims management in cyber security. Our model captures the trade-off between the incentive to exaggerate security posture during the application process and the possibility of punishment for non-compliance with reported security policies. Simulations demonstrate that common sense techniques are not as efficient at providing effective cyber insurance audit decisions as the ones computed using game theory.\"","summary":"\"The literature on economic theory of insurance fraud has developed two main approaches: and @cite_2 . The costly state falsification approach assesses the client's behaviour towards a claim. We consider the costly state verification approach, which focuses on the insurer identifying fraudulent claims. The insurer can verify the claims via auditing but has to bear a verification cost. The optimal claim handling usually involves random auditing @cite_30 .\"","":""}
{"id":"2968869838","dialogue":"\"Abstract We introduce a game-theoretic model to investigate the strategic interaction between a cyber insurance policyholder whose premium depends on her self-reported security level and an insurer with the power to audit the security level upon receiving an indemnity claim. Audits can reveal fraudulent (or simply careless) policyholders not following reported security procedures, in which case the insurer can refuse to indemnify the policyholder. However, the insurer has to bear an audit cost even when the policyholders have followed the prescribed security procedures. As audits can be expensive, a key problem insurers face is to devise an auditing strategy to deter policyholders from misrepresenting their security levels to gain a premium discount. This decision-making problem was motivated by conducting interviews with underwriters and reviewing regulatory filings in the US; we discovered that premiums are determined by security posture, yet this is often self-reported and insurers are concerned by whether security procedures are practised as reported by the policyholders. To address this problem, we model this interaction as a Bayesian game of incomplete information and devise optimal auditing strategies for the insurers considering the possibility that the policyholder may misrepresent her security level. To the best of our knowledge, this work is the first theoretical consideration of post-incident claims management in cyber security. Our model captures the trade-off between the incentive to exaggerate security posture during the application process and the possibility of punishment for non-compliance with reported security policies. Simulations demonstrate that common sense techniques are not as efficient at providing effective cyber insurance audit decisions as the ones computed using game theory.\"","summary":"Our contribution to the literature is the first theoretical consideration of post-incident claims management. Our model captures the trade-off between the incentive to exaggerate security posture to receive a premium discount and the possibility of punishment for non-compliance with the reported security policies. We consider misrepresenting security posture a strategic choice for the insured and allow the insurer to respond by auditing claims. Not allowing the insurer to do so leads to market collapse @cite_10 .","":""}
{"id":"2967058823","dialogue":"\"We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.\"","summary":"\"As far as the CSA is concerned, this component can be easily built from the BWT using small space as it is formed (in its simplest design) by just a BWT with rank select functionality enhanced with a suffix array sampling, see also @cite_21 .\"","":""}
{"id":"2967058823","dialogue":"\"We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.\"","summary":"We are aware of only one work building the LCP array in small space from the BWT: @cite_38 show how to build the LCP array in @math time and @math bits of working space on top of the input BWT and the output. Other works @cite_15 @cite_21 show how to build the LCP array directly from the text in @math time and @math bits of space (compact).","":""}
{"id":"2967058823","dialogue":"\"We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.\"","summary":"\"K \"\"a rkk \"\"a @cite_26 show that the PLCP bitvector can be built in @math time using @math bits of working space on top of the text","":""}
{"id":"2967058823","dialogue":"\"We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.\"","summary":"\"The remaining component required to build a compressed suffix tree (in the version described by Sadakane @cite_10 ) is the suffix tree topology","":""}
{"id":"2967058823","dialogue":"\"We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.\"","summary":"\"In this paper, we give new space-time trade-offs that allow building the CST's components in smaller working space (and in some cases even faster) with respect to the existing solutions. We start by combining 's algorithm @cite_38 with the suffix-tree enumeration procedure of Belazzougui @cite_21 to obtain an algorithm that enumerates (i) all pairs @math , and (ii) all suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. We use this procedure to obtain algorithms that build (working space is on top of the input BWT and the output):\"","":""}
{"id":"2967058823","dialogue":"\"We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.\"","summary":"\"Also contribution ) improves the state-of-the-art, due to @cite_21 @cite_31 . In those papers, the authors show how to merge the BWTs of two texts @math and obtain the BWT of the collection @math in @math time and @math bits of working space for any @math [Thm. 7] belazzougui2016linear . When @math , this running time is the same as our result ), but the working space is much higher on small alphabets.\"","":""}
{"id":"2968100992","dialogue":"\"Many real-world prediction tasks have outcome (a.k.a. target or response) variables that have characteristic heavy-tail distributions. Examples include copies of books sold, auction prices of art pieces, etc. By learning heavy-tailed distributions, big and rare'' instances (e.g., the best-sellers) will have accurate predictions. Most existing approaches are not dedicated to learning heavy-tailed distribution; thus, they heavily under-predict such instances. To tackle this problem, we introduce ( L2P ), which exploits the pairwise relationships between instances to learn from a proportionally higher number of rare instances. L2P consists of two stages. In Stage 1, L2P learns a pairwise preference classifier: . In Stage 2, L2P learns to place a new instance into an ordinal ranking of known instances. Based on its placement, the new instance is then assigned a value for its outcome variable. Experiments on real data show that L2P outperforms competing approaches in terms of accuracy and capability to reproduce heavy-tailed outcome distribution. In addition, L2P can provide an interpretable model with explainable outcomes by placing each predicted instance in context with its comparable neighbors.\"","summary":"\"In real-world applications like search engines and recommendation systems, systems provide ranked lists tailored to users and their queries @cite_11 @cite_16 @cite_5 . In some cases, mapping those preferences into an ordinal variable leads to better user experience. Such tasks require the use of regression and multi-class classification methods @cite_40 .\"","":""}
{"id":"2968100992","dialogue":"\"Many real-world prediction tasks have outcome (a.k.a. target or response) variables that have characteristic heavy-tail distributions. Examples include copies of books sold, auction prices of art pieces, etc. By learning heavy-tailed distributions, big and rare'' instances (e.g., the best-sellers) will have accurate predictions. Most existing approaches are not dedicated to learning heavy-tailed distribution; thus, they heavily under-predict such instances. To tackle this problem, we introduce ( L2P ), which exploits the pairwise relationships between instances to learn from a proportionally higher number of rare instances. L2P consists of two stages. In Stage 1, L2P learns a pairwise preference classifier: . In Stage 2, L2P learns to place a new instance into an ordinal ranking of known instances. Based on its placement, the new instance is then assigned a value for its outcome variable. Experiments on real data show that L2P outperforms competing approaches in terms of accuracy and capability to reproduce heavy-tailed outcome distribution. In addition, L2P can provide an interpretable model with explainable outcomes by placing each predicted instance in context with its comparable neighbors.\"","summary":"\"Regression problems are known to suffer from under-predicting rare instances @cite_17 . Approaches proposed to correct fitting models consider prior correction that introduces terms capturing a fraction of rare events in the observations and weighting the data to compensate for differences @cite_0 @cite_4 . Hsu and Sabato @cite_27 proposed a methodology for linear regression with possibly heavy-tailed responses. They split data into multiple pieces, repeat the estimation process several times, and select the estimators based on their performance. They analytically prove that their method can perform reasonably well on heavy-tailed datasets. Quantile regression related approaches are proposed as well. Wang @cite_24 proposed estimating the intermediate conditional quantiles using conventional quantile regression and extrapolating these estimates to capture the behavior at the tail of the distribution. Robust Regression for Asymmetric Tails (RRAT) @cite_33 was proposed to address the problem of asymmetric noise distribution by using conditional quantile estimators. Zhang and Zhou @cite_20 considered linear regression with heavy-tail distributions and showed that using @math loss with truncated minimization can have advantages over @math loss. Like all truncated based approaches, their method requires prior knowledge of distributional properties. None of these regression techniques can capture non-linear decision boundaries.\"","":""}
{"id":"2968100992","dialogue":"\"Many real-world prediction tasks have outcome (a.k.a. target or response) variables that have characteristic heavy-tail distributions. Examples include copies of books sold, auction prices of art pieces, etc. By learning heavy-tailed distributions, big and rare'' instances (e.g., the best-sellers) will have accurate predictions. Most existing approaches are not dedicated to learning heavy-tailed distribution; thus, they heavily under-predict such instances. To tackle this problem, we introduce ( L2P ), which exploits the pairwise relationships between instances to learn from a proportionally higher number of rare instances. L2P consists of two stages. In Stage 1, L2P learns a pairwise preference classifier: . In Stage 2, L2P learns to place a new instance into an ordinal ranking of known instances. Based on its placement, the new instance is then assigned a value for its outcome variable. Experiments on real data show that L2P outperforms competing approaches in terms of accuracy and capability to reproduce heavy-tailed outcome distribution. In addition, L2P can provide an interpretable model with explainable outcomes by placing each predicted instance in context with its comparable neighbors.\"","summary":"\"In literature, efficient methodologies were proposed to learn pairwise relations more efficiently than comparing all @math pairs exhaustively. Qian proposed using two-step hashing framework to retrieve relevant instance and nominate pairs whose ranking is uncertain @cite_21 . Similar approaches to efficiently search similar pairs and approximately learning pairwise distance are proposed in the literature for information retrieval and image search @cite_25 @cite_37 @cite_3 .\"","":""}
{"id":"2968848930","dialogue":"\"We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.\"","summary":"\"Recently, neural networks trained with a ranking loss considering image pairs @cite_13 , triplets @cite_26 , quadruplets @cite_31 or beyond @cite_2 , have been considered for metric learning @cite_4 @cite_26 and for a broad range of search tasks such as face person identification @cite_20 @cite_31 @cite_28 @cite_1 or instance retrieval @cite_27 @cite_13 . These learning-to-rank approaches have been generalised to two or more modalities. Standard examples include building a joint embedding for images and text @cite_3 @cite_25 , videos and audio @cite_10 and, more related to our work, for videos and action labels @cite_35 , videos and text @cite_40 @cite_8 @cite_21 or some of those combined @cite_16 @cite_0 @cite_33 .\"","":""}
{"id":"2968848930","dialogue":"\"We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.\"","summary":"\"Representing text Early works in image-to-text cross-modal retrieval @cite_12 @cite_3 @cite_25 used TF-IDF as a weighted bag-of-words model for text representations (either from a word embedding model or one-hot vectors) in order to aggregate variable length text captions into a single fixed sized representation. With the advent of neural networks, works shifted to use RNNs, Gated Recurrent Units (GRU) or Long Short-Term Memory (LSTM) units to extract textual features @cite_40 or to use these models within the embedding network @cite_35 @cite_18 @cite_0 @cite_16 @cite_43 for both modalities.\"","":""}
{"id":"2968848930","dialogue":"\"We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.\"","summary":"\"Hahn al @cite_35 use two LSTMs to directly project videos into the Word2Vec embedding space. This method is evaluated on higher-level activities, showing that such a visual embedding aligns well with the learned space of Word2Vec to perform zero-shot recognition of these coarser-grained classes. Miech al @cite_11 found that using NetVLAD @cite_29 results in an increase in accuracy over GRUs or LSTMs for aggregation of both visual and text features. A follow up on this work @cite_33 learns a mixture of experts embedding from multiple modalities such as appearance, motion, audio or face features. It learns a single output embedding which is the weighted similarity between the different implicit visual-text embeddings. Recently, Miech al @cite_19 propose the HowTo100M dataset: A large dataset collected automatically using generated captions from youtube of how to tasks'. They find that fine-tuning on these weakly-paired video clips allows for state-of-the-art performance on a number of different datasets.\"","":""}
{"id":"2968848930","dialogue":"\"We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.\"","summary":"\"Fine-grained action recognition Recently, several large-scale datasets have been published for the task of fine-grained action recognition @cite_17 @cite_15 @cite_23 @cite_36 @cite_24 . These generally focus on a closed vocabulary of class labels describing short and or specific actions.\"","":""}
{"id":"2968848930","dialogue":"\"We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.\"","summary":"\"Rohrbach al @cite_24 investigate hand and pose estimation techniques for fine-grained activity recognition. By compositing separate actions, and treating them as attributes, they can predict unseen activities via novel combinations of seen actions. Mahdisoltani al @cite_9 train for four different tasks, including both coarse and fine grain action recognition. They conclude that training on fine-grain labels allows for better learning of features for coarse-grain tasks.\"","":""}
{"id":"2966129987","dialogue":"\"We study the simulation of stellar mergers, which requires complex simulations with high computational demands. We have developed Octo-Tiger, a finite volume grid-based hydrodynamics simulation code with Adaptive Mesh Refinement which is unique in conserving both linear and angular momentum to machine precision. To face the challenge of increasingly complex, diverse, and heterogeneous HPC systems, Octo-Tiger relies on high-level programming abstractions. We use HPX with its futurization capabilities to ensure scalability both between nodes and within, and present first results replacing MPI with libfabric achieving up to a 2.8x speedup. We extend Octo-Tiger to heterogeneous GPU-accelerated supercomputers, demonstrating node-level performance and portability. We show scalability up to full system runs on Piz Daint. For the scenario's maximum resolution, the compute-critical parts (hydrodynamics and gravity) achieve 68.1 parallel efficiency at 2048 nodes.\"","summary":"\"There are several studies that investigate the structure of mass loss in V1309 Scorpii through computer simulation. One approach to modeling this system is smoothed-particle hydrodynamics (SPH). Notable SPH applications include StarSmasher @cite_44 @cite_18 (a fork of StarCrash @cite_8 ) and an unpublished code developed by a collaboration of researchers from Princeton University, Columbia University, and Osaka University @cite_45 @cite_31 . An alternative approach is to use the finite volume method to simulate mass transfer. Examples of such applications include Athena @cite_38 @cite_57 and its rewrite named Athena++ @cite_19 @cite_9 @cite_6 . Lastly, Enzo @cite_20 is a project that implements finite volume hydrodynamics along with a collisionless N-body module that can be used to simulate binary systems where one component is taken to be a point mass. With the exception of SPH codes using direct summation for gravity, is unique among three-dimensional self-gravitating hydrodynamics codes in that it simultaneously conserves both linear and angular momentum to machine precision. SPH codes using direct summation for gravity are limited to only a few thousand particles, making the better choice for high resolution simulations.\"","":""}
{"id":"2966129987","dialogue":"\"We study the simulation of stellar mergers, which requires complex simulations with high computational demands. We have developed Octo-Tiger, a finite volume grid-based hydrodynamics simulation code with Adaptive Mesh Refinement which is unique in conserving both linear and angular momentum to machine precision. To face the challenge of increasingly complex, diverse, and heterogeneous HPC systems, Octo-Tiger relies on high-level programming abstractions. We use HPX with its futurization capabilities to ensure scalability both between nodes and within, and present first results replacing MPI with libfabric achieving up to a 2.8x speedup. We extend Octo-Tiger to heterogeneous GPU-accelerated supercomputers, demonstrating node-level performance and portability. We show scalability up to full system runs on Piz Daint. For the scenario's maximum resolution, the compute-critical parts (hydrodynamics and gravity) achieve 68.1 parallel efficiency at 2048 nodes.\"","summary":"\"Adaptive multithreading systems such as HPX expose concurrency by using user-level threads. Some other notable solutions that take such an approach are Uintah @cite_2 , Chapel @cite_55 , Charm++ @cite_12 , Kokkos @cite_36 , Legion @cite_5 , and PaRSEC @cite_23 . Note that we only refer to distributed memory capable solutions, since we focus here on large distributed simulations. Different task-based parallel programming models, e.g. Cilk Plus, OpenMP, Intel TBB, Qthreads, StarPU, GASPI, Chapel, Charm++, and HPX, are compared in @cite_27 . Our requirements (distributed, task-based, asynchronous) are met by few, out of which HPX has the highest technology readiness level according to this review. It is furthermore the only one with a future-proof C++ standard conforming API and allows us to support the libfabric networking library without changing application code. For more details, see Sec. .\"","":""}
{"id":"2966129987","dialogue":"\"We study the simulation of stellar mergers, which requires complex simulations with high computational demands. We have developed Octo-Tiger, a finite volume grid-based hydrodynamics simulation code with Adaptive Mesh Refinement which is unique in conserving both linear and angular momentum to machine precision. To face the challenge of increasingly complex, diverse, and heterogeneous HPC systems, Octo-Tiger relies on high-level programming abstractions. We use HPX with its futurization capabilities to ensure scalability both between nodes and within, and present first results replacing MPI with libfabric achieving up to a 2.8x speedup. We extend Octo-Tiger to heterogeneous GPU-accelerated supercomputers, demonstrating node-level performance and portability. We show scalability up to full system runs on Piz Daint. For the scenario's maximum resolution, the compute-critical parts (hydrodynamics and gravity) achieve 68.1 parallel efficiency at 2048 nodes.\"","summary":"\"There are several particle-based FMM implementations utilizing task-based programming available. The approach described in @cite_15 uses the Quark runtime environment @cite_21 , the implementation in @cite_10 @cite_42 uses StarPu @cite_30 , whilst @cite_11 uses OpenMP @cite_35 , and @cite_16 compares Cilk @cite_3 , HPX-5, and OpenMP tasks @cite_52 . Our choice of HPX for the task-based runtime system is motivated by the same findings as the above mentioned review and the need to implement specialized kernels for energy conservation that require coupling between different parts of the solver.\"","":""}
{"id":"2965409261","dialogue":"\"Adversarial training has been recently employed for realizing structured semantic segmentation","summary":"in which the aim is to preserve higher-level scene structural consistencies in dense predictions. However","":""}
{"id":"2965409261","dialogue":"\"Adversarial training has been recently employed for realizing structured semantic segmentation","summary":"in which the aim is to preserve higher-level scene structural consistencies in dense predictions. However","":""}
{"id":"2966257000","dialogue":"\"Recently, the field of adversarial machine learning has been garnering attention by showing that state-of-the-art deep neural networks are vulnerable to adverserial examples, stemming from small perturbations being added to the input image. Adversarial examples are generated by a malicious adversary by obtaining access to the model parameters, such as gradient information, to alter the input or by attacking a substitute model and transferring those malicious examples over to attack the victim model. Specifically, one of these attack algorithms, Robust Physical Perturbations ( @math ), generates adverserial images of stop signs with black and white stickers to achieve high targeted misclassification rates against standard-architecture traffic sign classifiers. In this paper, we propose BlurNet, a defense against the @math attack. First, we motivate the defense with a frequency analysis of the first layer feature maps of the network on the LISA dataset by demonstrating high frequency noise is introduced into the input image by the @math algorithm. To alleviate the high frequency, we introduce a depthwise convolution layer of standard blur kernels after the first layer. Finally, we present a regularization scheme to incorporate this low-pass filtering behavior into the training regime of the network.\"","summary":"\"is the technique of injecting adverserial examples and the corresponding gold standard labels into the training set @cite_8 @cite_18 @cite_12 . The motivation of this methodology is that the network will learn the adverserial perturbations introduced by the attacker. The problem with adverserial training is that it doubles the training time of the classifier as new examples need to be generated. Moreover, as shown by , adversarial training needs all types of adverserial examples produced by all known attacks, as the training process is non-adaptive @cite_25 . Our method can be paired with any of these types of defenses.\"","":""}
{"id":"2965547769","dialogue":"\"We propose a general approach for change-point detection in dynamic networks. The proposed method is model-free and covers a wide range of dynamic networks. The key idea behind our approach is to effectively utilize the network structure in designing change-point detection algorithms. This is done via an initial step of graphon estimation, where we propose a modified neighborhood smoothing (MNBS) algorithm for estimating the link probability matrices of a dynamic network. Based on the initial graphon estimation, we then develop a screening and thresholding algorithm for multiple change-point detection in dynamic networks. The convergence rate and consistency for the change-point detection procedure are derived as well as those for MNBS. When the number of nodes is large (e.g., exceeds the number of temporal points), our approach yields a faster convergence rate in detecting change-points comparing with an algorithm that simply employs averaged information of the dynamic network across time. Numerical experiments demonstrate robust performance of the proposed algorithm for change-point detection under various types of dynamic networks, and superior performance over existing methods is observed. A real data example is provided to illustrate the effectiveness and practical impact of the procedure.\"","summary":"\"@cite_5 proposes a novel estimator for estimating the link probability matrix @math of an undirected network by neighborhood smoothing (NBS). The essential idea consists of the following: Given an adjacent matrix @math , the link probability @math between node @math and @math is estimated by where @math is a certain set of neighboring nodes of node @math , which consists of the nodes that exhibit similar connection patterns as node @math . With a well-designed neighborhood adaptive to the network structure, the smoothing achieves an accurate estimation for @math . NBS in @cite_5 estimates @math with a single adjacency matrix @math . For a dynamic network, a sequence of adjacency matrices @math is available, which provides extra information of the network. By aggregating information from repeated observations across time, in Section , we propose a modified NBS by carefully shrinking the neighborhood size, which yields a better convergence rate in estimating the link probability matrix @math and thus an improved rate in change-point detection.\"","":""}
{"id":"2965547769","dialogue":"\"We propose a general approach for change-point detection in dynamic networks. The proposed method is model-free and covers a wide range of dynamic networks. The key idea behind our approach is to effectively utilize the network structure in designing change-point detection algorithms. This is done via an initial step of graphon estimation, where we propose a modified neighborhood smoothing (MNBS) algorithm for estimating the link probability matrices of a dynamic network. Based on the initial graphon estimation, we then develop a screening and thresholding algorithm for multiple change-point detection in dynamic networks. The convergence rate and consistency for the change-point detection procedure are derived as well as those for MNBS. When the number of nodes is large (e.g., exceeds the number of temporal points), our approach yields a faster convergence rate in detecting change-points comparing with an algorithm that simply employs averaged information of the dynamic network across time. Numerical experiments demonstrate robust performance of the proposed algorithm for change-point detection under various types of dynamic networks, and superior performance over existing methods is observed. A real data example is provided to illustrate the effectiveness and practical impact of the procedure.\"","summary":"\"Another related area of research is anomaly detection in dynamic networks, where the task is to detect short abrupt deviation of the network behavior from its norm. This is not the focus of our paper and we refer the readers to @cite_1 for a comprehensive survey.\"","":""}
{"id":"2965290354","dialogue":"\"Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a naive representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.\"","summary":"\"Inflating convolutional layers to 3D for video tasks was first explored in @cite_15 , in which the authors chose to optimise an architecture for the video task, rather than adapt one from an image problem. Both @cite_1 and @cite_5 have adapted large image classification models (Inception and ResNet respectively) to activity recognition tasks, such as @cite_12 @cite_14 . Aside from the added dimensionality, these architectures are much the same as in image tasks, and intuitively find similar success in the spatio-temporal domain as they do in the spatial domain, achieving state-of-the-art performance. These models are as complex and black-box in nature as their 2D counterparts and as such the motivation to explain them also translates.\"","":""}
{"id":"2965290354","dialogue":"\"Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a naive representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.\"","summary":"\"A variety of approaches have been attempted for explaining decisions made by deep neural networks. For example, in @cite_8 the authors propose feature visualisation for CNNs, in which the input images are optimised to maximally activate each filter in the CNN convolutional layers, following work in @cite_3 on non-convolutional models. Local explanations, in the sense that they are local to a single input, explain the inputs contribution to the model decision using feature attribution; these have found much success in explaining deep image processing models. These methods in some way approximate the contribution to the models decision, most commonly in a supervised task, to its input variables, pixels or features at a higher level. This has been implemented in a number of ways, for example, through use of probability gradients @cite_11 , global average pooling @cite_7 and its generalisation to networks with hidden layers in @cite_6 , or through local relevance based around a decision-neutral root point @cite_10 @cite_13 @cite_4 . These works are all considered in that they use information from the model internal parameters, i.e., its weights and activations, in generating an explanation.\"","":""}
{"id":"2965290354","dialogue":"\"Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a naive representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.\"","summary":"\"Layer-wise relevance propagation (LRP) rules, as defined in @cite_10 , have found moderate success in explaining image recognition tasks. Multiple implementations and improvements have been made to these rules, with marginal winning probability (MWP) @cite_4 , to our knowledge being the first implementation of the rules. Deep Taylor decomposition, an implementation of LRP by the original authors themselves has become very popular, and as a result of its input-domain agnosticism, has been applied to other domains outside of image recognition, including activity recognition @cite_16 . It is for these reasons we choose the deep Taylor method as the exemplar technique for our proposed method\"","":""}
{"id":"2965290354","dialogue":"\"Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a naive representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.\"","summary":"\"In addition to MWP, the authors in @cite_4 also show that removing relevance for the dual of the signal improves the focus of the explanation. This contrastive MWP (cMWP) effectively removes relevance to all classes, by explaining all other outputs at the second logits layer, leaving only relevance contributing to the chosen output neuron. Our method is similar to cMWP, in that we make use of subtraction of separate LRP signals to remove unwanted relevance. However, we backpropagate both signals through the network fully before subtracting. Where the cMWP method removes relevance towards all classes from the explanation, our method removes relevance towards spatially salient features in the frame, such as edges and background objects.\"","":""}
{"id":"2965290354","dialogue":"\"Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a naive representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.\"","summary":"\"Work on explainability methods outside of image tasks is still developing. Papers such as @cite_1 use feature visualisation techniques to provide insight into the models they have trained, but to our knowledge @cite_16 is still one of the only instances of an LRP based method applied to a video task. In this work, the difference between frames in relevance is highlighted by flattening the explanation block and plotting the overall relevance, which shows frames at certain points in an activity are more relevant overall. Saliency tubes, as proposed in @cite_9 , adapts the CAM technique of @cite_7 @cite_6 to localise salient motion in video frames. This method is the most similar to ours in that it highlights motion in 3D CNNs.\"","":""}
{"id":"2966220025","dialogue":"\"Spurred by the potential of deep learning, computational music generation has gained renewed academic interest. A crucial issue in music generation is that of user control, especially in scenarios where the music generation process is conditioned on existing musical material. Here we propose a model for conditional kick drum track generation that takes existing musical material as input, in addition to a low-dimensional code that encodes the desired relation between the existing material and the new material to be generated. These relational codes are learned in an unsupervised manner from a music dataset. We show that codes can be sampled to create a variety of musically plausible kick drum tracks and that the model can be used to transfer kick drum patterns from one song to another. Lastly, we demonstrate that the learned codes are largely invariant to tempo and time-shift.\"","summary":"\"In addition to the VAE-based methods for control over music generation processes mentioned above, a number of other studies have applied deep learning methods to address the problem of music generation in general, as reviewed in @cite_19 . Drum track generation has been tackled using recurrent architectures @cite_4 @cite_15 , Restricted Boltzmann Machines @cite_5 , and Generative Adversarial Networks (GANs) @cite_3 . Approaches to the generation process may rely on sampling from some latent representation of the material to be generated @cite_20 @cite_11 , possibly in an incremental fashion @cite_6 , or conditioning on user-provided information (such as a style label @cite_13 , unary @cite_1 , or structural @cite_12 constraints). @cite_16 demonstrates style transfer for audio. GANs are used in @cite_3 @cite_2 , where the output of the generation process is determined by providing some (time-varying) noise, in combination with conditioning on existing material. Similar to our study, @cite_18 uses a GAE to model between musical material in an autoregressive prediction task. To our knowledge this is the first use of GAEs for conditional music generation.\"","":""}
{"id":"2966623897","dialogue":"\"In order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. As such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. In this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. The method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. This effectively maintains a constant training size across all tasks. We first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.\"","summary":"\"Recently, a number of approaches have been proposed to adapt a DNN model to the continual learning setting, from an adaptive model architecture perspective such as adding columns or neurons for new tasks @cite_4 @cite_6 @cite_9 ; model parameter adjustment or regularization techniques like, imposing restrictions on parameter updates @cite_23 @cite_1 @cite_32 @cite_33 ; memory revisit techniques which ensure model updates towards the optimal directions @cite_20 @cite_42 @cite_14 ; Bayesian approaches to model continuously acquired information @cite_28 @cite_31 @cite_24 ; or on broader domains with approaches targeted at different setups or goals such as few-shot learning or transfer learning @cite_16 @cite_26 .\"","":""}
{"id":"2966623897","dialogue":"\"In order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. As such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. In this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. The method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. This effectively maintains a constant training size across all tasks. We first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.\"","summary":"\"In order to demonstrate our idea in comparison with the state-of-the-art techniques, we briefly discuss the following three popular approaches to continual learning: I) : It constrains or regularizes the model parameters by adding additional terms in the loss function that prevent the model from deviating significantly from the parameters important to earlier tasks. Typical algorithms include elastic weight consolidation (EWC) @cite_23 and continual learning through synaptic intelligence (SI) @cite_2 . II) : It revises the model structure successively after each task in order to provide more memory and additional free parameters in the model for new task input. Recent examples in this direction are progressive neural networks @cite_4 and dynamically expanding networks @cite_10 . III) : It stores data samples from previous tasks in a separate memory buffer and retrains the new model based on both the new task input and the memory buffer. Popular algorithms here are gradient episodic memory (GEM) @cite_20 , incremental classifier and representation learning (iCaRL) @cite_5 .\"","":""}
{"id":"2965084509","dialogue":"\"Recently, there has been growing interest in developing learning-based methods to detect and utilize salient semi-global or global structures, such as junctions, lines, planes, cuboids, smooth surfaces, and all types of symmetries, for 3D scene modeling and understanding. However, the ground truth annotations are often obtained via human labor, which is particularly challenging and inefficient for such tasks due to the large number of 3D structure instances (e.g., line segments) and other factors such as viewpoints and occlusions. In this paper, we present a new synthetic dataset, Structured3D, with the aim to providing large-scale photo-realistic images with rich 3D structure annotations for a wide spectrum of structured 3D modeling tasks. We take advantage of the availability of millions of professional interior designs and automatically extract 3D structures from them. We generate high-quality images with an industry-leading rendering engine. We use our synthetic dataset in combination with real images to train deep neural networks for room layout estimation and demonstrate improved performance on benchmark datasets.\"","summary":"\"@PARASPLIT Note that our dataset is very different from other popular large-scale 3D datasets, such as NYU v2 @cite_16 , SUN RGB-D @cite_34 , 2D-3D-S @cite_19 @cite_22 , ScanNet @cite_2 , and Matterport3D @cite_17 , in which the ground truth 3D information is stored in the format of point clouds or meshes. These datasets lack ground truth annotations of semi-global or global structures. While it is theoretically possible to extract 3D structure by applying structure detection algorithms to the point clouds or meshes ( , extracting planes from ScanNet as did in @cite_4 ), the detection results are often noisy and even contain errors. In addition, for some types of structure like wireframes and room layouts, how to reliably detect them from raw sensor data remains an active research topic in computer vision.\"","":""}
{"id":"2965084509","dialogue":"\"Recently, there has been growing interest in developing learning-based methods to detect and utilize salient semi-global or global structures, such as junctions, lines, planes, cuboids, smooth surfaces, and all types of symmetries, for 3D scene modeling and understanding. However, the ground truth annotations are often obtained via human labor, which is particularly challenging and inefficient for such tasks due to the large number of 3D structure instances (e.g., line segments) and other factors such as viewpoints and occlusions. In this paper, we present a new synthetic dataset, Structured3D, with the aim to providing large-scale photo-realistic images with rich 3D structure annotations for a wide spectrum of structured 3D modeling tasks. We take advantage of the availability of millions of professional interior designs and automatically extract 3D structures from them. We generate high-quality images with an industry-leading rendering engine. We use our synthetic dataset in combination with real images to train deep neural networks for room layout estimation and demonstrate improved performance on benchmark datasets.\"","summary":"\"In recent years, synthetic datasets have played an important role in successful training of deep neural networks. Notable examples for indoor scene understanding include SUNCG @cite_10 , SceneNet RGB-D @cite_21 , and InteriorNet @cite_23 . These datasets exceed real datasets in terms of scene diversity and frame numbers. But just like their real counterparts, these datasets lack ground truth structure annotations. Another issue with some synthetic datasets is the degree of realism in both the 3D models and the 2D renderings. @cite_8 shows that physically-based rendering could boost the performance of various indoor scene understanding tasks. To ensure the quality of our dataset, we make use of 3D room models created by professional designers and the state-of-the-art industrial rendering engines in this work.\"","":""}
{"id":"2965084509","dialogue":"\"Recently, there has been growing interest in developing learning-based methods to detect and utilize salient semi-global or global structures, such as junctions, lines, planes, cuboids, smooth surfaces, and all types of symmetries, for 3D scene modeling and understanding. However, the ground truth annotations are often obtained via human labor, which is particularly challenging and inefficient for such tasks due to the large number of 3D structure instances (e.g., line segments) and other factors such as viewpoints and occlusions. In this paper, we present a new synthetic dataset, Structured3D, with the aim to providing large-scale photo-realistic images with rich 3D structure annotations for a wide spectrum of structured 3D modeling tasks. We take advantage of the availability of millions of professional interior designs and automatically extract 3D structures from them. We generate high-quality images with an industry-leading rendering engine. We use our synthetic dataset in combination with real images to train deep neural networks for room layout estimation and demonstrate improved performance on benchmark datasets.\"","summary":"\"Room layout estimation. Room layout estimation aims to reconstruct the enclosing structure of the indoor scene, consisting of walls, floor, and ceiling. Existing public datasets ( , PanoContext @cite_3 and LayoutNet @cite_0 ) assume a simple cuboid-shape layout. PanoContext @cite_3 collects about 500 panoramas from the SUN360 dataset @cite_11 , LayoutNet @cite_0 extends the layout annotations to include panoramas from 2D-3D-S @cite_22 . Recently, Realtor360 @cite_32 collects 2,500 indoor panoramas from SUN360 @cite_11 and a real-estate database, and provides annotation of a more general Manhattan layout. We note that all room layout in these real datasets is manually labeled by the human. Since the room structure may be occluded by furniture and other objects, the ground truth'' inferred by humans may be not consistent with the actual layout. In our dataset, all ground truth 3D annotations are automatically extracted from the original house design files.\"","":""}
{"id":"2965144741","dialogue":"\"Being motivated by ceiling inspection applications via unmanned aerial vehicles (UAVs) which require close proximity flight to surfaces, a systematic control approach enabling safe and accurate close proximity flight is proposed in this work. There are two main challenges for close proximity flights: (i) the trust characteristics varies drastically for the different distance from the ceiling which results in a complex nonlinear dynamics; (ii) the system needs to consider physical and environmental constraints to safely fly in close proximity. To address these challenges, a novel framework consisting of a constrained optimization-based force estimation and an optimization-based nonlinear controller is proposed. Experimental results illustrate that the performance of the proposed control approach can stabilize UAV down to 1 cm distance to the ceiling. Furthermore, we report that the UAV consumes up to 12.5 less power when it is operated 1 cm distance to ceiling, which is promising potential for more battery-efficient inspection flights.\"","summary":"\"The available approaches can handle the control of the flying robot when it does not engage with an interaction. However, the challenges associated with the aerodynamic interaction require the system to be more responsive, adaptive and resilient @cite_12 @cite_29 @cite_31 @cite_21 . This operation also brings system and environment based constraints including the level of the interaction. The available approaches that consider the constraints leverage individual multi-models for generic interaction problems which bring additional complexity @cite_5 . Moreover, nominal optimization-based approaches are considered in the UAV control for the interaction tasks, wherein the system lacks the ability to take external forces, changing parameters and unmodeled dynamics into account @cite_17 @cite_19 @cite_25 .\"","":""}
{"id":"2964708426","dialogue":"\"While many apps include built-in options to report bugs or request features, users still provide an increasing amount of feedback via social media, like Twitter. Compared to traditional issue trackers, the reporting process in social media is unstructured and the feedback often lacks basic context information, such as the app version or the device concerned when experiencing the issue. To make this feedback actionable to developers, support teams engage in recurring, effortful conversations with app users to clarify missing context items. This paper introduces a simple approach that accurately extracts basic context information from unstructured, informal user feedback on mobile apps, including the platform, device, app version, and system version. Evaluated against a truthset of 3014 tweets from official Twitter support accounts of the 3 popular apps Netflix, Snapchat, and Spotify, our approach achieved precisions from 81 to 99 and recalls from 86 to 98 for the different context item types. Combined with a chatbot that automatically requests missing context items from reporting users, our approach aims at auto-populating issue trackers with structured bug reports.\"","summary":"\"Research found that especially non-technical end-users are more likely to express their opinions on social networks, such as Twitter @cite_21 . Several studies have identified Twitter as an important source for crowd-based requirements engineering and software evolution @cite_43 @cite_37 @cite_4 . Similar to app reviews, tweets contain important information, such as feature requests or bug reports. By performing a survey with software engineering practitioners and researchers @cite_35 underlined the need for automatic analysis techniques to, e.g., summarize, classify, and prioritize tweets. The authors highlight that a manual analysis of the tweets is unfeasible due to its quantity, unstructured nature, and varying quality. @cite_43 found that tweets provide additional requirements-related information. Compared to app reviews, by mining tweets the authors extracted @math 22 authors have used tweets to crowdsource app features @cite_5 , to support release decisions @cite_14 , to categorize and summarize technical information included in tweets @cite_1 , or to rank the reported issues @cite_26 . These studies enforce the relevance of our approach.\"","":""}
{"id":"2964122540","dialogue":"\"Abstract A vast amount of valuable data is produced and is becoming available for analysis as a result of advancements in smart cyber-physical systems. The data comes from various sources, such as healthcare, smart homes, smart vehicles, and often includes private, potentially sensitive information that needs appropriate sanitization before being released for analysis. The incremental and fast nature of data generation in these systems necessitates scalable privacy-preserving mechanisms with high privacy and utility. However, privacy preservation often comes at the expense of data utility. We propose a new data perturbation algorithm, SEAL (Secure and Efficient data perturbation Algorithm utilizing Local differential privacy), based on Chebyshev interpolation and Laplacian noise, which provides a good balance between privacy and utility with high efficiency and scalability. Empirical comparisons with existing privacy-preserving algorithms show that SEAL excels in execution speed, scalability, accuracy, and attack resistance. SEAL provides flexibility in choosing the best possible privacy parameters, such as the amount of added noise, which can be tailored to the domain and dataset.\"","summary":"\"Smart cyber-physical systems (SCPS) have become an important part of the IT landscape. Often these systems include IoT devices that allow effective and easy acquisition of data in areas such as healthcare, smart cities, smart vehicles, and smart homes @cite_66 . Data mining and analysis are among the primary goals of collecting data from SCPS. The infrastructural extensions of SCPSs have contributed to the exponential growth in the number of IoT sensors, but security is often overlooked, and the devices become a source of privacy leak. The security and privacy concerns of big data and data streams are not entirely new, but require constant attention due to technological advancements of the environments and the devices used @cite_40 . Confidentiality, authentication, and authorization are just a few of the concerns @cite_44 @cite_83 @cite_32 . Many studies have raised the importance of privacy and security of SCPS due to their heavy use of personally identifiable information (PII) @cite_84 . Controlling access via authentication @cite_67 , attribute-based encryption @cite_74 , temporal and location-based access control @cite_67 and employing constraint-based protocols @cite_61 are some examples of improving privacy of SCPS.\"","":""}
{"id":"2966804802","dialogue":"\"Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function basing on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: this https URL\"","summary":"\"Among these existing works, PTGAN @cite_29 and SPGAN @cite_32 transfer source images into target-domain style by CycleGAN and then use translated images to train a model. However, due to unable to guarantee the identity of generated images, these style transfer learning methods can not result in satisfactory performance. Another line of unsupervised cross-domain person Re-ID works @cite_5 @cite_54 @cite_49 @cite_25 combine other auxiliary information as an assistant task to improve the model generalization. For instance, TFusion @cite_9 integrates spatio-temporal patterns to improve the Re-ID precision, while EANet @cite_36 uses pose segmentation. TJ-AIDL @cite_5 learns an attribute-semantic and identity discriminative feature representation space simultaneously, which can be transferred to any new target domain for re-id tasks. Similar as the difficulty of supervised learning, these domain adaptation approaches suffer from the requirement of collecting attribute annotations.\"","":""}
{"id":"2966804802","dialogue":"\"Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function basing on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: this https URL\"","summary":"\"Beyond the above methods, some approaches @cite_41 @cite_57 @cite_49 focus on estimating pseudo identity labels on the target domain so as to learn deep models in a supervised manner. Usually, clustering methods are used in the feature space to generate a series of clusters which are used to update networks with an embedding loss ( , triplet loss @cite_58 or contrastive loss) @cite_45 @cite_49 or classification loss ( , softmax cross-entropy loss) @cite_41 . Whereas, embedding loss functions suffer from the limitation of sub-optimal results and slow convergence, while classification loss extremely depends on the quality of pseudo labels. While the work in @cite_0 introduces a simple domain adaptation framework which also use both triplet loss and softmax cross-entropy loss jointly, it aims at solving one-shot leaning problem.\"","":""}
{"id":"2966292672","dialogue":"\"We introduce the metric using BERT (Bidirectional Encoder Representations from Transformers) (, 2019) for automatic machine translation evaluation. The experimental results of the WMT-2017 Metrics Shared Task dataset show that our metric achieves state-of-the-art performance in segment-level metrics task for all to-English language pairs.\"","summary":"\"ReVal https: github.com rohitguptacs ReVal @cite_5 is also a metric using sentence embeddings. ReVal trains sentence embeddings from labeled data in WMT Metrics Shared Task and semantic similarity estimation tasks, but can not achieve sufficient performance because it uses only small data. RUSE trains only regression models from labeled data using sentence embeddings pre-trained on large data such as Quick Thought @cite_8 .\"","":""}
{"id":"2966292672","dialogue":"\"We introduce the metric using BERT (Bidirectional Encoder Representations from Transformers) (, 2019) for automatic machine translation evaluation. The experimental results of the WMT-2017 Metrics Shared Task dataset show that our metric achieves state-of-the-art performance in segment-level metrics task for all to-English language pairs.\"","summary":"\"As shown in Figure , RUSE encodes an MT hypothesis and an reference translation by a sentence encoder, respectively. Then, following InferSent @cite_12 , a features are extracted by combining sentence embeddings of the two sentences, and the evaluation score is estimated by the regression model based on multi-layer perceptron (MLP).\"","":""}
{"id":"2965749255","dialogue":"\"In the last few years, Header Bidding (HB) has gained popularity among web publishers and is challenging the status quo in the ad ecosystem. Contrary to the traditional waterfall standard, HB aims to give back control of the ad inventory to publishers, increase transparency, fairness and competition among advertisers, thus, resulting in higher ad-slot prices. Although promising, little is known about this new ad-tech protocol: How does it work internally and what are the different implementations of HB? What is the performance overhead, and how does it affect the user experience? Does it, indeed, provide higher revenues to publishers than the waterfall model? Who are the dominating entities in this new protocol? To respond to all these questions and shed light on this new, buzzing ad-technology, we design and implement HBDetector: a holistic HB detection mechanism that can capture HB auctions independently of the implementation followed in a website. By running HBDetector across the top 35,000 Alexa websites, we collect and analyze a dataset of 800k auctions. Our results show that: (i) 14.28 of the top Alexa websites utilize HB. (ii) Publishers tend to collaborate mostly with a relatively low number of demand partners, which are already big players in waterfall standard, (iii) HB latency can be significantly higher than waterfall, with up to 3x latency in the median cases.\"","summary":"\"User data and their economics have long been an interesting topic and attracted a considerable body of research @cite_25 @cite_18 @cite_0 @cite_9 @cite_19 @cite_2 @cite_29 @cite_39 @cite_38 @cite_6 @cite_13 . In particular, in @cite_19 , Acquisti al discuss the value of privacy after defining two concepts (i) : the monetary amount users are willing to pay to protect their privacy, and (ii) : the compensation that users are willing to accept for their privacy loss. In two user-studies @cite_0 @cite_9 authors measure how much users value their own offline and online personal data, and consequently how much they would sell them to advertisers. @cite_2 , authors propose transactional'' privacy to allow users to decide what personal information can be released and receive compensation from selling them.\"","":""}
{"id":"2965749255","dialogue":"\"In the last few years, Header Bidding (HB) has gained popularity among web publishers and is challenging the status quo in the ad ecosystem. Contrary to the traditional waterfall standard, HB aims to give back control of the ad inventory to publishers, increase transparency, fairness and competition among advertisers, thus, resulting in higher ad-slot prices. Although promising, little is known about this new ad-tech protocol: How does it work internally and what are the different implementations of HB? What is the performance overhead, and how does it affect the user experience? Does it, indeed, provide higher revenues to publishers than the waterfall model? Who are the dominating entities in this new protocol? To respond to all these questions and shed light on this new, buzzing ad-technology, we design and implement HBDetector: a holistic HB detection mechanism that can capture HB auctions independently of the implementation followed in a website. By running HBDetector across the top 35,000 Alexa websites, we collect and analyze a dataset of 800k auctions. Our results show that: (i) 14.28 of the top Alexa websites utilize HB. (ii) Publishers tend to collaborate mostly with a relatively low number of demand partners, which are already big players in waterfall standard, (iii) HB latency can be significantly higher than waterfall, with up to 3x latency in the median cases.\"","summary":"\"Bashir al in @cite_30 , study the diffusion of user tracking caused by RTB-based programmatic ad-auctions. Results of their study show that under specific assumptions, no less than 52 tracking companies can observe at least 91 an attempt to shed light upon Facebook's ad ecosystem, Andreou al in @cite_4 investigate the level of transparency provided by the mechanisms Why am I seeing this?'' and Ad Preferences Page. The authors built a browser extension to collect Facebook ads and information extracted from these two mechanisms before performing their own ad campaigns and target users that used their browser extension. They show that ad explanations are often incomplete and misleading. @cite_23 , the authors aim to enhance the transparency in ad ecosystem with regards to information sharing, by developing a content agnostic methodology to detect client- and server- side flows of information between ad exchanges and leveraging retargeted ads. By using crawled data, the authors collected 35.4k ad impressions and identified 4 different kinds of information sharing behavior between ad exchanges.\"","":""}
{"id":"2966538158","dialogue":"\"We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels---the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out---instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.\"","summary":"\"On a high level, our work relates to the use of abstraction in creating effective visual representations, , the use of . Viola and Isenberg @cite_39 describe this concept as a process, which removes detail when transitioning from a lower-level to a higher-level representation, yet which preserves the overall concept. While they attribute the removed detail to natural variation, noise, etc.'' in the investigated multi-scale representation we actually deal with a different data scenario: DNA assemblies at different levels of scale. We thus technically do not deal with a concept-preserving transformation'' @cite_39 , but with a process in which the underlying representational concept (or parts of it) can change. Nonetheless, their view of abstraction as an interactive process that allows viewers to relate one representation (at one scale) to another one (at a different scale) is essential to our work.\"","":""}
{"id":"2966538158","dialogue":"\"We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels---the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out---instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.\"","summary":"\"Also important from Viola and Isenberg's discussion @cite_39 is their concept of , which are traversed in scale space. We also connect the DNA representations at different scales, facilitating a smooth transition between them. In creating this axis of abstraction, we focus primarily on changes of Viola and Isenberg's geometric axis, but without a geometric interpolation of different representations. Instead, we use visual embedding of one scale in another one.\"","":""}
{"id":"2966538158","dialogue":"\"We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels---the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out---instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.\"","summary":"\"We investigate multi-scale representations of the DNA, which relates to work in bio-molecular visualization. Several surveys have summarized work in this field @cite_14 @cite_3 @cite_25 @cite_17 , so below we only point out selected approaches. In addition, a large body of work by professional illustrators on mesoscale cell depiction inspired us such as visualizing the human chromosome down to the detail of individual parts of the molecule @cite_15 .\"","":""}
{"id":"2966538158","dialogue":"\"We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels---the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out---instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.\"","summary":"\"In general, as one navigates through large-scale 3D scenes, the underlying subject matter is intrinsically complex and requires appropriate interaction to aid intellection @cite_8 . The inspection of individual parts is challenging, in particular if the viewer is too far away to appreciate its visual details. Yet large, detailed datasets or procedural approaches are essential to create believable representations. To generate not only efficient but visualizations, we thus need to remove detail in Viola and Isenberg's @cite_39 visual abstraction sense. This allows us to render at interactive rates as well as to see the intended structures, which would otherwise be hidden due to cluttered views. Consequently, even most single-scale small-scale representations use some type of multi-scale approach and with it introduce abstraction. Generally we can distinguish three fundamental techniques: multi-scale representations by leaving out detail of a single data source, multi-scale techniques that actively represent preserved features at different scales, and multi-scale approaches that can also transit between representations of different scales. We discuss approaches for these three categories next.\"","":""}
{"id":"2964608056","dialogue":"\"Online updating a tracking model to adapt to object appearance variations is challenging. For SGD-based model optimization, using a large learning rate may help to converge the model faster but has the risk of letting the loss wander wildly. Thus traditional optimization methods usually choose a relatively small learning rate and iterate for more steps to converge the model, which is time-consuming. In this paper, we propose to offline train a recurrent neural optimizer to predict an adaptive learning rate for model updating in a meta-learning setting, which can converge the model in a few gradient steps. This substantially improves the convergence speed of updating the tracking model, while achieving better performance. Moreover, we also propose a simple yet effective training trick called Random Filter Scaling to prevent overfitting, which boosts the performance greatly. Finally, we extensively evaluate our tracker, ROAM, on the OTB, VOT, GOT-10K, TrackingNet and LaSOT benchmark and our method performs favorably against state-of-the-art algorithms.\"","summary":"\"Learning to learn or meta-learning has a long history @cite_46 @cite_17 @cite_42 . With the recent successes of applying meta-learning on few-shot classification @cite_55 @cite_53 and reinforcement learning @cite_15 @cite_28 , it has regained attention. The pioneering work @cite_37 designs an off-line learned optimizer using gradient decent and shows promising performance compared with traditional optimization methods. However, it does not generalize well for large numbers of descent step. To mitigate this problem, @cite_27 proposes several training techniques, including parameters scaling and combination with convex functions to coordinate the learning process of the optimizer. @cite_26 also addresses this issue by designing a hierarchical RNN architecture with dynamically adapted input and output scaling. In contrast to other works that output an increment for each parameter update, which is prone to overfitting due to different gradient scales, we instead associate an adaptive learning rate produced by a recurrent neural network with the computed gradient for fast convergence of the model update.\"","":""}
{"id":"2966209849","dialogue":"\"Exploring deep convolutional neural networks of high efficiency and low memory usage is very essential for a wide variety of machine learning tasks. Most of existing approaches used to accelerate deep models by manipulating parameters or filters without data, e.g., pruning and decomposition. In contrast, we study this problem from a different perspective by respecting the difference between data. An instance-wise feature pruning is developed by identifying informative features for different instances. Specifically, by investigating a feature decay regularization, we expect intermediate feature maps of each instance in deep neural networks to be sparse while preserving the overall network performance. During online inference, subtle features of input images extracted by intermediate layers of a well-trained neural network can be eliminated to accelerate the subsequent calculations. We further take coefficient of variation as a measure to select the layers that are appropriate for acceleration. Extensive experiments conducted on benchmark datasets and networks demonstrate the effectiveness of the proposed method.\"","summary":"\"In order to excavate the complexity of each instance, several works are proposed for assigning different parts of the designed network to different input data dynamically. For example, @cite_28 @cite_9 @cite_21 utilized attention and gate layers to evaluate each channel and discard some of them with subtle importances during the inference phrase. @cite_2 @cite_20 @cite_7 utilized a gate cell to discard some layers in pre-trained deep neural networks for efficient inference. @cite_31 @cite_4 @cite_22 @cite_5 @cite_24 further proposed the branch selection operation to allow the learned neural networks to change themselves according to different input data. @cite_30 @cite_3 @cite_27 applied the dynamic strategy on the activations of feature maps in neural networks.\"","":""}
{"id":"2964959430","dialogue":"\"With the emergence of diverse data collection techniques, objects in real applications can be represented as multi-modal features. What's more, objects may have multiple semantic meanings. Multi-modal and Multi-label [1] (MMML) problem becomes a universal phenomenon. The quality of data collected from different channels are inconsistent and some of them may not benefit for prediction. In real life, not all the modalities are needed for prediction. As a result, we propose a novel instance-oriented Multi-modal Classifier Chains (MCC) algorithm for MMML problem, which can make convince prediction with partial modalities. MCC extracts different modalities for different instances in the testing phase. Extensive experiments are performed on one real-world herbs dataset and two public datasets to validate our proposed algorithm, which reveals that it may be better to extract many instead of all of the modalities at hand.\"","summary":"\"In this section, we briefly present state-of-the-art methods in multi-modal and multi-label @cite_3 fields. As for modality extraction in multi-modal learning, it is closely related to feature extraction @cite_17 . Therefore, we briefly review some related work on these two aspects in this section.\"","":""}
{"id":"2964959430","dialogue":"\"With the emergence of diverse data collection techniques, objects in real applications can be represented as multi-modal features. What's more, objects may have multiple semantic meanings. Multi-modal and Multi-label [1] (MMML) problem becomes a universal phenomenon. The quality of data collected from different channels are inconsistent and some of them may not benefit for prediction. In real life, not all the modalities are needed for prediction. As a result, we propose a novel instance-oriented Multi-modal Classifier Chains (MCC) algorithm for MMML problem, which can make convince prediction with partial modalities. MCC extracts different modalities for different instances in the testing phase. Extensive experiments are performed on one real-world herbs dataset and two public datasets to validate our proposed algorithm, which reveals that it may be better to extract many instead of all of the modalities at hand.\"","summary":"\"Multi-label learning is a fundamental problem in machine leaning with a wide range of applications. In multi-label learning, each instance is associated with multiple interdependent labels. Binary Relevance (BR) @cite_11 algorithm is the most simple and efficient solution of multi-label algorithms. However, the effectiveness of the resulting approaches might be suboptimal due to the ignorance of label correlations. To tackle this problem, Classifier Chains (CC) @cite_6 was proposed as a high-order approach to consider correlations between labels. It is obviously that the performance of CC is seriously affected by the training order of labels. To account for the effect of ordering, Ensembles of Classifiers Chains (ECC) @cite_6 is an ensemble framework of CC, which can be built with @math random permutation instead of inducing one classifier chain. Entropy Chain Classifier (ETCC) @cite_1 extends CC by calculating the contribution between two labels using information entropy theory while Latent Dirichlet Allocation Multi-Label (LDAML) @cite_14 exploiting global correlations among labels. LDAML mainly solve the problem of large portion of single label instance in some special multi-label datasets. Due to high dimensionality of data , dimensionality reduction @cite_13 or feature extraction should be taken into consideration.\"","":""}
{"id":"2964959430","dialogue":"\"With the emergence of diverse data collection techniques, objects in real applications can be represented as multi-modal features. What's more, objects may have multiple semantic meanings. Multi-modal and Multi-label [1] (MMML) problem becomes a universal phenomenon. The quality of data collected from different channels are inconsistent and some of them may not benefit for prediction. In real life, not all the modalities are needed for prediction. As a result, we propose a novel instance-oriented Multi-modal Classifier Chains (MCC) algorithm for MMML problem, which can make convince prediction with partial modalities. MCC extracts different modalities for different instances in the testing phase. Extensive experiments are performed on one real-world herbs dataset and two public datasets to validate our proposed algorithm, which reveals that it may be better to extract many instead of all of the modalities at hand.\"","summary":"\"In this paper, taking both multi-label learning and feature extraction into consideration, we propose MCC model with an end-to-end approach @cite_7 for MMML problem, which is inspired by adaptive decision methods. Different from previous feature selection or dimensionality reduction methods, MCC extracts different modalities for different instances and different labels. Consequently, when presented with an unseen instance, we would extract the most informative and cost-effective modalities for it. Empirical study shows the efficiency and effectiveness of MCC, which can achieve better classification performance with less average modalities.\"","":""}
{"id":"2966260985","dialogue":"\"Video surveillance can be significantly enhanced by using both top-view data, e.g., those from drone-mounted cameras in the air, and horizontal-view data, e.g., those from wearable cameras on the ground. Collaborative analysis of different-view data can facilitate various kinds of applications, such as human tracking, person identification, and human activity recognition. However, for such collaborative analysis, the first step is to associate people, referred to as subjects in this paper, across these two views. This is a very challenging problem due to large human-appearance difference between top and horizontal views. In this paper, we present a new approach to address this problem by exploring and matching the subjects' spatial distributions between the two views. More specifically, on the top-view image, we model and match subjects' relative positions to the horizontal-view camera in both views and define a matching cost to decide the actual location of horizontal-view camera and its view angle in the top-view image. We collect a new dataset consisting of top-view and horizontal-view image pairs for performance evaluation and the experimental results show the effectiveness of the proposed method.\"","summary":"\"Our work can be regarded as a problem of associating first-person and third-person cameras, which has been studied by many researchers. For example, @cite_2 identify a first-person camera wearer in a third-person video by incorporating spatial and temporal information from the videos of both cameras. In @cite_15 , information from first- and third-person cameras, together with laser range data, is fused to improve depth perception and 3D reconstruction. @cite_9 predict gaze behavior in social scenes using both first- and third-person cameras. In @cite_7 , first- and third-person cameras are synchronized, followed by associating subjects between their videos. In @cite_8 , a first-person video is combined to multiple third-person videos for more reliable action recognition. The third-person cameras in these methods usually bear horizontal views or views with certain slope angle. Differently, in this paper the third-person camera is mounted on a drone and produces top-view images, making cross-view appearance matching a very difficult problem.\"","":""}
{"id":"2966260985","dialogue":"\"Video surveillance can be significantly enhanced by using both top-view data, e.g., those from drone-mounted cameras in the air, and horizontal-view data, e.g., those from wearable cameras on the ground. Collaborative analysis of different-view data can facilitate various kinds of applications, such as human tracking, person identification, and human activity recognition. However, for such collaborative analysis, the first step is to associate people, referred to as subjects in this paper, across these two views. This is a very challenging problem due to large human-appearance difference between top and horizontal views. In this paper, we present a new approach to address this problem by exploring and matching the subjects' spatial distributions between the two views. More specifically, on the top-view image, we model and match subjects' relative positions to the horizontal-view camera in both views and define a matching cost to decide the actual location of horizontal-view camera and its view angle in the top-view image. We collect a new dataset consisting of top-view and horizontal-view image pairs for performance evaluation and the experimental results show the effectiveness of the proposed method.\"","summary":"\"As mentioned above, cross-view subject association can be treated as a person re-id problem, which has been widely studied in recent years. Most existing re-id methods can be grouped into two classes: similarity learning and representation learning. The former focuses on learning the similarity metric, e.g., the invariant feature learning based models @cite_11 @cite_16 @cite_18 , classical metric learning models @cite_23 @cite_6 @cite_0 , and deep metric learning models @cite_17 @cite_4 . The latter focuses on feature learning, including low-level visual features such as color, shape, and texture @cite_14 @cite_22 , and more recent CNN deep features @cite_13 @cite_12 . These methods assume that all the data are taken from horizontal views, with similar or different horizontal view angles, and almost all of these methods are based on appearance matching. In this paper, we attempt to re-identify subjects across top and horizontal views, where appearance matching is not an appropriate choice.\"","":""}
{"id":"2966209912","dialogue":"\"Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes), by sharing information of attributes between different objects. Attributes are artificially annotated for objects and are treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impact on the ZSL system performance. This paper first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting key attributes set can improve the generalization performance of the original ZSL model which uses all the attributes. Unfortunately, previous attribute selection methods are conducted based on the seen data, their selected attributes have poor generalization capability to the unseen data, which is unavailable in training stage for ZSL tasks. Inspired by learning from pseudo relevance feedback, this paper introduces the out-of-the-box data, which is pseudo data generated by an attribute-guided generative model, to mimic the unseen data. After that, we present an iterative attribute selection (IAS) strategy which iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.\"","summary":"\"ZSL can recognize new objects using attributes as the intermediate semantic representation. Some researchers adopt the probability-prediction strategy to transfer information. @cite_12 proposed a popular baseline, i.e. direct attribute prediction (DAP). DAP learns probabilistic attribute classifiers using the seen data and infers the label of the unseen data by combining the results of pre-trained classifiers. Most recent works adopt the label-embedding strategy that directly learns a mapping function from the input features space to the semantic embedding space. One line of works is to learn linear compatibility functions. For example, @cite_0 presented an attribute label embedding (ALE) model which learns a compatibility function combined with ranking loss. Romera- @cite_16 proposed an approach that models the relationships among features, attributes and classes as a two linear layers network. Another direction is to learn nonlinear compatibility functions. @cite_30 presented a nonlinear embedding model that augments bilinear compatibility model by incorporating latent variables. @cite_15 proposed a first general kronecker product kernel-based learning model for ZSL tasks. In addition to the classification task, @cite_38 proposed an attribute network for zero-shot hashing retrieval task.\"","":""}
{"id":"2966209912","dialogue":"\"Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes), by sharing information of attributes between different objects. Attributes are artificially annotated for objects and are treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impact on the ZSL system performance. This paper first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting key attributes set can improve the generalization performance of the original ZSL model which uses all the attributes. Unfortunately, previous attribute selection methods are conducted based on the seen data, their selected attributes have poor generalization capability to the unseen data, which is unavailable in training stage for ZSL tasks. Inspired by learning from pseudo relevance feedback, this paper introduces the out-of-the-box data, which is pseudo data generated by an attribute-guided generative model, to mimic the unseen data. After that, we present an iterative attribute selection (IAS) strategy which iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.\"","summary":"\"Attributes, as popular semantic representation of visual objects, can be the appearance, a part or a property of objects @cite_31 . For example, object has the attribute and , object has the attribute . Attributes are widely used to transfer information to recognize new objects in ZSL tasks @cite_12 @cite_0 . Using attributes as the semantic representation, data of different categories locates in different boxes bounded by the attributes as shown in Fig. . Since the attribute representation of the seen classes and the unseen class are different, the boxes with respect to the seen data and the unseen data are disjoint.\"","":""}
{"id":"2966209912","dialogue":"\"Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes), by sharing information of attributes between different objects. Attributes are artificially annotated for objects and are treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impact on the ZSL system performance. This paper first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting key attributes set can improve the generalization performance of the original ZSL model which uses all the attributes. Unfortunately, previous attribute selection methods are conducted based on the seen data, their selected attributes have poor generalization capability to the unseen data, which is unavailable in training stage for ZSL tasks. Inspired by learning from pseudo relevance feedback, this paper introduces the out-of-the-box data, which is pseudo data generated by an attribute-guided generative model, to mimic the unseen data. After that, we present an iterative attribute selection (IAS) strategy which iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.\"","summary":"\"Deep generative models aim to estimate the joint distribution @math of samples and labels, by learning the class prior probability @math and the class-conditional density @math separately. Generative model can be extended to a conditional generative model if the generator is conditioned on some extra information, such as attributes in the proposed method. Mirza and Osindero @cite_27 introduced a conditional version of generative adversarial nets, i.e. CGAN, which can be constructed by simply feeding the data label. CGAN is conditioned on both the generator and discriminator and can generate samples conditioned on class labels. Conditional Variational Autoencoder (CVAE) @cite_14 , as an extension of Variational Autoencoder, is a deep conditional generative model for structured output prediction using Gaussian latent variables. We modify CVAE with the attribute representation to generate out-of-the-box data for the attribute selection.\"","":""}
{"id":"2966558078","dialogue":"\"Semantic segmentation for lightweight urban scene parsing is a very challenging task, because both accuracy and efficiency (e.g., execution speed, memory footprint, and computation complexity) should all be taken into account. However, most previous works pay too much attention to one-sided perspective, either accuracy or speed, and ignore others, which poses a great limitation to actual demands of intelligent devices. To tackle this dilemma, we propose a new lightweight architecture named Context-Integrated and Feature-Refined Network (CIFReNet). The core components of our architecture are the Long-skip Refinement Module (LRM) and the Multi-scale Contexts Integration Module (MCIM). With low additional computation cost, LRM is designed to ease the propagation of spatial information and boost the quality of feature refinement. Meanwhile, MCIM consists of three cascaded Dense Semantic Pyramid (DSP) blocks with a global constraint. It makes full use of sub-regions close to the target and enlarges the field of view in an economical yet powerful way. Comprehensive experiments have demonstrated that our proposed method reaches a reasonable trade-off among overall properties on Cityscapes and Camvid dataset. Specifically, with only 7.1 GFLOPs, CIFReNet that contains less than 1.9 M parameters obtains a competitive result of 70.9 MIoU on Cityscapes test set and 64.5 on Camvid test set at a real-time speed of 32.3 FPS, which is more cost-efficient than other state-of-the-art methods.\"","summary":"\"Some recent works based on Fully Convolution Networks (FCNs) @cite_4 have achieved promising results on public benchmarks @cite_8 , @cite_24 . We then review the latest deep-learning-based methods from lightweight-oriented and accuracy-oriented aspects for scene parsing tasks.\"","":""}
{"id":"2962894112","dialogue":"\"We address the challenging problem of generating facial attributes using a single image in an unconstrained pose. In contrast to prior works that largely consider generation on 2D near-frontal images, we propose a GAN-based framework to generate attributes directly on a dense 3D representation given by UV texture and position maps, resulting in photorealistic, geometrically-consistent and identity-preserving outputs. Starting from a self-occluded UV texture map obtained by applying an off-the-shelf 3D reconstruction method, we propose two novel components. First, a texture completion generative adversarial network (TC-GAN) completes the partial UV texture map. Second, a 3D attribute generation GAN (3DA-GAN) synthesizes the target attribute while obtaining an appearance consistent with 3D face geometry and preserving identity. Extensive experiments on CelebA, LFW and IJB-A show that our method achieves consistently better attribute generation accuracy than prior methods, a higher degree of qualitative photorealism and preserves face identity information.\"","summary":"\"Early works @cite_27 @cite_2 apply a 3D Morphable Model and search for dense point correspondence to complete the invisible face region. @cite_8 proposes a high fidelity pose and expression normalization approach based on 3DMM. @cite_40 formulate the frontalization as a low rank optimization problem. @cite_12 formulate the frontalization as a recurrent object rotation problem. @cite_29 propose a concatenate network structure to rotate faces with image-level reconstruction constraint. @cite_18 proposes using the identity perception feature to reconstruct normalized faces. Recently, GAN-based generative models @cite_31 @cite_30 @cite_15 @cite_25 @cite_41 @cite_3 have achieved high visual quality and preserve identity with large extent. Our method aligns in the GAN-based methods but works on 3D UV position and texture other than the 2D images.\"","":""}
{"id":"2962860923","dialogue":"\"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.\"","summary":"\"Recent image-conditioned models have shown promising results for numerous image-to-image translation tasks such as maps @math satellite images, sketches @math photos, labels @math images @cite_4 @cite_27 @cite_29 , future frame prediction @cite_23 , superresolution @cite_26 , and inpainting @cite_19 . Moreover, images can be stylized by disentangling the style and the content @cite_20 @cite_30 or by encoding styles into a stylebank (set of convolution filters) @cite_1 . Models @cite_34 @cite_12 for rendering a person's appearance onto a given pose have shown to be effective for person re-identification. Label-conditioned models @cite_37 @cite_21 have also been explored for generating images for specific categories.\"","":""}
{"id":"2962860923","dialogue":"\"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.\"","summary":"\"Proposed by @cite_8 , knowledge distillation is designed for transferring knowledge from a teacher classifier to a student classifier. The teacher classifier normally would have more privileged information @cite_6 compared with the student classifier. The privileged information includes two aspects. The first aspect is referred to as the learning power, namely the size of the neural networks. A student classifier could have a more compact network structure compared with the teacher classifier, and by distilling knowledge from the teacher classifier to student classifier, the student classifier would have similar or even better classification performance than the teacher network. Relevant applications include network compression @cite_32 and network training acceleration @cite_7 . The second aspect is the learning resources, namely the amount of input data. The teacher classifier could have more learning resources and see more data that the student cannot see. Compared with the first aspect, this aspect is relatively unexplored and is the focus of our work.\"","":""}
{"id":"2962860923","dialogue":"\"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.\"","summary":"Many techniques have been recently proposed for solving continuous learning problems in computer vision @cite_5 @cite_35 and robotics @cite_10 in both discriminative and generative settings.","":""}
{"id":"2962860923","dialogue":"\"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.\"","summary":"\"For discriminative settings, Shmelkov al @cite_5 employ a distillation loss that measures the discrepancy between the output of the old and new network for distilling knowledge learnt by the old network. In addition, Castro al @cite_35 propose to use a few exemplar images from previous tasks and perform knowledge distillation using new features from previous classification layers followed by a modified activation layer. For generative settings, continual learning has been primarily achieved using memory replay based methods. Replay was first proposed by @cite_17 , where the images for previous tasks are generated and combined together with the data for the new task to form a joint dataset, and a new model is trained on the joint dataset. A similar idea is also adopted by @cite_22 for label-conditioned image generation. Approaches based on elastic weight consolidation @cite_3 have also been explored for the task of label-conditioned image generation @cite_22 , but they have limited capability to remember previous categories and generate high quality images.\"","":""}
{"id":"2963636228","dialogue":"\"Most of object detection algorithms can be categorized into two classes: two-stage detectors and one-stage detectors. For two-stage detectors, a region proposal phase can filter massive background candidates in the first stage and it masks the classification task more balanced in the second stage. Recently, one-stage detectors have attracted much attention due to its simple yet effective architecture. Different from two-stage detectors, one-stage detectors have to identify foreground objects from all candidates in a single stage. This architecture is efficient but can suffer from the imbalance issue with respect to two aspects: the imbalance between classes and that in the distribution of background, where only a few candidates are hard to be identified. In this work, we propose to address the challenge by developing the distributional ranking (DR) loss. First, we convert the classification problem to a ranking problem to alleviate the class-imbalance problem. Then, we propose to rank the distribution of foreground candidates above that of background ones in the constrained worst-case scenario. This strategy not only handles the imbalance in background candidates but also improves the efficiency for the ranking algorithm. Besides the classification task, we also improve the regression loss by gradually approaching the @math loss as suggested in interior-point methods. To evaluate the proposed losses, we replace the corresponding losses in RetinaNet that reports the state-of-the-art performance as a one-stage detector. With the ResNet-101 as the backbone, our method can improve mAP on COCO data set from @math to @math by only changing the loss functions and it verifies the effectiveness of the proposed losses.\"","summary":"\"Detection is a fundamental task in computer vision. In conventional methods, hand crafted features, e.g., HOG @cite_3 and SIFT @cite_7 , are used for detection either with a sliding-window strategy which holds a dense set of candidates, e.g., DPM @cite_2 or with a region proposal method which keeps a sparse set of candidates, e.g., Selective Search @cite_8 . Recently, since deep neural networks have shown the dominating performance in classification tasks @cite_11 , the features obtained from neural networks are leveraged for detection tasks.\"","":""}
{"id":"2963636228","dialogue":"\"Most of object detection algorithms can be categorized into two classes: two-stage detectors and one-stage detectors. For two-stage detectors, a region proposal phase can filter massive background candidates in the first stage and it masks the classification task more balanced in the second stage. Recently, one-stage detectors have attracted much attention due to its simple yet effective architecture. Different from two-stage detectors, one-stage detectors have to identify foreground objects from all candidates in a single stage. This architecture is efficient but can suffer from the imbalance issue with respect to two aspects: the imbalance between classes and that in the distribution of background, where only a few candidates are hard to be identified. In this work, we propose to address the challenge by developing the distributional ranking (DR) loss. First, we convert the classification problem to a ranking problem to alleviate the class-imbalance problem. Then, we propose to rank the distribution of foreground candidates above that of background ones in the constrained worst-case scenario. This strategy not only handles the imbalance in background candidates but also improves the efficiency for the ranking algorithm. Besides the classification task, we also improve the regression loss by gradually approaching the @math loss as suggested in interior-point methods. To evaluate the proposed losses, we replace the corresponding losses in RetinaNet that reports the state-of-the-art performance as a one-stage detector. With the ResNet-101 as the backbone, our method can improve mAP on COCO data set from @math to @math by only changing the loss functions and it verifies the effectiveness of the proposed losses.\"","summary":"\"One-stage detectors are also developed for efficiency @cite_21 @cite_15 @cite_22 . Since there is no region proposal phase to sample background candidates, one-stage detectors can suffer from the imbalance issue both between classes and in the background distribution. To alleviate the challenge, SSD @cite_21 adopts hard example mining, which only keeps the hard background candidates for training. Recently, RetinaNet @cite_13 is proposed to address the problem by focal loss. Unlike SSD, it keeps all background candidates but re-weights them such that the hard example will be assigned with a large weight. Focal loss improves the performance of detection explicitly, but the imbalance problem in detection is still not explored sufficiently. In this work, we develop the distributional ranking loss that ranks the distributions of foreground and background. It can alleviate the imbalance issue and capture the data distribution better with a data dependent mechanism.\"","":""}
{"id":"2964096688","dialogue":"\"Comunicacio presentada a la 22a International Conference on Digital Audio Effects (DAFx-19) que se celebra del 2 al 6 de setembre de 2019 a Birmingham, Regne Unit.\"","summary":"\"Within the context of NSynth @cite_10 , a new high-quality dataset of one shot instrumental notes was presented, largely surpassing the size of the previous datasets, containing @math musical notes with unique pitch, timbre and envelope. The sounds were collected from @math instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. The dataset is available online https: magenta.tensorflow.org datasets nsynth and provides a good basis for training and evaluating one shot instrumental sound classifiers. This dataset is already split in training, validation and test set, where the instruments present in the training set do not overlap with the ones present in validation and test sets. However, to the best of our knowledge, no methods for instrument classification have so far been evaluated on this dataset.\"","":""}
{"id":"2963562418","dialogue":"\"Fast, non-destructive and on-site quality control tools, mainly high sensitive imaging techniques, are important to assess the reliability of photovoltaic plants. To minimize the risk of further damages and electrical yield losses, electroluminescence (EL) imaging is used to detect local defects in an early stage, which might cause future electric losses. For an automated defect recognition on EL measurements, a robust detection and rectification of modules, as well as an optional segmentation into cells is required. This paper introduces a method to detect solar modules and crossing points between solar cells in EL images. We only require 1-D image statistics for the detection, resulting in an approach that is computationally efficient. In addition, the method is able to detect the modules under perspective distortion and in scenarios, where multiple modules are visible in the image. We compare our method to the state of the art and show that it is superior in presence of perspective distortion while the performance on images, where the module is roughly coplanar to the detector, is similar to the reference method. Finally, we show that we greatly improve in terms of computational time in comparison to the reference method.\"","summary":"\"The detection of solar modules in an EL image is an object detection task. Traditionally, feature-based methods have been applied to solve the task of object detection. Especially, Haar wavelets have proven to be successful @cite_0 . For an the efficient computation, Viola and Jones @cite_12 made use of integral images, previously known as summed area tables @cite_9 . Integral images are also an essential part of our method.\"","":""}
{"id":"2963562418","dialogue":"\"Fast, non-destructive and on-site quality control tools, mainly high sensitive imaging techniques, are important to assess the reliability of photovoltaic plants. To minimize the risk of further damages and electrical yield losses, electroluminescence (EL) imaging is used to detect local defects in an early stage, which might cause future electric losses. For an automated defect recognition on EL measurements, a robust detection and rectification of modules, as well as an optional segmentation into cells is required. This paper introduces a method to detect solar modules and crossing points between solar cells in EL images. We only require 1-D image statistics for the detection, resulting in an approach that is computationally efficient. In addition, the method is able to detect the modules under perspective distortion and in scenarios, where multiple modules are visible in the image. We compare our method to the state of the art and show that it is superior in presence of perspective distortion while the performance on images, where the module is roughly coplanar to the detector, is similar to the reference method. Finally, we show that we greatly improve in terms of computational time in comparison to the reference method.\"","summary":"\"In the last years, convolutional neural networks (CNNs) have achieved superior performance in many computer vision tasks. For example, single-stage detectors like YOLO @cite_6 yield good detection performance with a tolerable computational cost. Multi-stage object detectors, such as R-CNN @cite_11 , achieve even better results but come with an increased computational cost. In contrast to CNN-based approaches, the proposed method does not require any training data and is computationally very efficient.\"","":""}
{"id":"2963562418","dialogue":"\"Fast, non-destructive and on-site quality control tools, mainly high sensitive imaging techniques, are important to assess the reliability of photovoltaic plants. To minimize the risk of further damages and electrical yield losses, electroluminescence (EL) imaging is used to detect local defects in an early stage, which might cause future electric losses. For an automated defect recognition on EL measurements, a robust detection and rectification of modules, as well as an optional segmentation into cells is required. This paper introduces a method to detect solar modules and crossing points between solar cells in EL images. We only require 1-D image statistics for the detection, resulting in an approach that is computationally efficient. In addition, the method is able to detect the modules under perspective distortion and in scenarios, where multiple modules are visible in the image. We compare our method to the state of the art and show that it is superior in presence of perspective distortion while the performance on images, where the module is roughly coplanar to the detector, is similar to the reference method. Finally, we show that we greatly improve in terms of computational time in comparison to the reference method.\"","summary":"\"There are not many preliminary works on the automated detection of solar modules. al Vetter @cite_3 proposed an object detection pipeline that consists of several stacked filters followed by a Hough transform to detect solar modules in noisy infrared thermography measurements. Recently, al Deitsch @cite_1 proposed a processing pipeline for solar modules that jointly detects the modules in an EL image, estimates the configuration ( the number of rows and columns of cells), estimates the lens distortion and performs segmentation into rectified cell images. Their approach consists of a preprocessing step, where a multiscale vesselness filter @cite_8 is used to extract ridges (separating lines between cells) and bus bars. Then, parabolic curves are fitted onto the result to obtain a parametric model of the module. Finally, the distortion is estimated and module corners are extracted. Since this is, to the best of our knowledge, the only method that automatically detects solar modules and cell crossing points in EL images, we use this as a reference method to assess the performance of our approach.\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"A recent work proposed reconstruction of dynamic fluids @cite_51 for static cameras. Another work used RGB-D cameras to obtain reconstruction of non-rigid surfaces @cite_52 . Pioneering research in general dynamic scene reconstruction from multiple handheld wide-baseline cameras @cite_31 @cite_34 exploited prior reconstruction of the background scene to allow dynamic foreground segmentation and reconstruction. Recent work @cite_11 estimates shape of dynamic objects from handheld cameras exploiting GANs. However these approaches either work for static indoor scenes or exploit strong prior assumptions such as silhouette information, known background or scene structure. Also all these approaches give per frame reconstruction leading to temporally incoherent geometries. Our aim is to perform temporally coherent dense reconstruction of unknown dynamic non-rigid scenes automatically without strong priors or limitations on scene structure.\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"Many of the existing multi-view reconstruction approaches rely on a two-stage sequential pipeline where foreground or background segmentation is initially performed independently with respect to each camera, and then used as input to obtain visual hull for multi-view reconstruction. The problem with this approach is that the errors introduced at the segmentation stage cannot be recovered and are propagated to the reconstruction stage reducing the final reconstruction quality. Segmentation from multiple wide-baseline views has been proposed by exploiting appearance similarity @cite_2 @cite_44 @cite_20 . These approaches assume static backgrounds and different colour distributions for the foreground and background @cite_16 @cite_2 which limits applicability for general scenes.\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"Joint segmentation and reconstruction methods incorporate estimation of segmentation or matting with reconstruction to provide a combined solution. Joint refinement avoids the propagation of errors between the two stages thereby making the solution more robust. Also, cues from segmentation and reconstruction can be combined efficiently to achieve more accurate results. The first multi-view joint estimation system was proposed by @cite_13 which used iterative gradient descent to perform an energy minimization. A number of approaches were introduced for joint formulation in static scenes and one recent work used training data to classify the segments @cite_40 . The focus shifted to joint segmentation and reconstruction for rigid objects in indoor and outdoor environments. These approaches used a variety of techniques such as patch-based refinement @cite_38 @cite_50 and fixating cameras on the object of interest @cite_60 for reconstructing rigid objects in the scene. However, these are either limited to static scenes @cite_40 @cite_1 or process each frame independently thereby failing to enforce temporal consistency @cite_60 @cite_30 .\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"An approach based on optical flow and graph cuts was shown to work well for non-rigid objects in indoor settings but requires known background segmentation to obtain silhouettes and is computationally expensive @cite_43 . Practical application of temporally coherent joint estimation requires approaches that work on non-rigid objects for general scenes in uncontrolled environments. A quantitative evaluation of techniques for multi-view reconstruction was presented in @cite_62 . These methods are able to produce high quality results, but rely on good initializations and strong prior assumptions with known and controlled (static) scene backgrounds.\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"Temporally coherent 4D reconstruction refers to aligning the 3D surfaces of non-rigid objects over time for a dynamic sequence. This is achieved by estimating point-to-point correspondences for the 3D surfaces to obtain 4D temporally coherent reconstruction. 4D models allows to create efficient representation for practical applications in film, broadcast and immersive content production such as virtual, augmented and mixed reality. The majority of existing approaches for reconstruction of dynamic scenes from multi-view videos process each time frame independently due to the difficulty of simultaneously estimating temporal correspondence for non-rigid objects. Independent per-frame reconstruction can result in errors due to the inherent visual ambiguity caused by occlusion and similar object appearance for general scenes. Recent research has shown that exploiting temporal information can improve reconstruction accuracy as well as achieving temporal coherence @cite_7 .\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"3D scene flow estimates frame to frame correspondence whereas 4D temporal coherence estimates correspondence across the complete sequence to obtain a single surface model. Methods to estimate 3D scene flow have been reported in the literature @cite_0 for autonomous vehicles. However this approach is limited to narrow baseline cameras. Other scene flow approaches are dependent on 2D optical flow @cite_64 @cite_46 and they require an accurate estimate for most of the pixels which fails in the case of large motion. However, 3D scene flow methods align two frames independently and do not produce temporally coherent 4D models.\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"Research investigating spatio-temporal reconstruction across multiple frames was proposed by @cite_28 @cite_15 @cite_43 exploiting the temporal information from the previous frames using optical flow. An approach for recovering space-time consistent depth maps from multiple video sequences captured by stationary, synchronized and calibrated cameras for depth based free viewpoint video rendering was proposed by @cite_67 . However these methods require accurate initialisation, fixed and calibrated cameras and are limited to simple scenes. Other approaches to temporally coherent reconstruction @cite_65 either requires a large number of closely spaced cameras or bi-layer segmentation @cite_27 @cite_55 as a constraint for reconstruction. Recent approaches for spatio-temporal reconstruction of multi-view data either work on indoor studio data @cite_35 .\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"In the field of image segmentation, approaches have been proposed to provide temporally consistent monocular video segmentation @cite_71 @cite_70 @cite_5 @cite_61 . Hierarchical segmentation based on graphs was proposed in @cite_71 , directed acyclic graph were used to propose an object followed by segmentation @cite_61 . Optical flow is used to identify and consistently segment objects @cite_5 @cite_70 . Recently a number of approaches have been proposed for multi-view foreground object segmentation by exploiting appearance similarity spatially across views @cite_24 @cite_49 @cite_44 @cite_20 . An approach for space-time multi-view segmentation was proposed by @cite_2 . However, multi-view approaches assume a static background and different colour distributions for the foreground and background which limits applicability for general scenes and non-rigid objects.\"","":""}
{"id":"2963385316","dialogue":"\"Existing techniques for dynamic scene reconstruction from multiple wide-baseline cameras primarily focus on reconstruction in controlled environments, with fixed calibrated cameras and strong prior constraints. This paper introduces a general approach to obtain a 4D representation of complex dynamic scenes from multi-view wide-baseline static or moving cameras without prior knowledge of the scene structure, appearance, or illumination. Contributions of the work are: An automatic method for initial coarse reconstruction to initialize joint estimation; Sparse-to-dense temporal correspondence integrated with joint multi-view segmentation and reconstruction to introduce temporal coherence; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes by introducing shape constraint. Comparison with state-of-the-art approaches on a variety of complex indoor and outdoor scenes, demonstrates improved accuracy in both multi-view segmentation and dense reconstruction. This paper demonstrates unsupervised reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction and its application to free-viewpoint rendering and virtual reality.\"","summary":"\"To address this issue we introduce a novel method for spatio-temporal multi-view segmentation of dynamic scenes using shape constraints. Single image segmentation techniques using shape constraints provide good results for complex scene segmentation @cite_54 (convex and concave shapes), but require manual interaction. The proposed approach performs automatic multi-view video segmentation by initializing the foreground object model using spatio-temporal information from wide-baseline feature correspondence followed by a multi-layer optimization framework. Geodesic star convexity previously used in single view segmentation @cite_54 is applied to constraint the segmentation in each view. Our multi-view formulation naturally enforces coherent segmentation between views and also resolves ambiguities such as the similarity of background and foreground in isolated views.\"","":""}
{"id":"2962977811","dialogue":"\"We introduce DaiMoN, a decentralized artificial intelligence model network, which incentivizes peer collaboration in improving the accuracy of machine learning models for a given classification problem. It is an autonomous network where peers may submit models with improved accuracy and other peers may verify the accuracy improvement. The system maintains an append-only decentralized ledger to keep the log of critical information, including who has trained the model and improved its accuracy, when it has been improved, by how much it has improved, and where to find the newly updated model. DaiMoN rewards these contributing peers with cryptographic tokens. A main feature of DaiMoN is that it allows peers to verify the accuracy improvement of submitted models without knowing the test labels. This is an essential component in order to mitigate intentional model overfitting by model-improving peers. To enable this model accuracy evaluation with hidden test labels, DaiMoN uses a novel learnable Distance Embedding for Labels (DEL) function proposed in this paper. Specific to each test dataset, DEL scrambles the test label vector by embedding it in a low-dimension space while approximately preserving the distance between the dataset's test label vector and a label vector inferred by the classifier. It therefore allows proof-of-improvement (PoI) by peers without providing them access to true test labels. We provide analysis and empirical evidence that under DEL, peers can accurately assess model accuracy. We also argue that it is hard to invert the embedding function and thus, DEL is resilient against attacks aiming to recover test labels in order to cheat. Our prototype implementation of DaiMoN is available at this https URL.\"","summary":"\"One area of related work is on data-independent locality sensitive hashing (LSH) @cite_29 and data-dependent locality preserving hashing (LPH) @cite_27 @cite_8 . LSH hashes input vectors so that similar vectors have the same hash value with high probability. There are many algorithms in the family of LSH. One of the most common LSH methods is the random projection method called SimHash @cite_32 , which uses a random hyperplane to hash input vectors.\"","":""}
{"id":"2951047368","dialogue":"\"An obstacle to the development of many natural language processing products is the vast amount of training examples necessary to get satisfactory results. The generation of these examples is often a tedious and time-consuming task. This paper this paper proposes a method to transform the sentiment of sentences in order to limit the work necessary to generate more training data. This means that one sentence can be transformed to an opposite sentiment sentence and should reduce by half the work required in the generation of text. The proposed pipeline consists of a sentiment classifier with an attention mechanism to highlight the short phrases that determine the sentiment of a sentence. Then, these phrases are changed to phrases of the opposite sentiment using a baseline model and an autoencoder approach. Experiments are run on both the separate parts of the pipeline as well as on the end-to-end model. The sentiment classifier is tested on its accuracy and is found to perform adequately. The autoencoder is tested on how well it is able to change the sentiment of an encoded phrase and it was found that such a task is possible. We use human evaluation to judge the performance of the full (end-to-end) pipeline and that reveals that a model using word vectors outperforms the encoder model. Numerical evaluation shows that a success rate of 54.7 is achieved on the sentiment change.\"","summary":"\"Sentiment analysis is a task in NLP that aims to predict the sentiment of a sentence @cite_26 . The task can range from a binary classification task where the aim is to predict whether a document is positive or negative to a fine-grained task with multiple classes. In sentiment analysis, state-of-the-art results have been achieved using neural network architectures such as convolutional neural networks @cite_9 and recurrent neural networks @cite_1 . Variants of RNNs; LSTMs and GRUs, have also been used to great success @cite_6 .\"","":""}
{"id":"2951047368","dialogue":"\"An obstacle to the development of many natural language processing products is the vast amount of training examples necessary to get satisfactory results. The generation of these examples is often a tedious and time-consuming task. This paper this paper proposes a method to transform the sentiment of sentences in order to limit the work necessary to generate more training data. This means that one sentence can be transformed to an opposite sentiment sentence and should reduce by half the work required in the generation of text. The proposed pipeline consists of a sentiment classifier with an attention mechanism to highlight the short phrases that determine the sentiment of a sentence. Then, these phrases are changed to phrases of the opposite sentiment using a baseline model and an autoencoder approach. Experiments are run on both the separate parts of the pipeline as well as on the end-to-end model. The sentiment classifier is tested on its accuracy and is found to perform adequately. The autoencoder is tested on how well it is able to change the sentiment of an encoded phrase and it was found that such a task is possible. We use human evaluation to judge the performance of the full (end-to-end) pipeline and that reveals that a model using word vectors outperforms the encoder model. Numerical evaluation shows that a success rate of 54.7 is achieved on the sentiment change.\"","summary":"\"The attention mechanism was first proposed for the task of machine translation @cite_0 . Attention allows a network to 'focus' on one part of the sentence at a time. This is done through keeping another vector which contains information on the impact of individual words. Attention has also been used in other tasks within NLP area such as document classification @cite_22 , sentiment analysis @cite_13 and teaching machines to read @cite_5\"","":""}
{"id":"2951047368","dialogue":"\"An obstacle to the development of many natural language processing products is the vast amount of training examples necessary to get satisfactory results. The generation of these examples is often a tedious and time-consuming task. This paper this paper proposes a method to transform the sentiment of sentences in order to limit the work necessary to generate more training data. This means that one sentence can be transformed to an opposite sentiment sentence and should reduce by half the work required in the generation of text. The proposed pipeline consists of a sentiment classifier with an attention mechanism to highlight the short phrases that determine the sentiment of a sentence. Then, these phrases are changed to phrases of the opposite sentiment using a baseline model and an autoencoder approach. Experiments are run on both the separate parts of the pipeline as well as on the end-to-end model. The sentiment classifier is tested on its accuracy and is found to perform adequately. The autoencoder is tested on how well it is able to change the sentiment of an encoded phrase and it was found that such a task is possible. We use human evaluation to judge the performance of the full (end-to-end) pipeline and that reveals that a model using word vectors outperforms the encoder model. Numerical evaluation shows that a success rate of 54.7 is achieved on the sentiment change.\"","summary":"\"Encoder-decoder networks @cite_10 @cite_6 are often used in neural machine translation to translate a sequence from one language to another. These networks use RNNs or other types of neural networks to encode the information in the sentence and the another network to decode this sequence to the target language. Since RNNs do not perform well on longer sequences, the LSTM @cite_10 unit is often used for their memory component. Gated Recurrent Units @cite_6 are simpler variants on the LSTM, as they do not have an output gate.\"","":""}
{"id":"2951047368","dialogue":"\"An obstacle to the development of many natural language processing products is the vast amount of training examples necessary to get satisfactory results. The generation of these examples is often a tedious and time-consuming task. This paper this paper proposes a method to transform the sentiment of sentences in order to limit the work necessary to generate more training data. This means that one sentence can be transformed to an opposite sentiment sentence and should reduce by half the work required in the generation of text. The proposed pipeline consists of a sentiment classifier with an attention mechanism to highlight the short phrases that determine the sentiment of a sentence. Then, these phrases are changed to phrases of the opposite sentiment using a baseline model and an autoencoder approach. Experiments are run on both the separate parts of the pipeline as well as on the end-to-end model. The sentiment classifier is tested on its accuracy and is found to perform adequately. The autoencoder is tested on how well it is able to change the sentiment of an encoded phrase and it was found that such a task is possible. We use human evaluation to judge the performance of the full (end-to-end) pipeline and that reveals that a model using word vectors outperforms the encoder model. Numerical evaluation shows that a success rate of 54.7 is achieved on the sentiment change.\"","summary":"\"Transforming the sentiment of sentences has not been systematically attempted, however there are some previous pieces of research into this particular topic. @cite_21 propose a method where a sentence or phrase with the target attribute, in this case sentiment, is extracted and either inserted in the new sentence or completely replacing the previous sentence. Their approach finds phrases based on how often they appear in text with a certain attribute and not in text with the other attribute. However, this approach can not take phrases into account that by themselves are not necessarily strongly leaning towards one sentiment, but still essential to the sentiment of the sentence.\"","":""}
{"id":"2914666366","dialogue":"\"Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10 ). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.\"","summary":"\"There are approaches which try to prioritize different examples to train on as the learning process goes on such as @cite_0 and @cite_23 . Although these techniques involve selecting examples to train on, they do not seek to identify redundant subsets of the data, but rather to sample the full dataset in a way that speeds up convergence.\"","":""}
{"id":"2914666366","dialogue":"\"Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10 ). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.\"","summary":"\"An early mention of trying to reduce the training dataset size can be seen in @cite_28 . Their proposed algorithm splits the training dataset into many smaller training sets and iteratively removes these smaller sets until the generalization performance falls below an acceptable threshold. However, the algorithm relies on creating many small sets out of the given training set, rendering it impractical for modern usage.\"","":""}
{"id":"2914666366","dialogue":"\"Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10 ). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.\"","summary":"\"@cite_32 pose the problem of subset selection as a constrained sub-modular maximization problem and use it to propose an active learning algorithm. The proposed techniques are used by @cite_3 in the context of image recognition tasks. These drawback however, is that when used with deep-neural networks, simple uncertainty based strategies out-perform the mentioned algorithm.\"","":""}
{"id":"2914666366","dialogue":"\"Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10 ). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.\"","summary":"\"Another example of trying to identify a smaller, more informative set can be seen in @cite_25 . Using their own definition of value of a training example, they demonstrate that prioritizing training over examples of high training value can result in improved performance for object detection tasks. The authors suggest that their definition of training value encourages prototypicality and thus results is better learning.\"","":""}
{"id":"2914666366","dialogue":"\"Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10 ). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.\"","summary":"Most recently @cite_11 attempts to find redundancies in image recognition datasets by analyzing gradient magnitudes as a measure of importance. They prioritize examples with high gradient magnitude according to a pre-trained classifier. Their method fails to find redundancies in and datasets.","":""}
{"id":"2972504565","dialogue":"\"Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.\"","summary":"\"Adaptive learning rates in online and stochastic optimization date back at least to @cite_8 and were popularized in @cite_14 @cite_11 , the former of which introduced the well-known AdaGrad algorithm. Several variants of AdaGrad have now been proposed in the optimization and machine learning literature (see and the references therein), the most notable of which is the Adam algorithm @cite_4 . All of these methods require (at least) linear space for maintaining various per-parameter statistics along their execution.\"","":""}
{"id":"2911758669","dialogue":"\"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.\"","summary":"\"Theoretically, recovering 3D shape from single-view images is an ill-posed problem. To address this issue, many attempts have been made, such as ShapeFromX @cite_18 @cite_13 , where X may represent silhouettes @cite_28 , shading @cite_26 , and texture @cite_8 . However, these methods are barely applicable to use in the real-world scenarios, because all of them require strong presumptions and abundant expertise in natural images @cite_9 .\"","":""}
{"id":"2911758669","dialogue":"\"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.\"","summary":"\"With the success of generative adversarial networks (GANs) @cite_30 and variational autoencoders (VAEs) @cite_16 , 3D-VAE-GAN @cite_4 adopts GAN and VAE to generate 3D objects by taking a single-view image as input. However, 3D-VAE-GAN requires class labels for reconstruction. MarrNet @cite_17 reconstructs 3D objects by estimating depth, surface normals, and silhouettes of 2D images, which is challenging and usually leads to severe distortion @cite_3 . OGN @cite_14 and O-CNN @cite_23 use octree to represent higher resolution volumetric 3D objects with a limited memory budget. However, OGN representations are complex and consume more computational resources due to the complexity of octree representations. PSGN @cite_7 and 3D-LMNet @cite_12 generate point clouds from single-view images. However, the points have a large degree of freedom in the point cloud representation because of the limited connections between points. Consequently, these methods cannot recover 3D volumes accurately @cite_2 .\"","":""}
{"id":"2911758669","dialogue":"\"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.\"","summary":"\"SfM @cite_11 and SLAM @cite_0 methods are successful in handling many scenarios. These methods match features among images and estimate the camera pose for each image. However, the matching process becomes difficult when multiple viewpoints are separated by a large margin. Besides, scanning all surfaces of an object before reconstruction is sometimes impossible, which leads to incomplete 3D shapes with occluded or hollowed-out areas @cite_27 .\"","":""}
{"id":"2911758669","dialogue":"\"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.\"","summary":"\"Powered by large-scale datasets of 3D CAD models (e.g., ShapeNet @cite_24 ), deep-learning-based methods have been proposed for 3D reconstruction. Both 3D-R2N2 @cite_10 and LSM @cite_6 use RNNs to infer 3D shape from single or multiple input images and achieve impressive results. However, RNNs are time-consuming and permutation-variant, which produce inconsistent reconstruction results. 3DensiNet @cite_22 uses max pooling to aggregate the features from multiple images. However, max pooling only extracts maximum values from features, which may ignore other valuable features that are useful for 3D reconstruction.\"","":""}
{"id":"2962723636","dialogue":"\"Link streams model interactions over time in a wide range of fields. Under this model, the challenge is to mine efficiently both temporal and topological structures. Community detection and change point detection are one of the most powerful tools to analyze such evolving interactions. In this paper, we build on both to detect stable community structures by identifying change points within meaningful communities. Unlike existing dynamic community detection algorithms, the proposed method is able to discover stable communities efficiently at multiple temporal scales. We test the effectiveness of our method on synthetic networks, and on high-resolution time-varying networks of contacts drawn from real social networks.\"","summary":"\"The problem of detecting communities in dynamic networks has attracted a lot of attention in recent years, with various approaches tackling different aspects of the problem, see @cite_13 for a recent survey. Most of these methods consider that the studied dynamic networks are represented as sequences of snapshots, with each snapshot being a well formed graph with meaningful community structure, see for instance @cite_7 @cite_11 . Some other methods work with interval graphs, and update the community structure at each network change, e.g., @cite_6 @cite_12 . However, all those methods are not adapted to deal with link streams, for which the network is usually not well formed at any given time. Using them on such a network would require to first aggregate the links of the stream by choosing an arbitrarily temporal scale (aggregation window).\"","":""}
{"id":"2962723636","dialogue":"\"Link streams model interactions over time in a wide range of fields. Under this model, the challenge is to mine efficiently both temporal and topological structures. Community detection and change point detection are one of the most powerful tools to analyze such evolving interactions. In this paper, we build on both to detect stable community structures by identifying change points within meaningful communities. Unlike existing dynamic community detection algorithms, the proposed method is able to discover stable communities efficiently at multiple temporal scales. We test the effectiveness of our method on synthetic networks, and on high-resolution time-varying networks of contacts drawn from real social networks.\"","summary":"\"Our work is also related to research conducted on change point detection considering community structures. In these approaches, given a sequence of snapshots, one wants to detect the periods during which the network organization and or the community structure remains stable. In @cite_4 , the authors proposed the first change-point detection method for evolving networks that uses generative network models and statistical hypothesis testing. @cite_17 proposed a hierarchical change point detection method to detect both inter-community(local change) and intra-community(global change) evolution. A recent work by @cite_16 used graph distance measures and hierarchical clustering to identify sequences of system state dynamics. From those methods, our proposal keeps the principle of stable periods delimited by change points, and the idea of detecting changes at local and global scales. But our method differs in two directions: @math we are searching for stable individual communities instead of stable graph periods, and @math we search for stable structures at multiple levels of temporal granularity.\"","":""}
{"id":"2924934677","dialogue":"\"Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance. Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Meanwhile, we also present an early attempt of training a metric-preserving network, thereby defending the metric against adversarial attacks. At last, by benchmarking various adversarial settings, we expect that our work can facilitate the development of adversarial attack and defense in metric-based applications.\"","summary":"\"Adversarial learning @cite_16 @cite_14 @cite_38 @cite_10 has been incorporated into the training procedure of re-ID systems in many previous works. In these works, generative adversarial networks (GAN) @cite_19 typically acts as a data augmentation strategy by generating photo-realistic person images to enhance the training set. For example, Zheng al @cite_17 applied GAN to generate unlabeled images and assigned a uniform label distribution during training. Wei al @cite_23 proposed Person Transfer Generative Adversarial Network (PTGAN) to bridge the gap between different datasets. Moreover, Ge al @cite_1 propose Feature Distilling Generative Adversarial Network to learn identity-related and pose-unrelated representations. @cite_36 , binary codes are learned for efficient pedestrian matching via the proposed Adversarial Binary Coding.\"","":""}
{"id":"2924934677","dialogue":"\"Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance. Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Meanwhile, we also present an early attempt of training a metric-preserving network, thereby defending the metric against adversarial attacks. At last, by benchmarking various adversarial settings, we expect that our work can facilitate the development of adversarial attack and defense in metric-based applications.\"","summary":"\"However, to the best of our knowledge, no prior work has rigorously considered the robustness of re-ID systems towards adversarial attacks, which have received wide attention in the context of classification-based tasks, including image classification @cite_46 , object detection @cite_6 and semantic segmentation @cite_3 . As these vision tasks aims to sort an into a , they are therefore special cases of the broader classification problem. On such systems, it has been demonstrated that adding carefully generated human-imperceptible perturbations to an input image can easily cause the network to misclassify the perturbed image with high confidence. These tampered images are known as adversarial examples. Great efforts have been devoted to the generation of adversarial examples @cite_28 @cite_46 @cite_26 . In contrast, our work focuses on adversarial attacks on metric learning systems, which analyze the relationship between two .\"","":""}
{"id":"2952416736","dialogue":"\"We present a novel method to explicitly incorporate topological prior knowledge into deep learning based segmentation, which is, to our knowledge, the first work to do so. Our method uses the concept of persistent homology, a tool from topological data analysis, to capture high-level topological characteristics of segmentation results in a way which is differentiable with respect to the pixelwise probability of being assigned to a given class. The topological prior knowledge consists of the sequence of desired Betti numbers of the segmentation. As a proof-of-concept we demonstrate our approach by applying it to the problem of left-ventricle segmentation of cardiac MR images of 500 subjects from the UK Biobank dataset, where we show that it improves segmentation performance in terms of topological correctness without sacrificing pixelwise accuracy.\"","summary":"\"Other approaches have involved encouraging the correct adjacencies of various object classes, whether they were learned from the data as in @cite_15 or provided as a prior as in @cite_0 . Such methods allow the introduction of this simple topological feature into a loss function when performing image segmentation but cannot be easily generalised to any other kinds of higher-order feature such as the presence of holes, handles or voids.\"","":""}
{"id":"2952416736","dialogue":"\"We present a novel method to explicitly incorporate topological prior knowledge into deep learning based segmentation, which is, to our knowledge, the first work to do so. Our method uses the concept of persistent homology, a tool from topological data analysis, to capture high-level topological characteristics of segmentation results in a way which is differentiable with respect to the pixelwise probability of being assigned to a given class. The topological prior knowledge consists of the sequence of desired Betti numbers of the segmentation. As a proof-of-concept we demonstrate our approach by applying it to the problem of left-ventricle segmentation of cardiac MR images of 500 subjects from the UK Biobank dataset, where we show that it improves segmentation performance in terms of topological correctness without sacrificing pixelwise accuracy.\"","summary":"\"The recent work of @cite_2 introduced a topological regulariser for classification problems by considering the stability of connected components of the classification boundary and can be extended to higher-order topological features. It also provided a differentiable loss function which can be incorporated in the training of a neural network. This approach differs from ours in that firstly, it imposes topological constraints on the shape of the classification boundary in the feature space of inputs to the network, rather than topological constraints in the space of the pixels in the image, and secondly it aims only to reduce overall topological complexity. Our approach aims to fit the desired absence or presence of certain features and so complex features can be penalised or rewarded, as is appropriate for the task at hand.\"","":""}
{"id":"2952416736","dialogue":"\"We present a novel method to explicitly incorporate topological prior knowledge into deep learning based segmentation, which is, to our knowledge, the first work to do so. Our method uses the concept of persistent homology, a tool from topological data analysis, to capture high-level topological characteristics of segmentation results in a way which is differentiable with respect to the pixelwise probability of being assigned to a given class. The topological prior knowledge consists of the sequence of desired Betti numbers of the segmentation. As a proof-of-concept we demonstrate our approach by applying it to the problem of left-ventricle segmentation of cardiac MR images of 500 subjects from the UK Biobank dataset, where we show that it improves segmentation performance in terms of topological correctness without sacrificing pixelwise accuracy.\"","summary":"\"Persistent homology has previously been applied to the problem of semantic segmentation, such as in @cite_19 @cite_12 @cite_14 . The important distinction between our method and these previous works is that they apply PH to the input image to extract features, which are then used as inputs to some other algorithm for training. Such approaches can capture complex features of the input images but require those topological features to be directly extractable from the raw image data. Our approach instead processes the image with a CNN and it is the output of the CNN, representing the pixelwise likelihood of the structure we want to segment, which has PH applied to it.\"","":""}
{"id":"2911594054","dialogue":"\"Multi-model fitting has been extensively studied from the random sampling and clustering perspectives. Most assume that only a single type class of model is present and their generalizations to fitting multiple types of models structures simultaneously are non-trivial. The inherent challenges include choice of types and numbers of models, sampling imbalance and parameter tuning, all of which render conventional approaches ineffective. In this work, we formulate the multi-model multi-type fitting problem as one of learning deep feature embedding that is clustering-friendly. In other words, points of the same clusters are embedded closer together through the network. For inference, we apply K-means to cluster the data in the embedded feature space and model selection is enabled by analyzing the K-means residuals. Experiments are carried out on both synthetic and real world multi-type fitting datasets, producing state-of-the-art results. Comparisons are also made on single-type multi-model fitting tasks with promising results as well.\"","summary":": Using deep learning to solve geometric model fitting has received growing considerations. The dense approaches start from raw image pairs to estimate models such as homography @cite_28 or non-rigid transformation @cite_53 . @cite_36 proposed to estimate the camera pose directly from image sequences.","":""}
{"id":"2911594054","dialogue":"\"Multi-model fitting has been extensively studied from the random sampling and clustering perspectives. Most assume that only a single type class of model is present and their generalizations to fitting multiple types of models structures simultaneously are non-trivial. The inherent challenges include choice of types and numbers of models, sampling imbalance and parameter tuning, all of which render conventional approaches ineffective. In this work, we formulate the multi-model multi-type fitting problem as one of learning deep feature embedding that is clustering-friendly. In other words, points of the same clusters are embedded closer together through the network. For inference, we apply K-means to cluster the data in the embedded feature space and model selection is enabled by analyzing the K-means residuals. Experiments are carried out on both synthetic and real world multi-type fitting datasets, producing state-of-the-art results. Comparisons are also made on single-type multi-model fitting tasks with promising results as well.\"","summary":"\"In contrast to the preceding works, DSAC @cite_34 learns to extract from sparse feature correspondences some geometric models in a manner akin to RANSAC. The ability to learn representations from sparse points was also developed recently @cite_0 @cite_49 . This ability was exploited by @cite_4 to fit camera motion (essential matrix) from noisy correspondences. Despite the promising results, none of the existing works have considered generic model fitting and, more importantly, fitting data of multiple models and even multiple types. In this work, we formulate the generic multi-model multi-type fitting problem as one of learning good representations for clustering.\"","":""}
{"id":"2912711185","dialogue":"\"Abstract Query expansion (QE) is a well-known technique used to enhance the effectiveness of information retrieval. QE reformulates the initial query by adding similar terms that help in retrieving more relevant results. Several approaches have been proposed in literature producing quite favorable results, but they are not evenly favorable for all types of queries (individual and phrase queries). One of the main reasons for this is the use of the same kind of data sources and weighting scheme while expanding both the individual and the phrase query terms. As a result, the holistic relationship among the query terms is not well captured or scored. To address this issue, we have presented a new approach for QE using Wikipedia and WordNet as data sources. Specifically, Wikipedia gives rich expansion terms for phrase terms, while WordNet does the same for individual terms. We have also proposed novel weighting schemes for expansion terms: in-link score (for terms extracted from Wikipedia) and a tf-idf based scheme (for terms extracted from WordNet). In the proposed Wikipedia-WordNet-based QE technique (WWQE), we weigh the expansion terms twice: first, they are scored by the weighting scheme individually, and then, the weighting scheme scores the selected expansion terms concerning the entire query using correlation score. The proposed approach gains improvements of 24 on the MAP score and 48 on the GMAP score over unexpanded queries on the FIRE dataset. Experimental results achieve a significant improvement over individual expansion and other related state-of-the-art approaches. We also analyzed the effect on retrieval effectiveness of the proposed technique by varying the number of expansion terms.\"","summary":"\"Query Expansion has rich literature in the area of Information Retrieval (IR). In the era of 1960s, @cite_94 was the first researcher who applied QE for literature indexing and searching in a mechanized library system. In 1971, Rocchio @cite_57 brought QE to spotlight through relevance feedback method'' and its characterization in a vector space model. This method is still used in its original and modified forms in automatic query expansion (AQE). Rocchio's work was further extended and applied in techniques such as collection-based term co-occurrence @cite_0 @cite_48 , cluster-based information retrieval @cite_104 @cite_38 , comparative analysis of term distribution @cite_89 @cite_36 @cite_20 and automatic text processing @cite_98 @cite_6 @cite_15 .\"","":""}
{"id":"2912711185","dialogue":"\"Abstract Query expansion (QE) is a well-known technique used to enhance the effectiveness of information retrieval. QE reformulates the initial query by adding similar terms that help in retrieving more relevant results. Several approaches have been proposed in literature producing quite favorable results, but they are not evenly favorable for all types of queries (individual and phrase queries). One of the main reasons for this is the use of the same kind of data sources and weighting scheme while expanding both the individual and the phrase query terms. As a result, the holistic relationship among the query terms is not well captured or scored. To address this issue, we have presented a new approach for QE using Wikipedia and WordNet as data sources. Specifically, Wikipedia gives rich expansion terms for phrase terms, while WordNet does the same for individual terms. We have also proposed novel weighting schemes for expansion terms: in-link score (for terms extracted from Wikipedia) and a tf-idf based scheme (for terms extracted from WordNet). In the proposed Wikipedia-WordNet-based QE technique (WWQE), we weigh the expansion terms twice: first, they are scored by the weighting scheme individually, and then, the weighting scheme scores the selected expansion terms concerning the entire query using correlation score. The proposed approach gains improvements of 24 on the MAP score and 48 on the GMAP score over unexpanded queries on the FIRE dataset. Experimental results achieve a significant improvement over individual expansion and other related state-of-the-art approaches. We also analyzed the effect on retrieval effectiveness of the proposed technique by varying the number of expansion terms.\"","summary":"\"Another popular approach is the use of Wikipedia articles, titles and hyper-links (in-link and out-link) @cite_85 @cite_8 . We have already mentioned the importance of Wikipedia as an ideal knowledge source for QE. Recently, quite a few research works have used it for QE (e.g., @cite_26 @cite_85 @cite_100 @cite_84 @cite_8 ). Article @cite_77 attempts to enrich initial queries using semantic annotations in Wikipedia articles combined with phrase-disambiguation. Their experiments show better results in comparison to the relevance based language model.\"","":""}
{"id":"2952051141","dialogue":"\"When facing large-scale image datasets","summary":"online hashing serves as a promising solution for online retrieval and prediction tasks. It encodes the online streaming data into compact binary codes","":""}
{"id":"2952051141","dialogue":"\"When facing large-scale image datasets","summary":"online hashing serves as a promising solution for online retrieval and prediction tasks. It encodes the online streaming data into compact binary codes","":""}
{"id":"2952051141","dialogue":"\"When facing large-scale image datasets","summary":"online hashing serves as a promising solution for online retrieval and prediction tasks. It encodes the online streaming data into compact binary codes","":""}
{"id":"2912634655","dialogue":"\"In this paper, we propose an auto-encoder based generative neural network model whose encoder compresses the inputs into vectors in the tangent space of a special Lie group manifold: upper triangular positive definite affine transform matrices (UTDATs). UTDATs are representations of Gaussian distributions and can straightforwardly generate Gaussian distributed samples. Therefore, the encoder is trained together with a decoder (generator) which takes Gaussian distributed latent vectors as input. Compared with related generative models such as variational auto-encoder, the proposed model incorporates the information on geometric properties of Gaussian distributions. As a special case, we derive an exponential mapping layer for diagonal Gaussian UTDATs which eliminates matrix exponential operator compared with general exponential mapping in Lie group theory. Moreover, we derive an intrinsic loss for UTDAT Lie group which can be calculated as l-2 loss in the tangent space. Furthermore, inspired by the Lie group theory, we propose to use the Lie algebra vectors rather than the raw parameters (e.g. mean) of Gaussian distributions as compressed representations of original inputs. Experimental results verity the effectiveness of the proposed new generative model and the benefits gained from the Lie group structural information of UTDATs.\"","summary":"\"GANs @cite_13 @cite_16 @cite_17 @cite_15 are proven effective in generating photo-realistic images in recent developments of neural networks. Because of the adversarial training approach, it is difficult for GANs to map inputs to latent vectors. Although some approaches @cite_7 @cite_14 are proposed to address this problem, it still remains open and requires further investigation. Compared to GANs, VAEs @cite_6 @cite_9 are generative models which can easily map an input to its corresponding latent vector. This advantage enables VAEs to be either used as data compressors or employed in application scenarios where manipulation of the latent space is required @cite_4 @cite_12 . Compared with AEs @cite_3 , VAEs encode inputs to Gaussian distributions instead of deterministic latent vectors, and thus enable them to generate examples. On one hand, Gaussian distributions do not form a vector space. Naively treating them as vectors will ignore its geometric properties. On the other hand, most machine learning models including neural networks are designed to work with vector outputs. To incorporate the geometric properties of Gaussian distributions, the type of space of Gaussian distributions needs to be identified first; then corresponding techniques from geometric theories will be adopted to design the neural networks.\"","":""}
{"id":"2912634655","dialogue":"\"In this paper, we propose an auto-encoder based generative neural network model whose encoder compresses the inputs into vectors in the tangent space of a special Lie group manifold: upper triangular positive definite affine transform matrices (UTDATs). UTDATs are representations of Gaussian distributions and can straightforwardly generate Gaussian distributed samples. Therefore, the encoder is trained together with a decoder (generator) which takes Gaussian distributed latent vectors as input. Compared with related generative models such as variational auto-encoder, the proposed model incorporates the information on geometric properties of Gaussian distributions. As a special case, we derive an exponential mapping layer for diagonal Gaussian UTDATs which eliminates matrix exponential operator compared with general exponential mapping in Lie group theory. Moreover, we derive an intrinsic loss for UTDAT Lie group which can be calculated as l-2 loss in the tangent space. Furthermore, inspired by the Lie group theory, we propose to use the Lie algebra vectors rather than the raw parameters (e.g. mean) of Gaussian distributions as compressed representations of original inputs. Experimental results verity the effectiveness of the proposed new generative model and the benefits gained from the Lie group structural information of UTDATs.\"","summary":"\"Geometric theories have been applied to analyze image feature space. In @cite_10 , covariance matrices are used as image feature representations for object detection. Because covariance matrices are symmetric positive definite (SPD) matrices, which form a Riemannian manifold, a corresponding boosting algorithm is designed for SPD inputs. In @cite_2 , Gaussian distributions are used to model image features and the input space is analyzed using Lie group theory.\"","":""}
{"id":"2962743035","dialogue":"\"The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, an adversarial learning approach is proposed. A generator network is used to infer the chromaticity of a given grayscale image. The same network also performs a semantic classification of the image. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way, achieving top-tier performances relative to the state-of-the-art.\"","summary":"\"In these methods the user provides local hints, as for instance color scribbles, which are then propagated to the whole image. They were initiated with the work of Levin al @cite_5 . They assume that spatial neighboring pixels having similar intensities should have similar colors. They formalize this premise optimizing a quadratic cost function constrained to the values given by the scribbles. Several improvements were proposed. Huang al @cite_8 improve the bleeding artifact using edge information of the grayscale image. Yatziv al @cite_34 propose a luminance-weighted chrominance blending to relax the dependency of the position of the scribbles. Then, Luan al @cite_45 use the input scribbles to segment the grayscale image and thus better propagate the colors. This class of methods suffer from requiring large amounts of user inputs in particular when dealing with complex textures. Moreover, choosing the correct color palette is not an easy task.\"","":""}
{"id":"2962743035","dialogue":"\"The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, an adversarial learning approach is proposed. A generator network is used to infer the chromaticity of a given grayscale image. The same network also performs a semantic classification of the image. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way, achieving top-tier performances relative to the state-of-the-art.\"","summary":"\"@cite_48 , a supervised learning method is proposed through a linear parametric model and a variational autoencoder which is computed by quadratic regression on a large dataset of color images. These approaches are improved by the use of CNNs and large-scale datasets. For instance, Iizuka al @cite_41 extract local and global features to predict the colorization. The network is trained jointly for classification and colorization in a labeled dataset.\"","":""}
{"id":"2962743035","dialogue":"\"The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, an adversarial learning approach is proposed. A generator network is used to infer the chromaticity of a given grayscale image. The same network also performs a semantic classification of the image. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way, achieving top-tier performances relative to the state-of-the-art.\"","summary":"\"Zhang al @cite_12 learn the color distribution of every pixel and infer the colorization from the learnt distribution. The network is trained with a multinomial cross entropy loss with rebalanced rare classes allowing for rare colors to appear in the colorized image. In a similar spirit, Larsson al @cite_1 train a deep CNN to learn per-pixel color histograms. They use a VGG network in order to interpret the semantic composition of the scene as well as the localization of objects and then predict the color histograms of every pixel based on this interpretation. They train the network with the Kullback-Leibler divergence. Again, the colorization is inferred from the color histrograms.\"","":""}
{"id":"2962743035","dialogue":"\"The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, an adversarial learning approach is proposed. A generator network is used to infer the chromaticity of a given grayscale image. The same network also performs a semantic classification of the image. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way, achieving top-tier performances relative to the state-of-the-art.\"","summary":"\"Other CNN based approaches are combined with user interactions. For instance, Zhang al @cite_32 propose to train a deep network given the grayscale version and a set of sparse user inputs. This allows the user to have more than one plausible solution. Also, He al @cite_33 propose an exemplar-based colorization method using a deep learning approach. The colorization network jointly learns faithful local colorization to a meaningful reference and plausible color prediction when a reliable reference is unavailable.\"","":""}
{"id":"2962743035","dialogue":"\"The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, an adversarial learning approach is proposed. A generator network is used to infer the chromaticity of a given grayscale image. The same network also performs a semantic classification of the image. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way, achieving top-tier performances relative to the state-of-the-art.\"","summary":"\"Some methods use GANs to colorize grayscale images. Isola al @cite_27 propose to use conditional GANs to map an input image to an output image using a U-Net based generator. They train their network by combining the @math -loss with an adapted GAN loss. An extension is proposed by Nazeri al @cite_23 generalizing the procedure to high resolution images, speeding up and stabilizing the training. Cao al @cite_26 also use conditional GANs but, to obtain diverse possible colorizations, they sample several times the input noise, which is incorporated in multiple layers in the proposed network architecture, which consists of a fully convolutional non-stride network. Their choice of the LSUN bedroom dataset helps their method to learn the diversity of bedroom colors. Notice, that none of these GANs based methods use additional information such as classification.\"","":""}
{"id":"2953343723","dialogue":"\"Along with the deraining performance improvement of deep networks, their structures and learning become more and more complicated and diverse, making it difficult to analyze the contribution of various network modules when developing new deraining networks. To handle this issue, this paper provides a better and simpler baseline deraining network by considering network architecture, input and output, and loss functions. Specifically, by repeatedly unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take advantage of recursive computation. A recurrent layer is further introduced to exploit the dependencies of deep features across stages, forming our progressive recurrent network (PReNet). Furthermore, intra-stage recursive computation of ResNet can be adopted in PRN and PReNet to notably reduce network parameters with graceful degradation in deraining performance. For network input and output, we take both stage-wise result and original rainy image as input to each ResNet and finally output the prediction of residual image . As for loss functions, single MSE or negative SSIM losses are sufficient to train PRN and PReNet. Experiments show that PRN and PReNet perform favorably on both synthetic and real rainy images. Considering its simplicity, efficiency and effectiveness, our models are expected to serve as a suitable baseline in future deraining research. The source codes are available at this https URL.\"","summary":"\"In general, a rainy image can be formed as the composition of a clean background image layer and a rain layer. On one hand, linear summation is usually adopted as the composition model @cite_21 @cite_3 @cite_27 . Then, image deraining can be formulated by incorporating with proper regularizers on both background image and rain layer, and solved by specific optimization algorithms. Among these methods, Gaussian mixture model (GMM) @cite_3 , sparse representation @cite_27 , and low rank representation @cite_21 have been adopted for modeling background image or a rain layers. Based on linear summation model, optimization-based methods have been also extended for video deraining @cite_32 @cite_2 @cite_24 @cite_17 @cite_14 . On the other hand, screen blend model @cite_16 is assumed to be more realistic for the composition of rainy image, based on which Luo al @cite_16 use the discriminative dictionary learning to separate rain streaks by enforcing the two layers share fewest dictionary atoms. However, the real composition generally is more complicated and the regularizers are still insufficient in characterizing background and rain layers, making optimization-based methods remain limited in deraining performance.\"","":""}
{"id":"2953343723","dialogue":"\"Along with the deraining performance improvement of deep networks, their structures and learning become more and more complicated and diverse, making it difficult to analyze the contribution of various network modules when developing new deraining networks. To handle this issue, this paper provides a better and simpler baseline deraining network by considering network architecture, input and output, and loss functions. Specifically, by repeatedly unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take advantage of recursive computation. A recurrent layer is further introduced to exploit the dependencies of deep features across stages, forming our progressive recurrent network (PReNet). Furthermore, intra-stage recursive computation of ResNet can be adopted in PRN and PReNet to notably reduce network parameters with graceful degradation in deraining performance. For network input and output, we take both stage-wise result and original rainy image as input to each ResNet and finally output the prediction of residual image . As for loss functions, single MSE or negative SSIM losses are sufficient to train PRN and PReNet. Experiments show that PRN and PReNet perform favorably on both synthetic and real rainy images. Considering its simplicity, efficiency and effectiveness, our models are expected to serve as a suitable baseline in future deraining research. The source codes are available at this https URL.\"","summary":"\"When applied deep network to single image deraining, one natural solution is to learn a direct mapping to predict clean background image @math from rainy image @math . However, it is suggested that plain fully convolutional networks (FCN) are ineffective in learning the direct mapping @cite_0 @cite_4 . Instead, Fu al @cite_0 @cite_4 apply a low-pass filter to decompose @math into a base layer @math and a detail layer @math . By assuming @math , FCNs are then deployed to predict @math from @math . In contrast, Li al @cite_10 adopt the residual learning formulation to predict rain layer @math from @math . More complicated learning formulations, such as joint detection and removal of rain streaks @cite_20 , and joint rain density estimation and deraining @cite_8 , are also suggested. And adversarial loss is also introduced to enhance the texture details of deraining result @cite_1 @cite_7 . In this work, we show that the improvement of deraining networks actually eases the difficulty of learning, and it is also feasible to train PRN and PReNet to learn either direct or residual mapping.\"","":""}
{"id":"2953343723","dialogue":"\"Along with the deraining performance improvement of deep networks, their structures and learning become more and more complicated and diverse, making it difficult to analyze the contribution of various network modules when developing new deraining networks. To handle this issue, this paper provides a better and simpler baseline deraining network by considering network architecture, input and output, and loss functions. Specifically, by repeatedly unfolding a shallow ResNet, progressive ResNet (PRN) is proposed to take advantage of recursive computation. A recurrent layer is further introduced to exploit the dependencies of deep features across stages, forming our progressive recurrent network (PReNet). Furthermore, intra-stage recursive computation of ResNet can be adopted in PRN and PReNet to notably reduce network parameters with graceful degradation in deraining performance. For network input and output, we take both stage-wise result and original rainy image as input to each ResNet and finally output the prediction of residual image . As for loss functions, single MSE or negative SSIM losses are sufficient to train PRN and PReNet. Experiments show that PRN and PReNet perform favorably on both synthetic and real rainy images. Considering its simplicity, efficiency and effectiveness, our models are expected to serve as a suitable baseline in future deraining research. The source codes are available at this https URL.\"","summary":"\"For the architecture of deraining network, Fu al first adopt a shallow CNN @cite_0 and then a deeper ResNet @cite_4 . In @cite_20 , a multi-task CNN architecture is designed for joint detection and removal of rain streaks, in which contextualized dilated convolution and recurrent structure are adopted to handle multi-scale and heavy rain steaks. Subsequently, Zhang al @cite_8 propose a density aware multi-stream densely connected CNN for joint estimating rain density and removing rain streaks. @cite_7 , attentive-recurrent network is developed for single image raindrop removal. Most recently, Li al @cite_10 recurrently utilize dilated CNN and squeeze-and-excitation blocks to remove heavy rain streaks. In comparison to these deeper and complex networks, our work incorporates ResNet, recurrent layer and multi-stage recursion to constitute a better, simpler and more efficient deraining network.\"","":""}
{"id":"2964040729","dialogue":"\"Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the node-orderless' property. Normally, we use an adjacent matrix to represent a graph, but an artificial and random node-order will be cast on the graphs, which renders the performance of deep models extremely erratic and not robust. In order to eliminate the unnecessary node-order constraint, in this paper, we propose a novel model named Isomorphic Neural Network (IsoNN), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates. IsoNN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. To further lower down the computational cost and identify the optimal subgraph patterns, IsoNN adopts two min-pooling layers to find the optimal matching. The first min-pooling layer aims at finding the best permutation matrix, whereas the second one is used to determine the best templates for the input graph data. Three fully-connected layers are used as the classification component in IsoNN. Extensive experiments are conducted on real-world datasets, and the experimental results demonstrate both the effectiveness and efficiency of IsoNN.\"","summary":"\"Graph classification is an important problem with many practical applications. Data like social networks, chemical compounds, brain networks can be represented as graphs naturally and they can have applications such as community detection @cite_31 , anti-cancer activity identification @cite_30 @cite_2 and Alzheimer's patients diagnosis @cite_32 @cite_33 respectively. Traditionally, researchers mine the subgraphs by DFS or BFS @cite_16 @cite_24 , and use them as the features. With the rapid development of deep learning (DL), many works are done based on DL methods. GAM builds the model by RNN with self-attention mechanism @cite_5 . DCNN extend CNN to general graph-structured data by introducing a ‘diffusion-convolution’ operation @cite_27 .\"","":""}
{"id":"2963324243","dialogue":"\"Recognition of objects with subtle differences has been used in many practical applications, such as car model recognition and maritime vessel identification. For discrimination of the objects in fine-grained detail, we focus on deep embedding learning by using a multi-task learning framework, in which the hierarchical labels (coarse and fine labels) of the samples are utilized both for classification and a quadruplet-based loss function. In order to improve the recognition strength of the learned features, we present a novel feature selection method specifically designed for four training samples of a quadruplet. By experiments, it is observed that the selection of very hard negative samples with relatively easy positive ones from the same coarse and fine classes significantly increases some performance metrics in a fine-grained dataset when compared to selecting the quadruplet samples randomly. The feature embedding learned by the proposed method achieves favorable performance against its state-of-the-art counterparts.\"","summary":"\"Earlier works on metric learning are based on @cite_19 . In that study, two identical neural networks extract the features of two arbitrary images. Next, these features are compared by a metric which is based on a radial function The distance between any two members in the feature space is defined as the cosine of the angle between them @cite_19 . . While their loss function forces the samples in the same class to be closer to each other in the sense of the selected distance function, the samples in the different classes are forced to be mapped far from each other. The cost function of such a network is given below @cite_2 where @math represents the operation of @math , and @math are distances in between samples.\"","":""}
{"id":"2963324243","dialogue":"\"Recognition of objects with subtle differences has been used in many practical applications, such as car model recognition and maritime vessel identification. For discrimination of the objects in fine-grained detail, we focus on deep embedding learning by using a multi-task learning framework, in which the hierarchical labels (coarse and fine labels) of the samples are utilized both for classification and a quadruplet-based loss function. In order to improve the recognition strength of the learned features, we present a novel feature selection method specifically designed for four training samples of a quadruplet. By experiments, it is observed that the selection of very hard negative samples with relatively easy positive ones from the same coarse and fine classes significantly increases some performance metrics in a fine-grained dataset when compared to selecting the quadruplet samples randomly. The feature embedding learned by the proposed method achieves favorable performance against its state-of-the-art counterparts.\"","summary":"\"Another approach is to utilize the hierarchical class labels of the training samples @cite_15 . In that method, samples with similar fine labels have the same coarse label, i.e. a sample has more than one label. The cost function is modified by considering both the coarse and fine labels. For this purpose, each quadruplet sample is constructed as follows: (1) Reference sample (anchor sample), @math , (2) Positive positive sample, @math , (3) Positive negative sample, @math , (4) Negative sample, @math . Similar to the triplet selection, the quadruplets are selected such that three constraints should be taken into account. First, both the coarse and fine classes of @math and @math should be the same. Second, although the coarse class of @math is the same as the coarse class of @math , the fine classes are different. Finally, the coarse class of @math and @math should be different.\"","":""}
{"id":"2963324243","dialogue":"\"Recognition of objects with subtle differences has been used in many practical applications, such as car model recognition and maritime vessel identification. For discrimination of the objects in fine-grained detail, we focus on deep embedding learning by using a multi-task learning framework, in which the hierarchical labels (coarse and fine labels) of the samples are utilized both for classification and a quadruplet-based loss function. In order to improve the recognition strength of the learned features, we present a novel feature selection method specifically designed for four training samples of a quadruplet. By experiments, it is observed that the selection of very hard negative samples with relatively easy positive ones from the same coarse and fine classes significantly increases some performance metrics in a fine-grained dataset when compared to selecting the quadruplet samples randomly. The feature embedding learned by the proposed method achieves favorable performance against its state-of-the-art counterparts.\"","summary":"\"Moreover, the loss function for the quadruplets is similar to the triplet based methods @cite_15 . On the other hand, in @cite_7 , the use of the global loss has been proposed, while the quadruplet samples are selected randomly (Note that these quadruplets hold the constraints). The global loss penalizes the network in case of the mean and variance of the distances between the samples in a quadruplet are not appropriate, as given in In , @math , @math , and @math , @math as defined in @cite_21 . , where @math and @math are the margins, similar to .\"","":""}
{"id":"2963324243","dialogue":"\"Recognition of objects with subtle differences has been used in many practical applications, such as car model recognition and maritime vessel identification. For discrimination of the objects in fine-grained detail, we focus on deep embedding learning by using a multi-task learning framework, in which the hierarchical labels (coarse and fine labels) of the samples are utilized both for classification and a quadruplet-based loss function. In order to improve the recognition strength of the learned features, we present a novel feature selection method specifically designed for four training samples of a quadruplet. By experiments, it is observed that the selection of very hard negative samples with relatively easy positive ones from the same coarse and fine classes significantly increases some performance metrics in a fine-grained dataset when compared to selecting the quadruplet samples randomly. The feature embedding learned by the proposed method achieves favorable performance against its state-of-the-art counterparts.\"","summary":"\"In @cite_15 , the hierarchical labels of the training samples are utilized. It should be noted that a model has difficulty in convergence when the samples are selected randomly since the most informative pairs are not effectively considered. Here, we propose two methods for sample selection to address this issue.\"","":""}
{"id":"2963549923","dialogue":"\"We consider the robust version of items selection problem, in which the goal is to choose representatives from a family of sets, preserving constraints on the allowed items' combinations. We prove NP-hardness of the deterministic version, and establish polynomially solvable special cases. Next, we consider the robust version in which we aim at minimizing the maximum regret of the solution under interval parameter uncertainty. We show that this problem is hard for the second level of polynomial-time hierarchy. We develop an exact solution algorithm for the robust problem, based on cut generation, and present the results of computational experiments.\"","summary":"\"The basic variant of this problem has been first considered in @cite_8 under the name Representatives Selection Problem, where we are allowed to select one item from each set of alternatives. In order to alleviate the effects of cost uncertainty on decision making, the min-max and min-max regret criteria @cite_11 @cite_17 have been proposed to assess the solution quality. The problem formulations using these criteria belong to the class of robust optimization problems @cite_4 . Such approach appears to be more suitable for large scale design projects than an alternative stochastic optimization approach @cite_1 , when: 1) decision makers do not have sufficient historical data for estimating probability distributions; 2) there is a high factor of risk involved in one-shot decisions, and a precautionary approach is preferred. The robust approach to discrete optimization problems has been applied in many areas of industrial engineering, such as: scheduling and sequencing @cite_9 @cite_0 @cite_21 , network optimization @cite_10 @cite_15 , assignment @cite_19 @cite_7 , and others @cite_20 .\"","":""}
{"id":"2963549923","dialogue":"\"We consider the robust version of items selection problem, in which the goal is to choose representatives from a family of sets, preserving constraints on the allowed items' combinations. We prove NP-hardness of the deterministic version, and establish polynomially solvable special cases. Next, we consider the robust version in which we aim at minimizing the maximum regret of the solution under interval parameter uncertainty. We show that this problem is hard for the second level of polynomial-time hierarchy. We develop an exact solution algorithm for the robust problem, based on cut generation, and present the results of computational experiments.\"","summary":"\"Note that deterministic version of Representatives Selection Problem is easily solvable in polynomial time. For interval uncertainty representation of cost parameters the problem can still be solved in polynomial time, both in case of minimizing the maximum regret and the relative regret @cite_8 . However, in case of discrete set of scenarios, the problem becomes NP-hard even for 2 scenarios, and strongly NP-hard when the number of scenarios @math is a part of the input. In @cite_6 authors prove that strong NP-hardness holds also when sets of eligible items are bounded. In @cite_2 an @math -approximation algorithm for this variant was given.\"","":""}
{"id":"2963297137","dialogue":"\"Industry 4.0 is becoming more and more important for manufacturers as the developments in the area of Internet of Things advance. Another technology gaining more attention is data stream processing systems. Although such streaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their application in this context is still low. The contributions in this paper are threefold. Firstly, we present industry findings that we derived from site inspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0 and important related aspects is elaborated. As a third contribution, we illustrate our opinion on why data stream processing technologies could act as an enabler for Industry 4.0 and point out possible obstacles on this way.\"","summary":"\"A recent work developed a framework called Production Assessment 4.0, which aims to support enterprises developing Industry 4.0 use cases. For doing so, they made use of the design thinking approach. After elaborating on the framework and its processes, a section about its evaluation is presented. Production Assessment 4.0 was evaluated in several consulting projects with enterprises. However, no details about, e.g., their data characteristics or their state of Industry 4.0 adoption progress are given @cite_17 .\"","":""}
{"id":"2963297137","dialogue":"\"Industry 4.0 is becoming more and more important for manufacturers as the developments in the area of Internet of Things advance. Another technology gaining more attention is data stream processing systems. Although such streaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their application in this context is still low. The contributions in this paper are threefold. Firstly, we present industry findings that we derived from site inspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0 and important related aspects is elaborated. As a third contribution, we illustrate our opinion on why data stream processing technologies could act as an enabler for Industry 4.0 and point out possible obstacles on this way.\"","summary":"\"With respect to Industry 4.0, there are many existing definitions and views published. An overview of selected perceptions of Industry 4.0 is presented in @cite_7 . Moreover, it also states that there is no generally accepted definition for the term Industry 4.0.\"","":""}
{"id":"2963297137","dialogue":"\"Industry 4.0 is becoming more and more important for manufacturers as the developments in the area of Internet of Things advance. Another technology gaining more attention is data stream processing systems. Although such streaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their application in this context is still low. The contributions in this paper are threefold. Firstly, we present industry findings that we derived from site inspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0 and important related aspects is elaborated. As a third contribution, we illustrate our opinion on why data stream processing technologies could act as an enabler for Industry 4.0 and point out possible obstacles on this way.\"","summary":"\"Another work presents design principles for Industry 4.0 that are derived through text analysis and literature studies @cite_5 . Thereby, it is aimed to help both, the scientific community and practitioners with this result. In total, four design principles were identified, namely technical assistance, interconnection, decentralized decisions, and information transparency.\"","":""}
{"id":"2963863924","dialogue":"\"We investigate the effectiveness of a simple solution to the common problem of deep learning in medical image analysis with limited quantities of labeled training data. The underlying idea is to assign artificial labels to abundantly available unlabeled medical images and, through a process known as surrogate supervision, pre-train a deep neural network model for the target medical image analysis task lacking sufficient labeled training data. In particular, we employ 3 surrogate supervision schemes, namely rotation, reconstruction, and colorization, in 4 different medical imaging applications representing classification and segmentation for both 2D and 3D medical images. 3 key findings emerge from our research: 1) pre-training with surrogate supervision is effective for small training sets; 2) deep models trained from initial weights pre-trained through surrogate supervision outperform the same models when trained from scratch, suggesting that pre-training with surrogate supervision should be considered prior to training any deep 3D models; 3) pre-training models in the medical domain with surrogate supervision is more effective than transfer learning from an unrelated domain (e.g., natural images), indicating the practical value of abundant unlabeled medical image data.\"","summary":"\"Self-supervised learning with surrogate supervision is a relatively new trend in computer vision, with promising schemes appearing only in recent years. Consequently, the literature on the effectiveness of surrogate supervision in medical imaging is meager. @cite_5 proposed longitudinal relationships between medical images as the surrogate task to pre-train model weights. To generate surrogate supervision, they assign a label of 1 if two longitudinal studies belong to the same patient and 0 otherwise. @cite_4 used noise removal in small image patches as the surrogate task, wherein the surrogate supervision was created by mapping the patches with user-injected noise to the original clean image patches. @cite_18 used image colorization as the surrogate task, wherein color colonoscopy images are converted to gray-scale and then recovered using a conditional Generative Adversarial Network (GAN).\"","":""}
{"id":"2912974420","dialogue":"\"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntactic features obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET. Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.\"","summary":"\"There are several studies for solving relation classification task. Early methods used handcrafted features through a series of NLP tools or manually designing kernels @cite_19 . These approaches use high-level lexical and syntactic features obtained from NLP tools and manually designing kernels, but the classification models relying on such features suffer from propagation of implicit error of the tools.\"","":""}
{"id":"2912974420","dialogue":"\"Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntactic features obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET. Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.\"","summary":"\"On the other hands, deep neural networks have shown outperform previous models using handcraft features. Especially, many researches tried to solve the problem based on end-to-end models using only raw sentences and pre-trained word representations learned by Skip-gram and Continuous Bag-of-Words @cite_16 @cite_8 @cite_26 . employed a deep convolutional neural network (CNN) for extracting lexical and sentence level features @cite_25 . Dos proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes @cite_15 . Zhang and Wang used bidirectional recurrent neural network (RNN) to learn long-term dependency between entity pairs @cite_21 . Furthermore, proposed bidirectional LSTM network (BLSTM) utilizing position of words, POS tags, named entity information, dependency parse @cite_24 . This model resolved vanishing gradient problem appeared in RNNs by using BLSTM.\"","":""}
{"id":"2911583982","dialogue":"\"Abstract To improve the performance of Intensive Care Units (ICUs), the field of bio-statistics has developed scores which try to predict the likelihood of negative outcomes. These help evaluate the effectiveness of treatments and clinical practice, and also help to identify patients with unexpected outcomes. However, they have been shown by several studies to offer sub-optimal performance. Alternatively, Deep Learning offers state of the art capabilities in certain prediction tasks and research suggests deep neural networks are able to outperform traditional techniques. Nevertheless, a main impediment for the adoption of Deep Learning in healthcare is its reduced interpretability, for in this field it is crucial to gain insight into the why of predictions, to assure that models are actually learning relevant features instead of spurious correlations. To address this, we propose a deep multi-scale convolutional architecture trained on the Medical Information Mart for Intensive Care III (MIMIC-III) for mortality prediction, and the use of concepts from coalitional game theory to construct visual explanations aimed to show how important these inputs are deemed by the network. Results show our model attains a ROC AUC of 0.8735 ( ± 0.0025) which is competitive with the state of the art of Deep Learning mortality models trained on MIMIC-III data, while remaining interpretable. Supporting code can be found at https: github.com williamcaicedo ISeeU .\"","summary":"\"Although the most natural application of Deep Learning algorithms to medical diagnosis is automated medical image diagnosis @cite_38 , the usage of Physiological Time Series (PTS) and Electronic Medical Record (EMR) data, is a more general source of data on which machine learning models can be trained. EMRs are very attractive as a potential data source since their use is widespread, which makes them abundant and accessible electronically. However, there are certain challenges associated with their “secondary use” in Machine Learning @cite_39 . Despite this, several works have reported the successful use of EMRs and PTS to train Machine Learning Deep Learning based models for diagnosis.\"","":""}
{"id":"2911565143","dialogue":"\"This paper presents a new method for medical diagnosis of neurodegenerative diseases, such as Parkinson's, by extracting and using latent information from trained Deep convolutional, or convolutional-recurrent Neural Networks (DNNs). In particular, our approach adopts a combination of transfer learning, k-means clustering and k-Nearest Neighbour classification of deep neural network learned representations to provide enriched prediction of the disease based on MRI and or DaT Scan data. A new loss function is introduced and used in the training of the DNNs, so as to perform adaptation of the generated learned representations between data from different medical environments. Results are presented using a recently published database of Parkinson's related information, which was generated and evaluated in a hospital environment.\"","summary":"\"A Parkinson's database comprising MRI and DaT Scan data from 78 subjects, 55 patients with Parkinson's and 23 non patients, has been recently released @cite_14 ; it includes, in total 41528 MRI data (31147 from patients and 10381 from non patients) and 925 DaT scans (595 and 330 respectively). Our developments next are based on this database.\"","":""}
{"id":"2911565143","dialogue":"\"This paper presents a new method for medical diagnosis of neurodegenerative diseases, such as Parkinson's, by extracting and using latent information from trained Deep convolutional, or convolutional-recurrent Neural Networks (DNNs). In particular, our approach adopts a combination of transfer learning, k-means clustering and k-Nearest Neighbour classification of deep neural network learned representations to provide enriched prediction of the disease based on MRI and or DaT Scan data. A new loss function is introduced and used in the training of the DNNs, so as to perform adaptation of the generated learned representations between data from different medical environments. Results are presented using a recently published database of Parkinson's related information, which was generated and evaluated in a hospital environment.\"","summary":"\"Recent advances in deep neural networks @cite_4 , @cite_6 , @cite_8 , @cite_16 have been explored in @cite_0 , where convolutional (CNN) and convolutional-recurrent (CNN-RNN) neural networks were developed and trained to classify the information in the above Parkinson's database in two categories, i.e., patients and non patients, based on either MRI inputs, or DaT Scan inputs, or together MRI and DaT Scan inputs.\"","":""}
{"id":"2911565143","dialogue":"\"This paper presents a new method for medical diagnosis of neurodegenerative diseases, such as Parkinson's, by extracting and using latent information from trained Deep convolutional, or convolutional-recurrent Neural Networks (DNNs). In particular, our approach adopts a combination of transfer learning, k-means clustering and k-Nearest Neighbour classification of deep neural network learned representations to provide enriched prediction of the disease based on MRI and or DaT Scan data. A new loss function is introduced and used in the training of the DNNs, so as to perform adaptation of the generated learned representations between data from different medical environments. Results are presented using a recently published database of Parkinson's related information, which was generated and evaluated in a hospital environment.\"","summary":"\"The developed networks included: transfer learning of the ResNet-50 network @cite_20 as far as the convolutional part of the networks was concerned, with retraining of the fully connected network layers; adding on top of this and training a recurrent network using Gated Recurrent Units (GRU) @cite_5 in an end-to-end manner.\"","":""}
{"id":"2914653242","dialogue":"\"Whole-body control (WBC) is a generic task-oriented control method for feedback control of loco-manipulation behaviors in humanoid robots. The combination of WBC and model-based walking controllers has been widely utilized in various humanoid robots. However, to date, the WBC method has not been employed for unsupported passive-ankle dynamic locomotion. As such, in this paper, we devise a new WBC, dubbed whole-body locomotion controller (WBLC), that can achieve experimental dynamic walking on unsupported passive-ankle biped robots. A key aspect of WBLC is the relaxation of contact constraints such that the control commands produce reduced jerk when switching foot contacts. To achieve robust dynamic locomotion, we conduct an in-depth analysis of uncertainty for our dynamic walking algorithm called time-to-velocity-reversal (TVR) planner. The uncertainty study is fundamental as it allows us to improve the control algorithms and mechanical structure of our robot to fulfill the tolerated uncertainty. In addition, we conduct extensive experimentation for: 1) unsupported dynamic balancing (i.e. in-place stepping) with a six degree-of-freedom (DoF) biped, Mercury; 2) unsupported directional walking with Mercury; 3) walking over an irregular and slippery terrain with Mercury; and 4) in-place walking with our newly designed ten-DoF viscoelastic liquid-cooled biped, DRACO. Overall, the main contributions of this work are on: a) achieving various modalities of unsupported dynamic locomotion of passive-ankle bipeds using a WBLC controller and a TVR planner, b) conducting an uncertainty analysis to improve the mechanical structure and the controllers of Mercury, and c) devising a whole-body control strategy that reduces movement jerk during walking.\"","summary":"\"Passive walking robots @cite_10 @cite_26 fall in the dynamic locomotion category too. These studies shed light on the important aspects of biped locomotion, but do not provide direct application for feedback control related to our methods. On the other hand, the progress made in actuated planar biped locomotion is impressive. @cite_21 @cite_23 show biped robots running and their capability to recover from disturbances on irregular terrains. However, there is an obvious gap between supported (or constrained) locomotion and unsupported walking. @cite_30 shows unsupported single leg hopping, which is a remarkable accomplishment. Besides the strong contribution in dynamic locomotion of that work, the study omitted several important aspects of unsupported biped locomotion such as body posture control, continuous interaction of the stance leg through the ground contact phases, and disturbances from the other limbs' motion, which are a focus of our paper.\"","":""}
{"id":"2914868659","dialogue":"\"With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2 on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9 , much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6 Mask AP.\"","summary":"\"Region-CNN family @cite_31 @cite_39 @cite_43 @cite_4 @cite_0 considers object detection as two sequential problems: first propose a (large) set of bounding box candidates, crop them, and use an image classification module to classify the cropped region or region feature. R-CNN @cite_31 uses selective search @cite_23 to generate region proposals and feeds them to an ImageNet classification network. SPP @cite_39 and Fast RCNN @cite_43 first feed an image through a convolutional network and crop an intermediate feature map to reduce computation. Faster RCNN @cite_4 further replaces region proposals @cite_23 with a Region Proposal Network. The detection-by-classification idea is intuitive and keeps the best performance so far @cite_14 @cite_32 @cite_21 @cite_25 @cite_33 @cite_40 @cite_38 @cite_41 @cite_12 @cite_26 .\"","":""}
{"id":"2914868659","dialogue":"\"With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2 on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9 , much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6 Mask AP.\"","summary":"One-stage object detectors @cite_48 @cite_5 @cite_42 @cite_15 @cite_20 @cite_6 @cite_18 do not have a region cropping module. They can be considered as region or anchor proposal networks and directly assign a class label to each positive anchor. SSD @cite_42 @cite_27 uses different scale anchors in different network layers. YOLOv2 @cite_15 learns category-specific anchor shape priors. RetinaNet @cite_48 proposes a focal loss to balance the training contribution between positive and negative anchors. RefineDet @cite_37 learns to early reject negative anchors. Well-designed single-stage object detectors achieve very close performance with two-stage ones at higher efficiency.","":""}
{"id":"2914868659","dialogue":"\"With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2 on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9 , much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6 Mask AP.\"","summary":"\"As a bottom-up object detection method, our idea of grouping center and extreme points is related to Deformable Part Model @cite_13 . Our center point detector functions similarly with the root filter in DPM @cite_13 , and our four extreme points can be considered as a universal part decomposition for all categories. Instead of learning the part configuration, our predicted center and four extreme points have a geometry structure. And we use a state-of-the-art keypoint detection network instead of low-level image filters for part detection.\"","":""}
{"id":"2914868659","dialogue":"\"With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2 on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9 , much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6 Mask AP.\"","summary":"\"Determining which keypoints are from the same person is an important component in bottom-up multi-person pose estimation. There are multiple solutions: Newell al @cite_1 proposes to learn an associative feature for each keypoint, which is trained using an embedding loss. Cao al @cite_30 learns an affinity field which resembles the edge between connected keypoints. Papandreous al @cite_2 learns the displacement to the parent joint on the human skeleton tree, as a 2-d feature for each keypoint. Nie al @cite_50 also learn a feature as the offset with respect to the object center.\"","":""}
{"id":"2914868659","dialogue":"\"With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2 on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9 , much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6 Mask AP.\"","summary":"\"Prevalent keypoint detection methods work on well-defined semantic keypoints, e.g., human joints. StarMap @cite_49 mixes all types of keypoints using a single heatmap for general keypoint detection. Our extreme and center points are a kind of such general implicit keypoints, but with more explicit geometry property.\"","":""}
{"id":"2952145720","dialogue":"\"Headline generation is a special type of text summarization task. While the amount of available training data for this task is almost unlimited, it still remains challenging, as learning to generate headlines for news articles implies that the model has strong reasoning about natural language. To overcome this issue, we applied recent Universal Transformer architecture paired with byte-pair encoding technique and achieved new state-of-the-art results on the New York Times Annotated corpus with ROUGE-L F1-score 24.84 and ROUGE-2 F1-score 13.48. We also present the new RIA corpus and reach ROUGE-L F1-score 36.81 and ROUGE-2 F1-score 22.15 on it.\"","summary":"\"In the recent work of Hayashi @cite_1 , an encoder-decoder approach was presented, where the first sentence was reformulated to a headline. Our Encoder-Decoder baseline (see section ) follows their setup.\"","":""}
{"id":"2914826069","dialogue":"This paper shows that every sublevel set of the loss function of a class of deep over-parameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.","summary":"\"Many interesting theoretical results have been developed on the loss surface of neural networks @cite_23 @cite_11 @cite_22 @cite_10 @cite_18 @cite_3 @cite_24 @cite_4 @cite_15 @cite_0 @cite_5 @cite_12 @cite_1 @cite_19 @cite_6 . There is also a whole line of researches studying convergence of learning algorithms in training neural networks and others studying generalization properties, which is however beyond the scope of this paper.\"","":""}
{"id":"2913669491","dialogue":"\"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10 of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.\"","summary":"\"One of the most related studies is the one by Xia et al @cite_33 . They investigated what developers search for on the Web, and found that developers search for explanations of unknown terminology, explanations for exceptions error messages (e.g., HTTP 404), reusable code snippets, solutions to common programming bugs, and suitable third-party libraries services. Furthermore, they found that searching for solutions to performance bugs, solutions to multi-threading bugs, public datasets to test newly developed algorithms or systems, reusable code snippets, best industrial practices, database optimization solutions, solutions to security bugs, and solutions to software configuration bugs are the most difficult search tasks that developers consider.\"","":""}
{"id":"2913669491","dialogue":"\"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10 of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.\"","summary":"\"Many researchers have made use of code comments in their work. @cite_24 automatically identify bugs by analyzing inconsistencies between code and comments. Ratol and Robillard @cite_38 used code comments to assist refactoring activities. Wong et al @cite_27 used code comments to map source code and Stack Overflow content. German et al @cite_25 developed the ninka tool that automatically identifies a software license in code comments. Goldman and Miller @cite_3 developed the tool CodeTrail, that demonstrates how the developer's use of web resources can be improved by connecting the Eclipse integrated development environment (IDE) and the Firefox web browser.\"","":""}
{"id":"2913669491","dialogue":"\"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10 of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.\"","summary":"Self-admitted technical debt is a commenting activity that has been well-studied in recent years @cite_20 . @cite_18 and @cite_7 studied the removal of self-admitted technical debt based on the modification of comments. Our finding of referencing bug reports for self-admitted technical debt could be another opportunity to study development activities around technical debt.","":""}
{"id":"2913669491","dialogue":"\"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10 of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.\"","summary":"There are also studies which analyze link sharing occurring in other software artifacts. Gomez et al @cite_34 investigated link sharing on Stack Overflow to gain insights into how software developers discover and disseminate innovations. Rath et al @cite_30 investigated links to issue tracking systems in commit comments. They reported that developers often do not provide external links to issues. They evaluated several methods to automatically recover links by searching issues related to a given commit. Alqahtani et al @cite_1 proposed a tool to automatically link dependent components in a system to online resources for analyzing their vulnerabilities. Chen et al @cite_12 proposed a tool to link problematic source code to relevant Stack Overflow questions using similarity of source code fragments.","":""}
{"id":"2913669491","dialogue":"\"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10 of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.\"","summary":"Traceability links between source code and documents is another related research topic. Scanniello et al @cite_13 reported that developers can understand source code effectively if they can refer to design models including source code element names. Their observation has been obtained through a controlled experiment of program comprehension tasks with UML models produced in a requirements engineering phase and a design phase. Antoniol et al @cite_15 proposed a method to identify links between source files and design documents because developers may update source file names without updating related documents. Their method uses similarity of attribute names of a class to identify its original class definition in design documents. Rahimi et al @cite_5 proposed a rule-based method to update links between source files and requirements documents. Their method recognizes a change scenario from semantic differences of source code and then updates links according to a rule corresponding to the change scenario. Those methods would be effective to automatically update traceability links. Similar tool support for external source referencing is a future direction of our research.","":""}
{"id":"2912586918","dialogue":"\"Advertisements are unavoidable in modern society. Times Square is notorious for its incessant display of advertisements. Its popularity is worldwide and smaller cities possess miniature versions of the display, such as Pittsburgh and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district recently rose to popularity due to its upscale shops and constant onslaught of advertisements to pedestrians. Advertisements arise in other mediums as well. For example, they help popular streaming services, such as Spotify, Hulu, and Youtube TV gather significant streams of revenue to reduce the cost of monthly subscriptions for consumers. Ads provide an additional source of money for companies and entire industries to allocate resources toward alternative business motives. They are attractive to companies and nearly unavoidable for consumers. One challenge for advertisers is examining a advertisement's effectiveness or usefulness in conveying a message to their targeted demographics. Rather than constructing a single, static image of content, a video advertisement possesses hundreds of frames of data with varying scenes, actors, objects, and complexity. Therefore, measuring effectiveness of video advertisements is important to impacting a billion-dollar industry. This paper explores the combination of human-annotated features and common video processing techniques to predict effectiveness ratings of advertisements collected from Youtube. This task is seen as a binary (effective vs. non-effective), four-way, and five-way machine learning classification task. The first findings in terms of accuracy and inference on this dataset, as well as some of the first ad research, on a small dataset are presented. Accuracies of 84 , 65 , and 55 are reached on the binary, four-way, and five-way tasks respectively.\"","summary":"\"Finally, in 2006, researchers investigated the use of neural networks to predict television ad effectiveness @cite_30 . They achieved an accuracy of 99\"","":""}
{"id":"2961524200","dialogue":"\"The phenomenon of residential segregation was captured by Schelling's famous segregation model where two types of agents are placed on a grid and an agent is content with her location if the fraction of her neighbors which have the same type as her is at least @math , for some @math . Discontent agents simply swap their location with a randomly chosen other discontent agent or jump to a random empty cell. We analyze a generalized game-theoretic model of Schelling segregation which allows more than two agent types and more general underlying graphs modeling the residential area. For this we show that both aspects heavily influence the dynamic properties and the tractability of finding an optimal placement. We map the boundary of when improving response dynamics (IRD), i.e., the natural approach for finding equilibrium states, are guaranteed to converge. For this we prove several sharp threshold results where guaranteed IRD convergence suddenly turns into the strongest possible non-convergence result: a violation of weak acyclicity. In particular, we show such threshold results also for Schelling's original model, which is in contrast to the standard assumption in many empirical papers. Furthermore, we show that in case of convergence, IRD find an equilibrium in @math steps, where @math is the number of edges in the underlying graph and show that this bound is met in empirical simulations starting from random initial agent placements.\"","summary":"\"Recently, a series of papers by Young @cite_24 , Zhang @cite_21 @cite_28 , @cite_13 , @cite_6 @cite_22 , @cite_18 @cite_27 and @cite_19 initiated a rigorous analysis of stochastic processes induced by Schelling's model. In these processes either two randomly chosen unhappy agents of different type swap positions @cite_24 @cite_21 @cite_28 or a randomly chosen agent changes her type with a certain probability @cite_6 @cite_18 @cite_19 @cite_27 @cite_22 . It is worth noticing that both types of processes are closely related but not identical to Schelling's original model where discontent agents move to different positions until they become content with their current location. The focus of the above mentioned works is on investigating the expected size of the obtained homogeneous regions, but it is also shown that the stochastic processes starting from a uniform random agent placement converge with high probability to a stable placement. The convergence time was considered by Mobius & Rosenblat @cite_12 who observe that the Markov chain analyzed in @cite_24 @cite_21 @cite_28 has a very high mixing time. @cite_19 show in the two-dimensional grid case a dichotomy in mixing times for high @math and very low @math values.\"","":""}
{"id":"2961524200","dialogue":"\"The phenomenon of residential segregation was captured by Schelling's famous segregation model where two types of agents are placed on a grid and an agent is content with her location if the fraction of her neighbors which have the same type as her is at least @math , for some @math . Discontent agents simply swap their location with a randomly chosen other discontent agent or jump to a random empty cell. We analyze a generalized game-theoretic model of Schelling segregation which allows more than two agent types and more general underlying graphs modeling the residential area. For this we show that both aspects heavily influence the dynamic properties and the tractability of finding an optimal placement. We map the boundary of when improving response dynamics (IRD), i.e., the natural approach for finding equilibrium states, are guaranteed to converge. For this we prove several sharp threshold results where guaranteed IRD convergence suddenly turns into the strongest possible non-convergence result: a violation of weak acyclicity. In particular, we show such threshold results also for Schelling's original model, which is in contrast to the standard assumption in many empirical papers. Furthermore, we show that in case of convergence, IRD find an equilibrium in @math steps, where @math is the number of edges in the underlying graph and show that this bound is met in empirical simulations starting from random initial agent placements.\"","summary":"\"Very recently, @cite_20 studied a variant of the model by @cite_8 , where the agents are partitioned into stubborn and strategic agents. The former agents do not move and the latter agents try to maximize the fraction of same-type agents in their neighborhood by jumping to a suitable empty location. This corresponds to a variant of the JSG with @math . They show that equilibria are not guaranteed to exist and that deciding equilibrium existence or the existence of an agent placement with certain social welfare is NP-hard. This relates to our hardness results for computing socially optimal states. They also prove that the price of anarchy and the price of stability can be unbounded.\"","":""}
{"id":"2960213605","dialogue":"\"In the time-decay model for data streams, elements of an underlying data set arrive sequentially with the recently arrived elements being more important. A common approach for handling large data sets is to maintain a , a succinct summary of the processed data that allows approximate recovery of a predetermined query. We provide a general framework that takes any offline-coreset and gives a time-decay coreset for polynomial time decay functions. We also consider the exponential time decay model for @math -median clustering, where we provide a constant factor approximation algorithm that utilizes the online facility location algorithm. Our algorithm stores @math points where @math is the half-life of the decay function and @math is the aspect ratio of the dataset. Our techniques extend to @math -means clustering and @math -estimators as well.\"","summary":"\"The first insertion-only streaming algorithm for the @math -median clustering problem was presented in 2000 by Guha, Mishra, Motwani, and O'Callaghan @cite_17 . Their algorithm uses @math space for a @math approximation, for some @math . Subsequently, Charikar al , @cite_0 present an @math -approximation algorithm for @math -means clustering that uses @math space. Their algorithm uses a number of phases, each corresponding to a different guess for the value of the cost of optimal solution. The guesses are then used in the online facility location ( ) algorithm of @cite_45 , which provides a set of centers whose number and cost allows the algorithm to reject or accept the guess. This technique is now one of the standard approaches for handling @math -service problems. Braverman al , @cite_3 improve the space usage of this technique to @math . @cite_20 and @cite_2 develop algorithms for @math -means clustering on sliding windows, in which expired data should not be included in determining the cost of a solution.\"","":""}
{"id":"2960213605","dialogue":"\"In the time-decay model for data streams, elements of an underlying data set arrive sequentially with the recently arrived elements being more important. A common approach for handling large data sets is to maintain a , a succinct summary of the processed data that allows approximate recovery of a predetermined query. We provide a general framework that takes any offline-coreset and gives a time-decay coreset for polynomial time decay functions. We also consider the exponential time decay model for @math -median clustering, where we provide a constant factor approximation algorithm that utilizes the online facility location algorithm. Our algorithm stores @math points where @math is the half-life of the decay function and @math is the aspect ratio of the dataset. Our techniques extend to @math -means clustering and @math -estimators as well.\"","summary":"\"Another line of approach for @math -service problems is the construction of coresets, in particular when the data points lie in the Euclidean space. Har-Peled and Mazumdar @cite_28 give an insertion-only streaming algorithm for @math -medians and @math -means that provides a @math -approximation, using space @math , where @math is the dimension of the space. Similarly, Chen @cite_40 introduced an algorithm using @math space, with the same approximation guarantees.\"","":""}
{"id":"2960213605","dialogue":"\"In the time-decay model for data streams, elements of an underlying data set arrive sequentially with the recently arrived elements being more important. A common approach for handling large data sets is to maintain a , a succinct summary of the processed data that allows approximate recovery of a predetermined query. We provide a general framework that takes any offline-coreset and gives a time-decay coreset for polynomial time decay functions. We also consider the exponential time decay model for @math -median clustering, where we provide a constant factor approximation algorithm that utilizes the online facility location algorithm. Our algorithm stores @math points where @math is the half-life of the decay function and @math is the aspect ratio of the dataset. Our techniques extend to @math -means clustering and @math -estimators as well.\"","summary":"\"Cohen and Strauss @cite_47 study problems in time-decaying data streams in 2003. There are a number of results @cite_49 @cite_41 @cite_46 @cite_24 in this line of work, but the most prominent time-decay model is the sliding window model. Datar al , @cite_4 introduced the exponential histogram as a framework in the sliding window for estimating statistics such as count, sum of positive integers, average, and @math norms. This initiated an active line of research, including improvements to count and sum @cite_26 , frequent itemsets @cite_33 @cite_12 , frequency counts and quantiles @cite_32 @cite_44 , rarity and similarity @cite_11 , variance and @math -medians @cite_36 and other geometric and numerical linear algebra problems @cite_10 @cite_25 @cite_18 .\"","":""}
{"id":"2962316770","dialogue":"\"This paper proposes a face anti-spoofing user-centered model (FAS-UCM). The major difficulty, in this case, is obtaining fraudulent images from all users to train the models. To overcome this problem, the proposed method is divided in three main parts: generation of new spoof images, based on style transfer and spoof image representation models; training of a Convolutional Neural Network (CNN) for liveness detection; evaluation of the live and spoof testing images for each subject. The generalization of the CNN to perform style transfer has shown promising qualitative results. Preliminary results have shown that the proposed method is capable of distinguishing between live and spoof images on the SiW database, with an average classification error rate of 0.22.\"","summary":"\"Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNNs) have been extensively used in the process of generating new images given a specific database as a reference. The modeling of new images can be learned from the probability distribution of any set of images @cite_36 . This process can be perceived in the literature in applications such as the generation of new images @cite_35 , the transfer of styles from one set of images to another @cite_22 , the modeling of new images combining features in the discriminative space @cite_32 , among others.\"","":""}
{"id":"2962316770","dialogue":"\"This paper proposes a face anti-spoofing user-centered model (FAS-UCM). The major difficulty, in this case, is obtaining fraudulent images from all users to train the models. To overcome this problem, the proposed method is divided in three main parts: generation of new spoof images, based on style transfer and spoof image representation models; training of a Convolutional Neural Network (CNN) for liveness detection; evaluation of the live and spoof testing images for each subject. The generalization of the CNN to perform style transfer has shown promising qualitative results. Preliminary results have shown that the proposed method is capable of distinguishing between live and spoof images on the SiW database, with an average classification error rate of 0.22.\"","summary":"\"@cite_1 presented a neural algorithm for style transfer based on the extraction of image style through convolutional layers. The authors showed that the deeper the convolutional layers, the more the content of the image and the artistic style could be separated -- and, as a result, more the artistic style could be extracted from the input image. Similar to this, the higher layers of the CNN can generate more robust, sharp and detailed artistic styles images @cite_1 . @cite_37 brought to light optimization to the neural algorithm proposed by @cite_1 , where the neural feed-forward network was trained with perceptual loss, instead of a per-pixel loss. Such an optimization had similar qualitative results in regards to the artistic style transfer, with three orders of magnitude faster @cite_37 . The optimization proposed by @cite_8 also showed that instance normalization could be applied to the CNN with improved results over batch normalization, in training and testing time.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Let us recall that Semantic GP uses the information in the target behavior, i.e., @math , to guide the search. Notably, Krawiec @cite_29 affirmed that aware semantic methods make search algorithms better informed. For example, Nguyen @cite_28 proposed Fitness Sharing, a technique that promotes dispersion and diversity of individuals. Their proposal consisted of calculating the individual fitness as @math , where @math is approximately equal to the number of individuals that behave similarly to individual @math .\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Some crossover and mutation operators have been developed with the use of semantics. Beadle and Johnson @cite_40 proposed a crossover operator that measures the semantic equivalence between parents and offsprings; and rejects the offspring that is semantically equivalent to its parents. Quang Uy @cite_42 proposed a semantic crossover and mutation. The crossover operator searches for a crossover point in each parent in such way that subtrees were semantically similar, and the mutation operator allows the replacement of an individual subtree only if the new subtree is semantically similar. Hara @cite_16 proposed the Semantic Control Crossover that uses the semantics to combine individuals where a global search was performed in the first generations and a local search in the last ones. Graff used subtrees semantics and partial derivatives to proposed crossover @cite_18 @cite_49 and mutation @cite_36 operators.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Moraglio @cite_21 @cite_27 proposed Geometric Semantic Genetic Programming (GSGP). Their work called the attention of the GP scientific community because the crossover operator produces an offspring that stands in the segment joining the parents' semantics. Therefore, offspring fitness cannot be worse than the worst fitness of the parents. Given two parents @math and @math , the crossover operator generates an offspring as @math , where @math is a real value between @math and @math . This property transforms the fitness landscape into a cone. Unfortunately, the offspring is always bigger than the sum of the size of its parents; this makes the operator unusable in practice. Later, some operators appear intending to improve Moraglio's GSGP. For example, Approximately Geometric Semantic Crossover (SX) @cite_10 , Deterministic Geometric Semantic Crossover @cite_16 , Locally Geometric Crossover (LGX) @cite_7 @cite_2 and Approximated Geometric Crossover (AGX) @cite_8 , Semantic Crossover and Mutation based on projections @cite_24 @cite_19 and Subtree Semantic Geometric Crossover (SSGX) @cite_47 .\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Pawlak @cite_8 proposed the Random Desired Operator (RDO). It propagates the target semantics to calculate the desired semantics in the node selected as mutation point. This desired behavior is used to search in a procedures library for the most similar subtree. Finally, it swaps the mutated node with the subtree. RDO was extended by Szubert @cite_0 introducing the Forward Propagation Mutation (FPM) which uses a combination of forward and back-propagation to find a combination of unitary and binary functions that is the most similar to the desired behavior.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Chen @cite_11 proposed the Angle-Driven Selection (ADS) where the first parent is selected using fitness and the second is with an angle-distance defined as @math . One of our selection heuristics is similar to ADS; however, there are significant differences, the first parent is randomly selected whereas the second parent is selected using an equivalent similarity with the difference that the target behavior is not considered in our approach.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Loveard and Ciesielski @cite_5 proposed different techniques for representing classification problems in GP; one of them assign the class based on a range, there were as many intervals as classes. Muni @cite_51 proposed to evolve a tree for each class following an equivalent strategy of one-vs-all approach. Jaben and Baig @cite_17 developed a two-stage method, the first one evolves a classifier for each class, and the second phases combine these classifiers.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Ingalalli @cite_13 introduced a GP framework called Multi-dimensional Multi-class Genetic Programming (M2GP). The main idea is to transform the original space into another one using functions evolved with GP, then, a centroid is calculated for each class, and the vectors are assigned to the class which corresponds to the nearest centroid using the Mahalanobis distance. M2GP takes as argument the dimension of the transform space; this parameter is evolved in M3GP @cite_1 by including specialized search operators that can increase or decrease the number of feature dimensions produced by each tree. They extended M3GP and proposed M4GP @cite_12 that uses a stack-based representation in addition to new selection methods, namely lexicase selection, and age-fitness Pareto survival.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Naredo @cite_33 use NS for evolving genetic programming classifiers based on M3GP where the difference is the procedure to compute the fitness. Each GP individual is represented as a binary vector whose length is the training set size and each vector element is set to 1 if the classifier assigns the class label correctly and 0 otherwise. Then, they use this binary vectors to measure the sparseness among individuals, and the more the sparseness the higher the fitness value. Their results show that all their NS variants achieve competitive results relative to the traditional objective-based.\"","":""}
{"id":"2959448612","dialogue":"\"In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.\"","summary":"\"Auto machine learning consists of obtaining automatically a classifier (regressor) that includes the steps of preprocessing, feature selection, classifier selection, and hyperparameters tuning. Feurer @cite_26 developed a robust automated machine learning (AutoML) technique using Bayesian optimization methods. It is based on scikit-learn @cite_23 , using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods; giving rise to a structured hypothesis space with 110 hyperparameters. Olson @cite_52 proposed the use of GP to develop a powerful algorithm that automatically constructs and optimizes machine learning pipelines through a Tree-based Pipeline Optimization Tool (TPOT). On classification, the objective consists of maximizing accuracy score performing a searching of the combinations of 14 preprocessors, five feature selectors, and 11 classifiers; all these techniques implemented on scikit-learn @cite_23 .\"","":""}
{"id":"2960160011","dialogue":"\"Dynamic malware analysis executes the program in an isolated environment and monitors its run-time behaviour (e.g.","summary":"system API calls) for malware detection. This technique has been proven to be effective against various code obfuscation techniques and newly released (\"\"zero-day\"\") malware. However","":""}
{"id":"2960160011","dialogue":"\"Dynamic malware analysis executes the program in an isolated environment and monitors its run-time behaviour (e.g.","summary":"system API calls) for malware detection. This technique has been proven to be effective against various code obfuscation techniques and newly released (\"\"zero-day\"\") malware. However","":""}
{"id":"2960160011","dialogue":"\"Dynamic malware analysis executes the program in an isolated environment and monitors its run-time behaviour (e.g.","summary":"system API calls) for malware detection. This technique has been proven to be effective against various code obfuscation techniques and newly released (\"\"zero-day\"\") malware. However","":""}
{"id":"2960160011","dialogue":"\"Dynamic malware analysis executes the program in an isolated environment and monitors its run-time behaviour (e.g.","summary":"system API calls) for malware detection. This technique has been proven to be effective against various code obfuscation techniques and newly released (\"\"zero-day\"\") malware. However","":""}
{"id":"2960160011","dialogue":"\"Dynamic malware analysis executes the program in an isolated environment and monitors its run-time behaviour (e.g.","summary":"system API calls) for malware detection. This technique has been proven to be effective against various code obfuscation techniques and newly released (\"\"zero-day\"\") malware. However","":""}
{"id":"2961578905","dialogue":"\"We present a data-driven framework for incorporating side information in dynamic optimization under uncertainty. Specifically, our approach uses predictive machine learning methods (such as k-nearest neighbors, kernel regression, and random forests) to weight the relative importance of various data-driven uncertainty sets in a robust optimization formulation. Through a novel measure concentration result for local machine learning methods, we prove that the proposed framework is asymptotically optimal for stochastic dynamic optimization with covariates. We also describe a general-purpose approximation for the proposed framework, based on overlapping linear decision rules, which is computationally tractable and produces high-quality solutions for dynamic problems with many stages. Across a variety of examples in shipment planning, inventory management, and finance, our method achieves improvements of up to 15 over alternatives and requires less than one minute of computation time on problems with twelve stages.\"","summary":"\"This paper follows a recent body of literature on data-driven optimization under uncertainty in operations research and management science. Much of this work has focused on the paradigm of distributionally robust optimization, in which the optimal solution is that which performs best in expectation over a worst-case probability distribution from an ambiguity set. Motivated by probabilistic guarantees, distributionally robust optimization has found particular applicability in data-driven settings in which the ambiguity set is constructed using historical data, such as @cite_7 @cite_9 @cite_5 @cite_14 . In particular, the final steps in our convergence result () draw heavily from similar techniques from @cite_5 and @cite_10 . In contrast to previous work, this paper develops a new measure concentration result for the weighted empirical distribution () which enables machine learning and covariates to be incorporated into sample robust optimization and Wasserstein-based distributionally robust optimization for the first time.\"","":""}
{"id":"2961578905","dialogue":"\"We present a data-driven framework for incorporating side information in dynamic optimization under uncertainty. Specifically, our approach uses predictive machine learning methods (such as k-nearest neighbors, kernel regression, and random forests) to weight the relative importance of various data-driven uncertainty sets in a robust optimization formulation. Through a novel measure concentration result for local machine learning methods, we prove that the proposed framework is asymptotically optimal for stochastic dynamic optimization with covariates. We also describe a general-purpose approximation for the proposed framework, based on overlapping linear decision rules, which is computationally tractable and produces high-quality solutions for dynamic problems with many stages. Across a variety of examples in shipment planning, inventory management, and finance, our method achieves improvements of up to 15 over alternatives and requires less than one minute of computation time on problems with twelve stages.\"","summary":"\"Several recent papers have focused on tractable approximations of two- and multi-stage and robust optimization. Many approaches are based around policy approximation schemes, including lifted linear decision rules , @math -adaptivity , and finite adaptability . Alternative approaches include tractable approximations of copositive formulations . Closest related to the approximation scheme in this paper are @cite_1 and @cite_13 , which address two-stage problems via overlapping decision rules. @cite_1 propose a modeling approach that leads to novel approximations of various distributionally robust applications, including two-stage distributionally robust optimization using Wasserstein ambiguity sets and expectations of piecewise convex objective functions in single-stage problems. Independently, @cite_13 investigate a of two-stage sample robust optimization by optimizing a separate linear decision rule for each uncertainty set and prove that this approximation gap converges to zero as the amount of data goes to infinity. In of this paper, we show how to extend similar techniques to dynamic problems with many stages for the first time.\"","":""}
{"id":"2961578905","dialogue":"\"We present a data-driven framework for incorporating side information in dynamic optimization under uncertainty. Specifically, our approach uses predictive machine learning methods (such as k-nearest neighbors, kernel regression, and random forests) to weight the relative importance of various data-driven uncertainty sets in a robust optimization formulation. Through a novel measure concentration result for local machine learning methods, we prove that the proposed framework is asymptotically optimal for stochastic dynamic optimization with covariates. We also describe a general-purpose approximation for the proposed framework, based on overlapping linear decision rules, which is computationally tractable and produces high-quality solutions for dynamic problems with many stages. Across a variety of examples in shipment planning, inventory management, and finance, our method achieves improvements of up to 15 over alternatives and requires less than one minute of computation time on problems with twelve stages.\"","summary":"\"As discussed previously, the methodology in this paper also follows recent work on incorporating covariates in optimization under uncertainty using local predictive methods (such as @math -nearest neighbor regression, kernel regression, and random forests). In particular, the asymptotic optimality justification of @cite_11 in single-stage settings relies on the strong universal consistency for local predictive models (, @cite_6 ). Our proof of asymptotic optimality instead relies on convergence guarantees rooted in distributionally robust optimization. The reason we use a different approach is that the arguments for the convergence for local predictive models from @cite_11 require finite dimensional decision variables. In contrast, the convergence guarantees in this paper apply for dynamic optimization over general spaces of policies.\"","":""}
{"id":"2911027578","dialogue":"\"A key property underlying the success of evolutionary algorithms (EAs) is their global search behavior, which allows the algorithms to jump' from a current state to other parts of the search space, thereby avoiding to get stuck in local optima. This property is obtained through a random choice of the radius at which offspring are sampled from previously evaluated solutions. It is well known that, thanks to this global search behavior, the probability that an EA using standard bit mutation finds a global optimum of an arbitrary function @math tends to one as the number of function evaluations grows. This advantage over heuristics using a fixed search radius, however, comes at the cost of using non-optimal step sizes also in those regimes in which the optimal rate is stable for a long time. This downside results in significant performance losses for many standard benchmark problems. We introduce in this work a simple way to interpolate between the random global search of EAs and their deterministic counterparts which sample from a fixed radius only. To this end, we introduce , in which the binomial choice of the search radius is replaced by a normal distribution. Normalized standard bit mutation allows a straightforward way to control its variance, and hence the degree of randomness involved. We experiment with a self-adjusting choice of this variance, and demonstrate its effectiveness for the two classic benchmark problems LeadingOnes and OneMax. Our work thereby also touches a largely ignored question in discrete evolutionary computation: multi-dimensional parameter control.\"","summary":"\"As reasoned above, normalized standard bit mutation offers an elegant way to interpolate between deterministic mutation strengths and regular standard bit mutation, thus showing that Randomized Local Search (RLS) variants with their deterministic search radii and the (1+1) EA with mutation rate @math are essentially just different instantiations of the same meta-algorithm. Similar results also extend to population-based @math EAs. Note that normalized standard bit mutation also allows other degrees of randomization, thereby offering a wide range for further experimentation. In this context we note that for the special case of standard RLS (i.e., the greedy (1+1) hill climber that flips in each iteration exactly one uniformly chosen bit) a similar meta-model allowing to interpolate between the (1+1) EA and RLS is the (1+1) EA @math introduced in @cite_2 @cite_5 . This model, however, is much less flexible, and does not allow, for example, deterministic search radii greater than one.\"","":""}
{"id":"2960433490","dialogue":"\"Recently","summary":"neural networks trained as optimizers under the \"\"learning to learn\"\" or meta-learning framework have been shown to be effective for a broad range of optimization tasks including derivative-free black-box function optimization. Recurrent neural networks (RNNs) trained to optimize a diverse set of synthetic non-convex differentiable functions via gradient descent have been effective at optimizing derivative-free black-box functions. In this work","":""}
{"id":"2960433490","dialogue":"\"Recently","summary":"neural networks trained as optimizers under the \"\"learning to learn\"\" or meta-learning framework have been shown to be effective for a broad range of optimization tasks including derivative-free black-box function optimization. Recurrent neural networks (RNNs) trained to optimize a diverse set of synthetic non-convex differentiable functions via gradient descent have been effective at optimizing derivative-free black-box functions. In this work","":""}
{"id":"2911003065","dialogue":"\"MAPF is the problem of finding paths for multiple agents such that every agent reaches its goal and the agents do not collide. Most prior work on MAPF were on grid, assumed all actions cost the same, agents do not have a volume, and considered discrete time steps. In this work we propose a MAPF algorithm that do not assume any of these assumptions, is complete, and provides provably optimal solutions. This algorithm is based on a novel combination of SIPP, a continuous time single agent planning algorithms, and CBS, a state of the art multi-agent pathfinding algorithm. We analyze this algorithm, discuss its pros and cons, and evaluate it experimentally on several standard benchmarks.\"","summary":"\"dRRT* is a MAPF algorithm designed for continuous spaces @cite_25 . It is a sample-based technique that is asymptotically complete and optimal. is optimal and complete, and is designed to run over a discrete graph. ORCA @cite_19 @cite_27 and ALAN @cite_18 are also MAPF algorithms designed for continuous space. They are fast and distributed, but do not provide optimality or completeness guarantees.\"","":""}
{"id":"2963550527","dialogue":"\"Recent research on face detection, which is focused primarily on improving accuracy of detecting smaller faces, attempt to develop new anchor design strategies to facilitate increased overlap between anchor boxes and ground truth faces of smaller sizes. In this work, we approach the problem of small face detection with the motivation of enriching the feature maps using a density map estimation module. This module, inspired by recent crowd counting density estimation techniques, performs the task of estimating the per pixel density of people faces present in the image. Output of this module is employed to accentuate the feature maps from the backbone network using a feature enrichment module before being used for detecting smaller faces. The proposed approach can be used to complement recent anchor-design based novel methods to further improve their results. Experiments conducted on different datasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of the proposed approach.\"","summary":"\"Zhang al @cite_0 proposed a single image-based method that involved multi-column network to extract features at different scales. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people head size due to perspective effect or image resolution. Onoro-Rubio and L 'o pez-Sastre in @cite_62 addressed the scale issue by proposing a scale aware counting model called Hydra CNN to estimate the object density maps. Sam al @cite_16 trained a Switching-CNN network to automatically choose the most optimal regressor among several independent regressors for a particular input patch. More recently, Sindagi and Patel @cite_3 proposed Contextual Pyramid CNN (CP-CNN), where they demonstrated significant improvements by fusing local and global context through classification networks.\"","":""}
{"id":"2957156153","dialogue":"\"We provide recovery guarantees for compressible signals that have been corrupted with noise and extend the framework introduced in [1] to defend neural networks against @math -norm and @math -norm attacks. Concretely, for a signal that is approximately sparse in some transform domain and has been perturbed with noise, we provide guarantees for accurately recovering the signal in the transform domain. We can then use the recovered signal to reconstruct the signal in its original domain while largely removing the noise. Our results are general as they can be directly applied to most unitary transforms used in practice and hold for both @math -norm bounded noise and @math -norm bounded noise. In the case of @math -norm bounded noise, we prove recovery guarantees for Iterative Hard Thresholding (IHT) and Basis Pursuit (BP). For the case of @math -norm bounded noise, we provide recovery guarantees for BP. These guarantees theoretically bolster the defense framework introduced in [1] for defending neural networks against adversarial inputs. Finally, we experimentally demonstrate this defense framework using both IHT and BP against the One Pixel Attack [21], Carlini-Wagner @math and @math attacks [3], Jacobian Saliency Based attack [18], and the DeepFool attack [17] on CIFAR-10 [12], MNIST [13], and Fashion-MNIST [27] datasets. This expands beyond the experimental demonstrations of [1].\"","summary":"\"The authors of @cite_0 introduced the CRD framework which inspired this work. The main theorem (Theorem 2.2) of @cite_0 is an analog of our Theorem and provides a similar bound the approximation error for recovery via IHT. First note that the statement of the Theorem 2.2 of @cite_0 is missing the required hypothesis @math . This hypothesis appears in Lemma 3.6 of @cite_0 , which is used to prove Theorem 2.2, but it appears to have been accidentally dropped from the statement of Theorem 2.2. We note that, by making the constants explicit, the proof of Lemma 3.6 of @cite_0 gives the same restricted isometry property that we do in Theorem . Therefore, the guarantees we obtain for IHT are essentially the same as in @cite_0 . The main difference is that, to derive recovery guarantees for IHT from the restricted isometry property, we utilize Theorem below (which is a modified version of Theorem 6.18 of @cite_15 ) while the authors of @cite_0 utilize Theorem 3.4 in @cite_0 (which is taken from @cite_19 ).\"","":""}
{"id":"2957156153","dialogue":"\"We provide recovery guarantees for compressible signals that have been corrupted with noise and extend the framework introduced in [1] to defend neural networks against @math -norm and @math -norm attacks. Concretely, for a signal that is approximately sparse in some transform domain and has been perturbed with noise, we provide guarantees for accurately recovering the signal in the transform domain. We can then use the recovered signal to reconstruct the signal in its original domain while largely removing the noise. Our results are general as they can be directly applied to most unitary transforms used in practice and hold for both @math -norm bounded noise and @math -norm bounded noise. In the case of @math -norm bounded noise, we prove recovery guarantees for Iterative Hard Thresholding (IHT) and Basis Pursuit (BP). For the case of @math -norm bounded noise, we provide recovery guarantees for BP. These guarantees theoretically bolster the defense framework introduced in [1] for defending neural networks against adversarial inputs. Finally, we experimentally demonstrate this defense framework using both IHT and BP against the One Pixel Attack [21], Carlini-Wagner @math and @math attacks [3], Jacobian Saliency Based attack [18], and the DeepFool attack [17] on CIFAR-10 [12], MNIST [13], and Fashion-MNIST [27] datasets. This expands beyond the experimental demonstrations of [1].\"","summary":"\"Other works that provide guarantees include @cite_1 and @cite_20 where the authors frame the problem as one of regularizing the Lipschitz constant of a network and provide a lower bound on the norm of the perturbation required to change the classifier decision. The authors of @cite_13 use robust optimization to perturb the training data and provide a training procedure that updates parameters based on worst case perturbations. A similar approach to @cite_13 is @cite_12 in which the authors use robust optimization to provide lower bounds on the norm of adversarial perturbations on the training data. In @cite_26 , the authors use techniques from Differential Privacy @cite_25 in order to augment the training procedure of the classifier to improve robustness to adversarial inputs. Another approach using randomization is @cite_10 in which the authors add i.i.d Gaussian noise to the input and provide guarantees of maintaining classifier predictions as long as the @math -norm of the attack vector is bounded by a function that depends on the output of the classifier.\"","":""}
{"id":"2957156153","dialogue":"\"We provide recovery guarantees for compressible signals that have been corrupted with noise and extend the framework introduced in [1] to defend neural networks against @math -norm and @math -norm attacks. Concretely, for a signal that is approximately sparse in some transform domain and has been perturbed with noise, we provide guarantees for accurately recovering the signal in the transform domain. We can then use the recovered signal to reconstruct the signal in its original domain while largely removing the noise. Our results are general as they can be directly applied to most unitary transforms used in practice and hold for both @math -norm bounded noise and @math -norm bounded noise. In the case of @math -norm bounded noise, we prove recovery guarantees for Iterative Hard Thresholding (IHT) and Basis Pursuit (BP). For the case of @math -norm bounded noise, we provide recovery guarantees for BP. These guarantees theoretically bolster the defense framework introduced in [1] for defending neural networks against adversarial inputs. Finally, we experimentally demonstrate this defense framework using both IHT and BP against the One Pixel Attack [21], Carlini-Wagner @math and @math attacks [3], Jacobian Saliency Based attack [18], and the DeepFool attack [17] on CIFAR-10 [12], MNIST [13], and Fashion-MNIST [27] datasets. This expands beyond the experimental demonstrations of [1].\"","summary":"\"Most defenses against adversarial inputs do not come with theoretical guarantees. Instead, a large body of research has focused on finding practical ways to improve robustness to adversarial inputs by either augmenting the training data @cite_22 , using adversarial inputs from various networks @cite_28 , or by reducing the dimensionality of the input @cite_14 . For instance, @cite_18 use robust optimization to make the network robust to worst case adversarial perturbations on the training data. However, the effectiveness of their approach is determined by the amount and quality of training data available and its similarity to the distribution of the test data. An approach similar to ours but without any theoretical guarantees is @cite_21 . In this work, the authors use Generative Adversarial Networks (GANs) to estimate the distribution of the training data and during inference, use a GAN to reconstruct an input that is most similar to a given test input and is not adversarial.\"","":""}
{"id":"2972864268","dialogue":"\"Modeling error or external disturbances can severely degrade the performance of Model Predictive Control (MPC) in real-world scenarios. Robust MPC (RMPC) addresses this limitation by optimizing over feedback policies but at the expense of increased computational complexity. Tube MPC is an approximate solution strategy in which a robust controller, designed offline, keeps the system in an invariant tube around a desired nominal trajectory, generated online. Naturally, this decomposition is suboptimal, especially for systems with changing objectives or operating conditions. In addition, many tube MPC approaches are unable to capture state-dependent uncertainty due to the complexity of calculating invariant tubes, resulting in overly-conservative approximations. This work presents the Dynamic Tube MPC (DTMPC) framework for nonlinear systems where both the tube geometry and open-loop trajectory are optimized simultaneously. By using boundary layer sliding control, the tube geometry can be expressed as a simple relation between control parameters and uncertainty bound; enabling the tube geometry dynamics to be added to the nominal MPC optimization with minimal increase in computational complexity. In addition, DTMPC is able to leverage state-dependent uncertainty to reduce conservativeness and improve optimization feasibility. DTMPC is demonstrated to robustly perform obstacle avoidance and modify the tube geometry in response to obstacle proximity.\"","summary":"\"A number of works have been published on the stability, feasibility, and performance of linear tube MPC @cite_10 @cite_13 @cite_8 . While this is an effective strategy to achieve robustness, decoupling the nominal MPC problem and controller design is suboptimal. Rakovi @cite_17 showed that the region of attraction can be enlarged by parameterizing the problem with the open-loop trajectory and tube size. The authors presented the homothetic tube MPC (HTMPC) algorithm that treated the state and control tubes as homothetic copies of a fixed cross-section shape, enabling the problem to be parameterized by the tube’s centers (i.e., open-loop trajectory) and a cross-section scaling factor. The work was extended to tubes with varying shapes, known as elastic tube MPC (ETMPC), but at the expense of computational complexity @cite_12 . Both HTMPC and ETMPC possess strong theoretical properties and have the potential to significantly improve performance but a nonlinear extension has yet to be developed.\"","":""}
{"id":"2909816737","dialogue":"\"This work proposes an Application-Specific System Processor (ASSP) hardware for the Secure Hash Algorithm 1 (SHA-1) algorithm. The proposed hardware was implemented in a Field Programmable Gate Array (FPGA) Xilinx Virtex 6 xc6vlx240t-1ff1156. The throughput and the occupied area were analyzed for several implementations in parallel instances of the hash algorithm. The results showed that the hardware proposed for the SHA-1 achieved a throughput of 0.644 Gbps for a single instance and slightly more than 28 Gbps for 48 instances in a single FPGA. Various applications such as password recovery, password validation, and high volume data integrity checking can be performed efficiently and quickly with an ASSP for SHA1.\"","summary":"Works with SHA-1 implementation on other hardware platforms can be found in @cite_5 and @cite_6 in which comparisons between Graphics Processing Units (GPUs) and CPUs were performed. The GPUs NVIDIA Tesla M2050 with @math CUDA cores and AMD FirePro V7800 with @math stream processors could achieve throughput peaks of up to @math Gbps.","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910298035","dialogue":"\"An @math -vector MDS code is a @math -linear subspace of @math (for some field @math ) of dimension @math","summary":"such that any @math (vector) symbols of the codeword suffice to determine the remaining @math (vector) symbols. The length @math of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR)","":""}
{"id":"2910420318","dialogue":"We report FPGA implementation results of low precision CNN convolution layers optimized for sparse and constant parameters. We describe techniques that amortizes the cost of common factor multiplication and automatically leverage dense hand tuned LUT structures. We apply this method to corner case residual blocks of Resnet on a sparse Resnet50 model to assess achievable utilization and frequency and demonstrate an effective performance of 131 and 23 TOP chip for the corner case blocks. The projected performance on a multichip persistent implementation of all Resnet50 convolution layers is 10k im s chip at batch size 2. This is 1.37x higher than V100 GPU upper bound at the same batch size after normalizing for sparsity.","summary":"\"In recent years, Convolutional Neural Networks(CNN) have demonstrated great efficacy on computer vision tasks such as classification @cite_8 , localization @cite_10 , and SRGAN @cite_6 . Together with Recurrent Neural Networks(RNN), it has motivated the development of custom silicon for Deep Learning(DL). For example, GPU Tensor Core @cite_1 , TPU @cite_2 and Graphcore @cite_4 .\"","":""}
{"id":"2910420318","dialogue":"We report FPGA implementation results of low precision CNN convolution layers optimized for sparse and constant parameters. We describe techniques that amortizes the cost of common factor multiplication and automatically leverage dense hand tuned LUT structures. We apply this method to corner case residual blocks of Resnet on a sparse Resnet50 model to assess achievable utilization and frequency and demonstrate an effective performance of 131 and 23 TOP chip for the corner case blocks. The projected performance on a multichip persistent implementation of all Resnet50 convolution layers is 10k im s chip at batch size 2. This is 1.37x higher than V100 GPU upper bound at the same batch size after normalizing for sparsity.","summary":"\"There has also been work to optimize DL on programmable logic. Notably, Song @cite_3 proposed software-hardware co-design. While silicon implementations must customize for a range of DL applications, an FPGA can customize to a single DL application. This enables application specific customization of precision, sparsity and network structure. However, this is not the limit of FPGA customization. FPGAs can be further customized to a specific instance of a DL application by implementing post training parameters as constants. We call this a Compiled CNN or RNN.\"","":""}
{"id":"2964234547","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Recently, there have been malware attacks on IoT devices, the most prominent one being that of Mirai. IoT devices such as IP cameras, DVRs and routers were compromised by the Mirai malware and later large-scale DDoS attacks were propagated using those infected devices (bots) in October 2016. In this research, we develop a network-based algorithm which can be used to detect IoT bots infected by Mirai or similar malware in large-scale networks (e.g. ISP network). The algorithm particularly targets bots scanning the network for vulnerable devices since the typical scanning phase for botnets lasts for months and the bots can be detected much before they are involved in an actual attack. We analyze the unique signatures of the Mirai malware to identify its presence in an IoT device. Further, to optimize the usage of computational resources, we use a two-dimensional (2D) packet sampling approach, wherein we sample the packets transmitted by IoT devices both across time and across the devices. Leveraging the Mirai signatures identified and the 2D packet sampling approach, a bot detection algorithm is proposed. We use testbed measurements and simulations to study the relationship between bot detection delays and the sampling frequencies for device packets. Subsequently, we derive insights from the obtained results and use them to design our proposed bot detection algorithm. Finally, we discuss the deployment of our bot detection algorithm and the countermeasures which can be taken post detection.\"","summary":"\"There has also been some research on intrusion detection and anomaly detection systems for IoT. A whitelist-based intrusion detection system for IoT devices (Heimdall) has been presented in @cite_1 . Heimdall is based on dynamic profile learning and is designed to work on routers acting as gateways for IoT devices. The authors in @cite_2 propose an intrusion detection model for IoT backbone networks leveraging two-layer dimension reduction and two-tier classification techniques to detect U2R (User-to-Root) and R2L (Remote-to-Local) attacks. In a recently published paper @cite_11 , deep-autoencoders based anomaly detection has been used to detect attacks launched from IoT botnets. The method consists of extraction of statistical features from behavioral snapshots of normal IoT device traffic captures, training of a deep learning-based autoencoder (for each IoT device) on the extracted features and comparison of the reconstruction error for traffic observations with a threshold for normal-anomalous classification. The proposed detection method was evaluated on Mirai and BASHLITE botnets formed using commercial IoT devices.\"","":""}
{"id":"2964234547","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Recently, there have been malware attacks on IoT devices, the most prominent one being that of Mirai. IoT devices such as IP cameras, DVRs and routers were compromised by the Mirai malware and later large-scale DDoS attacks were propagated using those infected devices (bots) in October 2016. In this research, we develop a network-based algorithm which can be used to detect IoT bots infected by Mirai or similar malware in large-scale networks (e.g. ISP network). The algorithm particularly targets bots scanning the network for vulnerable devices since the typical scanning phase for botnets lasts for months and the bots can be detected much before they are involved in an actual attack. We analyze the unique signatures of the Mirai malware to identify its presence in an IoT device. Further, to optimize the usage of computational resources, we use a two-dimensional (2D) packet sampling approach, wherein we sample the packets transmitted by IoT devices both across time and across the devices. Leveraging the Mirai signatures identified and the 2D packet sampling approach, a bot detection algorithm is proposed. We use testbed measurements and simulations to study the relationship between bot detection delays and the sampling frequencies for device packets. Subsequently, we derive insights from the obtained results and use them to design our proposed bot detection algorithm. Finally, we discuss the deployment of our bot detection algorithm and the countermeasures which can be taken post detection.\"","summary":"\"Fourth, we do not extract CnC communication features and use them to identify bot-CnC communications as done in @cite_10 @cite_24 @cite_29 . This is because we aim to detect bots infected by Mirai-like IoT malware, towards which much simpler features can be used as discussed in Section . Fifth, unlike @cite_11 , we aim to detect IoT bots much before the actual attack, during the scanning phase itself as explained in Section . Finally, most of the above cited works use quantifiers such as detection rate and false positive rates to evaluate the performance of their proposed botnet detection solutions. Instead, we use a quantity called average detection delay (defined in Section ) for the performance evaluation of our proposed bot detection solution since the features used by our solution eliminate the possibility of inaccurate detections or false positives. To the best of our knowledge, there are no existing papers on detecting IoT bots compromised by Mirai or its variants which exhibit port-based SYN scanning behavior.\"","":""}
{"id":"2909698198","dialogue":"\"We compare a novel Knowledge-based Reinforcement Learning (KB-RL) approach with the traditional Neural Network (NN) method in solving a classical task of the Artificial Intelligence (AI) field. Neural networks became very prominent in recent years and, combined with Reinforcement Learning, proved to be very effective for one of the frontier challenges in AI - playing the game of Go. Our experiment shows that a KB-RL system is able to outperform a NN in a task typical for NN, such as optimizing a regression problem. Furthermore, KB-RL offers a range of advantages in comparison to the traditional Machine Learning methods. Particularly, there is no need for a large dataset to start and succeed with this approach, its learning process takes considerably less effort, and its decisions are fully controllable, explicit and predictable.\"","summary":"\"As the result of NNs' popularity, little attention has been paid to other AI approaches, such as symbolism, evolutionarism, or Bayesian statistics @cite_0 . More recently, though, new studies emerge that show successful results in applying alternative approaches to AI tasks. For example, Denis G Willson at el. show that their evolutionary algorithm can outperform the deep neural network approach in playing Atari games @cite_2 . More studies are targeting General AI as, for instance, the CYC project @cite_19 . Some researchers advocate that the combination of different techniques into one powerful AI system is the way to go @cite_8 .\"","":""}
{"id":"2962163524","dialogue":"\"In common real-world robotic operations, action and state spaces can be vast and sometimes unknown, and observations are often relatively sparse. How do we learn the full topology of action and state spaces when given only few and sparse observations? Inspired by the properties of grid cells in mammalian brains, we build a generative model that enforces a normalized pairwise distance constraint between the latent space and output space to achieve data-efficient discovery of output spaces. This method achieves substantially better results than prior generative models, such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs). Prior models have the common issue of mode collapse and thus fail to explore the full topology of output space. We demonstrate the effectiveness of our model on various datasets both qualitatively and quantitatively.\"","summary":"\"Recent works have made substantial progress in imposing diversity constraint on the latent space of a generative model. In particular, Liu et. al. @cite_49 proposes the normalized diversification technique that effectively solves the problem of mode collapsing. Building on top of their prior work, we use a similar technique to learn an accurate encoding of action and state spaces in physical manipulation tasks. To our knowledge, our model is the first to use normalized diversification in these applications.\"","":""}
{"id":"2963432486","dialogue":"\"Due to the high resolution of pathological images, the automated semantic segmentation in the medical pathological images has shown greater challenges than that in natural images. Sliding Window method has shown its effect on solving problem caused by the high resolution of whole slide images (WSI). However, owing to its localization, Sliding Window method also suffers from lack of global information. In this paper, a dual input semantic segmentation network based on attention is proposed, in which, one input provides small-scale fine information, the other input provides large-scale coarse information. Compared with single input methods, our method based on dual inputs and attention: DA-RefineNet exhibits a dramatic performance improvement on ICIAR2018 breast cancer segmentation task.\"","summary":"\"In this paper we adopt Refinenet @cite_14 as the baseline. The main difference between Refinenet and Unet @cite_26 lies in the unique block \"\"Refine Block\"\". The Refine Block is a unique feature fusion block","":""}
{"id":"2957959342","dialogue":"\"We propose a method to learn unsupervised sentence representations in a non-compositional manner based on Generative Latent Optimization. Our approach does not impose any assumptions on how words are to be combined into a sentence representation. We discuss a simple Bag of Words model as well as a variant that models word positions. Both are trained to reconstruct the sentence based on a latent code and our model can be used to generate text. Experiments show large improvements over the related Paragraph Vectors. Compared to uSIF, we achieve a relative improvement of 5 when trained on the same data and our method performs competitively to Sent2vec while trained on 30 times less data.\"","summary":"\"Methods requiring labels generally use less training data as they can be more data efficient due to the better training signal that can obtained from labeled data. Examples include: InferSent which uses labelled entailment pairs, GenSen utilizing supervision from multiple tasks, and ParaNMT with paraphrase sentence pairs or conversational responses @cite_1 .\"","":""}
{"id":"2960316954","dialogue":"\"In this paper, we propose an end to end joint radio and virtual network function (VNF) resource allocation for next-generation networks providing different types of services with different requirements in term of latency and data rate. We consider both the access and core parts of the network and formulate a novel optimization problem whose aim is to perform the radio resource allocation jointly with VNF embedding, scheduling, and resource allocation such that the network cost, defined as the consumed energy and the number of utilized network servers, is minimized. The proposed optimization problem is non-convex, NP-hard, and mathematically intractable, and hence, we use an alternative search method (ASM) to decouple the main problem into some sub-problems of lower complexity. We propose a novel heuristic algorithm for embedding and scheduling of VNFs by proposing a novel admission control (AC) algorithm. We compare the performance of the proposed algorithm with a greedy-based solution in terms of the acceptance ratio and the number of active servers. Our simulation results show that the proposed algorithm outperforms the conventional ones.\"","summary":"\"T he fifth generation of wireless cellular networks (5G) provides a wide range of services black with various requirements that should be guaranteed in the network @cite_3 @cite_7 . In the traditional network, dedicated and specific hardware equipment are required. Therefore, in order to provide a new service in these networks, it is necessary for each operator to purchase the hardware resources and install it on the network @cite_25 .\"","":""}
{"id":"2960316954","dialogue":"\"In this paper, we propose an end to end joint radio and virtual network function (VNF) resource allocation for next-generation networks providing different types of services with different requirements in term of latency and data rate. We consider both the access and core parts of the network and formulate a novel optimization problem whose aim is to perform the radio resource allocation jointly with VNF embedding, scheduling, and resource allocation such that the network cost, defined as the consumed energy and the number of utilized network servers, is minimized. The proposed optimization problem is non-convex, NP-hard, and mathematically intractable, and hence, we use an alternative search method (ASM) to decouple the main problem into some sub-problems of lower complexity. We propose a novel heuristic algorithm for embedding and scheduling of VNFs by proposing a novel admission control (AC) algorithm. We compare the performance of the proposed algorithm with a greedy-based solution in terms of the acceptance ratio and the number of active servers. Our simulation results show that the proposed algorithm outperforms the conventional ones.\"","summary":"\"A dynamic service function chain deployment is proposed in @cite_13 in which the authors consider a trade-off between resource consumption and operational overhead. In @cite_22 , NF placement in the network is studied. Moreover, its impact on network performance with the aim of minimizing the cost of having virtual machines (VMs) In this paper, VM, node, and server have the same meaning. and the cost of steering traffic into servers are investigated. Service function chain (SFC) placement in the cloud-based network with the aim of minimizing end-to-end (E2E) latency of SFCs and enhancing QoS is investigated in @cite_30 . An automated decentralized method for online placement and optimization of VMs in NFV-based network is proposed in 8501940 . In @cite_8 , VNF embedding with the aim of minimizing physical machine and taking into consideration users' SFC requests and factors such as basic resource consumption and time-varying workload is studied.\"","":""}
{"id":"2960316954","dialogue":"\"In this paper, we propose an end to end joint radio and virtual network function (VNF) resource allocation for next-generation networks providing different types of services with different requirements in term of latency and data rate. We consider both the access and core parts of the network and formulate a novel optimization problem whose aim is to perform the radio resource allocation jointly with VNF embedding, scheduling, and resource allocation such that the network cost, defined as the consumed energy and the number of utilized network servers, is minimized. The proposed optimization problem is non-convex, NP-hard, and mathematically intractable, and hence, we use an alternative search method (ASM) to decouple the main problem into some sub-problems of lower complexity. We propose a novel heuristic algorithm for embedding and scheduling of VNFs by proposing a novel admission control (AC) algorithm. We compare the performance of the proposed algorithm with a greedy-based solution in terms of the acceptance ratio and the number of active servers. Our simulation results show that the proposed algorithm outperforms the conventional ones.\"","summary":"\"In @cite_3 , an online scheduling and embedding algorithm by considering the capacity of available buffers and the processing time of each VNF is proposed for NFV. The authors propose a set of greedy algorithms and tabu search algorithm for mapping and scheduling. Moreover, cost, revenue, and acceptance ratio of these algorithms are compared together. VNF placement in a network with several mobile virtual network operators (MVNOs) are investigated in @cite_16 in which the slice scheduling mechanism is introduced in order to isolate the traffic flow of MVNOs. In this paper, the goal is optimizing VNF placement based on the available radio resources. black Joint VNF placement and admission control (AC) are studied in @cite_24 . In this paper, the aim is to maximize networks provider revenue in terms of bandwidth and capacity. black The authors in @cite_0 , propose an RA algorithm which integrates placement and scheduling of VNF together. In @cite_15 , a VNF scheduling problem is investigated and joint VNF scheduling and traffic steering is formulated as a mixed integer linear program. In the optimization problem, both the processing latency of VNFs and service chain transmission latency at virtual links are considered.\"","":""}
{"id":"2957586579","dialogue":"\"With social media becoming increasingly pop-ular on which lots of news and real-time eventsare reported, developing automated questionanswering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets haveconcentrated on question answering (QA) forformal text like news and Wikipedia, wepresent the first large-scale dataset for QA oversocial media data. To ensure that the tweetswe collected are useful, we only gather tweetsused by journalists to write news articles. Wethen ask human annotators to write questionsand answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answersare extractive, we allow the answers to be ab-stractive. We show that two recently proposedneural models that perform well on formaltexts are limited in their performance when ap-plied to our dataset. In addition, even the fine-tuned BERT model is still lagging behind hu-man performance with a large margin. Our re-sults thus point to the need of improved QAsystems targeting social media text.\"","summary":"\"Traditional core NLP research typically focuses on English newswire datasets such as the Penn Treebank @cite_12 . In recent years, with the increasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed. For example, build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets. annotated 800 tweets, and performed an empirical study for part-of-speech tagging and chunking on a new Twitter dataset. They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets. annotated 929 tweets, and built the first dependency parser for tweets, whereas built the Chinese counterpart based on 1,000 annotated Weibo posts. To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets.\"","":""}
{"id":"2956802405","dialogue":"\"Subspace segmentation or subspace learning is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. Existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via self-expressiveness of samples. A closed form of representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is then addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, taking into account of solving both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data, and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method on retrieving minimal sample subspaces that are heavily intersected.\"","summary":"\"As shown in Theorem , the LRR gets the MSDR if and only if the subspaces are independent, that is, @math . This condition implies that each subspace does not intersect with the sum of other subspaces, or equivalently, @math , which is much stricter than that given in Theorem . It is proven by @cite_2 that the iPursuit can separate two subspaces ( @math ) with a high probability. This is one of the special cases shown in Corollary . The condition for LRSSC is similar with that of SSC in the same form and is stricter. We omit the comparison the condition with that of LRSSC, but a detailed comparison with SSC is given below.\"","":""}
{"id":"2956802405","dialogue":"\"Subspace segmentation or subspace learning is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. Existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via self-expressiveness of samples. A closed form of representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is then addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, taking into account of solving both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data, and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method on retrieving minimal sample subspaces that are heavily intersected.\"","summary":"\"For the SSC, @cite_15 showed that if the samples are uniformly distributed in a union of subspaces @math and for the basis matrices @math of @math , @math with where @math is a give parameter, then SSC could give a block-diagonal solution partitioned as the ideal subspace segmentation with a probability approximately equal to one, depending on @math , @math , @math , and @math . We remark this claim does not imply a connected solution as we have explained in the early discussion or mentioned by @cite_3 .\"","":""}
{"id":"2959959234","dialogue":"\"We present a novel method for motion segmentation called LAAV (Locally Affine Atom Voting). Our model's main novelty is using sets of features to segment motion for all features in the scene. LAAV acts as a pre-processing pipeline stage for features in the image, followed by a fine-tuned version of the state-of-the-art Random Voting (RV) method. Unlike standard approaches, LAAV segments motion using feature-set affinities instead of pair-wise affinities between all features; therefore, it significantly simplifies complex scenarios and reduces the computational cost without a loss of accuracy. We describe how the challenges encountered by using previously suggested approaches are addressed using our model. We then compare our algorithm with several state-of-the-art methods. Experiments shows that our approach achieves the most accurate motion segmentation results and, in the presence of measurement noise, achieves comparable results to the other algorithms.\"","summary":"\"The Two-view approach is derived merely from the relative camera poses from multiple views, called relative-pose constraints, without any additional assumptions of the scene. The epipolar constraint is such a constraint between two views @cite_18 .\"","":""}
{"id":"2959959234","dialogue":"\"We present a novel method for motion segmentation called LAAV (Locally Affine Atom Voting). Our model's main novelty is using sets of features to segment motion for all features in the scene. LAAV acts as a pre-processing pipeline stage for features in the image, followed by a fine-tuned version of the state-of-the-art Random Voting (RV) method. Unlike standard approaches, LAAV segments motion using feature-set affinities instead of pair-wise affinities between all features; therefore, it significantly simplifies complex scenarios and reduces the computational cost without a loss of accuracy. We describe how the challenges encountered by using previously suggested approaches are addressed using our model. We then compare our algorithm with several state-of-the-art methods. Experiments shows that our approach achieves the most accurate motion segmentation results and, in the presence of measurement noise, achieves comparable results to the other algorithms.\"","summary":"\"Random Voting (RV) @cite_31 , which is considered as the leading geometric method for motion segmentation partly because of its robustness to noise, has shown particularly successful results with a low computational cost. The algorithm, based on epipolar geometry, is an iterative process of randomized feature selection between two frames, estimating a fundamental matrix from the selected features and vote scores for the rest of the remaining features to be associated with a certain motion model. Since the method uses random initialization, it never loses any information even when the selected features do not represent a motion model. However, this approach only works well when the independent moving object is big enough, such that it consists of enough features to properly estimate the object's motion. In addition, objects in the scene need to be in a certain size so that the background object features ratio is not too high in order for the object's features to be selected in the randomized features selection. Finally, its accuracy rate results can vary due to the random initialization.\"","":""}
{"id":"2959959234","dialogue":"\"We present a novel method for motion segmentation called LAAV (Locally Affine Atom Voting). Our model's main novelty is using sets of features to segment motion for all features in the scene. LAAV acts as a pre-processing pipeline stage for features in the image, followed by a fine-tuned version of the state-of-the-art Random Voting (RV) method. Unlike standard approaches, LAAV segments motion using feature-set affinities instead of pair-wise affinities between all features; therefore, it significantly simplifies complex scenarios and reduces the computational cost without a loss of accuracy. We describe how the challenges encountered by using previously suggested approaches are addressed using our model. We then compare our algorithm with several state-of-the-art methods. Experiments shows that our approach achieves the most accurate motion segmentation results and, in the presence of measurement noise, achieves comparable results to the other algorithms.\"","summary":"\"The Multiview approach utilizes the trajectory of the feature points. PAC @cite_23 and SSC @cite_17 methods have quite accurate results in multiple motion cases in a sequence and are also robust to noise. However, those algorithms are extremely slow. Latent low-rank representation-based method (LatLRR) @cite_20 is faster and more accurate, but this method becomes degraded in extremely noisy environments. The ICLM-based approach @cite_16 is very fast, but has lower accuracy than other state-of-the-art approaches. In addition, while Multiview approaches are more accurate than Two-view approaches, they do not have good performance when there are only a few frames.\"","":""}
{"id":"2956358468","dialogue":"\"We study various discrete nonlinear combinatorial optimization problems in an online learning framework. In the first part, we address the question of whether there are negative results showing that getting a vanishing (or even vanishing approximate) regret is computational hard. We provide a general reduction showing that many (min-max) polynomial time solvable problems not only do not have a vanishing regret, but also no vanishing approximation @math -regret, for some @math (unless @math ). Then, we focus on a particular min-max problem, the min-max version of the vertex cover problem which is solvable in polynomial time in the offline case. The previous reduction proves that there is no @math -regret online algorithm, unless Unique Game is in @math ; we prove a matching upper bound providing an online algorithm based on the online gradient descent method. Then, we turn our attention to online learning algorithms that are based on an offline optimization oracle that, given a set of instances of the problem, is able to compute the optimum static solution. We show that for different nonlinear discrete optimization problems, it is strongly @math -hard to solve the offline optimization oracle, even for problems that can be solved in polynomial time in the static case (e.g. min-max vertex cover, min-max perfect matching, etc.). On the positive side, we present an online algorithm with vanishing regret that is based on the follow the perturbed leader algorithm for a generalized knapsack problem.\"","summary":"\"Online Learning, or Online Convex Optimization, is an active research domain. In this section, we only summarize works which are directly related to ours. We refer the reader to comprehensive books @cite_15 @cite_1 and references therein for a more complete overview. The first no-regret algorithm has been given by . Subsequently, and gave improved algorithms with regret @math where @math is the size of the action space. However, these algorithms have running-time @math which is exponential in the size of the input for many applications, in particular for combinatorial optimization problems. An intriguing question is whether there exists a no-regret online algorithm with running-time polynomial in @math . proved that no such algorithm exists in general settings without any assumption on the structure. Designing online polynomial-time algorithms with approximation and vanishing regret guarantees for combinatorial optimization problems is a major research agenda.\"","":""}
{"id":"2956358468","dialogue":"\"We study various discrete nonlinear combinatorial optimization problems in an online learning framework. In the first part, we address the question of whether there are negative results showing that getting a vanishing (or even vanishing approximate) regret is computational hard. We provide a general reduction showing that many (min-max) polynomial time solvable problems not only do not have a vanishing regret, but also no vanishing approximation @math -regret, for some @math (unless @math ). Then, we focus on a particular min-max problem, the min-max version of the vertex cover problem which is solvable in polynomial time in the offline case. The previous reduction proves that there is no @math -regret online algorithm, unless Unique Game is in @math ; we prove a matching upper bound providing an online algorithm based on the online gradient descent method. Then, we turn our attention to online learning algorithms that are based on an offline optimization oracle that, given a set of instances of the problem, is able to compute the optimum static solution. We show that for different nonlinear discrete optimization problems, it is strongly @math -hard to solve the offline optimization oracle, even for problems that can be solved in polynomial time in the static case (e.g. min-max vertex cover, min-max perfect matching, etc.). On the positive side, we present an online algorithm with vanishing regret that is based on the follow the perturbed leader algorithm for a generalized knapsack problem.\"","summary":"\"In their breakthrough paper, presented the first efficient online algorithm, called (FTPL), for linear objective functions. The strategy consists of adding perturbation to the cumulative gain (payoff) of each action and then selecting the action with the highest perturbed gain. This strategy has been generalized and successfully applied to several settings @cite_4 @cite_9 @cite_7 @cite_20 . Specifically, FTPL and its generalized versions have been used to design efficient online no-regret algorithms with oracles beyond linear settings: to submodular settings @cite_4 and non-convex settings @cite_6 . However, all these approaches require best-response oracles, and as we show in this paper, for several problems such best-response oracles require exponential time computation.\"","":""}
{"id":"2956358468","dialogue":"\"We study various discrete nonlinear combinatorial optimization problems in an online learning framework. In the first part, we address the question of whether there are negative results showing that getting a vanishing (or even vanishing approximate) regret is computational hard. We provide a general reduction showing that many (min-max) polynomial time solvable problems not only do not have a vanishing regret, but also no vanishing approximation @math -regret, for some @math (unless @math ). Then, we focus on a particular min-max problem, the min-max version of the vertex cover problem which is solvable in polynomial time in the offline case. The previous reduction proves that there is no @math -regret online algorithm, unless Unique Game is in @math ; we prove a matching upper bound providing an online algorithm based on the online gradient descent method. Then, we turn our attention to online learning algorithms that are based on an offline optimization oracle that, given a set of instances of the problem, is able to compute the optimum static solution. We show that for different nonlinear discrete optimization problems, it is strongly @math -hard to solve the offline optimization oracle, even for problems that can be solved in polynomial time in the static case (e.g. min-max vertex cover, min-max perfect matching, etc.). On the positive side, we present an online algorithm with vanishing regret that is based on the follow the perturbed leader algorithm for a generalized knapsack problem.\"","summary":"\"Another direction is to design online learning algorithms using (offline polynomial-time) approximation algorithms as oracles. provided an algorithm which is inspired by Zinkevich's algorithm @cite_14 (gradient descent): at every step, the algorithm updates the current solution in the direction of the gradient and project back to the feasible set using an approximation algorithm. They showed that given an @math -approximation algorithm for a optimization problem, after @math prediction rounds (time steps) the online algorithm achieves an @math -regret bound of @math using @math calls to the approximation algorithm per round in average. Later on, gave an algorithm with @math -regret bound of @math using only @math calls to the approximation algorithm per round in average. These algorithms rely crucially on the linearity of the objective functions and it remains an interesting open question to design algorithms for online non-linear optimization problems.\"","":""}
{"id":"2890654010","dialogue":"\"Abstract Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.\"","summary":"\"In computer science, with the constant networking and middleware development, scheduling in distributed processing systems is one of the topics which has gained attention in the last two decades. Casavant and Kuhl @cite_115 present a taxonomy of scheduling in general purpose distributed systems. The classification presented by the authors include local and global, static and dynamic, distributed and non-distributed, cooperative and non-cooperative scheduling, as well as some approaches to solve the problem, such as optimal and sub-optimal, heuristic, and approximate. This presented classification is complete in some sense, and it is still valid nowadays. However the current state of distributed systems indeed demands the addition of new branches in this taxonomy.\"","":""}
{"id":"2890654010","dialogue":"\"Abstract Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.\"","summary":"\"Kwok and Ahmad @cite_141 survey static scheduling algorithms for allocating tasks connected as directed task graphs (DAGs) into multiprocessors. The authors presented a simplified taxonomy for approaches to the problem, as well as the description and classification of @math scheduling algorithms. The DAG scheduling algorithms for multiprocessors have been adapted for scheduling in distributed systems, incorporating intrinsic characteristics of such systems for an enhanced performance. Therefore, Kwok and Ahmad presented static scheduling algorithms for multiprocessors, which are also applicable to distributed systems, and their classification. In this paper we review extensions of those algorithms as well as the their classification by including heterogeneous systems, dynamic scheduling algorithms, scheduling algorithms in modern distributed environments, and new scheduling techniques.\"","":""}
{"id":"2890654010","dialogue":"\"Abstract Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.\"","summary":"\"In the last decades, after Kwok and Ahmad's work, other surveys and taxonomies for solutions to the scheduling problem for parallel systems have been developed. Most of these works focus on heterogeneous distributed systems @cite_159 , which Ahmad and Kwok considered as one of the most challenging directions to follow @cite_141 . Job scheduling strategies for grid computing are evaluated by in @cite_168 . The authors present common scheduling structures, such as centralized, decentralized, and hierarchical. Within each scheduling structure, they present and evaluate @math processor selection strategies and three scheduling algorithms, namely (FCFS), , and . After this work, many dynamic scheduling strategies were developed to tackle with the grid dynamicity.\"","":""}
{"id":"2890654010","dialogue":"\"Abstract Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.\"","summary":"\"As claim in their scheduling review @cite_11 , parallel job scheduling reviews are needed in a regular basis. The purpose of their short review was to introduce clusters and grids into the parallel job scheduling literature. Indeed, the authors present an introduction to job scheduling in grids, highlighting differences between a parallel computer and the grid. They point out cross-domain load balancing and co-allocations as two main concerns when scheduling in grids. In our work, we introduce a classification of schedulers in distributed systems that comprises a more extensive view of grid computing algorithms. Moreover, we highlight new requirements for the cloud computing emergent paradigm as well as its differences to grid computing.\"","":""}
{"id":"2890654010","dialogue":"\"Abstract Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.\"","summary":"\"@cite_75 present a taxonomy in the scheduling problem for workflows considering multiple criteria optimization in grid computing environments. The authors separate the multi-criteria scheduling taxonomy in @math , namely , and , each facet describing the problem from a different point of view. These facets are expanded to classify existing works in a smaller granularity, pointing out where current research can be expanded and the work in each facet.\"","":""}
{"id":"2890654010","dialogue":"\"Abstract Scheduling is essentially a decision-making process that enables resource sharing among a number of activities by determining their execution order on the set of available resources. The emergence of distributed systems brought new challenges on scheduling in computer systems, including clusters, grids, and more recently clouds. On the other hand, the plethora of research makes it hard for both newcomers researchers to understand the relationship among different scheduling problems and strategies proposed in the literature, which hampers the identification of new and relevant research avenues. In this paper we introduce a classification of the scheduling problem in distributed systems by presenting a taxonomy that incorporates recent developments, especially those in cloud computing. We review the scheduling literature to corroborate the taxonomy and analyze the interest in different branches of the proposed taxonomy. Finally, we identify relevant future directions in scheduling for distributed systems.\"","summary":"\"We highlight two conclusions achieved by the authors in @cite_75 which are touched by contributions given by our survey: (i) grid workflow scheduling problem is still not fully addressed by existing work''; and (ii) there are almost no workflow scheduling approaches which are based on an adaptive cost model for criteria''. As a contribution to (i), in this survey we expand the general distributed system scheduling to comprise it. As a contribution to (ii), we include scheduling taxonomies for utility grids and cloud computing environments.\"","":""}
{"id":"2909346791","dialogue":"\"Linear mixed models (LMMs) are widely used for heritability estimation in genome-wide association studies (GWAS). In standard approaches to heritability estimation with LMMs, a genetic relationship matrix (GRM) must be specified. In GWAS, the GRM is frequently a correlation matrix estimated from the study population's genotypes, which corresponds to a normalized Euclidean distance kernel. In this paper, we show that reliance on the Euclidean distance kernel contributes to several unresolved modeling inconsistencies in heritability estimation for GWAS. These inconsistencies can cause biased heritability estimates in the presence of linkage disequilibrium (LD), depending on the distribution of causal variants. We show that these biases can be resolved (at least at the modeling level) if one adopts a Mahalanobis distance-based GRM for LMM analysis. Additionally, we propose a new definition of partitioned heritability -- the heritability attributable to a subset of genes or single nucleotide polymorphisms (SNPs) -- using the Mahalanobis GRM, and show that it inherits many of the nice consistency properties identified in our original analysis. Partitioned heritability is a relatively new area for GWAS analysis, where inconsistency issues related to LD have previously been known to be especially pernicious.\"","summary":"\"Recently, in independent work, @cite_0 proposed using the Mahalanobis kernel in a similar way for heritability esitmation with GWAS data. 's paper primarily focuses on empirical analysis, using both simulated and real datasets to illsutrate advantages of the Mahalanobis kernel. The present work contains more precise mathematical and statistical justification for much of the work in @cite_0 , and introduces statistical principles (e.g. @math -heritability in Section ) that can be extended to other targeted application areas and genetics (like partitioning heritability).\"","":""}
{"id":"2909148692","dialogue":"\"In the past decades, feature-learning-based 3D shape retrieval approaches have been received widespread attention in the computer graphic community. These approaches usually explored the hand-crafted distance metric or conventional distance metric learning methods to compute the similarity of the single feature. The single feature always contains onefold geometric information, which cannot characterize the 3D shapes well. Therefore, the multiple features should be used for the retrieval task to overcome the limitation of single feature and further improve the performance. However, most conventional distance metric learning methods fail to integrate the complementary information from multiple features to construct the distance metric. To address these issue, a novel multi-feature distance metric learning method for non-rigid 3D shape retrieval is presented in this study, which can make full use of the complimentary geometric information from multiple shape features by utilizing the KL-divergences. Minimizing KL-divergence between different metric of features and a common metric is a consistency constraints, which can lead the consistency shared latent feature space of the multiple features. We apply the proposed method to 3D model retrieval, and test our method on well known benchmark database. The results show that our method substantially outperforms the state-of-the-art non-rigid 3D shape retrieval methods.\"","summary":"\"Appropriate similarities between samples can improve the performances of the retrieval system. During the past decade, several well-known distance metric learning methods are proposed for various fields @cite_5 @cite_33 @cite_44 @cite_21 @cite_6 @cite_18 , such as ITML @cite_5 , LMNN @cite_33 , SVMs @cite_44 , PCA @cite_21 , LDA @cite_6 , etc. These algorithms have been used for many computer vision and computer graphic tasks, such as classification, retrieval, correspondence, etc. These algorithms solve the problem that most features lie in a complex high-dimensional spaces where Euclidean metric is ineffective. However, most distance metric learning methods fail to integrate compatible and complementary information from multiple features to construct a distance metric. In order to explore more useful information for various applications, many researchers invest many methods to combine multi-view setting to distance metric learning algorithm. Kan @cite_7 proposed a multi-view discriminant analysis as an extension of LDA, which has achieved excellent performances facing with multi-view features. Wu @cite_31 proposed an online multi-modal distance metric learning which has been successfully applied in image retrieval.\"","":""}
{"id":"2966078298","dialogue":"\"Discovering social relations in images can make machines better interpret the behavior of human beings. However, automatically recognizing social relations in images is a challenging task due to the significant gap between the domains of visual content and social relation. Existing studies separately process various features such as faces expressions, body appearance, and contextual objects, thus they cannot comprehensively capture the multi-granularity semantics, such as scenes, regional cues of persons, and interactions among persons and objects. To bridge the domain gap, we propose a Multi-Granularity Reasoning framework for social relation recognition from images. The global knowledge and mid-level details are learned from the whole scene and the regions of persons and objects, respectively. Most importantly, we explore the fine-granularity pose keypoints of persons to discover the interactions among persons and objects. Specifically, the pose-guided Person-Object Graph and Person-Pose Graph are proposed to model the actions from persons to object and the interactions between paired persons, respectively. Based on the graphs, social relation reasoning is performed by graph convolutional networks. Finally, the global features and reasoned knowledge are integrated as a comprehensive representation for social relation recognition. Extensive experiments on two public datasets show the effectiveness of the proposed framework.\"","summary":"\"The interdisciplinary research of multimedia and sociology has been studied for many years @cite_5 @cite_0 . Popular topics include social networks discovery @cite_25 , key actors detection @cite_1 , group activity recognition @cite_17 , and so on. In recent years, social recognition from images has attracted attention from researchers @cite_20 @cite_3 @cite_27 @cite_30 . For example, Zhang proposed to learn social relation traits from face images by CNNs @cite_13 . Sun proposed a social relation dataset based on the social domain theory @cite_19 and exploited CNNs to recognize social relations from a set of attributes @cite_20 . Li proposed to an attention-based dual-glance model for social relation recognition, in which the first glance extracted features from persons and the second glance focused on contextual cues @cite_2 . Wang proposed to model persons and objects in an image as a graph and perform relation reasoning by a Gated Graph Neural Network @cite_16 . However, they only considered the co-existence of persons and objects in a scene but neglected global information and interactions among persons and objects that are important knowledge for social relation recognition. Therefore, we propose a Multi-Granularity Reasoning framework to explore complementary cues for social relation recognition.\"","":""}
{"id":"2961663897","dialogue":"\"In this work, we present an end-to-end framework to settle data association in online Multiple-Object Tracking (MOT). Given detection responses, we formulate the frame-by-frame data association as Maximum Weighted Bipartite Matching problem, whose solution is learned using a neural network. The network incorporates an affinity learning module, wherein both appearance and motion cues are investigated to encode object feature representation and compute pairwise affinities. Employing the computed affinities as edge weights, the following matching problem on a bipartite graph is resolved by the optimization module, which leverages a graph neural network to adapt with the varying cardinalities of the association problem and solve the combinatorial hardness with favorable scalability and compatibility. To facilitate effective training of the proposed tracking network, we design a multi-level matrix loss in conjunction with the assembled supervision methodology. Being trained end-to-end, all modules in the tracker can co-adapt and co-operate collaboratively, resulting in improved model adaptiveness and less parameter-tuning efforts. Experiment results on the MOT benchmarks demonstrate the efficacy of the proposed approach.\"","summary":"\"Data association is the process of dividing a set of instances into different groups, such that to maximize the global cross-group similarities while maintaining one-to-one association constraint. This fundamental technique exists in various domains that involve correspondence matching @cite_65 , such as person re-identification @cite_102 @cite_26 @cite_20 , keypoint matching @cite_108 , 3D reconstruction @cite_59 , action recognition @cite_88 , and T-by-D based MOT @cite_15 .\"","":""}
{"id":"2909336075","dialogue":"\"As we enter into the AI era","summary":"the proliferation of deep learning approaches","":"and efficiency of GANs resulted in the development of \"\"deep fake\"\"s.\""}
{"id":"2909336075","dialogue":"\"As we enter into the AI era","summary":"the proliferation of deep learning approaches","":""}
{"id":"2907172645","dialogue":"\"Convolution is the core operation for many deep neural networks. The Winograd convolution algorithms have been shown to accelerate the widely-used small convolution sizes. Quantized neural networks can effectively reduce model sizes and improve inference speed, which leads to a wide variety of kernels and hardware accelerators that work with integer data. The state-of-the-art Winograd algorithms pose challenges for efficient implementation and execution by the integer kernels and accelerators. We introduce a new class of Winograd algorithms by extending the construction to the field of complex and propose optimizations that reduce the number of general multiplications. The new algorithm achieves an arithmetic complexity reduction of @math x over the direct method and an efficiency gain up to @math over the rational algorithms. Furthermore, we design and implement an integer-based filter scaling scheme to effectively reduce the filter bit width by @math without any significant accuracy loss.\"","summary":"\"The Winograd convolution algorithm was first used to accelerate convnets by @cite_10 . The authors derived several small fixed-size algorithms over the field of rationals based on the minimal filtering algorithm proposed by Winograd @cite_21 , which achieve arithmetic complexity reductions ranging from @math x to @math x for the popular filter sizes.\"","":""}
{"id":"2907178700","dialogue":"\"In this paper, we propose a method for clustering image-caption pairs by simultaneously learning image representations and text representations that are constrained to exhibit similar distributions. These image-caption pairs arise frequently in high-value applications where structured training data is expensive to produce but free-text descriptions are common. MultiDEC initializes parameters with stacked autoencoders, then iteratively minimizes the Kullback-Leibler divergence between the distribution of the images (and text) to that of a combined joint target distribution. We regularize by penalizing non-uniform distributions across clusters. The representations that minimize this objective produce clusters that outperform both single-view and multi-view techniques on large benchmark image-caption datasets.\"","summary":"\"Joint embedding of image and text models have been increasingly popular in applications including image captioning @cite_0 @cite_39 @cite_24 , question answering @cite_4 , and information retrieval @cite_24 @cite_35 @cite_16 . DeVise @cite_5 is the first method to generate visual-semantic embeddings that linearly transform a visual embedding from a pre-trained deep neural network into the embedding space of textual representation. The method begins with a pre-trained language model, then optimizes the visual-semantic model with a combination of dot-product similarity and hinge rank loss as the loss function. After DeVise, several visual semantic models have been developed by optimizing bi-directional pairwise ranking loss @cite_23 @cite_27 and maximum mean discrepancy loss @cite_7 . Maximizing CCA (Canonical Correlation Analysis) @cite_2 is also a common way to acquire cross-modal representation. @cite_29 address the problem of matching images and text in a joint latent space learned with deep canonical correlation analysis. @cite_35 develop a canonical correlation analysis layer and then apply pairwise ranking loss to learn a common representation of image and text for information retrieval tasks. However, most image-text multi-modal studies focus on matching image and text. Few methods study the problem of unsupervised clustering of image-text pairs.\"","":""}
{"id":"2907178700","dialogue":"\"In this paper, we propose a method for clustering image-caption pairs by simultaneously learning image representations and text representations that are constrained to exhibit similar distributions. These image-caption pairs arise frequently in high-value applications where structured training data is expensive to produce but free-text descriptions are common. MultiDEC initializes parameters with stacked autoencoders, then iteratively minimizes the Kullback-Leibler divergence between the distribution of the images (and text) to that of a combined joint target distribution. We regularize by penalizing non-uniform distributions across clusters. The representations that minimize this objective produce clusters that outperform both single-view and multi-view techniques on large benchmark image-caption datasets.\"","summary":"\"addressed a related problem where they aim to cluster images by integrating the multimodal feature generation with the Locality Linear Coding (LLC) and co-occurrence association network, multimodal feature fusion with CCA, and accelerated hierarchical k-means clustering @cite_25 . However, the text data they handled are tags instead of longer, noisy, and unreliable free-text descriptions as we do in MultiDEC. proposed EZLearn @cite_14 , a co-training framework which takes image-text data and an ontology to classify images using labels from the ontology. This model requires prior knowledge of the data in order to derive an ontology; this prior knowledge is not always available, and can significantly bias the results toward the clusters implied by the ontology.\"","":""}
{"id":"2908344088","dialogue":"\"In this paper, we study Forman's discrete Morse theory in the context of weighted homology. We develop weighted versions of classical theorems in discrete Morse theory. A key difference in the weighted case is that simplicial collapses do not necessarily preserve weighted homology. We work out some sufficient conditions for collapses to preserve weighted homology, as well as study the effect of elementary removals on weighted homology. An application to sequence analysis is included, where we study the weighted ordered complexes of sequences.\"","summary":"\"Other papers involving usage of weights and discrete Morse theory include @cite_23 , where weights are applied to different colors in the Red-Green-Blue (RGB) encoding. Discrete Morse theory is then used in combination with persistent homology for data analysis. In @cite_15 , discrete Morse theory is used to extract the extremal structure of scalar and vector fields on 2D manifolds embedded in @math . Weights @math are assigned to the edges of the cell graph, followed by computing the sequence of maximum weight matchings. An algorithmic pipeline computes a hierarchy of extremal structures, where the hierarchy is defined by an importance measure and enables the user to select an appropriate level of detail.\"","":""}
{"id":"2957560463","dialogue":"\"We propose to study unweighted graphs of constant distance VC-dimension as a broad generalization of many graph classes for which we can compute the diameter in truly subquadratic-time. In particular for any fixed @math , the class of @math -minor free graphs has distance VC-dimension at most @math . Our first main result is that on graphs of distance VC-dimension at most @math , for any fixed @math we can either compute the diameter or conclude that it is larger than @math in time @math , where @math only depends on @math . Then as a byproduct of our approach, we get the first truly subquadratic-time algorithm for constant diameter computation on all the nowhere dense graph classes. Finally, we show how to remove the dependency on @math for any graph class that excludes a fixed graph @math as a minor. More generally, our techniques apply to any graph with constant distance VC-dimension and polynomial expansion. As a result for all such graphs one obtains a truly subquadratic-time algorithm for computing their diameter. Our approach is based on the work of Chazelle and Welzl who proved the existence of spanning paths with strongly sublinear stabbing number for every hypergraph of constant VC-dimension. We show how to compute such paths efficiently by combining the best known approximation algorithms for the stabbing number problem with a clever use of @math -nets, region decomposition and other partition techniques.\"","summary":"\"An early example of linear-time solvable special case for diameter computation is the class of interval graphs @cite_0 . For every interval graph @math and for any integer @math , if we first compute an interval representation for @math in linear-time @cite_31 then we can compute by dynamic programming, for every vertex @math , the contiguous segment of all the vertices at a distance @math from @math in @math . It takes almost linear-time and it implies a straightforward quasi linear-time algorithm for diameter computation. More efficient algorithms for diameter computation on interval graphs and related graph classes were proposed in @cite_23 . Nevertheless we will show in what follows that interval orderings are a powerful tool for diameter computation on more general geometric graph classes.\"","":""}
{"id":"2763400571","dialogue":"\"In this paper, we propose PopNetCod, a popularity-based caching policy for network coding enabled Named Data Networking. PopNetCod is a distributed caching policy, in which each router measures the local popularity of the content objects by analyzing the requests that it receives. It then uses this information to decide which Data packets to cache or evict from its content store. Since network coding is used, partial caching of content objects is supported, which facilitates the management of the content store. The routers decide the Data packets that they cache or evict in an online manner when they receive requests for Data packets. This allows the most popular Data packets to be cached closer to the network edges. The evaluation of PopNetCod shows an improved cache-hit rate compared to the widely used Leave Copy Everywhere placement policy and the Least Recently Used eviction policy. The improved cache-hit rate helps the clients to achieve higher goodput, while it also reduces the load on the source servers.\"","summary":"\"None of the approaches above consider the use of network coding @cite_25 , and all are evaluated in single-path scenarios. Given the benefits that network coding brings to multipath communications in NDN @cite_17 @cite_9 @cite_10 @cite_3 , some approaches have been proposed to improve the benefits of caching in network coding enabled NDN architectures @cite_26 @cite_24 @cite_15 . @cite_26 and @cite_24 propose optimal solutions to the problem of efficiently caching in network coding enabled NDN. However, both approaches need a central entity that is aware of the network topology and the Interests, which does not scale well with the number of network nodes. @cite_15 is an eviction policy in which routers, before evicting a Data packet, apply network coding to the Data packet by means of combining it with other Data packets with the same name prefix that will remain in the cache. Due to the increased Data packet diversity in the network, the cache-hit rate is improved. However, in Interest aggregation and Interest pipelining are problematic, limiting the benefits that network coding brings to the NDN architecture.\"","":""}
{"id":"2961482815","dialogue":"\"Domain adaptation investigates the problem of leveraging knowledge from a well-labeled source domain to an unlabeled target domain, where the two domains are drawn from different data distributions. Because of the distribution shifts, different target samples have distinct degrees of difficulty in adaptation. However, existing domain adaptation approaches overwhelmingly neglect the degrees of difficulty and deploy exactly the same framework for all of the target samples. Generally, a simple or shadow framework is fast but rough. A sophisticated or deep framework, on the contrary, is accurate but slow. In this paper, we aim to challenge the fundamental contradiction between the accuracy and speed in domain adaptation tasks. We propose a novel approach, named agile domain adaptation , which agilely applies optimal frameworks to different target samples and classifies the target samples according to their adaptation difficulties. Specifically, we propose a paradigm which performs several early detections before the final classification. If a sample can be classified at one of the early stage with enough confidence, the sample would exit without the subsequent processes. Notably, the proposed method can significantly reduce the running cost of domain adaptation approaches, which can extend the application scenarios of domain adaptation to even mobile devices and real-time systems. Extensive experiments on two open benchmarks verify the effectiveness and efficiency of the proposed method.\"","summary":"\"A typical domain adaptation @cite_28 @cite_3 @cite_10 @cite_11 problem consists of two domains: a well-labeled source domain and an unlabeled target domain. The two domains generally have the same label space but different data distributions @cite_2 . Domain adaptation aims to mitigate the gap between the two data distributions, so that the knowledge, e.g., features and parameters, learned from the source domain can be transfered to the target domain. Recently, domain adaptation has been successfully applied to many real-world applications, such as image recognition @cite_21 @cite_25 , multimedia analysis @cite_26 @cite_31 and recommender systems @cite_5 @cite_0 .\"","":""}
{"id":"2961482815","dialogue":"\"Domain adaptation investigates the problem of leveraging knowledge from a well-labeled source domain to an unlabeled target domain, where the two domains are drawn from different data distributions. Because of the distribution shifts, different target samples have distinct degrees of difficulty in adaptation. However, existing domain adaptation approaches overwhelmingly neglect the degrees of difficulty and deploy exactly the same framework for all of the target samples. Generally, a simple or shadow framework is fast but rough. A sophisticated or deep framework, on the contrary, is accurate but slow. In this paper, we aim to challenge the fundamental contradiction between the accuracy and speed in domain adaptation tasks. We propose a novel approach, named agile domain adaptation , which agilely applies optimal frameworks to different target samples and classifies the target samples according to their adaptation difficulties. Specifically, we propose a paradigm which performs several early detections before the final classification. If a sample can be classified at one of the early stage with enough confidence, the sample would exit without the subsequent processes. Notably, the proposed method can significantly reduce the running cost of domain adaptation approaches, which can extend the application scenarios of domain adaptation to even mobile devices and real-time systems. Extensive experiments on two open benchmarks verify the effectiveness and efficiency of the proposed method.\"","summary":"\"Since domain adaptation aims to mitigate the distribution gap between the source domain and the target domain, it is vital to find a metric which can measure the data distribution divergence. Maximum mean discrepancy (MMD) @cite_27 is widely considered as a favorable criteria in previous work. For instance, deep adaptation networks (DAN) @cite_4 generalizes deep convolutional neural networks to the domain adaptation scenario. In DAN, the general (task-invariant) layers are shared by the two domains and the task-specific layers are adapted by multi-kernel MMD. Furthermore, joint adaptation networks (JAN) @cite_18 extends DAN by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion.\"","":""}
{"id":"2961482815","dialogue":"\"Domain adaptation investigates the problem of leveraging knowledge from a well-labeled source domain to an unlabeled target domain, where the two domains are drawn from different data distributions. Because of the distribution shifts, different target samples have distinct degrees of difficulty in adaptation. However, existing domain adaptation approaches overwhelmingly neglect the degrees of difficulty and deploy exactly the same framework for all of the target samples. Generally, a simple or shadow framework is fast but rough. A sophisticated or deep framework, on the contrary, is accurate but slow. In this paper, we aim to challenge the fundamental contradiction between the accuracy and speed in domain adaptation tasks. We propose a novel approach, named agile domain adaptation , which agilely applies optimal frameworks to different target samples and classifies the target samples according to their adaptation difficulties. Specifically, we propose a paradigm which performs several early detections before the final classification. If a sample can be classified at one of the early stage with enough confidence, the sample would exit without the subsequent processes. Notably, the proposed method can significantly reduce the running cost of domain adaptation approaches, which can extend the application scenarios of domain adaptation to even mobile devices and real-time systems. Extensive experiments on two open benchmarks verify the effectiveness and efficiency of the proposed method.\"","summary":"\"Recently, generative adversarial networks (GAN) @cite_14 has been introduced into domain adaptation. Compared with the distribution alignment methods, adversarial domain adaptation models @cite_6 @cite_25 are able to generate domain invariant features under the supervision of a discriminator. For instance, adversarial discriminative domain adaptation (ADDA) @cite_6 combines discriminative analysis, untied weight sharing and a GAN loss under a generalized framework. Coupled generative adversarial networks (CoGAN) @cite_15 minimize the domain shifts by simultaneously training two GANs to handle the source domain and the target domain.\"","":""}
{"id":"2961619217","dialogue":"\"With deep learning becoming a more prominent approach for automatic classification of three-dimensional point cloud data, a key bottleneck is the amount of high quality training data, especially when compared to that available for two-dimensional images. One potential solution is the use of synthetic data for pre-training networks, however the ability for models to generalise from synthetic data to real world data has been poorly studied for point clouds. Despite this, a huge wealth of 3D virtual environments exist which, if proved effective can be exploited. We therefore argue that research in this domain would be of significant use. In this paper we present SynthCity an open dataset to help aid research. SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Every point is assigned a label from one of nine categories. We generate our point cloud in a typical Urban Suburban environment using the Blensor plugin for Blender.\"","summary":"\"The need for outdoor labelled point clouds has been addressed by a range of researchers. , @cite_15 released the Paris-rue-Madame MLS dataset containing 20M points ( @math and reflectance), , @cite_10 the iQmulus dataset containing 300M points ( @math , time, reflectance and number of echoes) and , @cite_8 the Paris-Lille-3D containing 143.1M points ( @math , scanner @math , gps time, reflectance). However, many caveats exist within these datasets. For example, Paris-rue-Madame, whilst large enough for traditional machine learning algorithms (i.e. Support Vector Machines, Random Forest), does not meet the scale for a modern DNN, which number of parameters can easily exceed 10x the number of points available. The iQmulus is more suited in terms of size however due to a 2D semi-manual data labelling approach, many mislabelled ground truth points exist.\"","":""}
{"id":"2959360491","dialogue":"\"Given a family @math of graphs and a positive integer @math , a graph @math is called vertex @math -fault-tolerant with respect to @math , denoted by @math -FT @math , if @math contains some @math as a subgraph, for every @math with @math . Vertex-fault-tolerance has been introduced by Hayes [A graph model for fault-tolerant computing systems, IEEE Transactions on Computers, C-25 (1976), pp. 875-884.], and has been studied in view of potential applications in the design of interconnection networks operating correctly in the presence of faults. We define the Fault-Tolerant Complete Matching (FTCM) Problem in bipartite graphs of order @math : to design a bipartite @math , with @math , @math , @math , that has a FTCM, and the tuple @math , where @math and @math are the maximum degree in @math and @math , respectively, is lexicographically minimum. @math has a FTCM if deleting at most @math vertices from @math creates @math that has a complete matching, i.e., a matching of size @math . We show that if @math is integer, solutions of the FTCM Problem can be found among @math -regular bipartite graphs of order @math , with @math , and @math . If @math then all @math -regular bipartite graphs of order @math have a FTCM, and for @math , it is not the case. We characterize the values of @math , @math , @math , and @math that admit an @math -regular bipartite graph of order @math , with @math , and give a simple construction that creates such a graph with a FTCM whenever possible. Our techniques are based on Hall's marriage theorem, elementary number theory, linear Diophantine equations, properties of integer functions and congruences, and equations involving them.\"","summary":"\"Design of fault-tolerant bipartite graphs has potential applications in the design of flexible processes, where there are @math different request types and @math servers that should process them (see, for example, the work of @cite_0 for a review of the topic).\"","":""}
{"id":"2959360491","dialogue":"\"Given a family @math of graphs and a positive integer @math , a graph @math is called vertex @math -fault-tolerant with respect to @math , denoted by @math -FT @math , if @math contains some @math as a subgraph, for every @math with @math . Vertex-fault-tolerance has been introduced by Hayes [A graph model for fault-tolerant computing systems, IEEE Transactions on Computers, C-25 (1976), pp. 875-884.], and has been studied in view of potential applications in the design of interconnection networks operating correctly in the presence of faults. We define the Fault-Tolerant Complete Matching (FTCM) Problem in bipartite graphs of order @math : to design a bipartite @math , with @math , @math , @math , that has a FTCM, and the tuple @math , where @math and @math are the maximum degree in @math and @math , respectively, is lexicographically minimum. @math has a FTCM if deleting at most @math vertices from @math creates @math that has a complete matching, i.e., a matching of size @math . We show that if @math is integer, solutions of the FTCM Problem can be found among @math -regular bipartite graphs of order @math , with @math , and @math . If @math then all @math -regular bipartite graphs of order @math have a FTCM, and for @math , it is not the case. We characterize the values of @math , @math , @math , and @math that admit an @math -regular bipartite graph of order @math , with @math , and give a simple construction that creates such a graph with a FTCM whenever possible. Our techniques are based on Hall's marriage theorem, elementary number theory, linear Diophantine equations, properties of integer functions and congruences, and equations involving them.\"","summary":"\"Nevertheless, the relation of our model with the process flexibility literature is still remote, since the process flexibility community has so far focused on systems where a server can process different kinds of compatible requests within the same time period. Moreover, only recently unbalanced systems, i.e., with @math , have started to be considered (see, @cite_21 and @cite_4 for examples).\"","":""}
{"id":"2959360491","dialogue":"\"Given a family @math of graphs and a positive integer @math , a graph @math is called vertex @math -fault-tolerant with respect to @math , denoted by @math -FT @math , if @math contains some @math as a subgraph, for every @math with @math . Vertex-fault-tolerance has been introduced by Hayes [A graph model for fault-tolerant computing systems, IEEE Transactions on Computers, C-25 (1976), pp. 875-884.], and has been studied in view of potential applications in the design of interconnection networks operating correctly in the presence of faults. We define the Fault-Tolerant Complete Matching (FTCM) Problem in bipartite graphs of order @math : to design a bipartite @math , with @math , @math , @math , that has a FTCM, and the tuple @math , where @math and @math are the maximum degree in @math and @math , respectively, is lexicographically minimum. @math has a FTCM if deleting at most @math vertices from @math creates @math that has a complete matching, i.e., a matching of size @math . We show that if @math is integer, solutions of the FTCM Problem can be found among @math -regular bipartite graphs of order @math , with @math , and @math . If @math then all @math -regular bipartite graphs of order @math have a FTCM, and for @math , it is not the case. We characterize the values of @math , @math , @math , and @math that admit an @math -regular bipartite graph of order @math , with @math , and give a simple construction that creates such a graph with a FTCM whenever possible. Our techniques are based on Hall's marriage theorem, elementary number theory, linear Diophantine equations, properties of integer functions and congruences, and equations involving them.\"","summary":"\"A related line of research is not to design the smallest fault tolerant graphs (in terms of any metric, like the ones given above), but to analyze the level of fault tolerance assured by prescribed topologies @cite_2 @cite_12 . This topic is of particular interest for algorithm design in high performance computing. Supercomputers are comprised of many processing nodes (with some local memory) that use an interconnection network to communicate during the execution of distributed algorithms. An algorithm delegates computational tasks to different nodes, and uses some logical topology for its message passing. This logical topology has to be somehow embedded in the interconnection network provided by the supercomputer. So it is of practical interest to study if the message passing topologies most common in algorithm design (like cycles, and trees of certain types) can still be embedded in interconnection topologies provided by supercomputers (often similar to hypercubes) when the system presents some faults @cite_2 .\"","":""}
{"id":"2959360491","dialogue":"\"Given a family @math of graphs and a positive integer @math , a graph @math is called vertex @math -fault-tolerant with respect to @math , denoted by @math -FT @math , if @math contains some @math as a subgraph, for every @math with @math . Vertex-fault-tolerance has been introduced by Hayes [A graph model for fault-tolerant computing systems, IEEE Transactions on Computers, C-25 (1976), pp. 875-884.], and has been studied in view of potential applications in the design of interconnection networks operating correctly in the presence of faults. We define the Fault-Tolerant Complete Matching (FTCM) Problem in bipartite graphs of order @math : to design a bipartite @math , with @math , @math , @math , that has a FTCM, and the tuple @math , where @math and @math are the maximum degree in @math and @math , respectively, is lexicographically minimum. @math has a FTCM if deleting at most @math vertices from @math creates @math that has a complete matching, i.e., a matching of size @math . We show that if @math is integer, solutions of the FTCM Problem can be found among @math -regular bipartite graphs of order @math , with @math , and @math . If @math then all @math -regular bipartite graphs of order @math have a FTCM, and for @math , it is not the case. We characterize the values of @math , @math , @math , and @math that admit an @math -regular bipartite graph of order @math , with @math , and give a simple construction that creates such a graph with a FTCM whenever possible. Our techniques are based on Hall's marriage theorem, elementary number theory, linear Diophantine equations, properties of integer functions and congruences, and equations involving them.\"","summary":"A problem that is closely related to ours was presented by Perarnau and Petridis @cite_13 . The authors studied the existence of perfect matchings in induced balanced subgraphs of random biregular bipartite graphs.","":""}
{"id":"2962342247","dialogue":"\"It is well known that size-based scheduling policies","summary":"which take into account job size (i.e.","":""}
{"id":"2962342247","dialogue":"\"It is well known that size-based scheduling policies","summary":"which take into account job size (i.e.","":""}
{"id":"2962342247","dialogue":"\"It is well known that size-based scheduling policies","summary":"which take into account job size (i.e.","":""}
{"id":"2956124191","dialogue":"\"Traditionally, the automatic recognition of human activities is performed with supervised learning algorithms on limited sets of specific activities. This work proposes to recognize recurrent activity patterns, called routines, instead of precisely defined activities. The modeling of routines is defined as a metric learning problem, and an architecture, called SS2S, based on sequence-to-sequence models is proposed to learn a distance between time series. This approach only relies on inertial data and is thus non intrusive and preserves privacy. Experimental results show that a clustering algorithm provided with the learned distance is able to recover daily routines.\"","summary":"\"The traditional approach to compute distances between sequences (or time series, or trajectories) is to perform Dynamic Time Warping (DTW) @cite_5 which was introduced in 1978. Since then, several improvements of the algorithm have been published, notably a fast version by Salvador @cite_0 . DTW is considered one of the best metric to use for sequence classification @cite_21 combined with @math -nearest neighbors. Recently, Abid @cite_26 proposed a neural network architecture to learn the parameters of a warping distance accordingly to the euclidean distances in a projection space. However, DTW, as other shaped-based distances @cite_9 , is only able to retrieve local similarities when time series have a relatively small length and are just shifted or not well aligned.\"","":""}
{"id":"2907303408","dialogue":"\"Twitter has been a prominent social media platform for mining population-level health data and accurate clustering of health-related tweets into topics is important for extracting relevant health insights. In this work, we propose deep convolutional autoencoders for learning compact representations of health-related tweets, further to be employed in clustering. We compare our method to several conventional tweet representation methods including bag-of-words, term frequency-inverse document frequency, Latent Dirichlet Allocation and Non-negative Matrix Factorization with 3 different clustering algorithms. Our results show that the clustering performance using proposed representation learning scheme significantly outperforms that of conventional methods for all experiments of different number of clusters. In addition, we propose a constraint on the learned representations during the neural network training in order to further enhance the clustering performance. All in all, this study introduces utilization of deep neural network-based architectures, i.e., deep convolutional autoencoders, for learning informative representations of health-related tweets.\"","summary":"\"Devising efficient representations of tweets, i.e., features, for performing clustering has been studied extensively. Most frequently used features for representing the text in tweets as numerical vectors are (BoWs) and (tf-idf) features @cite_2 @cite_42 @cite_61 @cite_5 @cite_43 . Both of these feature extraction methods are based on word occurrence counts and eventually, result in a sparse (most elements being zero) document-term matrix. Proposed algorithms for clustering tweets into topics include variants of hierarchical, density-based and centroid-based clustering methods; k-means algorithm being the most frequently used one @cite_42 @cite_43 @cite_24 .\"","":""}
{"id":"2907303408","dialogue":"\"Twitter has been a prominent social media platform for mining population-level health data and accurate clustering of health-related tweets into topics is important for extracting relevant health insights. In this work, we propose deep convolutional autoencoders for learning compact representations of health-related tweets, further to be employed in clustering. We compare our method to several conventional tweet representation methods including bag-of-words, term frequency-inverse document frequency, Latent Dirichlet Allocation and Non-negative Matrix Factorization with 3 different clustering algorithms. Our results show that the clustering performance using proposed representation learning scheme significantly outperforms that of conventional methods for all experiments of different number of clusters. In addition, we propose a constraint on the learned representations during the neural network training in order to further enhance the clustering performance. All in all, this study introduces utilization of deep neural network-based architectures, i.e., deep convolutional autoencoders, for learning informative representations of health-related tweets.\"","summary":"\"Numerous works on topic modeling of tweets are available as well. Topic models are generative models, relying on the idea that a given tweet is a mixture of topics, where a topic is a probability distribution over words @cite_45 . Even though the objective in topic modeling is slightly different than that of pure clustering, representing each tweet as a topic vector is essentially a way of dimensionality reduction or feature extraction and can further be followed by a clustering algorithm. Proposed topic modeling methods include conventional approaches or variants of them such as Latent Dirichlet Allocation (LDA) @cite_8 @cite_2 @cite_42 @cite_32 @cite_20 @cite_48 @cite_54 @cite_27 @cite_43 @cite_40 @cite_29 and Non-negative Matrix Factorization (NMF) @cite_22 @cite_5 . Note that topic models such as LDA are based on the notion that words belonging to a topic are more likely to appear in the same document and do not assume a distance metric between discovered topics.\"","":""}
{"id":"2907303408","dialogue":"\"Twitter has been a prominent social media platform for mining population-level health data and accurate clustering of health-related tweets into topics is important for extracting relevant health insights. In this work, we propose deep convolutional autoencoders for learning compact representations of health-related tweets, further to be employed in clustering. We compare our method to several conventional tweet representation methods including bag-of-words, term frequency-inverse document frequency, Latent Dirichlet Allocation and Non-negative Matrix Factorization with 3 different clustering algorithms. Our results show that the clustering performance using proposed representation learning scheme significantly outperforms that of conventional methods for all experiments of different number of clusters. In addition, we propose a constraint on the learned representations during the neural network training in order to further enhance the clustering performance. All in all, this study introduces utilization of deep neural network-based architectures, i.e., deep convolutional autoencoders, for learning informative representations of health-related tweets.\"","summary":"\"In contrary to abovementioned feature extraction methods which are not specific to representation of tweets but rather generic in natural language processing, various works propose custom feature extraction methods for certain health-related information retrieval tasks from Twitter. For instance, engineered sentiment analysis features to discover latent infectious diseases from Twitter @cite_7 . In order to track public health condition trends from Twitter, specific features are proposed by Parker at al. employing Wikipedia article index, i.e., treating the retrieval of medically-related Wikipedia articles as an indicator of a health-related condition @cite_1 . Custom user similarity features calculated from tweets were also proposed for building a framework for recommending health-related topics @cite_27 .\"","":""}
{"id":"2907303408","dialogue":"\"Twitter has been a prominent social media platform for mining population-level health data and accurate clustering of health-related tweets into topics is important for extracting relevant health insights. In this work, we propose deep convolutional autoencoders for learning compact representations of health-related tweets, further to be employed in clustering. We compare our method to several conventional tweet representation methods including bag-of-words, term frequency-inverse document frequency, Latent Dirichlet Allocation and Non-negative Matrix Factorization with 3 different clustering algorithms. Our results show that the clustering performance using proposed representation learning scheme significantly outperforms that of conventional methods for all experiments of different number of clusters. In addition, we propose a constraint on the learned representations during the neural network training in order to further enhance the clustering performance. All in all, this study introduces utilization of deep neural network-based architectures, i.e., deep convolutional autoencoders, for learning informative representations of health-related tweets.\"","summary":"\"Metrics for evaluating the performance of clustering algorithms varies depending on whether the ground truth topic categories are available or not. If so, frequently used metrics are and . In the case of absence of ground truth labels, one has to use internal clustering criterions such as Calinski-Harabasz (CH) score @cite_58 and Davies-Bouldin index @cite_4 . provides an extensive comparative study of cluster validity indices @cite_28 .\"","":""}
{"id":"2957945053","dialogue":"\"When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data -- from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels and at each level, we offer a set of selection and filtering operations enabling the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.\"","summary":"\"Most of the currently available computational tools for protein-protein interactions are focusing on protein pairs and a comprehensible overview was published by Huang @cite_4 . Some of the existing approaches, such as ArDock @cite_29 , already combine the computational method with a basic visual representation of the predictions. There are even some solutions, such as DockingShop @cite_28 , which are enabling the user to interactively design an initial configuration for a protein docking prediction process through a molecular graphics interface.\"","":""}
{"id":"2957945053","dialogue":"\"When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data -- from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels and at each level, we offer a set of selection and filtering operations enabling the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.\"","summary":"\"One of the first tools designed primarily for multi-body docking was CombDock @cite_10 . The algorithm works on a principle of hierarchical construction of the complex from smaller subunits and a greedy selection of the best-ranking subunits. The combinatorial step is followed by the reduction of solutions based on RMSD and a scoring function. Multi--LZerD @cite_24 uses a genetic algorithm to generate complexes from initial pairwise docks and applies an energy minimization structure refinement procedure for the ranking of the solutions. @cite_13 proposed an ant colony optimization approach to solve the combinatorial problem. DockStar @cite_33 formulates the task of detecting the spatial conformation of a protein complex as an Integer Linear Program. Unlike other methods, it also integrates experimental data from mass spectrometry into the scoring of the solutions. Another tool reusing pairwise docks in combination with experimental data is PRISM-EM @cite_14 . It uses density maps from cryo-electron microscopy for guiding the placement of subunits.\"","":""}
{"id":"2957945053","dialogue":"\"When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data -- from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels and at each level, we offer a set of selection and filtering operations enabling the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.\"","summary":"\"Although these schematic representations are conveying the information about a single configuration, they do not support the comparison and interactive filtering of entire ensembles of configurations. This issue is addressed in the CoCoMaps @cite_22 and COZOID @cite_25 tools. Both tools come with linked visualizations, aiding the users in analyzing and comparing interactions between protein pairs. CoCoMaps and its successor CONS-COCOMAPS @cite_1 enable to measure and visualize the consensus in multiple docking solutions and display the conservation of residue contacts using intermolecular contact maps. The COZOID tool uses a set of linked views for the interactive exploration of large ensembles of protein pairs, supporting a visual drilldown approach for narrowing down the set of possibly relevant configurations. The main limitation of these approaches is that they are operating only on protein pairs (i.e., single ) and cannot be directly applied to multi-body complexes. The multiscale aspect in molecular visualization can be explored on different granularity levels, as shown in the recent survey of @cite_6 .\"","":""}
{"id":"2957945053","dialogue":"\"When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data -- from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels and at each level, we offer a set of selection and filtering operations enabling the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.\"","summary":"\"In our case, we were not only concerned with designing proper visual representations of the individual hierarchy levels of large ensembles of multi-body complexes, but also with how to interactively explore and filter these ensembles to support the identification of biochemically most relevant instances. @cite_34 focus on the problem of interactive visual steering of hierarchical simulation ensembles. In their substantially different application case, they also deal with linking representations on different levels of detail as well as with the challenge that the ensemble can grow during the exploration process.\"","":""}
{"id":"2907819829","dialogue":"\"Most text detection methods hypothesize texts are horizontal or multi-oriented and thus define quadrangles as the basic detection unit. However, text in the wild is usually perspectively distorted or curved, which can not be easily tackled by existing approaches. In this paper, we propose a deep character embedding network (CENet) which simultaneously predicts the bounding boxes of characters and their embedding vectors, thus making text detection a simple clustering task in the character embedding space. The proposed method does not require strong assumptions of forming a straight line on general text detection, which provides flexibility on arbitrarily curved or perspectively distorted text. For character detection task, a dense prediction subnetwork is designed to obtain the confidence score and bounding boxes of characters. For character embedding task, a subnet is trained with contrastive loss to project detected characters into embedding space. The two tasks share a backbone CNN from which the multi-scale feature maps are extracted. The final text regions can be easily achieved by a thresholding process on character confidence and embedding distance of character pairs. We evaluated our method on ICDAR13, ICDAR15, MSRA-TD500, and Total-Text. The proposed method achieves state-of-the-art or comparable performance on all these datasets, and shows substantial improvement in the irregular-text datasets, i.e. Total-Text.\"","summary":"\"MSER @cite_5 and SWT @cite_21 are classical text component extraction methods. In the era of deep learning, CTPN @cite_17 extracts horizontal text components with fixed-size width using a modified Faster R-CNN framework. Horizontal text lines are easily generated, since CTPN adjusted the Faster R-CNN @cite_7 framework to output dense text components. SegLink @cite_36 proposed a kind of oriented text component (i.e. segment) and a component-pair connection structure (i.e. link). A link indicates which two segments should be connected. Naturally, SegLink dealt better with multi-oriented texts than CTPN. PixelLink @cite_1 provided an instance segmentation based solution that detects text pixels and their linkage with neighbor pixels. Positive pixels with positive links are grouped as the collection of connected components. Besides, Markov Clustering Network @cite_11 regarded detected text pixels as nodes and associated them with computed attractors by a designed markov clustering networks. The above mentioned methods provided inspiring ideas on text detection. However, the regions between characters are sometimes in-discriminative with background in some cases, especially in text lines where distances between characters are large.\"","":""}
{"id":"2907819829","dialogue":"\"Most text detection methods hypothesize texts are horizontal or multi-oriented and thus define quadrangles as the basic detection unit. However, text in the wild is usually perspectively distorted or curved, which can not be easily tackled by existing approaches. In this paper, we propose a deep character embedding network (CENet) which simultaneously predicts the bounding boxes of characters and their embedding vectors, thus making text detection a simple clustering task in the character embedding space. The proposed method does not require strong assumptions of forming a straight line on general text detection, which provides flexibility on arbitrarily curved or perspectively distorted text. For character detection task, a dense prediction subnetwork is designed to obtain the confidence score and bounding boxes of characters. For character embedding task, a subnet is trained with contrastive loss to project detected characters into embedding space. The two tasks share a backbone CNN from which the multi-scale feature maps are extracted. The final text regions can be easily achieved by a thresholding process on character confidence and embedding distance of character pairs. We evaluated our method on ICDAR13, ICDAR15, MSRA-TD500, and Total-Text. The proposed method achieves state-of-the-art or comparable performance on all these datasets, and shows substantial improvement in the irregular-text datasets, i.e. Total-Text.\"","summary":"\"Recently, quite a few works @cite_9 @cite_38 @cite_27 @cite_0 @cite_15 @cite_31 @cite_22 @cite_32 have put emphasis on adjusting some popular object detection frameworks including Faster R-CNN @cite_7 , SSD @cite_23 and Densebox @cite_10 to detect word boundary. In contrast to general objects, texts appearing in the real-world have larger varieties of aspect ratios and orientations. @cite_27 and @cite_38 directly added more anchor boxes of large aspect ratio to cover texts of wider range. @cite_9 and @cite_15 added the angle property to the bounding box to deal with the problem of multiple orientations, while EAST @cite_0 and @cite_31 provided a looser representation namely quadrangle. These methods seem to easily achieve high performance on benchmarks with word-level annotations, but not on non-Latin scripts or curved text with polygon-level annotations.\"","":""}
{"id":"2907819829","dialogue":"\"Most text detection methods hypothesize texts are horizontal or multi-oriented and thus define quadrangles as the basic detection unit. However, text in the wild is usually perspectively distorted or curved, which can not be easily tackled by existing approaches. In this paper, we propose a deep character embedding network (CENet) which simultaneously predicts the bounding boxes of characters and their embedding vectors, thus making text detection a simple clustering task in the character embedding space. The proposed method does not require strong assumptions of forming a straight line on general text detection, which provides flexibility on arbitrarily curved or perspectively distorted text. For character detection task, a dense prediction subnetwork is designed to obtain the confidence score and bounding boxes of characters. For character embedding task, a subnet is trained with contrastive loss to project detected characters into embedding space. The two tasks share a backbone CNN from which the multi-scale feature maps are extracted. The final text regions can be easily achieved by a thresholding process on character confidence and embedding distance of character pairs. We evaluated our method on ICDAR13, ICDAR15, MSRA-TD500, and Total-Text. The proposed method achieves state-of-the-art or comparable performance on all these datasets, and shows substantial improvement in the irregular-text datasets, i.e. Total-Text.\"","summary":"\"The goal of metric learning or embedding methods @cite_3 @cite_18 @cite_4 is to learn a function that measures how similar two samples are. There are many successful applications of metric learning @cite_26 @cite_3 @cite_18 @cite_4 , such as ranking, image retrieval, face verification, speaker verification and so on. By far, applications of metric learning on document analysis or text reading were limited to the problem of word spotting and verification @cite_39 @cite_7 @cite_30 . In this work, we verify the effectiveness of deep metric learning in text detection task. Based on character candidates, we provide an end-to-end trainable network that can output the character bounding boxes and their embedding vectors simultaneously. Text regions could be easily detected by grouping characters which embedding distances are small.\"","":""}
{"id":"2949972480","dialogue":"\"Recommender systems have become important tools to support users in identifying relevant content in an overloaded information space. To ease the development of recommender systems, a number of recommender frameworks have been proposed that serve a wide range of application domains. Our TagRec framework is one of the few examples of an open-source framework tailored towards developing and evaluating tag-based recommender systems. In this paper, we present the current, updated state of TagRec, and we summarize and reflect on four use cases that have been implemented with TagRec: (i) tag recommendations, (ii) resource recommendations, (iii) recommendation evaluation, and (iv) hashtag recommendations. To date, TagRec served the development and or evaluation process of tag-based recommender systems in two large scale European research projects, which have been described in 17 research papers. Thus, we believe that this work is of interest for both researchers and practitioners of tag-based recommender systems.\"","summary":"\"A considerable contribution to this area is http: wiki.librec.net doku.php , a Java-based library that, so far, comprises around 70 resource recommendation algorithms and evaluation modules @cite_6 . Another Java-based, open-source framework is http: ranksys.org , which focuses on the evaluation of ranking problems and supports the investigation of novelty as well as diversity for academic research @cite_29 , which is reflected in its design (e.g., data input interfaces work with a triple of user, item and features).\"","":""}
{"id":"2949972480","dialogue":"\"Recommender systems have become important tools to support users in identifying relevant content in an overloaded information space. To ease the development of recommender systems, a number of recommender frameworks have been proposed that serve a wide range of application domains. Our TagRec framework is one of the few examples of an open-source framework tailored towards developing and evaluating tag-based recommender systems. In this paper, we present the current, updated state of TagRec, and we summarize and reflect on four use cases that have been implemented with TagRec: (i) tag recommendations, (ii) resource recommendations, (iii) recommendation evaluation, and (iv) hashtag recommendations. To date, TagRec served the development and or evaluation process of tag-based recommender systems in two large scale European research projects, which have been described in 17 research papers. Thus, we believe that this work is of interest for both researchers and practitioners of tag-based recommender systems.\"","summary":"\"Other examples of open-source recommender software are http: www.mymedialite.net , an item recommender library that focuses on rating and ranking predictions in collaborative filtering approaches @cite_31 , https: github.com irecsys CARSKit , a recommendation library specifically designed for context-aware recommendations, and http: www.libfm.org tagrec.html , a software component that implements Tensor Factorization models for personalized tag recommendations in C++ @cite_26 .\"","":""}
{"id":"2961566780","dialogue":"\"Semi-supervised learning (SSL) uses unlabeled data for training and has been shown to greatly improve performances when compared to a supervised approach on the labeled data available. This claim depends both on the amount of labeled data available and on the algorithm used. In this paper, we compute analytically the gap between the best fully-supervised approach on labeled data and the best semi-supervised approach using both labeled and unlabeled data. We quantify the best possible increase in performance obtained thanks to the unlabeled data, i.e. we compute the accuracy increase due to the information contained in the unlabeled data. Our work deals with a simple high-dimensional Gaussian mixture model for the data in a Bayesian setting. Our rigorous analysis builds on recent theoretical breakthroughs in high-dimensional inference and a large body of mathematical tools from statistical physics initially developed for spin glasses.\"","summary":"\"Using exact but non-rigorous methods from statistical physics, @cite_17 @cite_27 determines the critical values for @math and @math at which it becomes information-theoretically possible to reconstruct the membership into clusters better than chance. Rigorous results on this model are given in @cite_10 where bounds on the critical values are obtained. The precises thresholds were then determined in @cite_2 . Our analysis builds on the techniques derived in this last reference with two main modifications: additional work is required to compute the classification accuracy (as opposed to the mean squared error) and to incorporate the side information.\"","":""}
{"id":"2961566780","dialogue":"\"Semi-supervised learning (SSL) uses unlabeled data for training and has been shown to greatly improve performances when compared to a supervised approach on the labeled data available. This claim depends both on the amount of labeled data available and on the algorithm used. In this paper, we compute analytically the gap between the best fully-supervised approach on labeled data and the best semi-supervised approach using both labeled and unlabeled data. We quantify the best possible increase in performance obtained thanks to the unlabeled data, i.e. we compute the accuracy increase due to the information contained in the unlabeled data. Our work deals with a simple high-dimensional Gaussian mixture model for the data in a Bayesian setting. Our rigorous analysis builds on recent theoretical breakthroughs in high-dimensional inference and a large body of mathematical tools from statistical physics initially developed for spin glasses.\"","summary":"\"To the best of our knowledge, there are much fewer theoretical works dealing with a semi-supervised setting. @cite_18 studies a mixture model where the estimation problem is essentially reduced to the one of estimating the mixing parameter and shows that the information content of unlabeled examples decreases as classes overlap. More closely related to our work, @cite_3 provides the first information theoretic tight analysis for inference of latent community structure given a dense graph along with high dimensional node covariates, correlated with the same latent communities. @cite_20 studies a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data similar to our setting.\"","":""}
{"id":"2961566780","dialogue":"\"Semi-supervised learning (SSL) uses unlabeled data for training and has been shown to greatly improve performances when compared to a supervised approach on the labeled data available. This claim depends both on the amount of labeled data available and on the algorithm used. In this paper, we compute analytically the gap between the best fully-supervised approach on labeled data and the best semi-supervised approach using both labeled and unlabeled data. We quantify the best possible increase in performance obtained thanks to the unlabeled data, i.e. we compute the accuracy increase due to the information contained in the unlabeled data. Our work deals with a simple high-dimensional Gaussian mixture model for the data in a Bayesian setting. Our rigorous analysis builds on recent theoretical breakthroughs in high-dimensional inference and a large body of mathematical tools from statistical physics initially developed for spin glasses.\"","summary":"\"In contrast, there are a number of practical works and proposed algorithms for semi-supervised learning based on transductive models @cite_4 , graph-based method @cite_11 or generative modeling @cite_1 , see the surveys @cite_26 and @cite_5 . SSL methods based on training a neural network by adding an additional loss term to ensure consistency regularization are presented in @cite_19 , @cite_12 , @cite_6 . We refer in particular to the recent work @cite_8 for an overview of these SSL methods (currently the state-of-the-art for SSL on image classification datasets). The algorithm MixMAtch introduced in @cite_15 obtains impressive results on all standard image benchmarks. Given these recent improvements, natural questions arise: what is the best possible achievable performance? to what extend can we generalize those improvement to other domains? We believe that our work is a first step in a theoretical understanding of these questions.\"","":""}
{"id":"2953433119","dialogue":"\"In this paper, we propose the part-aware and aggregation neural network (Part-A^2 net) for 3D object detection from point cloud. The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage learns to simultaneously predict coarse 3D proposals and accurate intra-object part locations with the free-of-charge supervisions derived from 3D ground-truth boxes. The predicted intra-object part locations within the same proposals are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the features of 3D proposals. Then the part-aggregation stage learns to re-score the box and refine the box location based on the pooled part locations. We present extensive experiments on the KITTI 3D object detection dataset, which demonstrate that both the predicted intra-object part locations and the proposed RoI-aware point cloud pooling scheme benefit 3D object detection and our Part-A^2 net outperforms state-of-the-art methods by utilizing only point cloud data.\"","summary":"\"Point cloud feature learning for 3D object detection. There are generally three ways of learning features from point cloud for 3D detection. @cite_32 @cite_27 @cite_0 @cite_8 @cite_12 projected point cloud to bird-view map and utilized 2D CNN for feature extraction. @cite_34 @cite_42 @cite_41 conducted PointNet @cite_1 @cite_31 to learn the point cloud features directly from raw point cloud. @cite_9 proposed VoxelNet and @cite_20 applied sparse convolution to speed up the VoxelNet for feature learning. Inspired by VoxelNet, we designed a UNet-like @cite_39 backbone network by using sparse convolution and deconvolution to extract discriminative point features for predicting intra-object part locations and 3D object detection.\"","":""}
{"id":"2906722931","dialogue":"\"Recommendation system could help the companies to persuade users to visit or consume at a particular place, which was based on many traditional methods such as the set of collaborative filtering algorithms. Most research discusses the model design or feature engineering methods to minimize the root mean square error (RMSE) of rating prediction, but lacks exploring the ways to generate the reasons for recommendations. This paper proposed an integrated neural network based model which integrates rating scores prediction and explainable words generation. Based on the experimental results, this model presented lower RMSE compared with traditional methods, and generate the explanation of recommendation to convince customers to visit the recommended place.\"","summary":"\"Much empirical research focuses on either rating stars treating the scores as numerical data or explainable text generation offering the features of the POIs, and those studies adopt different models, features and evaluation methods @cite_25 @cite_21 @cite_7 @cite_14 @cite_24 @cite_0 . This project is motivated by these empirical studies on rating stars prediction, and we proposed an integrated model which can predict rating stars and generate explainable opinion-aspect pairs for users.\"","":""}
{"id":"2906722931","dialogue":"\"Recommendation system could help the companies to persuade users to visit or consume at a particular place, which was based on many traditional methods such as the set of collaborative filtering algorithms. Most research discusses the model design or feature engineering methods to minimize the root mean square error (RMSE) of rating prediction, but lacks exploring the ways to generate the reasons for recommendations. This paper proposed an integrated neural network based model which integrates rating scores prediction and explainable words generation. Based on the experimental results, this model presented lower RMSE compared with traditional methods, and generate the explanation of recommendation to convince customers to visit the recommended place.\"","summary":"\"0.05in Collaborative Filtering is commonly used in recommender system @cite_12 @cite_11 @cite_16 @cite_2 --- exploiting similarity among the preference of users to generate recommendations. Research is done using users’ historical data to predict ratings, and deliver the results of recommendation to users @cite_26 @cite_9 . The traditional methods in rating prediction are the user-based model, and matrix factorization(MF). According to @cite_18 , they introduced the user-based method which predicts the rating scores based on the other users who have similar preferences. Regarding matrix factorization, mentioned the applications of two commonly used methods: Singular Value Decomposition(SVD) and Non-negative Matrix Factorization(NMF) @cite_18 . Additionally, many researchers utilized neural-based collaborative filtering model for rating prediction @cite_1 @cite_23 . However, there are many limitations for the traditional algorithms such as scalability problems, and lacking bias terms. As for neural collaborative filtering, it performs better on rating prediction, but there is no description of generating explainable text in empirical works.\"","":""}
{"id":"2908069757","dialogue":"\"The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.\"","summary":"\"A considerable body of work is dedicated to automatic photo enhancement. However, it traditionally only focused on a specific subproblem, such as super-resolution, denoising, deblurring, or colorization. All of these subproblems are tackled simultaneously when we generate plausible high-quality photos from low-end ones. Furthermore, these older works commonly train with artifacts that have been artificially applied to the target image dataset. Recreating and simulating all the flaws in one camera given a picture from another is close to impossible, therefore in order to achieve real-world photo enhancement we use the photos simultaneously captured by a capture rig from Ignatov @cite_7 . Despite their limitations, the related works contain many useful ideas, which we briefly review in this section.\"","":""}
{"id":"2908069757","dialogue":"\"The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.\"","summary":"\"is the task of increasing the resolution of an image, which is usually trained with down-scaled versions of the target image as inputs. Many prior works have been dedicated to doing this using CNNs of progressively larger and more complex nature @cite_17 @cite_0 @cite_22 @cite_3 @cite_8 @cite_24 . Initially, a simple pixel-wise mean squared error (MSE) loss was often used to guarantee high fidelity of the reconstructed images, but this often led to blurry results due to uncertainty in pixel intensity space. Recent works @cite_25 aim at perceptual quality and employ losses based on VGG layers @cite_13 , and generative adversarial networks (GANs) @cite_11 @cite_16 , which seem to be well suited to generating plausible-looking, realistic high-frequency details.\"","":""}
{"id":"2908069757","dialogue":"\"The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.\"","summary":"\"In , the aim is to hallucinate color for each pixel, given only its luminosity. It is trained on images with their color artificially removed. Isola @cite_15 achieve state of the art performance using a GAN to solve the more general problem of image-to-image translation.\"","":""}
{"id":"2908069757","dialogue":"\"The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.\"","summary":"\"aim to remove optical distortions from photos that have been taken out of focus, while the camera was moving, or of faraway geographical or astronomical features. The neural models employed are CNNs, typically trained on images with artificially added blur or haze, using a MSE loss function @cite_26 @cite_23 @cite_5 @cite_14 @cite_9 . Recently, datasets with both hazy and haze-free images were introduced @cite_4 and solutions such as the one of Ki @cite_6 were proposed, which use a GAN, in addition to L1 and perceptual losses. Similar techniques are effective for as well @cite_27 @cite_18 @cite_19 @cite_10 .\"","":""}
{"id":"2908069757","dialogue":"\"The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.\"","summary":"\"The use of GANs has progressed towards the development of general purpose image-to-image translation. Isola @cite_15 propose a conditional GAN architecture for paired data, where the discriminator is conditioned on the input image. Zhu @cite_20 relax this requirement, introducing the cycle consistency loss which allows the GAN to train on unpaired data. These two approaches work on many surprising datasets, however, the image quality is too low for our purpose of photo-realistic image enhancement. This is why Ignatov introduce paired @cite_7 and unpaired @cite_12 GAN architectures that are specially designed for this purpose.\"","":""}
{"id":"2908069757","dialogue":"\"The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.\"","summary":"\"The DPED dataset @cite_7 consists of photos taken simultaneously by three different cell phone cameras, as well as a Canon 70D DSLR camera. In addition, these photographs are aligned and cut into 100x100 pixel patches, and compared such that patches that differ too much are rejected. In this work, only the iPhone 3GS data is considered. This results in 160k pairs of images.\"","":""}
{"id":"2800824532","dialogue":"\"The development of self-interference (SI) cancelation technology makes full-duplex (FD) communication possible. Considering the quality of service (QoS) of flows in small cells densely deployed scenario with limited time slot resources, this paper introduces the FD communication into the concurrent scheduling problem of millimeter-wave wireless backhaul network. We propose a QoS-aware FD concurrent scheduling algorithm to maximize the number of flows with their QoS requirements satisfied. Based on the contention graph, the algorithm makes full use of the FD condition. Both residual SI and multi-user interference are considered. Besides, it also fully considers the QoS requirements of flows and ensures the flows can be transmitted at high rates. Extensive simulations at 60 GHz demonstrate that with high SI cancelation level and appropriate contention threshold, the proposed FD algorithm can achieve superior performance in terms of the number of flows with their QoS requirements satisfied and the system throughput compared with other state-of-the-art schemes.\"","summary":"\"Compared with the serial TDMA scheme, concurrent transmission scheduling can significantly increase the system throughput, and thus has been extensively studied @cite_7 , @cite_0 - @cite_19 . Cai @cite_0 proposed a scheduling algorithm based on exclusive region to support concurrent transmissions. To maximize the number of flows scheduled in the network so that the QoS requirement of each flow is satisfied, Qiao . @cite_4 proposed a flip-based scheduling algorithm. In @cite_7 , Zhu proposed a Maximum QoS aware Independent Set (MQIS) based scheduling algorithm for mmWave backhaul networks to maximize the number of flows with their QoS requirements satisfied. In MQIS, the concurrent transmission and the QoS aware priority are exploited to achieve more successfully scheduled flows and higher network throughput. In @cite_18 , based on Stackelberg game, Li proposed a distributed transmission power control solution for the concurrent transmission scheduling between interference D2D links to further enhance the network throughput. Niu @cite_19 proposed an energy efficient scheduling scheme for the mmWave backhaul network, which exploits concurrent transmissions to achieve higher energy efficiency. However, all the above scheduling algorithms assume the devices are HD.\"","":""}
{"id":"2800824532","dialogue":"\"The development of self-interference (SI) cancelation technology makes full-duplex (FD) communication possible. Considering the quality of service (QoS) of flows in small cells densely deployed scenario with limited time slot resources, this paper introduces the FD communication into the concurrent scheduling problem of millimeter-wave wireless backhaul network. We propose a QoS-aware FD concurrent scheduling algorithm to maximize the number of flows with their QoS requirements satisfied. Based on the contention graph, the algorithm makes full use of the FD condition. Both residual SI and multi-user interference are considered. Besides, it also fully considers the QoS requirements of flows and ensures the flows can be transmitted at high rates. Extensive simulations at 60 GHz demonstrate that with high SI cancelation level and appropriate contention threshold, the proposed FD algorithm can achieve superior performance in terms of the number of flows with their QoS requirements satisfied and the system throughput compared with other state-of-the-art schemes.\"","summary":"\"Recently, the development of SI cancelation technology has made FD communication possible. Jain @cite_5 proposed the signal inversion and adaptive cancelation. Combining signal inversion cancelation with digital cancelation can reduce SI by up to 73dB. Everett @cite_8 showed the BS could exploit directional diversity by using directional antennas to achieve additional passive suppression of the SI. Besides, Miura @cite_3 proposed a novel node architecture introducing directional antennas into FD wireless technology. Rajagopal @cite_11 proved enabling backhaul transmission on one panel while simultaneously receiving backhaul on an adjacent panel is attainable for next generation backhaul designs. In @cite_12 , Xiao showed the configuration with separate Tx Rx antenna arrays appeared more flexible in SI suppression, and proposed the beamforming cancelation in FD mmWave communication.\"","":""}
{"id":"2800824532","dialogue":"\"The development of self-interference (SI) cancelation technology makes full-duplex (FD) communication possible. Considering the quality of service (QoS) of flows in small cells densely deployed scenario with limited time slot resources, this paper introduces the FD communication into the concurrent scheduling problem of millimeter-wave wireless backhaul network. We propose a QoS-aware FD concurrent scheduling algorithm to maximize the number of flows with their QoS requirements satisfied. Based on the contention graph, the algorithm makes full use of the FD condition. Both residual SI and multi-user interference are considered. Besides, it also fully considers the QoS requirements of flows and ensures the flows can be transmitted at high rates. Extensive simulations at 60 GHz demonstrate that with high SI cancelation level and appropriate contention threshold, the proposed FD algorithm can achieve superior performance in terms of the number of flows with their QoS requirements satisfied and the system throughput compared with other state-of-the-art schemes.\"","summary":"\"Considering the potential of the FD communication in increasing network performance, Feng @cite_10 proposed a design framework for 5G mmWave backhaul, which combined FD transmissions and hybrid beamforming with routing and scheduling schemes. However, the scheduling solution in @cite_10 was for the system with sufficient TS resources and aimed at accomplishing all of the transmissions with the minimum time. Thus, there was no special consideration for the QoS requirements of flows in limited time. Therefore, for mmWave backhaul networks with limited TS resources, a more QoS-favorable FD scheduling algorithm is needed.\"","":""}
{"id":"2906796011","dialogue":"\"We prove an invariance principle for a random Lorentz-gas particle in 3 dimensions under the Boltzmann-Grad limit and simultaneous diffusive scaling. That is, for the trajectory of a point-like particle moving among infinite-mass, hard-core, spherical scatterers of radius @math , placed according to a Poisson point process of density @math , in the limit @math , @math , @math up to time scales of order @math . To our knowledge this represents the first significant progress towards solving this problem in classical nonequilibrium statistical physics, since the groundbreaking work of Gallavotti (1970), Spohn (1978) and Boldrighini-Bunimovich-Sinai (1983). The novelty is that the diffusive scaling of particle trajectory and the kinetic (Boltzmann-Grad) limit are taken simulataneously. The main ingredients are a coupling of the mechanical trajectory with the Markovian random flight process, and probabilistic and geometric controls on the efficiency of this coupling.\"","summary":"\"In the case of infinite horizon (e.g. the plain @math arrangement of the spherical scatterers of diameter less than the lattice spacing) the free flight distribution of a particle flying in a uniformly sampled random direction has a heavy tail which causes a different type of long time behaviour of the particle displacement. The arguments of @cite_20 indicated that in the two-dimensional case super-diffusive scaling of order @math is expected. A central limit theorem with this anomalous scaling was proved with full rigour in @cite_7 , for the Lorentz-particle displacement in the @math -dimensional periodic case with infinite horizon. The periodic infinite horizon case in dimensions @math remains open.\"","":""}
{"id":"2906796011","dialogue":"\"We prove an invariance principle for a random Lorentz-gas particle in 3 dimensions under the Boltzmann-Grad limit and simultaneous diffusive scaling. That is, for the trajectory of a point-like particle moving among infinite-mass, hard-core, spherical scatterers of radius @math , placed according to a Poisson point process of density @math , in the limit @math , @math , @math up to time scales of order @math . To our knowledge this represents the first significant progress towards solving this problem in classical nonequilibrium statistical physics, since the groundbreaking work of Gallavotti (1970), Spohn (1978) and Boldrighini-Bunimovich-Sinai (1983). The novelty is that the diffusive scaling of particle trajectory and the kinetic (Boltzmann-Grad) limit are taken simulataneously. The main ingredients are a coupling of the mechanical trajectory with the Markovian random flight process, and probabilistic and geometric controls on the efficiency of this coupling.\"","summary":"\"In @cite_12 and @cite_14 it is proved that in the Boltzmann-Grad limit the trajectory of the Lorentz particle in any compact time interval @math with @math fixed, converges weakly to a non-Markovian flight process which has, however, a complete description in terms of a Markov chain of the successive collision impact parameters and, conditionally on this random sequence, independent flight lengths. (For a full description in these terms see @cite_21 .) As a second limit, an invariance principle is proved in @cite_21 for this non-Markovian random flight process, with superdiffusive scaling @math . Note that in this case the second limit doesn't just drop out from Donsker's theorem as it did in the random scatterer setting. The results of @cite_12 are valid in @math while those of @cite_14 and @cite_21 in arbitrary dimension.\"","":""}
{"id":"2891409106","dialogue":"\"The problem of entity-typing has been studied predominantly in supervised learning fashion","summary":"mostly with task-specific annotations (for coarse types) and sometimes with distant supervision (for fine types). While such approaches have strong performance within datasets","":""}
{"id":"2891409106","dialogue":"\"The problem of entity-typing has been studied predominantly in supervised learning fashion","summary":"mostly with task-specific annotations (for coarse types) and sometimes with distant supervision (for fine types). While such approaches have strong performance within datasets","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"The resource provisioning from cloud computing infrastructures using Spot Instances or similar mechanisms has been addressed profusely in the scientific literature in the last years @cite_10 . However, the vast majority of this work has been done from the users' perspective when using and consuming Spot Instances @cite_31 and few works tackle the problem from the resource provider standpoint.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"Due to the unpredictable nature of the Spot Instances, there are several research papers that try to improve the task completion time ---making the task resilient against termination--- and reduce the costs for the user. @cite_26 propose a probabilistic model to obtain the bid prices so that the costs and performance and reliability can be improved. In @cite_13 @cite_3 @cite_19 @cite_30 the task checkpointing is addressed so as to minimize costs and improve the whole completion time.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"Related with the previous works, have studied the usage of Spot Instances to deploy reliable virtual clusters @cite_17 @cite_37 , managing the allocated instances on behalf of the users. They focus on the execution of compute intensive tasks on top of a pool of Spot Instances, in order to find the most effective way to minimize both the execution time of a given workload and the price of the allocated resources. Similarly, in @cite_21 the autors develop a workflow scheduling scheme that reduces the completion time using Spot Instances.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"Regarding Big Data analysis, several authors have studied how the usage of Spot Instances could be used to execute MapReduce workloads reducing the monetary costs, such as in @cite_4 @cite_32 . The usage of Spot Instances for opportunistic computing is another usage that has awaken a lot of interest, especially regarding the design of an optimal bidding algorithm that would reduce the costs for the users @cite_15 @cite_20 . There are already existing applications such as the vCluster framework @cite_35 that can consume resources from heterogeneous cloud infrastructures in a fashion that could take advantage of the lower price that the Spot Instances should provide.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"@cite_22 delivered an implementation of preemptible instances for the Nimbus toolkit in order to utilize those instances for backfilling of idle resources, focusing on HTC fault-tolerant tasks. However, they did not focus on offering this functionality to the end-users, but rather to the operators of the infrastructure, as a way to maximize their resource utilization. In this work, it was the responsibility of the provider to configure the backfill tasks that were to be executed on the idle resources.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"Nadjaran have developed a Spot Instances as a Service (SIPaaS) framework, a set of web services that makes possible to run a Spot market on top of an OpenStack cloud @cite_24 . However, even if this framework aims to deliver preemptible instances on OpenStack cloud, it is designed to utilize normal resources to provide this functionality. SIPaaS utilizes normal resources to create the Spot market that is provided to the users by means of a thin layer on top of a given OpenStack, providing a different API to interact with the resources. From the CMF point of view, all resources are of the same type, being SIPaaS the responsible of handling them, in different ways. In contrast, our work leverages two different kind of instances at the CMF level, performing different scheduling strategies depending on which kind of resource it is being requested. SIPaaS also delivers a price market similar to the Amazon EC2 Spot Instances market, therefore they also provide the Ex-CORE auction algorithm @cite_5 in order to govern the price fluctuations.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"have proposed @cite_25 a capacity planning method combined with an admission service for IaaS cloud providers offering different service classes. This method allows providers to tackle the challenge of estimating the minimum capacity required to deliver an agreed Service Level Objective (SLO) across all the defined service classes. In the aforementioned paper lean on their previous work @cite_0 @cite_29 , where they proposed a way to reclaim unused cloud resources to offer a new class. This class, in contrast with the preemptible instances described here, still offer a SLO to the users, being the work on focused on the reduction of the changes that the SLO is violated due to an instance reclamation because of a capacity shortage.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"Generally speaking, existing Cloud Management Frameworks (CMFs) do not implement full-fledged queuing mechanism as other computing models do (like the Grid or traditional batch systems). Clouds are normally more focused on the rapid scaling of the resources rather than in batch processing, where systems are governed by queuing systems @cite_11 . The default scheduling strategies in the current CMFs are mostly based on the immediate allocation or resources following a fist-come, first-served basis. The cloud schedulers provision them when requested, or they are not provisioned at all (except in some CMFs that implement a FIFO queuing mechanism) @cite_33 .\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"However, some users require for a queuing system ---or some more advanced features like advance reservations--- for running virtual machines. In those cases, there are some external services such as Haizea @cite_34 for OpenNebula or Blazar https: launchpad.net blazar for OpenStack. Those systems lay between the CMF and the users, intercepting their requests and interacting with the cloud system on their behalf, implementing the required functionality.\"","":""}
{"id":"2906853528","dialogue":"\"Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.\"","summary":"\"Besides simplistic scheduling policies like first-fit or random chance node selection @cite_33 , current CMF implement a scheduling algorithm that is based on a rank selection of hosts, as we will explain in what follows:\"","":""}
{"id":"2950902462","dialogue":"\"Capturing the interesting components of an image is a key aspect of image understanding. When a speaker annotates an image, selecting labels that are informative greatly depends on the prior knowledge of a prospective listener. Motivated by cognitive theories of categorization and communication, we present a new unsupervised approach to model this prior knowledge and quantify the informativeness of a description. Specifically, we compute how knowledge of a label reduces uncertainty over the space of labels and utilize this to rank candidate labels for describing an image. While the full estimation problem is intractable, we describe an efficient algorithm to approximate entropy reduction using a tree-structured graphical model. We evaluate our approach on the open-images dataset using a new evaluation set of 10K ground-truth ratings and find that it achieves 65 agreement with human raters, largely outperforming other unsupervised baseline approaches.\"","summary":"\"* Image importance and object saliency. The problem of deciding which components in an image are important has been studied intensively. The main approaches involved identifying characteristics of objects and images that could contribute to importance, and use labeled data for predicting object importance. Elazary and Itti @cite_8 considered the order of object naming in the LabelMe dataset @cite_7 as a measure of the interest of an object and compare that to salient locations predicted by computational models of bottom-up attention. The elegant work of Spain and Perona @cite_18 examined which factors can predict the order in which objects will be mentioned given an image. @cite_19 characterized factors related to semantics, to composition and to the likelihood of attribute-object, and investigated how these affected the measures of importance. @cite_6 focused on predicting entry-level classes using a supervised approach. These studies also make it clear that the object saliency is strongly correlated with its perceived importance @cite_16 @cite_4 .\"","":""}
{"id":"2906539509","dialogue":"\"High bandwidth requirements are an obstacle for accelerating the training and inference of deep neural networks. Most previous research focuses on reducing the size of kernel maps for inference. We analyze parameter sparsity of six popular convolutional neural networks - AlexNet, MobileNet, ResNet-50, SqueezeNet, TinyNet, and VGG16. Of the networks considered, those using ReLU (AlexNet, SqueezeNet, VGG16) contain a high percentage of 0-valued parameters and can be statically pruned. Networks with Non-ReLU activation functions in some cases may not contain any 0-valued parameters (ResNet-50, TinyNet). We also investigate runtime feature map usage and find that input feature maps comprise the majority of bandwidth requirements when depth-wise convolution and point-wise convolutions used. We introduce dynamic runtime pruning of feature maps and show that 10 of dynamic feature map execution can be removed without loss of accuracy. We then extend dynamic pruning to allow for values within an epsilon of zero and show a further 5 reduction of feature map loading with a 1 loss of accuracy in top-1.\"","summary":"\"Rather, we look at all the feature maps and remove the maps that are dynamically determined not to be participating in the classification. Rhu @cite_10 recently described a compressing DMA engine (cDMA) that improved virtualized DNNs (vDNN) performance by 32 technique prunes by channel rather than elements. This benefits instruction set processors, particularly signal processors, because data can be easily loaded into the processor using sliding windows.\"","":""}
{"id":"2955034357","dialogue":"\"Download fraud is a prevalent threat in mobile App markets, where fraudsters manipulate the number of downloads of Apps via various cheating approaches. Purchased fake downloads can mislead recommendation and search algorithms and further lead to bad user experience in App markets. In this paper, we investigate download fraud problem based on a company's App Market, which is one of the most popular Android App markets. We release a honeypot App on the App Market and purchase fake downloads from fraudster agents to track fraud activities in the wild. Based on our interaction with the fraudsters, we categorize download fraud activities into three types according to their intentions: boosting front end downloads, optimizing App search ranking, and enhancing user acquisition&retention rate. For the download fraud aimed at optimizing App search ranking, we select, evaluate, and validate several features in identifying fake downloads based on billions of download data. To get a comprehensive understanding of download fraud, we further gather stances of App marketers, fraudster agencies, and market operators on download fraud. The followed analysis and suggestions shed light on the ways to mitigate download fraud in App markets and other social platforms. To the best of our knowledge, this is the first work that investigates the download fraud problem in mobile App markets.\"","summary":"\"Previous works have investigated various kinds of security issues in App markets. Chen @cite_14 and Rahman @cite_32 analyzed malware dissemination in Google Play. Zhu @cite_25 and Chen @cite_22 studied the suspicious Apps involved in search ranking in iOS App Store. @cite_15 delved crowdsourced spam reviews in both Google Play and iOS App Store. @cite_3 gave a longitudinal analysis of Apps in Google Play and provided suggestions on detecting search ranking fraud. According to @cite_31 , Google Play does not eliminate all fake downloads. Moreover, few previous works have investigated this problem either. It is mainly due to the lack of the ground truth of fraud activities. The data crawled from the front end, which has limited information, also hinders previous work for a comprehensive study on the download fraud. In this work, with server-side data and device vendor information as the ground truth, we could take a holistic approach to probe the download fraud in App market.\"","":""}
{"id":"2955034357","dialogue":"\"Download fraud is a prevalent threat in mobile App markets, where fraudsters manipulate the number of downloads of Apps via various cheating approaches. Purchased fake downloads can mislead recommendation and search algorithms and further lead to bad user experience in App markets. In this paper, we investigate download fraud problem based on a company's App Market, which is one of the most popular Android App markets. We release a honeypot App on the App Market and purchase fake downloads from fraudster agents to track fraud activities in the wild. Based on our interaction with the fraudsters, we categorize download fraud activities into three types according to their intentions: boosting front end downloads, optimizing App search ranking, and enhancing user acquisition&retention rate. For the download fraud aimed at optimizing App search ranking, we select, evaluate, and validate several features in identifying fake downloads based on billions of download data. To get a comprehensive understanding of download fraud, we further gather stances of App marketers, fraudster agencies, and market operators on download fraud. The followed analysis and suggestions shed light on the ways to mitigate download fraud in App markets and other social platforms. To the best of our knowledge, this is the first work that investigates the download fraud problem in mobile App markets.\"","summary":"\"The outcome of download fraud is similar to click fraud, which is a type of fraud that occurs in pay-per-click online advertising @cite_38 . Click fraudsters usually inject fake clicks to target URLs using click bots and steal money from advertisers. To detect click fraud, @cite_38 employed peer-to-peer measurements, command-and-control telemetry, and contemporaneous click data to analyze click fraud on botnets. @cite_28 devised various temporal and statistical patterns to detect click fraud in online advertising. @cite_35 leveraged behavior features and click patterns to detect spam URL sharing. The download fraud we investigated in this paper is more complicated than click fraud (i.e., mixed with human and bot activities). Inspired by the click fraud detection works mentioned above, we propose to model the download fraud activities in a multiview and feature-based perspective.\"","":""}
{"id":"2955034357","dialogue":"\"Download fraud is a prevalent threat in mobile App markets, where fraudsters manipulate the number of downloads of Apps via various cheating approaches. Purchased fake downloads can mislead recommendation and search algorithms and further lead to bad user experience in App markets. In this paper, we investigate download fraud problem based on a company's App Market, which is one of the most popular Android App markets. We release a honeypot App on the App Market and purchase fake downloads from fraudster agents to track fraud activities in the wild. Based on our interaction with the fraudsters, we categorize download fraud activities into three types according to their intentions: boosting front end downloads, optimizing App search ranking, and enhancing user acquisition&retention rate. For the download fraud aimed at optimizing App search ranking, we select, evaluate, and validate several features in identifying fake downloads based on billions of download data. To get a comprehensive understanding of download fraud, we further gather stances of App marketers, fraudster agencies, and market operators on download fraud. The followed analysis and suggestions shed light on the ways to mitigate download fraud in App markets and other social platforms. To the best of our knowledge, this is the first work that investigates the download fraud problem in mobile App markets.\"","summary":"\"For the black markets investigation, several works @cite_7 @cite_23 @cite_37 @cite_17 have probed the crowdsourcing websites and devised machine learning approaches to detect crowdturfing campaigns and crowd workers. Other works like @cite_4 inspected the transactions over trading App reviews and @cite_2 investigated the crowd fraud in Internet advertisement. However, seldom previous work has studied the black markets targeting download fraud. Like @cite_8 @cite_5 @cite_24 , we launch a honeypot App in App market to acquire reliable ground truth of download fraud activities. Moreover, we infiltrate into the black market and reap useful information from fraudsters to help our analysis.\"","":""}
{"id":"2950827124","dialogue":"\"This paper presents a \"\"learning to learn\"\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects","summary":"our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically","":""}
{"id":"2950827124","dialogue":"\"This paper presents a \"\"learning to learn\"\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects","summary":"our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically","":""}
{"id":"2950827124","dialogue":"\"This paper presents a \"\"learning to learn\"\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects","summary":"our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically","":""}
{"id":"2950827124","dialogue":"\"This paper presents a \"\"learning to learn\"\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects","summary":"our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically","":""}
{"id":"2950827124","dialogue":"\"This paper presents a \"\"learning to learn\"\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects","summary":"our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically","":""}
{"id":"2950827124","dialogue":"\"This paper presents a \"\"learning to learn\"\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects","summary":"our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically","":""}
{"id":"2953654207","dialogue":"\"In the recent years, convolutional neural networks have transformed the field of medical image analysis due to their capacity to learn discriminative image features for a variety of classification and regression tasks. However, successfully learning these features requires a large amount of manually annotated data, which is expensive to acquire and limited by the available resources of expert image analysts. Therefore, unsupervised, weakly-supervised and self-supervised feature learning techniques receive a lot of attention, which aim to utilise the vast amount of available data, while at the same time avoid or substantially reduce the effort of manual annotation. In this paper, we propose a novel way for training a cardiac MR image segmentation network, in which features are learnt in a self-supervised manner by predicting anatomical positions. The anatomical positions serve as a supervisory signal and do not require extra manual annotation. We demonstrate that this seemingly simple task provides a strong signal for feature learning and with self-supervised learning, we achieve a high segmentation accuracy that is better than or comparable to a U-net trained from scratch, especially at a small data setting. When only five annotated subjects are available, the proposed method improves the mean Dice metric from 0.811 to 0.852 for short-axis image segmentation, compared to the baseline U-net.\"","summary":"\"For natural image and video analysis problems, a number of pretext tasks have been explored, including prediction of image rotation @cite_8 , relative position @cite_6 , colorisation @cite_10 and image impainting @cite_3 etc. In medical imaging domain, self-supervised learning has also been explored but to a less extent. proposed a pretext task for subject identification @cite_4 . A Siamese network was trained to classify whether two spinal MR images came from the same subject or not. The pretrained features were used to initialise a disease grade classification network. defined re-colourisation of surgical videos as a pretext task and used the pretrained features to initialise a surgical instrument segmentation network @cite_11 . used rotation prediction as a pretext task and the self-learnt features were transferred to lung lobe segmentation and nodule detection tasks @cite_2 . Different from previous works in the medical imaging domain, we propose a novel pretext task, which is to predict anatomical positions. In particular, we leverage the rich information encoded in the cardiac MR scan view planes and DICOM headers to define the anatomical positions for the task.\"","":""}
{"id":"2883454930","dialogue":"\"Smartphone apps usually have access to sensitive user data such as contacts","summary":"geo-location","":""}
{"id":"2883454930","dialogue":"\"Smartphone apps usually have access to sensitive user data such as contacts","summary":"geo-location","":""}
{"id":"2883454930","dialogue":"\"Smartphone apps usually have access to sensitive user data such as contacts","summary":"geo-location","":""}
{"id":"2883454930","dialogue":"\"Smartphone apps usually have access to sensitive user data such as contacts","summary":"geo-location","":""}
{"id":"2883454930","dialogue":"\"Smartphone apps usually have access to sensitive user data such as contacts","summary":"geo-location","":""}
{"id":"2951007713","dialogue":"\"Highly Autonomous Driving (HAD) systems rely on deep neural networks for the visual perception of the driving environment. Such networks are trained on large manually annotated databases. In this work, a semi-parametric approach to one-shot learning is proposed, with the aim of bypassing the manual annotation step required for training perceptions systems used in autonomous driving. The proposed generative framework, coined Generative One-Shot Learning (GOL), takes as input single one-shot objects, or generic patterns, and a small set of so-called regularization samples used to drive the generative process. New synthetic data is generated as Pareto optimal solutions from one-shot objects using a set of generalization functions built into a generalization generator. GOL has been evaluated on environment perception challenges encountered in autonomous vision.\"","summary":"\"OpenAI published an interesting robotics application of one-shot learning, described in @cite_1 . Their algorithm, named , uses a meta-learning framework for training robotic systems in performing certain tasks based only on a couple of demonstrations. Also, the large amount of data required by Deep Reinforcement Learning systems has been approached in @cite_24 through the introduction of their algorithm.\"","":""}
{"id":"2951007713","dialogue":"\"Highly Autonomous Driving (HAD) systems rely on deep neural networks for the visual perception of the driving environment. Such networks are trained on large manually annotated databases. In this work, a semi-parametric approach to one-shot learning is proposed, with the aim of bypassing the manual annotation step required for training perceptions systems used in autonomous driving. The proposed generative framework, coined Generative One-Shot Learning (GOL), takes as input single one-shot objects, or generic patterns, and a small set of so-called regularization samples used to drive the generative process. New synthetic data is generated as Pareto optimal solutions from one-shot objects using a set of generalization functions built into a generalization generator. GOL has been evaluated on environment perception challenges encountered in autonomous vision.\"","summary":"\"One of the most influential work on GOL is the research on (GAN) @cite_27 . The major difference between GOL and the adversarial nets is that, within GOL, the generation of synthetic information is performed based on the generalization functions which generate new data from a one-shot object, thus making GOL a pure one-shot learning framework.\"","":""}
{"id":"2950720513","dialogue":"\"This paper presents a novel deep learning framework for human trajectory prediction and detecting social group membership in crowds. We introduce a generative adversarial pipeline which preserves the spatio-temporal structure of the pedestrian's neighbourhood, enabling us to extract relevant attributes describing their social identity. We formulate the group detection task as an unsupervised learning problem, obviating the need for supervised learning of group memberships via hand labeled databases, allowing us to directly employ the proposed framework in different surveillance settings. We evaluate the proposed trajectory prediction and group detection frameworks on multiple public benchmarks, and for both tasks the proposed method demonstrates its capability to better anticipate human sociological behaviour compared to the existing state-of-the-art methods.\"","summary":"\"One of the most popular deep learning methods is the social LSTM @cite_24 model which represents the pedestrians in the local neighbourhood using LSTMs and then generates their future trajectory by systematically pooling the relavant information. This removes the need for handcrafted features and learns the required feature vectors automatically through the encoded trajectory representation. This architecture is further augmented in @cite_6 where the authors propose a more efficient method to embed the local neighbourhood information via a soft and hardwired attention framework. They demonstrate the importance of fully capturing the context information, which includes the short-term history of the pedestrian of interest as well as their neighbours.\"","":""}
{"id":"2904303242","dialogue":"\"Mode collapse is one of the key challenges in the training of Generative Adversarial Networks(GANs). Previous approaches have tried to address this challenge either by changing the loss of GANs, or by modifying optimization strategies. We argue that it is more desirable if we can find the underlying structure of real data and build a structured generative model to further get rid of mode collapse. To this end, we propose Latent Dirichlet Allocation based Generative Adversarial Networks (LDAGAN), which have high capacity of modeling complex image data. Moreover, we optimize our model by combing variational expectation-maximization (EM) algorithm with adversarial learning. Stochastic optimization strategy ensures the training process of LDAGAN is not time consuming. Experimental results demonstrate our method outperforms the existing standard CNN based GANs on the task of image generation.\"","summary":"\"Although mixture GANs seem to further get rid of problems like mode collapse, they exhibit two drawbacks. Firstly, mixing weights ( mode distribution) @math in mixture GANs, for example in MGAN and MixGAN @cite_21 @cite_12 , are fixed. Such a fixed mixing scheme limits the flexibility of model distribution @math , probably leading to an undesired large divergence between real and model distributions. Besides, @math sometimes is predefined, such as in @cite_21 , which constraints the model capacity, and thus causes mode dropping. Secondly, some mixture GANs @cite_21 plausibly encourage mode diversity of generated samples, resulting in intra-class mode dropping.\"","":""}
{"id":"2904130053","dialogue":"\"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e. non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our proposed construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks with two hidden layers, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.\"","summary":"\"We discuss related work on suboptimal minima of the loss surface. In addition, we refer the reader to the overview article @cite_9 for a discussion on the non-convexity in neural network training.\"","":""}
{"id":"2904130053","dialogue":"\"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e. non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our proposed construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks with two hidden layers, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.\"","summary":"\"It is known that learning the parameters of neural networks is, in general, a hard problem. Blum and Rivest @cite_0 prove NP-completeness for a specific neural network. It has also been shown that local minima and other critical points exist in the loss function of neural network training (see e.g. @cite_36 @cite_23 @cite_16 @cite_7 @cite_28 @cite_26 ). The understanding of these critical points has led to significant improvements in neural network training. This includes weight initialization techniques (e.g. @cite_6 ), improved backpropagation algorithms to avoid saturation effects in neurons @cite_29 , entirely new activation functions, or the use of second order information @cite_34 @cite_18 .\"","":""}
{"id":"2904130053","dialogue":"\"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e. non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our proposed construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks with two hidden layers, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.\"","summary":"\"That suboptimal local minima must become rather degenerate if the neural network becomes sufficiently large was observed for networks with one hidden layer in @cite_5 and @cite_13 . Recently, Nguyen and Hein @cite_15 generalized this result to deeper networks containing an extremely wide hidden layer. Our contribution can be considered as a continuation of this work.\"","":""}
{"id":"2904130053","dialogue":"\"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e. non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our proposed construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks with two hidden layers, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.\"","summary":"\"To gain better insight into theoretical aspects, some papers consider linear networks, where the activation function is the identity. The classic result by Baldi and Hornik @cite_32 shows that linear two-layer neural networks have a unique global minimum and all other critical values are saddle points. Kawaguchi, @cite_37 , Lu and Kawaguchi @cite_3 and @cite_4 discuss generalizations of @cite_32 to deep linear networks.\"","":""}
{"id":"2904130053","dialogue":"\"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e. non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our proposed construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks with two hidden layers, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.\"","summary":"\"Finally, worth mentioning is the study of Liao and Poggio @cite_27 who use polynomial approximations to argue, by relying on Bezout's theorem, that the loss function should have many local minima with zero empirical loss. Also relevant is the observation by @cite_38 showing that, if the global minimum is not of zero loss, then a perfect predictor may have a larger loss in training than one producing worse classification results.\"","":""}
{"id":"2904146064","dialogue":"\"[Context.] The success of deep learning makes its usage more and more tempting in safety-critical applications. However such applications have historical standards (e.g., DO178, ISO26262) which typically do not envision the usage of machine learning. We focus in particular on of software artifacts, i.e., code modules, functions, or statements (depending on the desired granularity). [Problem.] Both code and requirements are a problem when dealing with deep neural networks: code constituting the network is not comparable to classical code; furthermore, requirements for applications where neural networks are required are typically very hard to specify: even though high-level requirements can be defined, it is very hard to make such requirements concrete enough, that one can qualify them of low-level requirements. An additional problem is that deep learning is in practice very much based on trial-and-error, which makes the final result hard to explain without the previous iterations. [Proposed solution.] We investigate which artifacts could play a similar role to code or low-level requirements in neural network development and propose various traces which one could possibly consider as a replacement for classical notions. We also propose a form of traceability (and new artifacts) in order to deal with the particular trial-and-error development process for deep learning.\"","summary":"\"In general, the safety of DNNs is commonly recognized as a huge challenge @cite_1 . There are more and more attempts towards the certification, verification, or explainability of DNNs, of which we provide now a short overview. None of them however (and, as far as we know, no other work either) addresses the traceability of DNNs.\"","":""}
{"id":"2904146064","dialogue":"\"[Context.] The success of deep learning makes its usage more and more tempting in safety-critical applications. However such applications have historical standards (e.g., DO178, ISO26262) which typically do not envision the usage of machine learning. We focus in particular on of software artifacts, i.e., code modules, functions, or statements (depending on the desired granularity). [Problem.] Both code and requirements are a problem when dealing with deep neural networks: code constituting the network is not comparable to classical code; furthermore, requirements for applications where neural networks are required are typically very hard to specify: even though high-level requirements can be defined, it is very hard to make such requirements concrete enough, that one can qualify them of low-level requirements. An additional problem is that deep learning is in practice very much based on trial-and-error, which makes the final result hard to explain without the previous iterations. [Proposed solution.] We investigate which artifacts could play a similar role to code or low-level requirements in neural network development and propose various traces which one could possibly consider as a replacement for classical notions. We also propose a form of traceability (and new artifacts) in order to deal with the particular trial-and-error development process for deep learning.\"","summary":"\"There has been attempts to apply principles of software engineering (or even of engineering in general) to NNs @cite_14 . It particularly attempts to address the lack of reproducibility in the development of NNs. Even though the terminology and techniques have changed a lot since 2004, the identified problems are still relevant. Still, the solutions of the paper do not answer the need for traceability and seem to hardly match nowadays' practice.\"","":""}
{"id":"2904146064","dialogue":"\"[Context.] The success of deep learning makes its usage more and more tempting in safety-critical applications. However such applications have historical standards (e.g., DO178, ISO26262) which typically do not envision the usage of machine learning. We focus in particular on of software artifacts, i.e., code modules, functions, or statements (depending on the desired granularity). [Problem.] Both code and requirements are a problem when dealing with deep neural networks: code constituting the network is not comparable to classical code; furthermore, requirements for applications where neural networks are required are typically very hard to specify: even though high-level requirements can be defined, it is very hard to make such requirements concrete enough, that one can qualify them of low-level requirements. An additional problem is that deep learning is in practice very much based on trial-and-error, which makes the final result hard to explain without the previous iterations. [Proposed solution.] We investigate which artifacts could play a similar role to code or low-level requirements in neural network development and propose various traces which one could possibly consider as a replacement for classical notions. We also propose a form of traceability (and new artifacts) in order to deal with the particular trial-and-error development process for deep learning.\"","summary":"The discrepancy between the recommendations of the ISO 26262 and the methods actually used in practice was analyzed in @cite_2 . This cannot be directly used for traceability but is indirectly a very useful source of information.","":""}
{"id":"2904146064","dialogue":"\"[Context.] The success of deep learning makes its usage more and more tempting in safety-critical applications. However such applications have historical standards (e.g., DO178, ISO26262) which typically do not envision the usage of machine learning. We focus in particular on of software artifacts, i.e., code modules, functions, or statements (depending on the desired granularity). [Problem.] Both code and requirements are a problem when dealing with deep neural networks: code constituting the network is not comparable to classical code; furthermore, requirements for applications where neural networks are required are typically very hard to specify: even though high-level requirements can be defined, it is very hard to make such requirements concrete enough, that one can qualify them of low-level requirements. An additional problem is that deep learning is in practice very much based on trial-and-error, which makes the final result hard to explain without the previous iterations. [Proposed solution.] We investigate which artifacts could play a similar role to code or low-level requirements in neural network development and propose various traces which one could possibly consider as a replacement for classical notions. We also propose a form of traceability (and new artifacts) in order to deal with the particular trial-and-error development process for deep learning.\"","summary":"\"There has been attempts to set grounds for a rigorous science'' of machine learning @cite_13 , again very useful, but not related to traceability. Finally, many safety-related problems have been identified for AI, especially for reinforcement learning @cite_31 . The identified challenges are relevant and the paper proposes a few attempts of solutions. Most of them are a source of inspiration to identify sources of problem and to analyze whether those sources can be tackled with traceability (even though it turns out not to be really the case for solutions identified in the present paper).\"","":""}
{"id":"2954739441","dialogue":"\"Firms implementing digital advertising campaigns face a complex problem in determining the right match between their advertising creatives and target audiences. Typical solutions to the problem have leveraged non-experimental methods","summary":"or used \"\"split-testing\"\" strategies that have not explicitly addressed the complexities induced by targeted audiences that can potentially overlap with one another. This paper presents an adaptive algorithm that addresses the problem via online experimentation. The algorithm is set up as a contextual bandit and addresses the overlap issue by partitioning the target audiences into disjoint","":""}
{"id":"2954302879","dialogue":"\"We address the problem of unsupervised domain adaptation (UDA) by learning a cross-domain agnostic embedding space, where the distance between the probability distributions of the two source and target visual domains is minimized. We use the output space of a shared cross-domain deep encoder to model the embedding space anduse the Sliced-Wasserstein Distance (SWD) to measure and minimize the distance between the embedded distributions of two source and target domains to enforce the embedding to be domain-agnostic.Additionally, we use the source domain labeled data to train a deep classifier from the embedding space to the label space to enforce the embedding space to be this http URL a result of this training scheme, we provide an effective solution to train the deep classification network on the source domain such that it will generalize well on the target domain, where only unlabeled training data is accessible. To mitigate the challenge of class matching, we also align corresponding classes in the embedding space by using high confidence pseudo-labels for the target domain, i.e. assigning the class for which the source classifier has a high prediction probability. We provide theoretical justification as well as experimental results on UDA benchmark tasks to demonstrate that our method is effective and leads to state-of-the-art performance.\"","summary":"\"There are two major approaches in the literature to address domain adaption. The approach for a group of methods is based on preprocessing the target domain data points. The target data is mapped from the target domain to the source domain such that the target data structure is preserved in the source @cite_26 . Another common approach is to map data from both domains to a latent domain invariant space @cite_29 . Early methods within the second approach learn a linear subspace as the invariant space @cite_21 @cite_37 where the target domain data points distribute similar to the source domain data points. A linear subspace is not suitable for capturing complex distributions. For this reason, recently deep neural networks have been used to model the intermediate space as the output of the network. The network is trained such that the source and the target domain distributions in its output possess minimal discrepancy. Training procedure can be done both by adversarial learning @cite_20 or directly minimizing the distance between the two distributions @cite_27 .\"","":""}
{"id":"2954302879","dialogue":"\"We address the problem of unsupervised domain adaptation (UDA) by learning a cross-domain agnostic embedding space, where the distance between the probability distributions of the two source and target visual domains is minimized. We use the output space of a shared cross-domain deep encoder to model the embedding space anduse the Sliced-Wasserstein Distance (SWD) to measure and minimize the distance between the embedded distributions of two source and target domains to enforce the embedding to be domain-agnostic.Additionally, we use the source domain labeled data to train a deep classifier from the embedding space to the label space to enforce the embedding space to be this http URL a result of this training scheme, we provide an effective solution to train the deep classification network on the source domain such that it will generalize well on the target domain, where only unlabeled training data is accessible. To mitigate the challenge of class matching, we also align corresponding classes in the embedding space by using high confidence pseudo-labels for the target domain, i.e. assigning the class for which the source classifier has a high prediction probability. We provide theoretical justification as well as experimental results on UDA benchmark tasks to demonstrate that our method is effective and leads to state-of-the-art performance.\"","summary":"\"Several important UDA methods use adversarial learning. @cite_30 pioneered and developed an effective method to match two distributions indirectly by using adversarial learning. @cite_22 and @cite_5 use the Generative Adversarial Networks (GAN) structure @cite_20 to tackle domain adaptation. The idea is to train two competing (i.e., adversarial) deep neural networks to match the source and the target distributions. A generator network maps data points from both domains to the domain-invariant space and a binary discriminator network is trained to classify the data points, with each domain considered as a class, based on the representations of the target and the source data points. The generator network is trained such that eventually the discriminator cannot distinguish between the two domains, i.e. classification rate becomes $50\"","":""}
{"id":"2954302879","dialogue":"\"We address the problem of unsupervised domain adaptation (UDA) by learning a cross-domain agnostic embedding space, where the distance between the probability distributions of the two source and target visual domains is minimized. We use the output space of a shared cross-domain deep encoder to model the embedding space anduse the Sliced-Wasserstein Distance (SWD) to measure and minimize the distance between the embedded distributions of two source and target domains to enforce the embedding to be domain-agnostic.Additionally, we use the source domain labeled data to train a deep classifier from the embedding space to the label space to enforce the embedding space to be this http URL a result of this training scheme, we provide an effective solution to train the deep classification network on the source domain such that it will generalize well on the target domain, where only unlabeled training data is accessible. To mitigate the challenge of class matching, we also align corresponding classes in the embedding space by using high confidence pseudo-labels for the target domain, i.e. assigning the class for which the source classifier has a high prediction probability. We provide theoretical justification as well as experimental results on UDA benchmark tasks to demonstrate that our method is effective and leads to state-of-the-art performance.\"","summary":"\"As Wasserstein distance is finding more applications in deep learning, efficient computation of Wasserstein distance has become an active area of research. The reason is that Wasserstein distance is defined in form of a linear programming optimization and solving this optimization problem is computationally expensive for high-dimensional data. Although computationally efficient variations and approximations of the Wasserstein distance have been recently proposed @cite_34 @cite_39 @cite_3 , these variations still require an additional optimization in each iteration of the stochastic gradient descent (SGD) steps to match distributions. @cite_27 used a regularized version of the optimal transport for domain adaptation. @cite_13 used a dual stochastic gradient algorithm for solving the regularized optimal transport problem. Alternatively, we propose to address the above challenges using Sliced Wasserstein Distance (SWD). Definition of SWD is motivated by the fact that in contrast to higher dimensions, the Wasserstein distance for one-dimensional distributions has a closed form solution which can be computed efficiently. This fact is used to approximate Wasserstein distance by SWD, which is a computationally efficient approximation and has recently drawn interest from the machine learning and computer vision communities @cite_28 @cite_35 @cite_32 @cite_41 @cite_42 .\"","":""}
{"id":"2954514064","dialogue":"\"Discovering communities in complex networks means grouping nodes similar to each other","summary":"to uncover latent information about them. There are hundreds of different algorithms to solve the community detection task","":""}
{"id":"2954514064","dialogue":"\"Discovering communities in complex networks means grouping nodes similar to each other","summary":"to uncover latent information about them. There are hundreds of different algorithms to solve the community detection task","":""}
{"id":"2954514064","dialogue":"\"Discovering communities in complex networks means grouping nodes similar to each other","summary":"to uncover latent information about them. There are hundreds of different algorithms to solve the community detection task","":""}
{"id":"2953622591","dialogue":"\"We present LumiereNet, a simple, modular, and completely deep-learning based architecture that synthesizes, high quality, full-pose headshot lecture videos from instructor's new audio narration of any length. Unlike prior works, LumiereNet is entirely composed of trainable neural network modules to learn mapping functions from the audio to video through (intermediate) estimated pose-based compact and abstract latent codes. Our video demos are available at [22] and [23].\"","summary":"\"* Visual speech synthesis Over the last two decades, there has been extensive study dedicated towards creating realistic animations for speech @cite_27 in 2D or 3D. 2D has the advantage that video cutouts of the mouth area can be used and combined leading to realistic visualizations @cite_0 @cite_19 . 3D approaches are much more versatile, as viewpoints and illumination can be changed at will @cite_16 . Given that our goal is to produce 2D animation based on audio, instead of formulating entire mapping as an end-to-end optimization task, we are inherently interested in the intermediate representations. Recent advances in this line were more or less focused on synthesizing only the parts of the face (around the mouth) and borrowing the rest of the subject from existing video footage @cite_9 @cite_8 @cite_24 .\"","":""}
{"id":"2953622591","dialogue":"\"We present LumiereNet, a simple, modular, and completely deep-learning based architecture that synthesizes, high quality, full-pose headshot lecture videos from instructor's new audio narration of any length. Unlike prior works, LumiereNet is entirely composed of trainable neural network modules to learn mapping functions from the audio to video through (intermediate) estimated pose-based compact and abstract latent codes. Our video demos are available at [22] and [23].\"","summary":"\"* Human pose estimation Human pose estimation is a general problem in computer vision to detect human figures in images and video. Recent deep-learning based algorithmic advances enable not only the detection and localization of major body points, but detailed surface-based human body representation (e.g.,OpenPose @cite_20 and DensePose @cite_22 ). We use a pre-trained DensePose estimator to create body figure RGB images from video frames.\"","":""}
{"id":"2953622591","dialogue":"\"We present LumiereNet, a simple, modular, and completely deep-learning based architecture that synthesizes, high quality, full-pose headshot lecture videos from instructor's new audio narration of any length. Unlike prior works, LumiereNet is entirely composed of trainable neural network modules to learn mapping functions from the audio to video through (intermediate) estimated pose-based compact and abstract latent codes. Our video demos are available at [22] and [23].\"","summary":"\"* Image-to-Image translation Several recent frameworks have used generative adversarial networks (GANs) @cite_10 to learn a parametric translation function between input and output images @cite_1 . Similar ideas have been applied to various tasks, such as generating photographs from sketches, or even video applications for both paired and unpaired cases @cite_21 @cite_6 @cite_5 @cite_30 . However, none of these techniques are fool-proof, and some amount of limitations often remain.\"","":""}
{"id":"2953137836","dialogue":"\"Since the labelling for the positive images videos is ambiguous in weakly supervised segment annotation, negative mining based methods that only use the intra-class information emerge. In these methods, negative instances are utilized to penalize unknown instances to rank their likelihood of being an object, which can be considered as a voting in terms of similarity. However, these methods 1) ignore the information contained in positive bags, 2) only rank the likelihood but cannot generate an explicit decision function. In this paper, we propose a voting scheme involving not only the definite negative instances but also the ambiguous positive instances to make use of the extra useful information in the weakly labelled positive bags. In the scheme, each instance votes for its label with a magnitude arising from the similarity, and the ambiguous positive instances are assigned soft labels that are iteratively updated during the voting. It overcomes the limitations of voting using only the negative bags. We also propose an expectation kernel density estimation (eKDE) algorithm to gain further insight into the voting mechanism. Experimental results demonstrate the superiority of our scheme beyond the baselines.\"","summary":"\"Negative mining methods train a classifier based on the strongly labelled negative training data. For each instance in a positive bag, based on the inter-class information, NegMin @cite_30 compute their similarities with all of the negative instances, and select the instance that has minimum max-similarity as of interest. CRANE @cite_33 selects negative instances to vote against an unknown instance by specifying some similarity threshold, and improves the robustness of labelling noise among negative instances. @cite_22 also make use of the similarity information as a pre-processing heuristic for a bag-level classification. They select instances with least similarity to the negative bags and use them to initialize cluster centers, which are then used to create the bag level feature descriptors of @cite_23 . Moreover, Jiang @cite_20 trains a one-class SVM based on negative instances, then ranks the saliency according to the distances to the decision boundary.\"","":""}
{"id":"2953137836","dialogue":"\"Since the labelling for the positive images videos is ambiguous in weakly supervised segment annotation, negative mining based methods that only use the intra-class information emerge. In these methods, negative instances are utilized to penalize unknown instances to rank their likelihood of being an object, which can be considered as a voting in terms of similarity. However, these methods 1) ignore the information contained in positive bags, 2) only rank the likelihood but cannot generate an explicit decision function. In this paper, we propose a voting scheme involving not only the definite negative instances but also the ambiguous positive instances to make use of the extra useful information in the weakly labelled positive bags. In the scheme, each instance votes for its label with a magnitude arising from the similarity, and the ambiguous positive instances are assigned soft labels that are iteratively updated during the voting. It overcomes the limitations of voting using only the negative bags. We also propose an expectation kernel density estimation (eKDE) algorithm to gain further insight into the voting mechanism. Experimental results demonstrate the superiority of our scheme beyond the baselines.\"","summary":"\"Besides using the inter-class information, key instance detection can be accomplished by searching similar patterns among diverse positive bags. The most classical framework is diverse density (DD) @cite_8 . It defines a conditional probability with similarity, and uses the noisy-or model to define a diverse density to select instances with high similarities to diverse positive bags and low similarities to negative bags. DD has been widely used as a basis for many methods including EM-DD @cite_18 , GEM-DD @cite_12 and DD-SVM @cite_4 . However, DD is sensitive to labelling noise. Evidence confidence @cite_3 is proposed to seek the mode on observed instances rather than in a continuous space to facilitate the computation and alleviate the sensitivity. @cite_17 exploit similarity among class-specific features to decide prototypes, which are used in a voting-based mechanism to select instances with a high diverse occurrence.\"","":""}
{"id":"2953137836","dialogue":"\"Since the labelling for the positive images videos is ambiguous in weakly supervised segment annotation, negative mining based methods that only use the intra-class information emerge. In these methods, negative instances are utilized to penalize unknown instances to rank their likelihood of being an object, which can be considered as a voting in terms of similarity. However, these methods 1) ignore the information contained in positive bags, 2) only rank the likelihood but cannot generate an explicit decision function. In this paper, we propose a voting scheme involving not only the definite negative instances but also the ambiguous positive instances to make use of the extra useful information in the weakly labelled positive bags. In the scheme, each instance votes for its label with a magnitude arising from the similarity, and the ambiguous positive instances are assigned soft labels that are iteratively updated during the voting. It overcomes the limitations of voting using only the negative bags. We also propose an expectation kernel density estimation (eKDE) algorithm to gain further insight into the voting mechanism. Experimental results demonstrate the superiority of our scheme beyond the baselines.\"","summary":"\"Since we derive a KDE interpretation for our voting scheme, we also make a literature review on this subject. KDE possesses the advantages of nonparametric method for unsupervised density estimation. @cite_35 propose a supervised KDE to make use of labels, and extend the mean shift @cite_32 to a supervised version to seek modes. In order to make full use of unlabelled data, @cite_36 @cite_21 propose a semi-supervised KDE to estimate class-specific density based on a little fraction of labelled data. SSKDE is later extended to a manifold structure @cite_5 .\"","":""}
{"id":"2953137836","dialogue":"\"Since the labelling for the positive images videos is ambiguous in weakly supervised segment annotation, negative mining based methods that only use the intra-class information emerge. In these methods, negative instances are utilized to penalize unknown instances to rank their likelihood of being an object, which can be considered as a voting in terms of similarity. However, these methods 1) ignore the information contained in positive bags, 2) only rank the likelihood but cannot generate an explicit decision function. In this paper, we propose a voting scheme involving not only the definite negative instances but also the ambiguous positive instances to make use of the extra useful information in the weakly labelled positive bags. In the scheme, each instance votes for its label with a magnitude arising from the similarity, and the ambiguous positive instances are assigned soft labels that are iteratively updated during the voting. It overcomes the limitations of voting using only the negative bags. We also propose an expectation kernel density estimation (eKDE) algorithm to gain further insight into the voting mechanism. Experimental results demonstrate the superiority of our scheme beyond the baselines.\"","summary":"\"Shallow learning methods have been outperformed by deep convolutional neural networks (DCNN) significantly on the visual recognition tasks resulting from their powerful feature representation @cite_24 . One approach to boosting the performance of shallow methods is using deep features from pre-trained DCNN models. R-CNN @cite_28 combined SVM with DCNN features to boost the object detection performance. DCNN features have also been incorporated into weakly supervised visual recognition tasks. @cite_7 concatenate multiple convolutional outputs and max-pool them to represent the super-pixel features. Observing that a region probably belongs to an object if many channels of the hidden-layer activation fire simultaneously, @cite_27 select the object regions using aggregation map, then max-pool the concatenation of multiple-layer activations to represent the image. Similarly, based on the findings that the hidden-layer activations of a pre-trained object recognition network usually fire up on objects rather than background, @cite_2 leverage these masks for weakly supervised semantic segmentation.\"","":""}
{"id":"2954062199","dialogue":"\"We propose a novel approach for generating region proposals for performing face-detection. Instead of classifying anchor boxes using features from a pixel in the convolutional feature map, we adopt a pooling-based approach for generating region proposals. However, pooling hundreds of thousands of anchors which are evaluated for generating proposals becomes a computational bottleneck during inference. To this end, an efficient anchor placement strategy for reducing the number of anchor-boxes is proposed. We then show that proposals generated by our network (Floating Anchor Region Proposal Network, FA-RPN) are better than RPN for generating region proposals for face detection. We discuss several beneficial features of FA-RPN proposals like iterative refinement, placement of fractional anchors and changing anchors which can be enabled without making any changes to the trained model. Our face detector based on FA-RPN obtains 89.4 mAP with a ResNet-50 backbone on the WIDER dataset.\"","summary":"\"Generating class agnostic region proposals has been investigated in computer vision for more than a decade. Initial methods include multi-scale combinatorial grouping @cite_6 , constrained parametric min-cuts @cite_21 , selective search @cite_16 . These methods generate region proposals which obtain high recall for objects in a category agnostic fashion. They were also very successful in the pre-deep learning era and obtained state-of-the-art performance even with a bag-of-words model @cite_21 . Using region proposals based on selective search @cite_21 , R-CNN @cite_26 was the first deep learning based detector. Unsupervised region proposals were also used in later detectors like Fast-RCNN @cite_36 but since the Faster-RCNN detector @cite_2 generated region proposals using a convolutional neural network, it has become the de-facto algorithm for generating region proposals.\"","":""}
{"id":"2954062199","dialogue":"\"We propose a novel approach for generating region proposals for performing face-detection. Instead of classifying anchor boxes using features from a pixel in the convolutional feature map, we adopt a pooling-based approach for generating region proposals. However, pooling hundreds of thousands of anchors which are evaluated for generating proposals becomes a computational bottleneck during inference. To this end, an efficient anchor placement strategy for reducing the number of anchor-boxes is proposed. We then show that proposals generated by our network (Floating Anchor Region Proposal Network, FA-RPN) are better than RPN for generating region proposals for face detection. We discuss several beneficial features of FA-RPN proposals like iterative refinement, placement of fractional anchors and changing anchors which can be enabled without making any changes to the trained model. Our face detector based on FA-RPN obtains 89.4 mAP with a ResNet-50 backbone on the WIDER dataset.\"","summary":"\"To improve RPN, several modifications have been proposed. State-of-the-art detectors can also detect objects in a single step. Detectors like SSH @cite_24 , SSD @cite_33 , RetinaNet @cite_7 , MS-CNN @cite_31 generate multi-scale feature maps to classify and regress anchors placed on these feature-maps. These single-shot detectors are closely related to the region proposal network as they have specific filters to detect objects of different sizes and aspect ratios but also combine feature-maps from multiple layers of the deep neural network. No further refinement is performed after the initial offsets generated by the network are applied. Another class of detectors are iterative, like G-CNN @cite_4 , Cascade-RCNN @cite_17 , LocNet @cite_1 , FPN @cite_28 , RFCN-3000 @cite_14 , Faster-RCNN @cite_2 . These detectors refine a pre-defined set of anchor-boxes in multiple stages and have more layers to further improve classification and localization of regressed anchors. One should note that even in these networks, the first stage comprises of the region proposal network which eliminates the major chunk of background regions. FA-RPN is closer to this line of work but, in contrast, it supports iterative refinement of region proposals during inference.\"","":""}
{"id":"2953574061","dialogue":"\"Being able to check whether an online advertisement has been targeted is essential for resolving privacy controversies and implementing in practice data protection regulations like GDPR, CCPA, and COPPA. In this paper we describe the design, implementation, and deployment of an advertisement auditing system called iWnder that uses crowdsourcing to reveal in real time whether a display advertisement has been targeted or not. Crowdsourcing simplifies the detection of targeted advertising, but requires reporting to a central repository the impressions seen by different users, thereby jeopardising their privacy. We break this deadlock with a privacy preserving data sharing protocol that allows iWnder to compute global statistics required to detect targeting, while keeping the advertisements seen by individual users and their browsing history private. We conduct a simulation study to explore the effect of different parameters and a live validation to demonstrate the accuracy of our approach. Unlike previous solutions, iWnder can even detect indirect targeting, i.e., marketing campaigns that promote a product or service whose description bears no semantic overlap with its targeted audience.\"","summary":"\"Topic-based solutions perform content-based analysis to extract the relevant topics on a user's browsing history and the ads he receives. Then, using different heuristics and statistical means, targeted ads are identified as those having topics that share some semantic overlap with the user's browsing history. Topic-based detection could, in principle, be applied to real users, as we have done for evaluation purposes in . Existing work, however, has only used it in conjunction with artificially constructed , , robots that browse the web imitating very specific (single-topic) demographic groups @cite_14 @cite_17 , or to emulate real-users offline using click-streams @cite_5 .\"","":""}
{"id":"2953574061","dialogue":"\"Being able to check whether an online advertisement has been targeted is essential for resolving privacy controversies and implementing in practice data protection regulations like GDPR, CCPA, and COPPA. In this paper we describe the design, implementation, and deployment of an advertisement auditing system called iWnder that uses crowdsourcing to reveal in real time whether a display advertisement has been targeted or not. Crowdsourcing simplifies the detection of targeted advertising, but requires reporting to a central repository the impressions seen by different users, thereby jeopardising their privacy. We break this deadlock with a privacy preserving data sharing protocol that allows iWnder to compute global statistics required to detect targeting, while keeping the advertisements seen by individual users and their browsing history private. We conduct a simulation study to explore the effect of different parameters and a live validation to demonstrate the accuracy of our approach. Unlike previous solutions, iWnder can even detect indirect targeting, i.e., marketing campaigns that promote a product or service whose description bears no semantic overlap with its targeted audience.\"","summary":"\"The only topic-based solution meant to be used by real users is MyAdchoice @cite_18 , which has been implemented in the form of a browser extension. This extension is available only under request, and based on the information reported in the paper, it has been only used in a beta-testing phase by few tens of friends and colleagues. Independently of the specific pros and cons of individual solutions, topic-based detection presents some common limitations. The most important being that it can only detect direct interest-based targeted advertising. It is unable to detect other forms of targeting based on demographic or geographic parameters, as well as indirect targeting (see for definitions).\"","":""}
{"id":"2953574061","dialogue":"\"Being able to check whether an online advertisement has been targeted is essential for resolving privacy controversies and implementing in practice data protection regulations like GDPR, CCPA, and COPPA. In this paper we describe the design, implementation, and deployment of an advertisement auditing system called iWnder that uses crowdsourcing to reveal in real time whether a display advertisement has been targeted or not. Crowdsourcing simplifies the detection of targeted advertising, but requires reporting to a central repository the impressions seen by different users, thereby jeopardising their privacy. We break this deadlock with a privacy preserving data sharing protocol that allows iWnder to compute global statistics required to detect targeting, while keeping the advertisements seen by individual users and their browsing history private. We conduct a simulation study to explore the effect of different parameters and a live validation to demonstrate the accuracy of our approach. Unlike previous solutions, iWnder can even detect indirect targeting, i.e., marketing campaigns that promote a product or service whose description bears no semantic overlap with its targeted audience.\"","summary":"\"Correlation-based solutions treat the online advertising ecosystem as a blackbox and apply machine learning and statistical methods to detect correlations between the browsing behavior and other characteristics of a user (OS, device type, location, ) and the ads he sees. For instance, XRay @cite_37 and Sunlight @cite_47 create for each persona several . Each shadow account performs a subset of the actions performed by the original persona. By analyzing the common actions performed by shadow accounts receiving the same reaction from the ecosystem ( , the same ad), the authors can infer the cause of a targeting event. AdFisher @cite_53 uses similar concepts to find discrimination practices, for instance, in the ads shown to men vs. women. As with topic-based detection, this technique presents important challenges related to scalability and practical implementation. Moreover, they are not suitable for real-time targeting detection. With the exception of @cite_18 , no previous work has been implemented as a tool for end-users. Most of them, including @cite_18 , rely on content-based analysis, thereby suffering from scalability issues and inability to detect indirect targeting.\"","":""}
{"id":"2954312627","dialogue":"\"Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.\"","summary":"\"Another type of method has been proposed to alleviate the cost of the comparison function and to shift the burden to the audio features extraction function -- which can be done offline and stored. The general principle is to encode each audio track as a single scalar or vector -- its embedding -- and to reduce the similarity computation to a simple Euclidean distance between embeddings. Originally, embeddings were for instance computed as a single hash encoding a succession of pitch landmarks @cite_43 , or as a vector obtained by PCA dimensionality reduction of a chromagram's 2D-DFT @cite_32 or with locality-sensitive hashing of melodic excerpts @cite_25 .\"","":""}
{"id":"2954312627","dialogue":"\"Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.\"","summary":"\"The principle is to learn a mapping between the input space and a latent manifold where a simple distance measure (such as Euclidean distance) should approximate the neighborhood relationships in the input space. There is however a trivial solution to the problem, where the function ends up mapping all the examples to the same point. Contrastive Loss was introduced to circumvent this problem, aiming at simultaneously similar pairs together and dissimilar pairs apart @cite_35 .\"","":""}
{"id":"2954312627","dialogue":"\"Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.\"","summary":"\"However, when the amount of labels becomes larger, the number of dissimilar pairs becomes quickly intractable. It was moreover observed in practice that once the network has become reasonably good, negative pairs become relatively easy to discern, which stalls the training of the discriminative model. is the strategy of training the model only with hard pairs, i.e. positive (resp. negative) pairs with large (resp. small) distances @cite_39 . Further improvement was introduced with the triplet loss, which is used to train a model to map each sample to an embedding that is closer to all of its positive counterparts than it is to all of its negative counterparts @cite_22 . Formally, for all triplets @math , @math , @math where @math is an anchor, and @math or @math is one of its positive or negative example, respectively, the loss to minimize is expressed as @math , where @math is a margin and @math and @math are the distances between each anchor @math and @math or @math , respectively.\"","":""}
{"id":"2954312627","dialogue":"\"Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.\"","summary":"\"In a recent work @cite_42 , we suggested in an analogy with image processing that dominant melody extraction can be seen as a type of image segmentation, where contours of the melody have to be isolated from the surrounding background. We have thus proposed for dominant melody estimation an adaptation of U-Net @cite_40 -- a model originally designed for medical image segmentation -- which slightly improves over @cite_45 .\"","":""}
{"id":"2955363999","dialogue":"\"State-of-the-art end-to-end automatic speech recognition (ASR) extracts acoustic features from input speech signal every 10 ms which corresponds to a frame rate of 100 frames second. In this report, we investigate the use of high-frame-rate features extraction in end-to-end ASR. High frame rates of 200 and 400 frames second are used in the features extraction and provide additional information for end-to-end ASR. The effectiveness of high-frame-rate features extraction is evaluated independently and in combination with speed perturbation based data augmentation. Experiments performed on two speech corpora, Wall Street Journal (WSJ) and CHiME-5, show that using high-frame-rate features extraction yields improved performance for end-to-end ASR, both independently and in combination with speed perturbation. On WSJ corpus, the relative reduction of word error rate (WER) yielded by high-frame-rate features extraction independently and in combination with speed perturbation are up to 21.3 and 24.1 , respectively. On CHiME-5 corpus, the corresponding relative WER reductions are up to 2.8 and 7.9 , respectively, on the test data recorded by microphone arrays and up to 11.8 and 21.2 , respectively, on the test data recorded by binaural microphones.\"","summary":"\"Speed perturbation @cite_5 is a data augmentation technique which creates warped time signals in addition to the original speech signals. Given an audio signal of length @math and a warping factor @math , speed perturbation creates a new signal with duration @math by resampling the original signal with a sampling rate of @math , where @math is the sampling rate of the original signal. Speed perturbation shifts the speech spectrum and also results in change in number of frames as the duration of the resulting signal is different @cite_5 .\"","":""}
{"id":"2963359379","dialogue":"\"As the availability and the inter-connectivity of RDF datasets grow, so does the necessity to understand the structure of the data. Understanding the topology of RDF graphs can guide and inform the development of, e.g. synthetic dataset generators, sampling methods, index structures, or query optimizers. In this work, we propose two resources: (i) a software framework (Resource URL of the framework: https: doi.org 10.5281 zenodo.2109469) able to acquire, prepare, and perform a graph-based analysis on the topology of large RDF graphs, and (ii) results on a graph-based analysis of 280 datasets (Resource URL of the datasets: https: doi.org 10.5281 zenodo.1214433) from the LOD Cloud with values for 28 graph measures computed with the framework. We present a preliminary analysis based on the proposed resources and point out implications for synthetic dataset generators. Finally, we identify a set of measures, that can be used to characterize graphs in the Semantic Web.\"","summary":"\"This category includes studies about the general structure of RDF graphs at instance, schema, and metadata levels. @cite_11 present the status of RDF datasets in the LOD Cloud in terms of size, linking, vocabulary usage, and metadata. LODStats @cite_3 and the large-scale approach DistLODStats @cite_9 report on statistics about RDF datasets on the web, including number of triples, RDF terms, and properties per entity, and usage of vocabularies across datasets. Loupe @cite_18 is an online tool that reports on the usage of classes and properties in RDF datasets. Fern ' @cite_6 define measures to describe the relatedness between nodes and edges using subject-object, subject-predicate, and predicate-object ratios. @cite_0 study the distribution of RDF terms, classes, instances, and datatypes to measure the quality of public RDF data. In summary, the study of RDF-specific properties of publicly available RDF datasets have been extensively covered and is currently supported by online services and tools such as LODStats and Loupe. Therefore, in addition to these works, we focus on analyzing graph invariants in RDF datasets.\"","":""}
{"id":"2963359379","dialogue":"\"As the availability and the inter-connectivity of RDF datasets grow, so does the necessity to understand the structure of the data. Understanding the topology of RDF graphs can guide and inform the development of, e.g. synthetic dataset generators, sampling methods, index structures, or query optimizers. In this work, we propose two resources: (i) a software framework (Resource URL of the framework: https: doi.org 10.5281 zenodo.2109469) able to acquire, prepare, and perform a graph-based analysis on the topology of large RDF graphs, and (ii) results on a graph-based analysis of 280 datasets (Resource URL of the datasets: https: doi.org 10.5281 zenodo.1214433) from the LOD Cloud with values for 28 graph measures computed with the framework. We present a preliminary analysis based on the proposed resources and point out implications for synthetic dataset generators. Finally, we identify a set of measures, that can be used to characterize graphs in the Semantic Web.\"","summary":"\"In the area of structural network analysis, it is common to study the distribution of certain graph measures in order to characterize a graph. RDF datasets have also been subject to these studies. The study by @cite_20 reveals that the power-law distribution is prevalent across graph invariants in RDF graphs obtained from 1.7 million documents. Also, the small-world phenomenon, known from experiments on social networks were studied within the Semantic Web @cite_19 . More recently, Fern ' @cite_6 have studied the structural features of real-world RDF data. Fern ' also propose measures in terms of in- and -out degrees for subjects, objects, and predicates and analyze the structure of @math RDF graphs from different knowledge domains. Most of these works focus on studying different in- and out-degree distributions and are limited to a rather small collection of RDF datasets. Moreover, the work by @cite_14 analyze further relevant graph invariants in RDF graphs including @math index and reciprocity. The work by applied graph-based metrics on synthetic RDF datasets. Complementary to these works, we present an study on @math RDF datasets from the LOD Cloud and analyze their structure based on the average degree, @math -index, and powerlaw exponent.\"","":""}
{"id":"2905153446","dialogue":"\"In this work, we propose a domain flow generation(DLOW) approach to model the domain shift between two domains by generating a continuous sequence of intermediate domains flowing from one domain to the other. The benefits of our DLOW model are two-fold. First, it is able to transfer source images into different styles in the intermediate domains. The transferred images smoothly bridge the gap between source and target domains, thus easing the domain adaptation task. Second, when multiple target domains are provided in the training phase, our DLOW model can be learnt to generate new styles of images that are unseen in the training data. We implement our DLOW model based on the state-of-the-art CycleGAN. A domainness variable is introduced to guide the model to generate the desired intermediate domain images. In the inference phase, a flow of various styles of images can be obtained by varying the domainness variable. We demonstrate the effectiveness of our approach for both cross-domain semantic segmentation and the style generalization tasks on benchmark datasets.\"","summary":"\"Our work is partially inspired by SGF @cite_29 and GFK @cite_55 , which have shown that the intermediate domains between source and target domains are useful for addressing the domain adaptation problem. They represented each domain as a subspace, and then connected them on Grassmannian manifold to model intermediate domains. Different from them, we model the intermediate domains by directly translate images on pixel level. This allows us to easily improve the existing deep domain adaptation models by using the translated images as training data. Moreover, our model can also be applied to image-level domain generalization by generating mixed-style images.\"","":""}
{"id":"2904221347","dialogue":"\"Metric graphs are meaningful objects for modeling complex structures that arise in many real-world applications, such as road networks, river systems, earthquake faults, blood vessels, and filamentary structures in galaxies. To study metric graphs in the context of comparison, we are interested in determining the relative discriminative capabilities of two topology-based distances between a pair of arbitrary finite metric graphs: the persistence distortion distance and the intrinsic Cech distance. We explicitly show how to compute the intrinsic Cech distance between two metric graphs based solely on knowledge of the shortest systems of loops for the graphs. Our main theorem establishes an inequality between the intrinsic Cech and persistence distortion distances in the case when one of the graphs is a bouquet graph and the other is arbitrary. The relationship also holds when both graphs are constructed via wedge sums of cycles and edges.\"","summary":"\"Well-known methods for comparing graphs using distance measures include combinatorial (e.g., graph edit distance @cite_14 ) and spectral (e.g., eigenvalue decomposition @cite_4 ) approaches. Graph edit distance minimizes the cost of transforming one graph to another via a set of elementary operators such as node edge insertions deletions, while spectral approaches optimize objective functions based on properties of the graph spectra.\"","":""}
{"id":"2904221347","dialogue":"\"Metric graphs are meaningful objects for modeling complex structures that arise in many real-world applications, such as road networks, river systems, earthquake faults, blood vessels, and filamentary structures in galaxies. To study metric graphs in the context of comparison, we are interested in determining the relative discriminative capabilities of two topology-based distances between a pair of arbitrary finite metric graphs: the persistence distortion distance and the intrinsic Cech distance. We explicitly show how to compute the intrinsic Cech distance between two metric graphs based solely on knowledge of the shortest systems of loops for the graphs. Our main theorem establishes an inequality between the intrinsic Cech and persistence distortion distances in the case when one of the graphs is a bouquet graph and the other is arbitrary. The relationship also holds when both graphs are constructed via wedge sums of cycles and edges.\"","summary":"\"Recently, several distances for comparing metric graphs have been proposed based on ideas from computational topology. In the case of a special type of metric graph called a Reeb graph, these distances include: the functional distortion distance @cite_5 , the combinatorial edit distance @cite_24 , the interleaving distance @cite_3 , and its variant in the setting of merge trees @cite_26 . In particular, the functional distortion distance can be considered as a variation of the Gromov-Hausdorff distance between two metric spaces @cite_5 . The interleaving distance is defined via algebraic topology and utilizes the equivalence between Reeb graphs and cosheaves @cite_3 . For metric graphs in general, both the persistence distortion distance @cite_6 and the intrinsic distance @cite_18 take into consideration the structure of metric graphs, independent of their geometric embeddings, by treating them as continuous metric spaces. In @cite_10 , Oudot and Solomon point out that since compact geodesic spaces can be approximated by finite metric graphs in the Gromov--Hausdorff sense @cite_13 (see also the recent work of M 'emoli and Okutan @cite_16 ), one can study potentially complicated length spaces by studying the persistence distortion of a sequence of approximating graphs.\"","":""}
{"id":"2904221347","dialogue":"\"Metric graphs are meaningful objects for modeling complex structures that arise in many real-world applications, such as road networks, river systems, earthquake faults, blood vessels, and filamentary structures in galaxies. To study metric graphs in the context of comparison, we are interested in determining the relative discriminative capabilities of two topology-based distances between a pair of arbitrary finite metric graphs: the persistence distortion distance and the intrinsic Cech distance. We explicitly show how to compute the intrinsic Cech distance between two metric graphs based solely on knowledge of the shortest systems of loops for the graphs. Our main theorem establishes an inequality between the intrinsic Cech and persistence distortion distances in the case when one of the graphs is a bouquet graph and the other is arbitrary. The relationship also holds when both graphs are constructed via wedge sums of cycles and edges.\"","summary":"\"In the context of comparing the relative discriminative capabilities of these distances, Bauer, Ge, and Wang @cite_5 show that the functional distortion distance between two Reeb graphs is bounded from below by the bottleneck distance between the persistence diagrams of the Reeb graphs. Bauer, Munch, and Wang @cite_0 establish a strong equivalence between the functional distortion distance and the interleaving distance on the space of all Reeb graphs, which implies the two distances are within a constant factor of one another. Carri ere and Oudot @cite_9 consider the intrinsic versions of the aforementioned distances and prove that they are all globally equivalent. They also establish a lower bound for the bottleneck distance in terms of a constant multiple of the functional distortion distance. In @cite_6 , Dey, Shi, and Wang show that the persistence distortion distance is stable with respect to changes to input metric graphs as measured by the Gromov-Hausdorff distance. In other words, the persistence distortion distance is bounded above by a constant factor of the Gromov-Hausdorff distance. Furthermore, the intrinsic distance is also bounded from above by the Gromov-Hausdorff distance for general metric spaces @cite_18 .\"","":""}
{"id":"2954142106","dialogue":"\"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.\"","summary":"\"Another line of work that is conceptually close to our method copes with intrinsic motivation which is used to drive the agent's exploration. Examples of such works include empowerment @cite_10 @cite_2 , count-based exploration @cite_6 @cite_9 @cite_13 @cite_7 , information gain about agent's dynamics @cite_1 and forward-inverse dynamics models @cite_12 . While our method uses an information-theoretic objective that is similar to these approaches, it is used to learn a variety of skills that can be directly used for model-based planning, which is in contrast to learning a better exploration policy for a single skill. We provide a discussion on the connection between empowerment and DADS in Appendix .\"","":""}
{"id":"2949767467","dialogue":"\"We present a new algorithm for predicting the near-term trajectories of road-agents in dense traffic videos. Our approach is designed for heterogeneous traffic, where the road-agents may correspond to buses, cars, scooters, bicycles, or pedestrians. We model the interactions between different road-agents using a novel LSTM-CNN hybrid network for trajectory prediction. In particular, we take into account heterogeneous interactions that implicitly accounts for the varying shapes, dynamics, and behaviors of different road agents. In addition, we model horizon-based interactions which are used to implicitly model the driving behavior of each road-agent. We evaluate the performance of our prediction algorithm, TraPHic, on the standard datasets and also introduce a new dense, heterogeneous traffic dataset corresponding to urban Asian videos and agent trajectories. We outperform state-of-the-art methods on dense traffic datasets by 30 .\"","summary":"\"Methods that do not model road-agent interactions are regarded as sub-optimal or as less accurate than methods that model the interactions between road-agents in the scene @cite_3 . Examples of methods that explicitly model road-agent interaction include techniques based on social forces @cite_28 @cite_18 , velocity obstacles @cite_11 , LTA @cite_32 , etc. Many of these models were designed to account for interactions between pedestrians in a crowds (i.e. homogeneous interactions) and to improve the prediction accuracy @cite_35 . Techniques based on velocity obstacles have been extended using kinematic constraints to model the interactions between heterogeneous road-agents @cite_29 . Our learning approach does not use any explicit pairwise motion model. Rather, we model the heterogeneous interactions between road-agents implicitly.\"","":""}
{"id":"2904148778","dialogue":"\"Efficient Reinforcement Learning usually takes advantage of demonstration or good exploration strategy. By applying posterior sampling in model-free RL under the hypothesis of GP, we propose Gaussian Process Posterior Sampling Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving theoretical justifications and empirical results. We also provide theoretical and empirical results that various demonstration could lower expected uncertainty and benefit posterior sampling exploration. In this way, we combined the demonstration and exploration process together to achieve a more efficient reinforcement learning.\"","summary":"\"Two typical methods of learning from demonstration, are inverse reinforcement learning (IRL) and imitation learning (IL). Inverse reinforcement learning was introduced in ng2000algorithms . Its goal is to infer the underlying reward function given the optimal demonstration behavior. Further IRL algorithm includes Bayesian IRL @cite_13 @cite_21 , Maximum Entropy IRL @cite_1 @cite_12 , Repeated IRL @cite_25 , etc. But IRL can be intractable when problem scale is large. Earlier imitation learning indicates behavior cloning, which could fail when agent encounters untrained states. Later representative IL algorithm includes Data Aggregation (DAgger) @cite_28 , Generative Adversarial Imitation Learning (GAIL) , etc. However, their work focuses on imitating optimal demonstration, regarding mediocre and failed demonstration unusable. They also never consider exploration problem after imitating.\"","":""}
{"id":"2905421523","dialogue":"\"We investigate learning to probabilistically bypass computations in a network architecture. Our approach is motivated by AIG, where layers are conditionally executed depending on their inputs, and the network is trained against a target bypass rate using a per-layer loss. We propose a per-batch loss function, and describe strategies for handling probabilistic bypass during inference as well as training. Per-batch loss allows the network additional flexibility. In particular, a form of mode collapse becomes plausible, where some layers are nearly always bypassed and some almost never; such a configuration is strongly discouraged by AIG's per-layer loss. We explore several inference-time strategies, including the natural MAP approach. With data-dependent bypass, we demonstrate improved performance over AIG. With data-independent bypass, as in stochastic depth, we observe mode collapse and effectively prune layers. We demonstrate our techniques on ResNet-50 and ResNet-101 for ImageNet , where our techniques produce improved accuracy (.15--.41 in precision@1) with substantially less computation (bypassing 25--40 of the layers).\"","summary":"\"Conditional computation has been well studied in computer vision. Cascaded classifiers @cite_26 shorten computation by identifying easy negatives and have recently been adapted to deep learning @cite_32 @cite_12 . More directly, @cite_34 and @cite_30 both propose a cascading architecture which computes features at multiple scales and allows for dynamic evaluation, where at inference time the user can trade off speed for accuracy. Similarly, @cite_6 adds intermediate classifiers and returns a label once the network reaches a specified confidence. @cite_21 @cite_23 both use the state of the network to adaptively decrease the number of computational steps during inference. @cite_23 uses an intermediate state sequence and a halting unit to limit the number of blocks that can be executed in an RNN; @cite_21 learns an image dependent stopping condition for each ResNet block that conditionally bypasses the rest of the layers in the block. @cite_8 trains a large number of small networks, called Experts, and then uses gates to select a sparse combination of the experts for a given input.\"","":""}
{"id":"2905421523","dialogue":"\"We investigate learning to probabilistically bypass computations in a network architecture. Our approach is motivated by AIG, where layers are conditionally executed depending on their inputs, and the network is trained against a target bypass rate using a per-layer loss. We propose a per-batch loss function, and describe strategies for handling probabilistic bypass during inference as well as training. Per-batch loss allows the network additional flexibility. In particular, a form of mode collapse becomes plausible, where some layers are nearly always bypassed and some almost never; such a configuration is strongly discouraged by AIG's per-layer loss. We explore several inference-time strategies, including the natural MAP approach. With data-dependent bypass, we demonstrate improved performance over AIG. With data-independent bypass, as in stochastic depth, we observe mode collapse and effectively prune layers. We demonstrate our techniques on ResNet-50 and ResNet-101 for ImageNet , where our techniques produce improved accuracy (.15--.41 in precision@1) with substantially less computation (bypassing 25--40 of the layers).\"","summary":"\"During train time, the activation rate, a hyperparameter set by the user, determines how frequently each individual gate should be open and the target loss is added to the classification loss, where activation loss is L2 of the difference between the target rate and current rate. Our work differs from @cite_31 several ways. We reformulate the loss to a per-batch loss and consider both data-dependent and data-independent layer bypass. Data-independent per-batch loss results in the network trying to remove enough layers to reach the target rate while retaining accuracy (similar to pruning techniques), while data-dependent per-batch loss has more flexibility to utilize layers.\"","":""}
{"id":"2905421523","dialogue":"\"We investigate learning to probabilistically bypass computations in a network architecture. Our approach is motivated by AIG, where layers are conditionally executed depending on their inputs, and the network is trained against a target bypass rate using a per-layer loss. We propose a per-batch loss function, and describe strategies for handling probabilistic bypass during inference as well as training. Per-batch loss allows the network additional flexibility. In particular, a form of mode collapse becomes plausible, where some layers are nearly always bypassed and some almost never; such a configuration is strongly discouraged by AIG's per-layer loss. We explore several inference-time strategies, including the natural MAP approach. With data-dependent bypass, we demonstrate improved performance over AIG. With data-independent bypass, as in stochastic depth, we observe mode collapse and effectively prune layers. We demonstrate our techniques on ResNet-50 and ResNet-101 for ImageNet , where our techniques produce improved accuracy (.15--.41 in precision@1) with substantially less computation (bypassing 25--40 of the layers).\"","summary":"\"Another approach to decreasing the computation time is network pruning. The earliest works attempted to determine the importance of specific weights @cite_7 @cite_44 or hidden units @cite_0 and remove those which are unimportant or redundant. Weight-based pruning on CNNs follows the same fundamental approach; @cite_9 prunes weights with small magnitude and @cite_48 incorporates these into a pipeline which also includes quantization and Huffman coding. Numerous techniques prune at the channel level, whether through heuristics @cite_42 @cite_19 or approximations to importance @cite_14 @cite_45 @cite_47 . @cite_49 prunes at the filter level using statistics from the following layer. @cite_4 applies binary mask variables to a layer's weight tensors, sorts the weights during train time, and then sends the lowest to zero.\"","":""}
{"id":"2905421523","dialogue":"\"We investigate learning to probabilistically bypass computations in a network architecture. Our approach is motivated by AIG, where layers are conditionally executed depending on their inputs, and the network is trained against a target bypass rate using a per-layer loss. We propose a per-batch loss function, and describe strategies for handling probabilistic bypass during inference as well as training. Per-batch loss allows the network additional flexibility. In particular, a form of mode collapse becomes plausible, where some layers are nearly always bypassed and some almost never; such a configuration is strongly discouraged by AIG's per-layer loss. We explore several inference-time strategies, including the natural MAP approach. With data-dependent bypass, we demonstrate improved performance over AIG. With data-independent bypass, as in stochastic depth, we observe mode collapse and effectively prune layers. We demonstrate our techniques on ResNet-50 and ResNet-101 for ImageNet , where our techniques produce improved accuracy (.15--.41 in precision@1) with substantially less computation (bypassing 25--40 of the layers).\"","summary":"\"@cite_25 is the most related to our data-independent bypass. They add a sparsity regularization and then modifies stochastic Accelerated Proximal Gradient to prune the network in an end-to-end fashion. Our work differs from @cite_25 by using GS to integrate the sparsity constraint into an additive loss which can be trained by any optimization technique; we use unmodified stochastic gradient descent with momentum (SGD), the typical technique for training classification. Recently, @cite_29 suggests that the main benefits of pruning come primarily from the identified architecture.\"","":""}
{"id":"2905421523","dialogue":"\"We investigate learning to probabilistically bypass computations in a network architecture. Our approach is motivated by AIG, where layers are conditionally executed depending on their inputs, and the network is trained against a target bypass rate using a per-layer loss. We propose a per-batch loss function, and describe strategies for handling probabilistic bypass during inference as well as training. Per-batch loss allows the network additional flexibility. In particular, a form of mode collapse becomes plausible, where some layers are nearly always bypassed and some almost never; such a configuration is strongly discouraged by AIG's per-layer loss. We explore several inference-time strategies, including the natural MAP approach. With data-dependent bypass, we demonstrate improved performance over AIG. With data-independent bypass, as in stochastic depth, we observe mode collapse and effectively prune layers. We demonstrate our techniques on ResNet-50 and ResNet-101 for ImageNet , where our techniques produce improved accuracy (.15--.41 in precision@1) with substantially less computation (bypassing 25--40 of the layers).\"","summary":"\"Our work is also related to regularization techniques such as Dropout @cite_33 and Stochastic Depth @cite_17 . Both techniques try to induce redundancy through stochastically removing parts of the network during training time. Dropout ignores individual units and Stochastic Depth (as described above) skips entire layers. Both provide evidence that the increased redundancy improves helps to prevent overfitting. These techniques can be seen as applying stochastic gates to units or layers, respectively, where the gate probabilities are hyperparameters.\"","":""}
{"id":"2953257876","dialogue":"\"We propose Top-N-Rank, a novel family of list-wise Learning-to-Rank models for reliably recommending the N top-ranked items. The proposed models optimize a variant of the widely used discounted cumulative gain (DCG) objective function which differs from DCG in two important aspects: (i) It limits the evaluation of DCG only on the top N items in the ranked lists, thereby eliminating the impact of low-ranked items on the learned ranking function; and (ii) it incorporates weights that allow the model to leverage multiple types of implicit feedback with differing levels of reliability or trustworthiness. Because the resulting objective function is non-smooth and hence challenging to optimize, we consider two smooth approximations of the objective function, using the traditional sigmoid function and the rectified linear unit (ReLU). We propose a family of learning-to-rank algorithms (Top-N-Rank) that work with any smooth objective function. Then, a more efficient variant, Top-N-Rank.ReLU, is introduced, which effectively exploits the properties of ReLU function to reduce the computational complexity of Top-N-Rank from quadratic to linear in the average number of items rated by users. The results of our experiments using two widely used benchmarks, namely, the MovieLens data set and the Amazon Video Games data set demonstrate that: (i) The top-N truncation' of the objective function substantially improves the ranking quality of the top N recommendations; (ii) using the ReLU for smoothing the objective function yields significant improvement in both ranking quality as well as runtime as compared to using the sigmoid; and (iii) Top-N-Rank.ReLU substantially outperforms the well-performing list-wise ranking methods in terms of ranking quality.\"","summary":"\"Existing LTR approaches suffer from several limitations. Although, in practical applications, only the top (say N) items in the ranked list are of interest, and the lower-ranked ratings in the list are less reliable, most existing LTR methods are optimized on the ranks of the entire lists, which, could potentially reduce the ranking quality of the top-ranked items. Furthermore, the computational complexity of straightforward approaches to optimizing ranking measures (e.g., DCG @cite_5 , MRR @cite_20 , AUC @cite_11 or MAP @cite_6 ), scale quadratically with @math (the average number of observed items across all users), which renders such methods impractical in large-scale real-world settings.\"","":""}
{"id":"2954153097","dialogue":"\"A significant amount of connection establishments on the web require a prior domain name resolution by the client. Especially on high-latency access networks, these DNS lookups cause a significant delay on the client's connection establishment with a server. To reduce the overhead of QUIC's connection establishment with prior DNS lookup on these networks, we propose a novel QuicSocks proxy. Basically, the client delegates the domain name resolution towards the QuicSocks proxy. Our results indicate, that colocating our proxy with real-world ISP-provided DNS resolvers provides great performance gains. For example, 10 of our 474 sample nodes distributed across ISP's in Germany would save at least 30ms per QUIC connection establishment. The design of our proposal aims to be readily deployable on the Internet by avoiding IP address spoofing, anticipating Network Address Translators and using the standard DNS and QUIC protocols. In summary, our proposal fosters a faster establishment of QUIC connections for clients on high-latency access networks.\"","summary":"\"Furthermore, Miniproxy @cite_2 can be used to accelerate TCP's connection establishment. This approach places a proxy between the client and the web server, which doubles the number of required TCP handshakes. Miniproxy can provide a faster TCP connection establishment in case of a favorable network topology and significant RTTs between client and web server. The QUIC protocol includes computationally expensive cryptographic handshakes causing a significant delay compared to TCP's handshake @cite_20 . Therefore, this approach seems less feasible when QUIC is used.\"","":""}
{"id":"2954153097","dialogue":"\"A significant amount of connection establishments on the web require a prior domain name resolution by the client. Especially on high-latency access networks, these DNS lookups cause a significant delay on the client's connection establishment with a server. To reduce the overhead of QUIC's connection establishment with prior DNS lookup on these networks, we propose a novel QuicSocks proxy. Basically, the client delegates the domain name resolution towards the QuicSocks proxy. Our results indicate, that colocating our proxy with real-world ISP-provided DNS resolvers provides great performance gains. For example, 10 of our 474 sample nodes distributed across ISP's in Germany would save at least 30ms per QUIC connection establishment. The design of our proposal aims to be readily deployable on the Internet by avoiding IP address spoofing, anticipating Network Address Translators and using the standard DNS and QUIC protocols. In summary, our proposal fosters a faster establishment of QUIC connections for clients on high-latency access networks.\"","summary":"\"The ASAP @cite_16 protocol piggybacks the first transport packet within the client's DNS query and the DNS server forwards it to the web server after resolving the IP address. However, this approach requires the DNS server to spoof the clients IP address which leads to a violation of the Best Current Practice RFC 2827 @cite_7 . Furthermore, a deployment of ASAP requires significant infrastructural changes to the Internet because it uses a custom transport protocol.\"","":""}
{"id":"2954153097","dialogue":"\"A significant amount of connection establishments on the web require a prior domain name resolution by the client. Especially on high-latency access networks, these DNS lookups cause a significant delay on the client's connection establishment with a server. To reduce the overhead of QUIC's connection establishment with prior DNS lookup on these networks, we propose a novel QuicSocks proxy. Basically, the client delegates the domain name resolution towards the QuicSocks proxy. Our results indicate, that colocating our proxy with real-world ISP-provided DNS resolvers provides great performance gains. For example, 10 of our 474 sample nodes distributed across ISP's in Germany would save at least 30ms per QUIC connection establishment. The design of our proposal aims to be readily deployable on the Internet by avoiding IP address spoofing, anticipating Network Address Translators and using the standard DNS and QUIC protocols. In summary, our proposal fosters a faster establishment of QUIC connections for clients on high-latency access networks.\"","summary":"\"Further possible performance improvements can be achieved by sending replicated DNS queries to several DNS resolvers and occasionally receiving a faster response @cite_8 . Another DNS-based mechanism aiming to reduce latency uses Server Push @cite_5 where the resolver provides speculative DNS responses prior to the client's query. In total, these approaches tradeoff a higher system utilization versus a possibly reduced latency.\"","":""}
{"id":"2954765265","dialogue":"\"Knowledge base construction is crucial for summarising, understanding and inferring relationships between biomedical entities. However, for many practical applications such as drug discovery, the scarcity of relevant facts (e.g. gene X is therapeutic target for disease Y) severely limits a domain expert's ability to create a usable knowledge base, either directly or by training a relation extraction model. In this paper, we present a simple and effective method of extracting new facts with a pre-specified binary relationship type from the biomedical literature, without requiring any training data or hand-crafted rules. Our system discovers, ranks and presents the most salient patterns to domain experts in an interpretable form. By marking patterns as compatible with the desired relationship type, experts indirectly batch-annotate candidate pairs whose relationship is expressed with such patterns in the literature. Even with a complete absence of seed data, experts are able to discover thousands of high-quality pairs with the desired relationship within minutes. When a small number of relevant pairs do exist - even when their relationship is more general (e.g. gene X is biologically associated with disease Y) than the relationship of interest - our system leverages them in order to i) learn a better ranking of the patterns to be annotated or ii) generate weakly labelled pairs in a fully automated manner. We evaluate our method both intrinsically and via a downstream knowledge base completion task, and show that it is an effective way of constructing knowledge bases when few or no relevant facts are already available.\"","summary":"\"The idea of extracting entity pairs by discovering textual patterns dates back to early work on bootstrapping for relation extraction with the DIPRE system @cite_1 . This system was designed to find co-occurrences of seed entity pairs of a known relationship type inside unlabelled text, then extract simple patterns (exact string matches) from these occurrences and use them to discover new entity pairs. introduced a pattern evaluation methodology based on the precision of a pattern on the set of entity pairs which had already been discovered; they also used the dot product between word vectors instead of an exact string match to allow for slight variations in text. Later work @cite_3 @cite_2 @cite_5 has proposed more sophisticated pattern extraction methods (based on dependency graphs or kernel methods on word vectors) and different pattern evaluation frameworks (document relevance scores).\"","":""}
{"id":"2954765265","dialogue":"\"Knowledge base construction is crucial for summarising, understanding and inferring relationships between biomedical entities. However, for many practical applications such as drug discovery, the scarcity of relevant facts (e.g. gene X is therapeutic target for disease Y) severely limits a domain expert's ability to create a usable knowledge base, either directly or by training a relation extraction model. In this paper, we present a simple and effective method of extracting new facts with a pre-specified binary relationship type from the biomedical literature, without requiring any training data or hand-crafted rules. Our system discovers, ranks and presents the most salient patterns to domain experts in an interpretable form. By marking patterns as compatible with the desired relationship type, experts indirectly batch-annotate candidate pairs whose relationship is expressed with such patterns in the literature. Even with a complete absence of seed data, experts are able to discover thousands of high-quality pairs with the desired relationship within minutes. When a small number of relevant pairs do exist - even when their relationship is more general (e.g. gene X is biologically associated with disease Y) than the relationship of interest - our system leverages them in order to i) learn a better ranking of the patterns to be annotated or ii) generate weakly labelled pairs in a fully automated manner. We evaluate our method both intrinsically and via a downstream knowledge base completion task, and show that it is an effective way of constructing knowledge bases when few or no relevant facts are already available.\"","summary":"\"A well known body of work, OpenIE @cite_18 @cite_0 @cite_10 @cite_15 aims to extract patterns between entity mentions in sentences, thereby discovering new surface forms which can be clustered @cite_12 @cite_9 in order to reveal new meaningful relationship types. In the biomedical domain, Percha and Altman attempt something similar by extracting and clustering dependency patterns between pairs of biomedical entities (e.g. chemical-gene, chemical-disease, gene-disease). Our work differs from these approaches in that we extract pairs for a pre-specified relationship type (either from scratch or by augmenting existing data written with specific guidelines), which is not guaranteed to correspond to a cluster of discovered surface forms.\"","":""}
{"id":"2953924693","dialogue":"\"We propose a novel multi-scale template matching method which is robust against both scaling and rotation in unconstrained environments. The key component behind is a similarity measure referred to as scalable diversity similarity (SDS). Specifically, SDS exploits bidirectional diversity of the nearest neighbor (NN) matches between two sets of points. To address the scale-robustness of the similarity measure, local appearance and rank information are jointly used for the NN search. Furthermore, by introducing penalty term on the scale change, and polar radius term into the similarity measure, SDS is shown to be a well-performing similarity measure against overall size and rotation changes, as well as non-rigid geometric deformations, background clutter, and occlusions. The properties of SDS are statistically justified, and experiments on both synthetic and real-world data show that SDS can significantly outperform state-of-the-art methods.\"","summary":"\"In unconstrained environments, to deal with nonrigid transformations and other noises, involving global information instead of pixel-wise local information for designing a robust similarity is a key cue. Histogram matching (HM) @cite_18 @cite_21 @cite_17 , which mainly measure the similarity between two color histograms, is not restricted by geometric transformation. However, it is usually not a good choice when background clutter and occlusions appear within the windows. Earth mover's distance (EMD) @cite_9 is proposed to measure the similarity between two probability distributions. Furthermore, a more robust approach @cite_6 is proposed by using spatial-appearance representation to measure the EMD. Tone mapping similarity measure @cite_12 is proposed for handling noise, which is approximated by a piece-wise constant linear function. Asymmetric correlation @cite_7 is proposed to deal with both the noise and illumination changes. Other measures focus on improving the robustness against noise as proposed in M-estimator @cite_0 @cite_5 and Hamming-based distance @cite_10 @cite_14 . We refer the interested readers to a comprehensive survey @cite_4 .\"","":""}
{"id":"2953924693","dialogue":"\"We propose a novel multi-scale template matching method which is robust against both scaling and rotation in unconstrained environments. The key component behind is a similarity measure referred to as scalable diversity similarity (SDS). Specifically, SDS exploits bidirectional diversity of the nearest neighbor (NN) matches between two sets of points. To address the scale-robustness of the similarity measure, local appearance and rank information are jointly used for the NN search. Furthermore, by introducing penalty term on the scale change, and polar radius term into the similarity measure, SDS is shown to be a well-performing similarity measure against overall size and rotation changes, as well as non-rigid geometric deformations, background clutter, and occlusions. The properties of SDS are statistically justified, and experiments on both synthetic and real-world data show that SDS can significantly outperform state-of-the-art methods.\"","summary":"\"An eye-catching family of similarity measures in recent years is to explore a global statistic property over the two point sets. Bi-directional similarity (BDS) @cite_1 proposes that two point sets are considered similar if all points of one set are contained in the other, and vice versa. Best-buddies-similarity (BBS) @cite_8 @cite_3 counts the two-side NNs as a similarity statistic. Deformable diversity similarity (DDIS) @cite_11 measures the diversity of feature matches between the two sets and is reported to outperform BBS by revealing the deformation'' of the NN field. Despite the robustness of BBS and DDIS against the transformations within the search windows, scaling and rotation on the whole search windows have not been considered. In this paper, we propose a scaling and rotation independent similarity measure which leads to a significant improvement and allows multi-scale template matching in unconstrained environments.\"","":""}
{"id":"2955307856","dialogue":"\"Terahertz (THz) sensing is a promising imaging technology for a wide variety of different applications. Extracting the interpretable and physically meaningful parameters for such applications, however, requires solving an inverse problem in which a model function determined by these parameters needs to be fitted to the measured data. Since the underlying optimization problem is nonconvex and very costly to solve, we propose learning the prediction of suitable parameters from the measured data directly. More precisely, we develop a model-based autoencoder in which the encoder network predicts suitable parameters and the decoder is fixed to a physically meaningful model function, such that we can train the encoding network in an unsupervised way. We illustrate numerically that the resulting network is more than 140 times faster than classical optimization techniques while making predictions with only slightly higher objective values. Using such predictions as starting points of local optimization techniques allows us to converge to better local minima about twice as fast as optimization without the network-based initialization.\"","summary":"\"Due to the revolutionary success (convolutional) neural networks have had on computer vision problems over the last decade, researchers have extended the fields of applications of neural networks significantly. A particularly interesting concept is to learn the solution of complex, possibly nonconvex, optimization problems. Different lines of research have considered directly learning the optimizer itself, e.g. modelled as a recurrent neural network @cite_12 , or rolling out optimization algorithms and learning the incremental steps, e.g. in the form of parameterized proximal operators in @cite_10 . Further hybrid approaches include optimization problems in the networks' architecture, e.g. @cite_29 , or combining optimizers with networks that have been trained individually @cite_25 @cite_2 . The recent work of @cite_19 trains a network to predict descent directions to a given energy in order to give provable convergence results on the learned optimizer.\"","":""}
{"id":"2955307856","dialogue":"\"Terahertz (THz) sensing is a promising imaging technology for a wide variety of different applications. Extracting the interpretable and physically meaningful parameters for such applications, however, requires solving an inverse problem in which a model function determined by these parameters needs to be fitted to the measured data. Since the underlying optimization problem is nonconvex and very costly to solve, we propose learning the prediction of suitable parameters from the measured data directly. More precisely, we develop a model-based autoencoder in which the encoder network predicts suitable parameters and the decoder is fixed to a physically meaningful model function, such that we can train the encoding network in an unsupervised way. We illustrate numerically that the resulting network is more than 140 times faster than classical optimization techniques while making predictions with only slightly higher objective values. Using such predictions as starting points of local optimization techniques allows us to converge to better local minima about twice as fast as optimization without the network-based initialization.\"","summary":"\"The most related prior work is the 3D face reconstruction network from Tewari al @cite_9 . They aimed at finding a semantic code vector from a given facial image such that feeding this code vector into a rending engine yields an image similar to the input image itself. While this problem had been addressed using optimization algorithms a long time ago @cite_23 (also known under the name of analysis-by-synthesis approaches), the Tewari al @cite_9 replaced the optimizer with a neural network and kept the original cost function to train the network in an unsupervised way. The resulting structure resembles an AE in which the decoder fixed to the forward model and was therefore coined model-based AE. As we will discuss in the next section, the idea of model-based AEs generalizes far beyond 3D face reconstruction and can be used to boost the THz parameter identification problem significantly.\"","":""}
{"id":"2955307856","dialogue":"\"Terahertz (THz) sensing is a promising imaging technology for a wide variety of different applications. Extracting the interpretable and physically meaningful parameters for such applications, however, requires solving an inverse problem in which a model function determined by these parameters needs to be fitted to the measured data. Since the underlying optimization problem is nonconvex and very costly to solve, we propose learning the prediction of suitable parameters from the measured data directly. More precisely, we develop a model-based autoencoder in which the encoder network predicts suitable parameters and the decoder is fixed to a physically meaningful model function, such that we can train the encoding network in an unsupervised way. We illustrate numerically that the resulting network is more than 140 times faster than classical optimization techniques while making predictions with only slightly higher objective values. Using such predictions as starting points of local optimization techniques allows us to converge to better local minima about twice as fast as optimization without the network-based initialization.\"","summary":"\"Finally, a recent work has exploited deep learning techniques in Terahertz imaging in @cite_20 , but the considered application of super-resolving the THz amplitude image @math by training a convolutional neural network on synthetically blurred images is not directly related to our proposed approach.\"","":""}
{"id":"2903664137","dialogue":"\"While much progress has been made in capturing high-quality facial performances using motion capture markers and shape-from-shading, high-end systems typically also rely on rotoscope curves hand-drawn on the image. These curves are subjective and difficult to draw consistently; moreover, ad-hoc procedural methods are required for generating matching rotoscope curves on synthetic renders embedded in the optimization used to determine three-dimensional facial pose and expression. We propose an alternative approach whereby these curves and other keypoints are detected automatically on both the image and the synthetic renders using trained neural networks, eliminating artist subjectivity and the ad-hoc procedures meant to mimic it. More generally, we propose using machine learning networks to implicitly define deep energies which when minimized using classical optimization techniques lead to three-dimensional facial pose and expression estimation.\"","summary":"\"The earliest approaches to regression-based face alignment trained a cascade of regressors to detect face landmarks @cite_67 @cite_58 @cite_36 @cite_16 @cite_35 . More recently, deep convolutional neural networks (CNNs) have been used for both 2D and 3D facial landmark detection from 2D images @cite_27 @cite_7 . These methods are generally classified into coordinate regression models @cite_37 @cite_27 @cite_41 @cite_60 , where a direct mapping is learned between the image and the landmark coordinates, and heatmap regression models @cite_30 @cite_0 @cite_48 , where prediction heatmaps are learned for each landmark. Heatmap-based architectures are generally derived from stacked hourglass @cite_30 @cite_0 @cite_69 @cite_55 or convolutional pose machine @cite_59 architectures used for human body pose estimation. Pixel coordinates can be obtained from the heatmaps by applying the argmax operation; however, @cite_68 @cite_11 use soft-argmax to achieve end-to-end differentiability. A more comprehensive overview of face alignment methods can be found in @cite_49 .\"","":""}
{"id":"2903664137","dialogue":"\"While much progress has been made in capturing high-quality facial performances using motion capture markers and shape-from-shading, high-end systems typically also rely on rotoscope curves hand-drawn on the image. These curves are subjective and difficult to draw consistently; moreover, ad-hoc procedural methods are required for generating matching rotoscope curves on synthetic renders embedded in the optimization used to determine three-dimensional facial pose and expression. We propose an alternative approach whereby these curves and other keypoints are detected automatically on both the image and the synthetic renders using trained neural networks, eliminating artist subjectivity and the ad-hoc procedures meant to mimic it. More generally, we propose using machine learning networks to implicitly define deep energies which when minimized using classical optimization techniques lead to three-dimensional facial pose and expression estimation.\"","summary":"\"Neural networks have been used for various other face image analysis tasks such as gender determination @cite_13 and face detection @cite_18 . More recently, deep CNNs have been used to improve face detection results especially in uncontrolled environments and with more extreme poses @cite_31 @cite_50 . Additionally, CNNs have been employed for face segmentation @cite_17 @cite_22 @cite_51 , facial pose and reflectance acquisition @cite_10 @cite_54 , and face recognition @cite_33 @cite_65 .\"","":""}
{"id":"2903664137","dialogue":"\"While much progress has been made in capturing high-quality facial performances using motion capture markers and shape-from-shading, high-end systems typically also rely on rotoscope curves hand-drawn on the image. These curves are subjective and difficult to draw consistently; moreover, ad-hoc procedural methods are required for generating matching rotoscope curves on synthetic renders embedded in the optimization used to determine three-dimensional facial pose and expression. We propose an alternative approach whereby these curves and other keypoints are detected automatically on both the image and the synthetic renders using trained neural networks, eliminating artist subjectivity and the ad-hoc procedures meant to mimic it. More generally, we propose using machine learning networks to implicitly define deep energies which when minimized using classical optimization techniques lead to three-dimensional facial pose and expression estimation.\"","summary":"\"Using deep networks such as VGG-16 @cite_15 for losses has been shown to be effective for training other deep networks for tasks such as style transfer and super-resolution @cite_26 . Such techniques have also been used for image generation @cite_61 and face swapping @cite_56 . Furthermore, deep networks have been used in energies for traditional optimization problems for style transfer @cite_52 , texture synthesis @cite_62 , and image generation @cite_19 @cite_14 . While @cite_52 @cite_62 use the L-BFGS @cite_34 method to minimize the optimization problem, @cite_19 @cite_14 use gradient descent methods @cite_32 .\"","":""}
{"id":"2904815624","dialogue":"\"Deep reinforcement-learning methods have achieved remarkable performance on challenging control tasks. Observations of the resulting behavior give the impression that the agent has constructed a generalized representation that supports insightful action decisions. We re-examine what is meant by generalization in RL, and propose several definitions based on an agent's performance in on-policy, off-policy, and unreachable states. We propose a set of practical methods for evaluating agents with these definitions of generalization. We demonstrate these techniques on a common benchmark task for deep RL, and we show that the learned networks make poor decisions for states that differ only slightly from on-policy states, even though those states are not selected adversarially. Taken together, these results call into question the extent to which deep Q-networks learn generalized representations, and suggest that more experimentation and analysis is necessary before claims of representation learning can be supported.\"","summary":"\"Generalization has been cast as avoiding overfitting to a particular training environment, implying that sampling from diverse environments is necessary for generalization @cite_8 @cite_2 . Other work has focused on generalization as improved performance in off-policy states, a framework much closer to standard approaches in supervised learning. Techniques such as adding stochasticity to the policy @cite_7 , having the agent take random steps, no-ops, steps from human play @cite_11 , or probabilistically repeating the agent's previous action @cite_6 , all force the agent to transition to off-policy states.\"","":""}
{"id":"2903301099","dialogue":"\"In this paper, a unified approach is presented to transfer learning that addresses several source and target domain label-space and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.\"","summary":"\"Transfer learning (TL) aims to transfer knowledge from one domain task to improve performance on the another @cite_6 . The most widely used TL technique for deep networks is fine-tuning @cite_9 @cite_0 @cite_7 . Instead of training a target network from scratch, its weights are initialised by a pre-trained model from another task such as ImageNet @cite_19 classification. While fine-tuning reduces label requirement compared to learning the target problem from scratch, it is prone to over-fitting if target labels are very few @cite_9 . Therefore, it is ineffective for very sparsely supervised DLSTL, and not applicable to unsupervised DLSTL. Moreover, vanilla TL does not exploit available unlabelled samples for the target problem (i.e. semi-supervised TL). The most related method to ours is @cite_17 which does exploit both unlabelled and few labelled data, i.e., semi-supervised DLSTL. However like other TL methods, it does not generalise to the unsupervised DLSTL setting where no target annotations are available.\"","":""}
{"id":"2903301099","dialogue":"\"In this paper, a unified approach is presented to transfer learning that addresses several source and target domain label-space and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.\"","summary":"Entropy loss for unlabelled data is another widely used SSL regulariser @cite_42 @cite_12 . It is applied at the classification layer in problems where the unlabelled and labelled data share the same label-space -- and reflects the inductive bias that a classification boundary should not cut through the dense unlabelled data regions. Its typical use is on softmax classifier outputs where it encourages a classifier to pick a single label. In contrast we use entropy-loss to solve DLSTL problems by applying it element-wise on our intermediate CFS layer in order to weakly align domains by encouraging them to share a near-binary representation.","":""}
{"id":"2903322478","dialogue":"\"Clustering is a fundamental machine learning task and can be used in many applications. With the development of deep neural networks (DNNs), combining techniques from DNNs with clustering has become a new research direction and achieved some success. However, few studies have focused on the imbalanced-data problem which commonly occurs in real-world applications. In this paper, we propose a clustering method, regularized deep embedding clustering (RDEC), that integrates virtual adversarial training (VAT), a network regularization technique, with a clustering method called deep embedding clustering (DEC). DEC optimizes cluster assignments by pushing data more densely around centroids in latent space, but it is sometimes sensitive to the initial location of centroids, especially in the case of imbalanced data, where the minor class has less chance to be assigned a good centroid. RDEC introduces regularization using VAT to ensure the model's robustness to local perturbations of data. VAT pushes data that are similar in the original space closer together in the latent space, bunching together data from minor classes and thereby facilitating cluster identification by RDEC. Combining the advantages of DEC and VAT, RDEC attains state-of-the-art performance on both balanced and imbalanced benchmark real-world datasets. For example, accuracies are as high as 98.41 on MNIST dataset and 85.45 on a highly imbalanced dataset derived from the MNIST, which is nearly 8 higher than the current best result.\"","summary":"\"In view of this problem, representation learning and clustering are performed simultaneously in recent works such as [ @cite_25 ], [ @cite_18 ], and [ @cite_1 ]. However, these works do not sufficiently address the imbalanced-data problem. DEC [ @cite_18 ] exhibits some degrees of robustness to imbalanced datasets, but work is needed to further improve its robustness. Several methods based on generative models have also been proposed [ @cite_23 ], [ @cite_9 ]. VaDE [ @cite_23 ] models a data generation procedure based on the variational autoencoder, where the data distribution in the latent space is modeled by GMM, the representations are sampled and then mapped into the space via the DNN. This approach is novel and can work well in some cases. However, because the class distribution is unknown in imbalanced dataset, it is difficult to learn a good generative model, which may lead to low versatility and robustness.\"","":""}
{"id":"2902850193","dialogue":"\"In this report, we describe the design and implementation of Ibdxnet, a low-latency and high-throughput transport providing the benefits of InfiniBand networks to Java applications. Ibdxnet is part of the Java-based DXNet library, a highly concurrent and simple to use messaging stack with transparent serialization of messaging objects and focus on very small messages (< 64 bytes). Ibdxnet implements the transport interface of DXNet in Java and a custom C++ library in native space using JNI. Several optimizations in both spaces minimize context switching overhead between Java and C++ and are not burdening message latency or throughput. Communication is implemented using the messaging verbs of the ibverbs library complemented by an automatic connection management in the native library. We compared DXNet with the Ibdxnet transport to the MPI implementations FastMPJ and MVAPICH2. For small messages up to 64 bytes using multiple threads, DXNet with the Ibdxnet transport achieves a bi-directional message rate of 10 million messages per second and surpasses FastMPJ by a factor of 4 and MVAPICH by a factor of 2. Furthermore, DXNet scales well on a high load all-to-all communication with up to 8 nodes achieving a total aggregated message rate of 43.4 million messages per second for small messages and a throughput saturation of 33.6 GB s with only 2 kb message size.\"","summary":"\"The message passing interface @cite_38 defines a standard for high level networking primitives to send and receive data between local and remote processes, typically used for HPC applications.\"","":""}
{"id":"2902850193","dialogue":"\"In this report, we describe the design and implementation of Ibdxnet, a low-latency and high-throughput transport providing the benefits of InfiniBand networks to Java applications. Ibdxnet is part of the Java-based DXNet library, a highly concurrent and simple to use messaging stack with transparent serialization of messaging objects and focus on very small messages (< 64 bytes). Ibdxnet implements the transport interface of DXNet in Java and a custom C++ library in native space using JNI. Several optimizations in both spaces minimize context switching overhead between Java and C++ and are not burdening message latency or throughput. Communication is implemented using the messaging verbs of the ibverbs library complemented by an automatic connection management in the native library. We compared DXNet with the Ibdxnet transport to the MPI implementations FastMPJ and MVAPICH2. For small messages up to 64 bytes using multiple threads, DXNet with the Ibdxnet transport achieves a bi-directional message rate of 10 million messages per second and surpasses FastMPJ by a factor of 4 and MVAPICH by a factor of 2. Furthermore, DXNet scales well on a high load all-to-all communication with up to 8 nodes achieving a total aggregated message rate of 43.4 million messages per second for small messages and a throughput saturation of 33.6 GB s with only 2 kb message size.\"","summary":"\"@cite_32 is a network stack designed for next generation systems for applications with an highly multi-threaded environment. It provides three independent layers: UCS is a service layer with different cross platform utilities, such as atomic operations, thread safety, memory management and data structures. The transport layer UCT abstracts different hardware architectures and their low-level APIs, and provides an API to implement communication primitives. UCP implements high level protocols such as MPI or PGAS programming models by using UCT.\"","":""}
{"id":"2902850193","dialogue":"\"In this report, we describe the design and implementation of Ibdxnet, a low-latency and high-throughput transport providing the benefits of InfiniBand networks to Java applications. Ibdxnet is part of the Java-based DXNet library, a highly concurrent and simple to use messaging stack with transparent serialization of messaging objects and focus on very small messages (< 64 bytes). Ibdxnet implements the transport interface of DXNet in Java and a custom C++ library in native space using JNI. Several optimizations in both spaces minimize context switching overhead between Java and C++ and are not burdening message latency or throughput. Communication is implemented using the messaging verbs of the ibverbs library complemented by an automatic connection management in the native library. We compared DXNet with the Ibdxnet transport to the MPI implementations FastMPJ and MVAPICH2. For small messages up to 64 bytes using multiple threads, DXNet with the Ibdxnet transport achieves a bi-directional message rate of 10 million messages per second and surpasses FastMPJ by a factor of 4 and MVAPICH by a factor of 2. Furthermore, DXNet scales well on a high load all-to-all communication with up to 8 nodes achieving a total aggregated message rate of 43.4 million messages per second for small messages and a throughput saturation of 33.6 GB s with only 2 kb message size.\"","summary":"\"@cite_11 is a distributed key-value storage optimized for low latency data access using InfiniBand with messaging verbs. Multiple transports are implemented for network communication, e.g. using reliable and unreliable connections with InfiniBand and Ethernet with unreliable connections. @cite_26 implements a key-value and graph storage using a shared memory architecture with RDMA. It performs well with a throughput of 167 million key-value lookups and 31 us latency using 20 machines. @cite_17 also implements a key-value storage using RDMA for get operations and messaging verbs for put operations. @cite_25 implements a key-value storage with a focus on NUMA architectures. It maps each CPU core to a partition of data and communicates using a request-response approach using unreliable connections. @cite_30 borrows the design of MICA and implements networking using RDMA writes for the request to the server and messaging verbs for the response back to the client.\"","":""}
{"id":"2952049456","dialogue":"\"We revisit the notion of deniability in quantum key exchange (QKE), a topic that remains largely unexplored. In the only work on this subject by Donald Beaver, it is argued that QKE is not necessarily deniable due to an eavesdropping attack that limits key equivocation. We provide more insight into the nature of this attack and how it extends to other constructions such as QKE obtained from uncloneable encryption. We then adopt the framework for quantum authenticated key exchange, developed by , and extend it to introduce the notion of coercer-deniable QKE, formalized in terms of the indistinguishability of real and fake coercer views. Next, we apply results from a recent work by Arrazola and Scarani on covert quantum communication to establish a connection between covert QKE and deniability. We propose DC-QKE, a simple deniable covert QKE protocol, and prove its deniability via a reduction to the security of covert QKE. Finally, we consider how entanglement distillation can be used to enable information-theoretically deniable protocols for QKE and tasks beyond key exchange.\"","summary":"\"In a framework based on the simulation paradigm, introduced the notion of deniable authentication @cite_10 , followed by the work of Di on the formalization of deniable key exchange @cite_24 . Both works rely on the formalism of zero-knowledge (ZK) proofs, with definitions formalized in terms of a simulator that can produce a simulated view that is indistinguishable from the real one. In a subsequent work, Di Raimondo and Gennaro gave a formal definition of forward deniability @cite_31 , requiring that indistinguishability remain intact even when a (corrupted) party reveals real coins after a session. Among other things, they showed that statistical ZK protocols are forward deniable.\"","":""}
{"id":"2952049456","dialogue":"\"We revisit the notion of deniability in quantum key exchange (QKE), a topic that remains largely unexplored. In the only work on this subject by Donald Beaver, it is argued that QKE is not necessarily deniable due to an eavesdropping attack that limits key equivocation. We provide more insight into the nature of this attack and how it extends to other constructions such as QKE obtained from uncloneable encryption. We then adopt the framework for quantum authenticated key exchange, developed by , and extend it to introduce the notion of coercer-deniable QKE, formalized in terms of the indistinguishability of real and fake coercer views. Next, we apply results from a recent work by Arrazola and Scarani on covert quantum communication to establish a connection between covert QKE and deniability. We propose DC-QKE, a simple deniable covert QKE protocol, and prove its deniability via a reduction to the security of covert QKE. Finally, we consider how entanglement distillation can be used to enable information-theoretically deniable protocols for QKE and tasks beyond key exchange.\"","summary":"\"Pass @cite_5 formally defines the notion of deniable zero-knowledge and presents positive and negative results in the common reference string and random oracle model. In @cite_0 , establish a link between deniability and ideal authentication and further model a situation in which deniability should hold even when a corrupted party colludes with the adversary during the execution of a protocol. They show an impossibility result in the PKI model if adaptive corruptions are allowed. Cremers and Feltz introduced another variant for key exchange referred to as peer and time deniability @cite_1 , while also capturing perfect forward secrecy. More recently, Unger and Goldberg studied deniable authenticated key exchange (DAKE) in the context of secure messaging @cite_4 .\"","":""}
{"id":"2952049456","dialogue":"\"We revisit the notion of deniability in quantum key exchange (QKE), a topic that remains largely unexplored. In the only work on this subject by Donald Beaver, it is argued that QKE is not necessarily deniable due to an eavesdropping attack that limits key equivocation. We provide more insight into the nature of this attack and how it extends to other constructions such as QKE obtained from uncloneable encryption. We then adopt the framework for quantum authenticated key exchange, developed by , and extend it to introduce the notion of coercer-deniable QKE, formalized in terms of the indistinguishability of real and fake coercer views. Next, we apply results from a recent work by Arrazola and Scarani on covert quantum communication to establish a connection between covert QKE and deniability. We propose DC-QKE, a simple deniable covert QKE protocol, and prove its deniability via a reduction to the security of covert QKE. Finally, we consider how entanglement distillation can be used to enable information-theoretically deniable protocols for QKE and tasks beyond key exchange.\"","summary":"\"To the best of our knowledge, the only work related to deniability in QKE is a single paper by Beaver @cite_32 , in which the author suggests a negative result arguing that existing QKE schemes are not necessarily deniable.\"","":""}
{"id":"2902517736","dialogue":"\"This paper presents a region-partition based attraction field dual representation for line segment maps, and thus poses the problem of line segment detection (LSD) as the region coloring problem. The latter is then addressed by learning deep convolutional neural networks (ConvNets) for accuracy, robustness and efficiency. For a 2D line segment map, our dual representation consists of three components: (i) A region-partition map in which every pixel is assigned to one and only one line segment; (ii) An attraction field map in which every pixel in a partition region is encoded by its 2D projection vector w.r.t. the associated line segment; and (iii) A squeeze module which squashes the attraction field to a line segment map that almost perfectly recovers the input one. By leveraging the duality, we learn ConvNets to compute the attraction field maps for raw in-put images, followed by the squeeze module for LSD, in an end-to-end manner. Our method rigorously addresses several challenges in LSD such as local ambiguity and class imbalance. Our method also harnesses the best practices developed in ConvNets based semantic segmentation methods such as the encoder-decoder architecture and the a-trous convolution. In experiments, our method is tested on the WireFrame dataset and the YorkUrban dataset with state-of-the-art performance obtained. Especially, we advance the performance by 4.5 percents on the WireFrame dataset. Our method is also fast with 6.6 10.4 FPS, outperforming most of existing line segment detectors.\"","summary":"\"The study of line segment detection has a very long history since 1980s @cite_9 . The early pioneers tried to detect line segments based upon the edge map estimation. Then, the perception grouping approaches based on the are proposed. Both of these methods concentrate on the hand-crafted low-level features for the detection, which have become a limitation. Recently, the line segment detection and its related problem edge detection have been studied under the perspective of deep learning, which dramatically improved the detection performance and brings us of great practical importance for real applications.\"","":""}
{"id":"2902517736","dialogue":"\"This paper presents a region-partition based attraction field dual representation for line segment maps, and thus poses the problem of line segment detection (LSD) as the region coloring problem. The latter is then addressed by learning deep convolutional neural networks (ConvNets) for accuracy, robustness and efficiency. For a 2D line segment map, our dual representation consists of three components: (i) A region-partition map in which every pixel is assigned to one and only one line segment; (ii) An attraction field map in which every pixel in a partition region is encoded by its 2D projection vector w.r.t. the associated line segment; and (iii) A squeeze module which squashes the attraction field to a line segment map that almost perfectly recovers the input one. By leveraging the duality, we learn ConvNets to compute the attraction field maps for raw in-put images, followed by the squeeze module for LSD, in an end-to-end manner. Our method rigorously addresses several challenges in LSD such as local ambiguity and class imbalance. Our method also harnesses the best practices developed in ConvNets based semantic segmentation methods such as the encoder-decoder architecture and the a-trous convolution. In experiments, our method is tested on the WireFrame dataset and the YorkUrban dataset with state-of-the-art performance obtained. Especially, we advance the performance by 4.5 percents on the WireFrame dataset. Our method is also fast with 6.6 10.4 FPS, outperforming most of existing line segment detectors.\"","summary":"\"In a long range of time, the hand-crafted low-level features (especially for image gradients) are heavily used for line segment detection. These approaches can be divided into edge map based approaches @cite_15 @cite_16 @cite_34 @cite_0 @cite_6 @cite_8 and perception grouping approaches @cite_21 @cite_12 @cite_26 . The edge map based approaches treat the visual features as a discriminated feature for edge map estimation and subsequently applying the Hough transform @cite_9 to globally search line configurations and then cutting them by using thresholds. In contrast to the edge map based approaches, the grouping methods directly use the image gradients as local geometry cues to group pixels into line segment candidates and filter out the false positives @cite_12 @cite_26 .\"","":""}
{"id":"1986573745","dialogue":"\"The aim of tool path planning is to maximize the efficiency against some given precision criteria. In practice, scallop height should be kept constant to avoid unnecessary cutting, while the tool path should be smooth enough to maintain a high feed rate. However, iso-scallop and smoothness often conflict with each other. Existing methods smooth iso-scallop paths one-by-one, which make the final tool path far from being globally optimal. This paper proposes a new framework for tool path optimization. It views a family of iso-level curves of a scalar function defined over the surface as tool path so that desired tool path can be generated by finding the function that minimizes certain energy functional and different objectives can be considered simultaneously. We use the framework to plan globally optimal tool path with respect to iso-scallop and smoothness. The energy functionals for planning iso-scallop, smoothness, and optimal tool path are respectively derived, and the path topology is studied too. Experimental results are given to show effectiveness of the proposed methods.\"","summary":"\"Last decade has seen a great deal of literature on tool path planning for free-form surfaces, such as iso-parametric method @cite_25 @cite_12 @cite_1 , iso-planar method @cite_19 @cite_9 @cite_18 , iso-scallop method @cite_23 @cite_32 @cite_13 @cite_17 @cite_3 @cite_8 @cite_21 , iso-phote method @cite_4 and C-space method @cite_33 , to name a few. Surveys of much more work about tool path planning research can be found in @cite_0 @cite_30 . Since we aim at optimal tool paths with respect to iso-scallop and smoothness, we put special interest in the iso-scallop method, which means the height of the points at the scallop curves remains as high as a given value so that the tool path has no unnecessary cutting. Conventionally, constant scallop height is obtained by varying the offset magnitude along each path. A mathematical method for generating iso-scallop tool paths following such strategy was first proposed by @cite_23 . Afterwards, methods to improve the computing efficiency @cite_2 @cite_3 and accuracy @cite_16 @cite_17 @cite_21 were proposed. In 2007, Kim @cite_27 reformulated the iso-scallop tool path as geodesic parallel curves on the design surface by defining a new Riemannian metric.\"","":""}
{"id":"1986573745","dialogue":"\"The aim of tool path planning is to maximize the efficiency against some given precision criteria. In practice, scallop height should be kept constant to avoid unnecessary cutting, while the tool path should be smooth enough to maintain a high feed rate. However, iso-scallop and smoothness often conflict with each other. Existing methods smooth iso-scallop paths one-by-one, which make the final tool path far from being globally optimal. This paper proposes a new framework for tool path optimization. It views a family of iso-level curves of a scalar function defined over the surface as tool path so that desired tool path can be generated by finding the function that minimizes certain energy functional and different objectives can be considered simultaneously. We use the framework to plan globally optimal tool path with respect to iso-scallop and smoothness. The energy functionals for planning iso-scallop, smoothness, and optimal tool path are respectively derived, and the path topology is studied too. Experimental results are given to show effectiveness of the proposed methods.\"","summary":"\"There also exist some efforts to generate smooth tool paths without considering the overlapping between neighbor machining strips (i.e., the iso-scallop condition). Generally, such methods are based on the Laplacian. For example, Bieterman and Sandstrom @cite_10 proposed a Laplacian based contour parallel tool path generation method by selecting the level sets of a harmonic function defined over a pocket as the tool path. But how to choose the level sets for it still remains an open problem, namely there is no formula for path interval calculation so far. Similarly, Chuang and Yang @cite_34 combined the Laplacian method and iso-parametric method to generate tool paths for pockets with complex topology, i.e., complex boundaries and islands. However, the smoothness of the tool path cannot be guaranteed through Laplacian energy as small Laplacian value does not necessarily mean small curvature of the level set curves. And solving a Laplace equation over a surface can only generate a unique and uncontrollable scalar function (scaling has no impact on the shape of tool paths). Another drawback of the Laplacian based approach is the severe overlapping between machining strips of neighbor paths, especially for paths near the boundary, which results in too much redundant machining.\"","":""}
{"id":"2901595091","dialogue":"\"We consider active learning of deep neural networks. Most active learning works in this context have focused on studying effective querying mechanisms and assumed that an appropriate network architecture is a priori known for the problem at hand. We challenge this assumption and propose a novel active strategy whereby the learning algorithm searches for effective architectures on the fly, while actively learning. We apply our strategy using three known querying techniques (softmax response, MC-dropout, and coresets) and show that the proposed approach overwhelmingly outperforms active learning using fixed architectures.\"","summary":"\"In (NAS), the goal is to devise algorithms that automatically optimize the neural architecture for a given problem. Several NAS papers have recently proposed a number of approaches. In @cite_25 , a reinforcement learning algorithm was used to optimize the architecture of a neural network. In @cite_8 , a genetic algorithm is used to optimize the structure of two types of blocks'' (a combination of neural network layers and building components) that have been used for constructing architectures. The number of blocks comprising the full architecture was manually optimized. It was observed that the optimal number of blocks is mostly dependent on the size of the training set. More efficient optimization techniques were proposed in @cite_13 @cite_27 @cite_1 @cite_0 . In all these works, the architecture search algorithms were focused on optimizing the structure of one (or two) blocks that were manually connected together to span the full architecture. The algorithm proposed in @cite_14 optimizes both the block structure and the number of blocks simultaneously.\"","":""}
{"id":"2901595091","dialogue":"\"We consider active learning of deep neural networks. Most active learning works in this context have focused on studying effective querying mechanisms and assumed that an appropriate network architecture is a priori known for the problem at hand. We challenge this assumption and propose a novel active strategy whereby the learning algorithm searches for effective architectures on the fly, while actively learning. We apply our strategy using three known querying techniques (softmax response, MC-dropout, and coresets) and show that the proposed approach overwhelmingly outperforms active learning using fixed architectures.\"","summary":"\"When considering NAS for fully-connected networks, @cite_30 proposed an algorithm that iteratively adds neurons to an existing layer or to initiate a new layer. Their algorithm iteratively optimizes the width and depth of a network. For a comprehensive survey on NAS techniques, see @cite_19 . To the best of our knowledge, no work has been done on architecture searches for active learning.\"","":""}
{"id":"2950851765","dialogue":"\"In this paper we present CMRNet, a realtime approach based on a Convolutional Neural Network to localize an RGB image of a scene in a map built from LiDAR data. Our network is not trained on the working area, i.e. CMRNet does not learn the map. Instead it learns to match an image to the map. We validate our approach on the KITTI dataset, processing each frame independently without any tracking procedure. CMRNet achieves 0.26m and 1.05deg median localization accuracy on the sequence 00 of the odometry dataset, starting from a rough pose estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is the first CNN-based approach that learns to match images from a monocular camera to a given, preexisting 3D LiDAR-map.\"","summary":"\"-map approaches The second category of localization techniques leverages existing maps, in order to solve the localization problem. In particular, two classes of approaches have been presented in the literature: geometry-based and projection-based methods. Caselitz al @cite_23 proposed a geometry-based method that solves the visual localization problem by comparing a set of 3D points, the point cloud reconstructed from a sequence of images and the existing map. Wolcott al @cite_30 , instead, developed a projection-based method that uses meshes built from intensity data associated to the 3D points of the maps, projected into an image plane, to perform a comparison with the camera image using the (NMI) measure. Neubert al @cite_14 proposed to use the similarity between depth images generated by synthetic views and the camera image as a score function for a particle filter, in order to localize the camera in indoor scenes.\"","":""}
{"id":"2950851765","dialogue":"\"In this paper we present CMRNet, a realtime approach based on a Convolutional Neural Network to localize an RGB image of a scene in a map built from LiDAR data. Our network is not trained on the working area, i.e. CMRNet does not learn the map. Instead it learns to match an image to the map. We validate our approach on the KITTI dataset, processing each frame independently without any tracking procedure. CMRNet achieves 0.26m and 1.05deg median localization accuracy on the sequence 00 of the odometry dataset, starting from a rough pose estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is the first CNN-based approach that learns to match images from a monocular camera to a given, preexisting 3D LiDAR-map.\"","summary":"\"The work presented in this paper has been inspired by Schneider al @cite_10 , which used 3D scans from a LIDAR and RGB images as the input of a novel CNN, RegNet. Their goal was to provide a CNN-based method for calibrating the extrinsic parameters of a camera a LIDAR sensor. Taking inspiration from that work, in this paper we propose a novel approach that has the advantages of both the categories described above. Differently from the aforementioned literature contribution, which exploits the data gathered from a synchronized single activation of a 3D LIDAR and a camera image, the inputs of our approach are a complete 3D LIDAR map of the environment, together with a single image and a rough initial guess of the camera pose. Eventually, the output consists of an accurate 6-DoF camera pose localization. It is worth to notice that having a single LIDAR scan taken at the same time as the image imply that the observed scene is exactly the same. In our case, instead, the 3D map usually depicts a different configuration, road users are not present, making the matching more challenging.\"","":""}
{"id":"2955907793","dialogue":"\"Detailed knowledge about the electrical power consumption in industrial production environments is a prerequisite to reduce and optimize their power consumption. Today's industrial production sites are equipped with a variety of sensors that, inter alia, monitor electrical power consumption in detail. However, these environments often lack an automated data collation and analysis. We present a system architecture that integrates different sensors and analyzes and visualizes the power consumption of devices, machines, and production plants. It is designed with a focus on scalability to support production environments of various sizes and to handle varying loads. We argue that a scalable architecture in this context must meet requirements for fault tolerance, extensibility, real-time data processing, and resource efficiency. As a solution, we propose a microservice-based architecture augmented by big data and stream processing techniques. Applying the fog computing paradigm, parts of it are deployed in an elastic, central cloud while other parts run directly, decentralized in the production environment. A prototype implementation of this architecture presents solutions how different kinds of sensors can be integrated and their measurements can be continuously aggregated. In order to make analyzed data comprehensible, it features a single-page web application that provides different forms of data visualization. We deploy this pilot implementation in the data center of a medium-sized enterprise, where we successfully monitor the power consumption of 16 servers. Furthermore, we show the scalability of our architecture with 20,000 simulated sensors.\"","summary":"\"Shrouf and Miragliotta @cite_2 report on different approaches for energy management enabled by Internet of Things (IoT) technologies. Based on literature, expert interviews, and reports of manufactures, they summarize different IoT architectures for power monitoring and present a general abstraction of them. The resulting architecture primarily focuses on network interconnections and integration of other systems. As in our approach, it respects real-time data processing and the challenge of integrating data of different sensors and data formats. However, data is only processed in a cloud or local server infrastructure and does not follow fog computing paradigms. The architecture represents a general approach and is therefore too abstract to offer a reference implementation.\"","":""}
{"id":"2955907793","dialogue":"\"Detailed knowledge about the electrical power consumption in industrial production environments is a prerequisite to reduce and optimize their power consumption. Today's industrial production sites are equipped with a variety of sensors that, inter alia, monitor electrical power consumption in detail. However, these environments often lack an automated data collation and analysis. We present a system architecture that integrates different sensors and analyzes and visualizes the power consumption of devices, machines, and production plants. It is designed with a focus on scalability to support production environments of various sizes and to handle varying loads. We argue that a scalable architecture in this context must meet requirements for fault tolerance, extensibility, real-time data processing, and resource efficiency. As a solution, we propose a microservice-based architecture augmented by big data and stream processing techniques. Applying the fog computing paradigm, parts of it are deployed in an elastic, central cloud while other parts run directly, decentralized in the production environment. A prototype implementation of this architecture presents solutions how different kinds of sensors can be integrated and their measurements can be continuously aggregated. In order to make analyzed data comprehensible, it features a single-page web application that provides different forms of data visualization. We deploy this pilot implementation in the data center of a medium-sized enterprise, where we successfully monitor the power consumption of 16 servers. Furthermore, we show the scalability of our architecture with 20,000 simulated sensors.\"","summary":"\"We did not find any monitoring approaches for production environments designed in a microservices architecture. However, microservice-based approaches exist for other applications of the Internet of Things, such as @cite_22 and @cite_9 . As we propose in our architecture, these approaches intend to deploy microservices decentralized for flexibility and extendibility. Moreover, they use an asynchronous messaging bus for the exchange of sensor data as in our approach. Both approaches do not focus on scalability and, therefore, do not evaluate this.\"","":""}
{"id":"2953607269","dialogue":"\"In this study, we investigated multi-modal approaches using images, descriptions, and title to categorize e-commerce products on this http URL. Specifically, we examined late fusion models, where the modalities are fused at the decision level. Products were each assigned multiple labels, and the hierarchy in the labels were flattened and filtered. For our individual baseline models, we modified a CNN architecture to classify the description and title, and then modified Keras' ResNet-50 to classify the images, achieving F1 scores of 77.0 , 82.7 , and 61.0 , respectively. In comparison, our tri-modal late fusion model can classify products more accurately than single modal models can, improving the F1 score to 88.2 . Each modality complemented the shortcomings of the other modalities, demonstrating that increasing the number of modalities can be an effective method for improving the accuracy of multi-label classification problems.\"","summary":"\"@cite_22 used a convolutional neural network (CNN) architecture based on the architecture of Kim @cite_21 to classify the title of the products. The first layer uses random word embedding. In addition they used a VGG network for image classification @cite_4 . While they experimented with both early and late fusion, only the late fusion resulted in an improvement in accuracy. The image and text classifiers were trained separately to achieve maximal performance individually before being combined by a policy network. The policy network which achieved the highest accuracy is a neural network with 2 fully connected layers and takes in the top-3 class probabilities from the image and text CNNs as input. Their dataset contained 1.2 million images and 2890 possible shelves. On average, each product falls in 3 shelves. Their model is considered accurate when the network correctly outputs one of the three shelves.\"","":""}
{"id":"2953607269","dialogue":"\"In this study, we investigated multi-modal approaches using images, descriptions, and title to categorize e-commerce products on this http URL. Specifically, we examined late fusion models, where the modalities are fused at the decision level. Products were each assigned multiple labels, and the hierarchy in the labels were flattened and filtered. For our individual baseline models, we modified a CNN architecture to classify the description and title, and then modified Keras' ResNet-50 to classify the images, achieving F1 scores of 77.0 , 82.7 , and 61.0 , respectively. In comparison, our tri-modal late fusion model can classify products more accurately than single modal models can, improving the F1 score to 88.2 . Each modality complemented the shortcomings of the other modalities, demonstrating that increasing the number of modalities can be an effective method for improving the accuracy of multi-label classification problems.\"","summary":"\"The dataset contained 96,806 products belonging to 193 different classes. Note that each product was assigned to one class. Hence @math berg applied a softmax function in the final layer before outputting the class probabilities. Similar to @cite_22 , both late and early fusion were explored, and late fusion yielded better results. Both heuristic policies and network policies were explored. Heuristic policies refer to some static rule; as an example, the mean of the probabilities from different modals. Network policies refer to training a neural network that takes the output probabilities from different networks and produces a new probability vector.\"","":""}
{"id":"2954440353","dialogue":"\"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.\"","summary":"\"The difference between the various surrogate approaches lies into the model space assumption and the acquisition function. Sequential Model-based Algorithm Configuration ( ) @cite_19 @cite_7 is based on Random Forest, so are frameworks based on such as @cite_29 @cite_18 or @cite_6 . @cite_15 uses a Tree-structured Parzen Estimator (TPE) while @cite_20 is based on Gaussian processes (GP).\"","":""}
{"id":"2954440353","dialogue":"\"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.\"","summary":"An acquisition function is used to determined the next configuration to be sampled. Most of those functions are based on Bayesian optimization @cite_32 @cite_16 . One popular strategy is to select @math such that it maximizes the expected improvement @cite_28 .","":""}
{"id":"2954440353","dialogue":"\"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.\"","summary":"\"As an alternative to Bayesian optimization, @cite_26 proposes to use Monte-Carlo Tree Search to iteratively explore a tree-structured search space while pruning the less promising configurations.\"","":""}
{"id":"2954440353","dialogue":"\"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.\"","summary":"\"In @cite_9 , the authors uses a genetic algorithm to sample a subset of representative input vectors in order to speed-up the model training while increasing the model performances. Genetic algorithms are also used to search for the whole pipeline as in @cite_36 or @cite_2 .\"","":""}
{"id":"2954440353","dialogue":"\"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.\"","summary":"\"The intrinsic difficulty of building a machine learning pipeline lies in the nature of the search space: the objective is non-separable i.e., the marginal performance of an operator @math depends on all the operators in all the paths leading to @math from the source, within the configuration space of a specific operator @math , there might be some dependencies between the hyperparameters (e.g. for Neural Networks, the coefficients @math , @math and @math make sense only for Adam solver @cite_10 ). Therefore, building a machine learning pipeline is a mix between selecting a proper sequence of operations and, for each operation, selecting the proper configuration in a structured and conditional space. On the contrary, most systems handle the problem by aggregating the whole search space, losing the sequential aspect of it. A notable exception is @cite_26 , inspired by , that explores the search space in terms of actions on operators (insertion, deletion, etc.).\"","":""}
{"id":"2954440353","dialogue":"\"Machines learning techniques plays a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as autoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.\"","summary":"\"To the best of our knowledge, the only approach that uses a non-predetermined sequence of operators is @cite_36 , but it is not possible to add additional constraints.\"","":""}
{"id":"2946521116","dialogue":"\"We take interest in the early assessment of risk for depression in social media users. We focus on the eRisk 2018 dataset, which represents users as a sequence of their written online contributions. We implement four RNN-based systems to classify the users. We explore several aggregations methods to combine predictions on individual posts. Our best model reads through all writings of a user in parallel but uses an attention mechanism to prioritize the most important ones at each timestep.\"","summary":"\"@cite_1 used a more classical approach to classify Twitter users as being at risk of depression or not. They first manually crafted features that describe users' online behavior and characterize their speech. The measures were computed daily, so a user is represented as the time series of the features. Then, the training and predictions were done by a svm with PCA for dimensionality reduction.\"","":""}
{"id":"2946521116","dialogue":"\"We take interest in the early assessment of risk for depression in social media users. We focus on the eRisk 2018 dataset, which represents users as a sequence of their written online contributions. We implement four RNN-based systems to classify the users. We explore several aggregations methods to combine predictions on individual posts. Our best model reads through all writings of a user in parallel but uses an attention mechanism to prioritize the most important ones at each timestep.\"","summary":"\"More similarly to our approach, @cite_4 used Hierarchical Attention Networks @cite_18 to represent user-generated documents. Sentence representations are learned using a rnn with an attention mechanism and are then used to learn the document's representation using the same network architecture. The computation of the attention weights they use is different from ours as it is non-parametric. Their equivalent of equation would be This means that the rnn learn the attention weights along with the representation of the sequences themselves. This attention function has been introduced in @cite_21 under that name of .\"","":""}
{"id":"2946521116","dialogue":"\"We take interest in the early assessment of risk for depression in social media users. We focus on the eRisk 2018 dataset, which represents users as a sequence of their written online contributions. We implement four RNN-based systems to classify the users. We explore several aggregations methods to combine predictions on individual posts. Our best model reads through all writings of a user in parallel but uses an attention mechanism to prioritize the most important ones at each timestep.\"","summary":"\"The @cite_21 is a simpler version of the that we used, that only takes into account the target hidden state. It is stated as such :\"","":""}
{"id":"2946521116","dialogue":"\"We take interest in the early assessment of risk for depression in social media users. We focus on the eRisk 2018 dataset, which represents users as a sequence of their written online contributions. We implement four RNN-based systems to classify the users. We explore several aggregations methods to combine predictions on individual posts. Our best model reads through all writings of a user in parallel but uses an attention mechanism to prioritize the most important ones at each timestep.\"","summary":"\"The function introduced in @cite_2 , has been improved in @cite_21 . use a concatenation layer to combine the information of the hidden state and the context vector.\"","":""}
{"id":"2946521116","dialogue":"\"We take interest in the early assessment of risk for depression in social media users. We focus on the eRisk 2018 dataset, which represents users as a sequence of their written online contributions. We implement four RNN-based systems to classify the users. We explore several aggregations methods to combine predictions on individual posts. Our best model reads through all writings of a user in parallel but uses an attention mechanism to prioritize the most important ones at each timestep.\"","summary":"\"was developed as part of Neural Turing Machines @cite_23 , where the attention is focused on inputs that are similar to the values in memory.\"","":""}
{"id":"2770651296","dialogue":"\"Scalability is a major issue for Internet of Things (IoT) as the total amount of traffic data collected and or the number of sensors deployed grow. In some IoT applications such as healthcare, power consumption is also a key design factor for the IoT devices. In this paper, a multi-signal compression and encoding method based on Analog Joint Source Channel Coding (AJSCC) is proposed that works fully in the analog domain without the need for power-hungry Analog-to-Digital Converters (ADCs). Compression is achieved by quantizing all the input signals but one. While saving power, this method can also reduce the number of devices by combining one or more sensing functionalities into a single device (called 'AJSCC device'). Apart from analog encoding, AJSCC devices communicate to an aggregator node (FPMM receiver) using a novel Frequency Position Modulation and Multiplexing (FPMM) technique. Such joint modulation and multiplexing technique presents three mayor advantages&#x2014;it is robust to interference at particular frequency bands, it protects against eavesdropping, and it consumes low power due to a very low Signal-to-Noise Ratio (SNR) operating region at the receiver. Performance of the proposed multi-signal compression method and FPMM technique is evaluated via simulations in terms of Mean Square Error (MSE) and Miss Detection Rate (MDR), respectively.\"","summary":"\"In the IoT healthcare and BAN domain, all of the existing solutions do sensing and communication in the digital domain (using ADCs DACs Microprocessors), which needs more power than analog sensing and communication. For example, Yang's group @cite_18 developed a miniaturized node that incorporates wireless communication, on-board processing, nine-axis motion tracking, and other sensors @cite_0 . It also developed an e-AR sensor @cite_27 , a small device to be worn behind the ear that captures information about the balance of the wearer such as gait, posture, skelet al joint shock-wave transmission, and activity of the individual. Yuce's group developed techniques based on Ultra Wide Band (UWB) wireless technology to reduce the power consumption of body-worn sensors @cite_11 @cite_8 . Another important example is the activity recognition of the user using various body sensors viz., accelerometer and gyroscope data. The data from the sensors is digitally processed and stored into a wrist-band device that syncs the data with the mobile phone using Bluetooth technology @cite_19 . Unlike these approaches, we adopt an entirely different approach based on analog sensing and communication that does not use any power-hungry ADCs (see Table ).\"","":""}
{"id":"2770651296","dialogue":"\"Scalability is a major issue for Internet of Things (IoT) as the total amount of traffic data collected and or the number of sensors deployed grow. In some IoT applications such as healthcare, power consumption is also a key design factor for the IoT devices. In this paper, a multi-signal compression and encoding method based on Analog Joint Source Channel Coding (AJSCC) is proposed that works fully in the analog domain without the need for power-hungry Analog-to-Digital Converters (ADCs). Compression is achieved by quantizing all the input signals but one. While saving power, this method can also reduce the number of devices by combining one or more sensing functionalities into a single device (called 'AJSCC device'). Apart from analog encoding, AJSCC devices communicate to an aggregator node (FPMM receiver) using a novel Frequency Position Modulation and Multiplexing (FPMM) technique. Such joint modulation and multiplexing technique presents three mayor advantages&#x2014;it is robust to interference at particular frequency bands, it protects against eavesdropping, and it consumes low power due to a very low Signal-to-Noise Ratio (SNR) operating region at the receiver. Performance of the proposed multi-signal compression method and FPMM technique is evaluated via simulations in terms of Mean Square Error (MSE) and Miss Detection Rate (MDR), respectively.\"","summary":"\"Shannon mapping has been applied in a number of applications such as Software-Defined Radio (SDR) systems @cite_6 , optical digital communications @cite_5 , Compressive Sensing (CS) @cite_16 , and digital video transmissions @cite_20 . All these applications use power-hungry ADCs and other digital components making such implementations unsuitable for healthcare and other low-power IoT solutions. Some works have studied the N:1 spiral-type mapping @cite_15 . The advantage of considering rectangular-type Shannon mapping is that there are existing low-power, all-analog circuit realizations for rectangular-type mapping (our previous work @cite_7 ). Using this approach, sensors can be designed using all-analog circuits that can compress multiple signals into one signal, thereby consuming less power. The signals from multiple sensors are multiplexed at different frequency locations in an interleaved pattern. Similar pattern has been studied for topics of pilot placement @cite_12 @cite_23 .\"","":""}
{"id":"2770651296","dialogue":"\"Scalability is a major issue for Internet of Things (IoT) as the total amount of traffic data collected and or the number of sensors deployed grow. In some IoT applications such as healthcare, power consumption is also a key design factor for the IoT devices. In this paper, a multi-signal compression and encoding method based on Analog Joint Source Channel Coding (AJSCC) is proposed that works fully in the analog domain without the need for power-hungry Analog-to-Digital Converters (ADCs). Compression is achieved by quantizing all the input signals but one. While saving power, this method can also reduce the number of devices by combining one or more sensing functionalities into a single device (called 'AJSCC device'). Apart from analog encoding, AJSCC devices communicate to an aggregator node (FPMM receiver) using a novel Frequency Position Modulation and Multiplexing (FPMM) technique. Such joint modulation and multiplexing technique presents three mayor advantages&#x2014;it is robust to interference at particular frequency bands, it protects against eavesdropping, and it consumes low power due to a very low Signal-to-Noise Ratio (SNR) operating region at the receiver. Performance of the proposed multi-signal compression method and FPMM technique is evaluated via simulations in terms of Mean Square Error (MSE) and Miss Detection Rate (MDR), respectively.\"","summary":"\"This paper is the first work to propose this structure for multiplexing data of AJSCC sensors. Table compares the power numbers of our circuit @cite_7 with state-of-the-art wireless sensor motes (all of which are digital). We can notice that @math is possible with our circuit, which is essential in low-power applications. The existing circuit realizations of spiral-type mapping also are all based on digital circuits and systems @cite_6 . In this scenario, it is worth noting that the Hybrid Digital Analog (HDA) coding can also perform signal compression @cite_26 @cite_17 . However, the digital part still needs digitization of the signals. Contrary to all these approaches, we propose signal compression and encoding in the analog domain with no need for ADCs DACs Microprocessors. To show the feasibility of our vision (analog compression and encoding), we previously developed a novel circuit to compress two signals @cite_7 and verified its applicability to two pathological signals (molecular biomarkers and physiological signal) @cite_21 . In this paper, we extend the theory for N-dimensional signal compression and propose novel multiplexing techniques that address the above mentioned challenges of scalability and power in the context of healthcare IoT as one of the key applications.\"","":""}
{"id":"2956033525","dialogue":"\"In this paper, a 1d convolutional neural network is designed for classification tasks of leaves with centroid contour distance curve (CCDC) as the single feature. With this classifier, simple feature as CCDC shows more discriminating power than people thought previously. The same architecture can also be applied for classifying 1 dimensional time series with little changes. Experiments on some benchmark datasets shows this architecture can provide classification accuracies that are higher than some existing methods. Code for the paper is available at this https URL Project.\"","summary":"\"On the side of shape features, they can be extracted based on botanical characteristics @cite_20 @cite_11 . These features may include: Aspect Ratio, Rectangularity, Convex Area, Ratio, Convex Perimeter Ratio, Sphericity, Circularity, Eccentricity, Form Factor, etc. @cite_1 discussed some other features applied on leave shapes and introduced two new multiscale triangle representations. There are also a lot of other work done with more in-depth design aiming for general shapes than just leaves. @cite_25 defines inner distance of shape contours to build shape descriptors. @cite_12 develops the visual descriptor called CENTRIST (CENsus TRansform hISTogram) for scene recognitions, it get good performance when applied to leave images. Authors of @cite_21 uses the transformation form shape contours to 1 dimensional time series and present the method of shapelet for shape recognition. @cite_24 describes a hierarchical representation for two dimensional objects that captures shape information at multiple levels of resolution for matching deformable shapes. Features coming from different method can be stacked together, these bagged features can usually help provide better performance as discussed in @cite_3 .\"","":""}
{"id":"2956033525","dialogue":"\"In this paper, a 1d convolutional neural network is designed for classification tasks of leaves with centroid contour distance curve (CCDC) as the single feature. With this classifier, simple feature as CCDC shows more discriminating power than people thought previously. The same architecture can also be applied for classifying 1 dimensional time series with little changes. Experiments on some benchmark datasets shows this architecture can provide classification accuracies that are higher than some existing methods. Code for the paper is available at this https URL Project.\"","summary":"\"Compared with methods mentioned above which tackles the difficulty in classification by designing complicated hand crafted deep features, convolutional neural networks (CNN) @cite_0 can take simple features as input and automatically abstracts useful features through its early convolutional blocks for later classification tasks @cite_8 . In this way, the difficulty is transferred into heavy computation where modern hardware now can provide sufficient support. It is more straightforward if we apply a CNN directly on leave images combining feature extraction task and classification task together, but this will make a model of unnecessary large size with a lot of parameters and they usually require a lot of data and time to be trained well with more risk of overfitting the data at hand. The key idea of this paper is to take the advantage of convolutional architecture, but apply it on the extracted single 1d CCDC feature to reduce the computational cost.\"","":""}
{"id":"2902948489","dialogue":"\"Millimeter-wave (mmWave) communications rely on directional transmissions to overcome severe path loss. Nevertheless, the use of narrow beams complicates the initial access procedure and increase t ...\"","summary":"\"The need for the design of new initial cell-search phase in mmWave communication has brought great research interest in this field recently. In IEEE 802.11ad standard, a coarse-grained sector matching is followed by a second beam training stage that provides a further refinement of the beamforming vectors @cite_20 @cite_32 . The authors in @cite_34 proposed a similar hierarchical design with multi-resolution codebook based on ideas from compressive sensing. Reference @cite_6 provided a framework to evaluate the performance of mmWave IA using 3GPP new radio (NR) scenario configurations. In @cite_19 , the authors analyzed various design options for IA given different scanning and signaling procedures. Specifically, the synchronization and cell-search consists of a sending a series of directional pilots to enable a joint time-frequency-spatial synchronization to occur jointly. @cite_16 and @cite_11 investigated initial cell-search based on context information which uses external localization service to get positioning information. In @cite_0 , the performance of these schemes are summarized in terms of cell detection failure probability and latency. In contrast to the aforementioned link-level studies, @cite_14 and @cite_2 provide a system-level analysis of IA protocols in terms of cell-search latency under different user equipment (UE) status.\"","":""}
{"id":"2952677293","dialogue":"\"Online communities provide a unique way for individuals to access information from those in similar circumstances, which can be critical for health conditions that require daily and personalized management. As these groups and topics often arise organically, identifying the types of topics discussed is necessary to understand their needs. As well, these communities and people in them can be quite diverse, and existing community detection methods have not been extended towards evaluating these heterogeneities. This has been limited as community detection methodologies have not focused on community detection based on semantic relations between textual features of the user-generated content. Thus here we develop an approach, NeuroCom, that optimally finds dense groups of users as communities in a latent space inferred by neural representation of published contents of users. By embedding of words and messages, we show that NeuroCom demonstrates improved clustering and identifies more nuanced discussion topics in contrast to other common unsupervised learning approaches.\"","summary":"\"Methods for identifying relevant groups is an active area of research; a great deal of work is on graph-based data such as social or information networks. Community detection is then based on choosing an objective function that captures the intuition of a community as a set of nodes with better internal connectivity than external connectivity @cite_9 . This is a rich area of research and briefly summarizing, there is work across spectral algorithms @cite_16 , measures of centrality @cite_0 and matrix factorization @cite_11 .\"","":""}
{"id":"2952677293","dialogue":"\"Online communities provide a unique way for individuals to access information from those in similar circumstances, which can be critical for health conditions that require daily and personalized management. As these groups and topics often arise organically, identifying the types of topics discussed is necessary to understand their needs. As well, these communities and people in them can be quite diverse, and existing community detection methods have not been extended towards evaluating these heterogeneities. This has been limited as community detection methodologies have not focused on community detection based on semantic relations between textual features of the user-generated content. Thus here we develop an approach, NeuroCom, that optimally finds dense groups of users as communities in a latent space inferred by neural representation of published contents of users. By embedding of words and messages, we show that NeuroCom demonstrates improved clustering and identifies more nuanced discussion topics in contrast to other common unsupervised learning approaches.\"","summary":"\"Neural representation has been implemented for social media short messages albeit in different ways than we propose @cite_1 . Gaussian Restricted Boltzmann Machines (RBM) have been used for modeling user's posts within a social network to identify their topics of interest, and finally construct communities @cite_1 . However, a parametric approach was used in which it was necessary to specify the number of clusters communities. In addition to not requiring such initialization, this approach is conceptually different than our proposed work in that it directly maps individuals to communities (instead of mapping the content of their posts, which may better capture heterogeneous community memberships). Further, content-based density approachefs as proposed, versus parametric ones could potentially learn a more organic number of communities. Given this gap, and the fact that content-based community detection (opposed to graph-based) may be more pertinent in health-related communities, here we explore content-based clustering of health communities.\"","":""}
{"id":"2902347647","dialogue":"\"De novo protein structure prediction from amino acid sequence is one of the most challenging problems in computational biology. As one of the extensively explored mathematical models for protein folding, Hydrophobic-Polar (HP) model enables thorough investigation of protein structure formation and evolution. Although HP model discretizes the conformational space and simplifies the folding energy function, it has been proven to be an NP-complete problem. In this paper, we propose a novel protein folding framework FoldingZero, self-folding a de novo protein 2D HP structure from scratch based on deep reinforcement learning. FoldingZero features the coupled approach of a two-head (policy and value heads) deep convolutional neural network (HPNet) and a regularized Upper Confidence Bounds for Trees (R-UCT). It is trained solely by a reinforcement learning algorithm, which improves HPNet and R-UCT iteratively through iterative policy optimization. Without any supervision and domain knowledge, FoldingZero not only achieves comparable results, but also learns the latent folding knowledge to stabilize the structure. Without exponential computation, FoldingZero shows promising potential to be adopted for real-world protein properties prediction.\"","summary":"\"Approximation algorithms offer rigorous mathematical tools and fold a protein structure within polynomial time. However, it may lead to a weak approximation ratio, resulting in a structure far from the optimal solution. Hart and Istrail proposed an approximation algorithm with ratio 3 8 of the optimal score for the 3D cubic lattice structure @cite_16 . An improved approximation algorithm with 2 5 performance guarantees was further developed by the same authors @cite_1 . For the 2D square lattice, an approximation algorithm @cite_7 can achieve the approximation ratio of 1 3.\"","":""}
{"id":"2902347647","dialogue":"\"De novo protein structure prediction from amino acid sequence is one of the most challenging problems in computational biology. As one of the extensively explored mathematical models for protein folding, Hydrophobic-Polar (HP) model enables thorough investigation of protein structure formation and evolution. Although HP model discretizes the conformational space and simplifies the folding energy function, it has been proven to be an NP-complete problem. In this paper, we propose a novel protein folding framework FoldingZero, self-folding a de novo protein 2D HP structure from scratch based on deep reinforcement learning. FoldingZero features the coupled approach of a two-head (policy and value heads) deep convolutional neural network (HPNet) and a regularized Upper Confidence Bounds for Trees (R-UCT). It is trained solely by a reinforcement learning algorithm, which improves HPNet and R-UCT iteratively through iterative policy optimization. Without any supervision and domain knowledge, FoldingZero not only achieves comparable results, but also learns the latent folding knowledge to stabilize the structure. Without exponential computation, FoldingZero shows promising potential to be adopted for real-world protein properties prediction.\"","summary":"\"Heuristic algorithms cannot guarantee the optimal solution, but they usually obtain an approximation solution in a reasonable time frame. Beutler and Dill introduced a Core-directed chain Growth method (CG) using a heuristic bias function to help assemble a hydrophobic core @cite_27 . Ant colony optimization based algorithms were developed by Shmygelska @cite_25 and Thalheim @cite_3 . proposed a new Monte Carlo method, fragment regrowth via energy-guided sequential sampling @cite_19 . Other techniques, such as simulated annealing @cite_26 , quantum annealing @cite_9 , genetic algorithms @cite_22 and reinforcement learning @cite_5 , were also applied to the HP model with limited success and scalability.\"","":""}
{"id":"2950959563","dialogue":"\"Given a set of observations generated by an optimization process, the goal of inverse optimization is to determine likely parameters of that process. We cast inverse optimization as a form of deep learning. Our method, called deep inverse optimization, is to unroll an iterative optimization process and then use backpropagation to learn parameters that generate the observations. We demonstrate that by backpropagating through the interior point algorithm we can learn the coefficients determining the cost vector and the constraints, independently or jointly, for both non-parametric and parametric linear programs, starting from one or multiple observations. With this approach, inverse optimization can leverage concepts and algorithms from deep learning.\"","summary":"\"In the parametric optimization setting, @cite_16 develop an optimization model that encodes KKT optimality conditions for imputing objective function coefficients of a convex optimization problem. @cite_28 focus on the same problem under the assumption of noisy measurements, developing a bilevel problem and two algorithms which are shown to maintain statistical consistency. Saez-Gallego and Morales @cite_20 address the case of learning @math and @math jointly in a parametric setting where the @math vector is assumed to be an affine function of a regressor. The general case of learning the weights of a parametric linear optimization problem where @math , @math and @math are functions of @math (Figure (iii)) has not been addressed in the literature.\"","":""}
{"id":"2950959563","dialogue":"\"Given a set of observations generated by an optimization process, the goal of inverse optimization is to determine likely parameters of that process. We cast inverse optimization as a form of deep learning. Our method, called deep inverse optimization, is to unroll an iterative optimization process and then use backpropagation to learn parameters that generate the observations. We demonstrate that by backpropagating through the interior point algorithm we can learn the coefficients determining the cost vector and the constraints, independently or jointly, for both non-parametric and parametric linear programs, starting from one or multiple observations. With this approach, inverse optimization can leverage concepts and algorithms from deep learning.\"","summary":"\"Recent work in machine learning @cite_27 @cite_23 @cite_33 views inverse optimization through the lens of online learning, where new observations appear over time rather than as one batch. Our approach may be applicable in online settings, but we focus on generality in the batch setting and do not investigate real-time cases.\"","":""}
{"id":"2950959563","dialogue":"\"Given a set of observations generated by an optimization process, the goal of inverse optimization is to determine likely parameters of that process. We cast inverse optimization as a form of deep learning. Our method, called deep inverse optimization, is to unroll an iterative optimization process and then use backpropagation to learn parameters that generate the observations. We demonstrate that by backpropagating through the interior point algorithm we can learn the coefficients determining the cost vector and the constraints, independently or jointly, for both non-parametric and parametric linear programs, starting from one or multiple observations. With this approach, inverse optimization can leverage concepts and algorithms from deep learning.\"","summary":"\"Methodologically, our unrolling strategy is similar to @cite_26 who directly optimize the hyperparameters of a neural network training procedure with gradient descent. Conceptually, the closest papers to our work are by Amos and Kolter @cite_7 and Donti, Amos and Kolter @cite_11 . However, these papers are written independently of the inverse optimization literature. Amos and Kolter @cite_7 present the OptNet framework, which integrates a quadratic optimization layer in a deep neural network. The gradients for updating the coefficients of the optimization problem are derived through implicit differentiation. This approach involves taking matrix differentials of the KKT conditions for the optimization problem in question, while our strategy is based on allowing a deep learning framework to unroll an existing optimization procedure. Their method has efficiency advantages, while our unrolling approach is easily applicable, including to processes for which the KKT conditions may not hold or are difficult to implicitly differentiate. We include a more in-depth discussion in .\"","":""}
{"id":"2902303115","dialogue":"\"We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.\"","summary":"\"@cite_25 @cite_27 proposed to extract deep features to divide the problem space and use simple probabilistic distribution at leaf nodes. These models enabled traditional decision regression trees with deep representation learning ability. Leaf node update rules were proposed based on convex optimization techniques, and they out-performed deep models without divide-and-conquer strategy. However, since the last layer of a deep model was used to divide the problem space, every path in the tree needs to be computed. Even when a branch of computation contributes little to the final prediction, it stills need evaluation because each splitting node requires the full forward-pass of the deep neural network. A model structure where each splitting node is separately evaluated was used @cite_7 for depth estimation, but a general framework was missing and the effect of computation path pruning was not investigated.\"","":""}
{"id":"2902303115","dialogue":"\"We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.\"","summary":"\"Pixel-difference feature is a special type of hand-crafted feature where only several pixels from an input are considered during its evaluation. They are thus efficient to compute and succeeded in computer vision tasks such as face detection @cite_17 , face alignment @cite_9 @cite_5 @cite_6 @cite_10 @cite_13 , pose estimation @cite_1 @cite_21 and body part classification @cite_24 . These features were also naturally incorporated into decision regression trees to divide the input feature space. A counterpart of sparse feature extraction process in CNNs is sparse convolution where the few non-zero entries in the convolution kernel determine the feature extraction process. To obtain a sparse convolution kernel, sparse decomposition @cite_3 and pruning @cite_11 techniques were proposed to sparsify a pre-trained dense CNN. @cite_8 proposed an alternative where random sparse kernel was initialized before the training process. While they focus on speeding up CNNs, there have not been study on using these sparse convolutional layers in problem space dividing process, as traditional pixel-difference feature was used in decision trees.\"","":""}
{"id":"2902657131","dialogue":"\"For precision medicine and personalized treatment, we need to identify predictive markers of disease. We focus on Alzheimer's disease (AD), where magnetic resonance imaging scans provide information about the disease status. By combining imaging with genome sequencing, we aim at identifying rare genetic markers associated with quantitative traits predicted from convolutional neural networks (CNNs), which traditionally have been derived manually by experts. Kernel-based tests are a powerful tool for associating sets of genetic variants, but how to optimally model rare genetic variants is still an open research question. We propose a generalized set of kernels that incorporate prior information from various annotations and multi-omics data. In the analysis of data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we evaluate whether (i) CNNs yield precise and reliable brain traits, and (ii) the novel kernel-based tests can help to identify loci associated with AD. The results indicate that CNNs provide a fast, scalable and precise tool to derive quantitative AD traits and that new kernels integrating domain knowledge can yield higher power in association tests of very rare variants.\"","summary":"\"Popular kernel-based tests include FaST-LMM-Set @cite_0 @cite_19 , the sequence kernel association test (SKAT, @cite_7 ) and optimal SKAT (SKAT-O, @cite_4 ), which are based on weighted linear kernels @math @cite_7 @cite_8 @cite_19 , or a linear combination of weighted linear and collapsing kernels @cite_4 . Newer approaches @cite_17 derive further data-adaptive combinations of linear, quadratic, IBS, and collapsing kernels. However, all these kernels provide suboptimal performance for the analysis of very rare genetic variants. Here, linear and quadratic kernels yield uninformative similarity measures (i.e., diagonal kernel matrices for singletons, which are variants with only one observed copy of the minor allele) and collapsing kernels often yield unspecific signals and aggregate noise.\"","":""}
{"id":"2903401627","dialogue":"\"Understanding web instructional videos is an essential branch of video understanding in two aspects. First, most existing video methods focus on short-term actions for a-few-second-long video clips; these methods are not directly applicable to long videos. Second, unlike unconstrained long videos, e.g., movies, instructional videos are more structured in that they have step-by-step procedure constraining the understanding task. In this paper, we study reasoning on instructional videos via question-answering (QA). Surprisingly, it has not been an emphasis in the video community despite its rich applications. We thereby introduce YouQuek, an annotated QA dataset for instructional videos based on the recent YouCook2. The questions in YouQuek are not limited to cues on one frame but related to logical reasoning in the temporal dimension. Observing the lack of effective representations for modeling long videos, we propose a set of carefully designed models including a novel Recurrent Graph Convolutional Network (RGCN) that captures both temporal order and relation information. Furthermore, we study multiple modalities including description and transcripts for the purpose of boosting video understanding. Extensive experiments on YouQuek suggest that RGCN performs the best in terms of QA accuracy and a better performance is gained by introducing human annotated description.\"","summary":"\"Instructional video understanding has received much attention recently. @cite_26 and @cite_1 both leverage the natural language annotation of the videos to learn the instructional procedure in videos. @cite_19 , however, propose to learn the temporal boundaries of different steps in a supervised manner without the aid of textual information. Dense captioning is also posed on instructional videos in @cite_24 , which aims at localizing temporal events from a video, and describing them with natural language sentences. Visual-linguistic ambiguities can be a common problem in instructional videos with narratives. @cite_7 focus on such ambiguities caused by the changing in visual appearance and referring expression, and aim to resolve references with no supervision. @cite_14 perform visual grounding task in instructional videos, also coping with visual-linguistic ambiguities. Yet, none of these works have tackled the QA problem on instructional videos, despite the uniqueness for instructional videos to perform reasoning.\"","":""}
{"id":"2903401627","dialogue":"\"Understanding web instructional videos is an essential branch of video understanding in two aspects. First, most existing video methods focus on short-term actions for a-few-second-long video clips; these methods are not directly applicable to long videos. Second, unlike unconstrained long videos, e.g., movies, instructional videos are more structured in that they have step-by-step procedure constraining the understanding task. In this paper, we study reasoning on instructional videos via question-answering (QA). Surprisingly, it has not been an emphasis in the video community despite its rich applications. We thereby introduce YouQuek, an annotated QA dataset for instructional videos based on the recent YouCook2. The questions in YouQuek are not limited to cues on one frame but related to logical reasoning in the temporal dimension. Observing the lack of effective representations for modeling long videos, we propose a set of carefully designed models including a novel Recurrent Graph Convolutional Network (RGCN) that captures both temporal order and relation information. Furthermore, we study multiple modalities including description and transcripts for the purpose of boosting video understanding. Extensive experiments on YouQuek suggest that RGCN performs the best in terms of QA accuracy and a better performance is gained by introducing human annotated description.\"","summary":"\"People are gaining interests in video question answering (VideoQA) in recent years. Most of the current VideoQA tasks are focusing on direct facts in short videos @cite_10 @cite_21 @cite_3 @cite_22 @cite_11 . They all automatically generate QA pairs using a state-of-the-art question generation algorithm proposed in @cite_29 . However, such auto-generation mechanism often generates QA pairs with poor quality and low diversity, though grammatically correct. Worse still, auto-generated QA pairs cannot involve reasoning. From the reasoning point of view, MovieQA @cite_31 use human annotated QA pairs on movies to evaluate automatic story comprehension. SVQA @cite_23 , following the step of @cite_2 , extend the CLEVR dataset to the video version. Yet, it still focuses on short-term relations, and does not fit natural settings.\"","":""}
{"id":"2902377675","dialogue":"\"Current Visual Question Answering (VQA) systems can answer intelligent questions about Known' visual content. However, their performance drops significantly when questions about visually and linguistically Unknown' concepts are presented during inference ( Open-world' scenario). A practical VQA system should be able to deal with novel concepts in real world settings. To address this problem, we propose an exemplar-based approach that transfers learning (i.e., knowledge) from previously Known' concepts to answer questions about the Unknown'. We learn a highly discriminative joint embedding space, where visual and semantic features are fused to give a unified representation. Once novel concepts are presented to the model, it looks for the closest match from an exemplar set in the joint embedding space. This auxiliary information is used alongside the given Image-Question pair to refine visual attention in a hierarchical fashion. Since handling the high dimensional exemplars on large datasets can be a significant challenge, we introduce an efficient matching scheme that uses a compact feature description for search and retrieval. To evaluate our model, we propose a new split for VQA, separating Unknown visual and semantic concepts from the training set. Our approach shows significant improvements over state-of-the-art VQA models on the proposed Open-World VQA dataset and standard VQA datasets.\"","summary":"\"VQA is an AI complete task that requires high-level multimodal reasoning both in visual and language domains. The recent literature in VQA mostly focus on the optimal mechanisms to fuse multimodal cues. A simple fusion approach was used by Lu al @cite_23 that progressively combines multimodal features using concatenations and sum-pooling operations. Xu al @cite_28 proposed a recurrent neural network to generate intelligent image captions by considering the previously predicted words and targeted visual content. Bilinear models provide an effective way to model complex interactions, but impose restrictions due to computational intractability for high-dimensional inputs. Efficient versions of bilinear pooling were used in @cite_2 @cite_24 to learn the second-order interactions between visual and language features. To further speed-up the computations, Ben-younes al @cite_25 introduced a Tucker fusion scheme that first projects the individual modalities to low dimensions and subsequently learns full bilinear relationships. Recently, Farazi al @cite_8 fused complementary object level features alongside image level descriptors to achieve superior performance.\"","":""}
{"id":"2902377675","dialogue":"\"Current Visual Question Answering (VQA) systems can answer intelligent questions about Known' visual content. However, their performance drops significantly when questions about visually and linguistically Unknown' concepts are presented during inference ( Open-world' scenario). A practical VQA system should be able to deal with novel concepts in real world settings. To address this problem, we propose an exemplar-based approach that transfers learning (i.e., knowledge) from previously Known' concepts to answer questions about the Unknown'. We learn a highly discriminative joint embedding space, where visual and semantic features are fused to give a unified representation. Once novel concepts are presented to the model, it looks for the closest match from an exemplar set in the joint embedding space. This auxiliary information is used alongside the given Image-Question pair to refine visual attention in a hierarchical fashion. Since handling the high dimensional exemplars on large datasets can be a significant challenge, we introduce an efficient matching scheme that uses a compact feature description for search and retrieval. To evaluate our model, we propose a new split for VQA, separating Unknown visual and semantic concepts from the training set. Our approach shows significant improvements over state-of-the-art VQA models on the proposed Open-World VQA dataset and standard VQA datasets.\"","summary":"\"Although most VQA approaches only work with the given training set, some efforts explore the use of supplementary information to help the VQA system. Generally, such methods employ external knowledge sources (both textual and visual) to augment the training set. For example, Teney al in @cite_30 @cite_21 used web searches to find related images which were used for answer prediction. Language based external knowledge bases were used by Wang al @cite_7 and Wu al @cite_20 to provide logical reasons for each answer choice and to answer a more diverse set of questions. More recently, Teney al @cite_16 proposed a meta-learning approach that learns to use an externally supplied support set comprising of example questions-answers. In contrast to these approaches, we do not use any external data, rather learn an attention function that learns to use similar examples from the training set to provide better inference-time predictions. Patro al @cite_0 proposed a differential attention mechanism that uses an exemplar from the training set to generates human-like attention maps, however does not consider a transferable attention function that can reason about new visual semantic concepts.\"","":""}
{"id":"2902055211","dialogue":"\"This paper builds upon the current methods to increase their capability and automation for 3D surface construction from noisy and potentially sparse point clouds. It presents an analysis of an artificial neural network surface regression and mapping method, describing caveats, improvements and justification for the different approach.\"","summary":"\"An approach was suggested by @cite_8 using an image processing inspiration for surface de-noising. It removes noise by applying a Weiner filter which approximates the components of the surface as a statistical distribution. There are two problems with this algorithm in our context. First, it needs to be decided this algorithm should be applied, unnecessary smoothing might remove features that describe the underlying geometry, although there is some attempt to apply a surface based anisotropic diffusion to preserve edges. In addition, the formula used requires the user to both know and supply the noise denoted by the variance @math @cite_8 . It may not be possible to determine the noise of the data as it is an unstructured point cloud.\"","":""}
{"id":"2902055211","dialogue":"\"This paper builds upon the current methods to increase their capability and automation for 3D surface construction from noisy and potentially sparse point clouds. It presents an analysis of an artificial neural network surface regression and mapping method, describing caveats, improvements and justification for the different approach.\"","summary":"\"Yumer and Kara suggest a NN regression method of surface fitting and hole stitching. The flexibility achieved by an adaptive neural network topology differs from previous attempts as the ideal topology of the network obtained (the hyperaparmeters) are not fixed @cite_11 , meaning the network can be tailored to each point cloud automatically. This method is good for removing noise as the underlying geometry of the point data and not random noise is represented in the final surface.\"","":""}
{"id":"2902055211","dialogue":"\"This paper builds upon the current methods to increase their capability and automation for 3D surface construction from noisy and potentially sparse point clouds. It presents an analysis of an artificial neural network surface regression and mapping method, describing caveats, improvements and justification for the different approach.\"","summary":"\"In a slightly different problem, where a NN is used to reconstruct the shape of a 3D object from its shading in a 2D @cite_3 . show from experiment that quantitative improvement does not necessarily lead to quantitative improvement. This is something to consider when using a 'black box' function like a neural network, especially where there could be some information loss. In this regard we must ensure that the final model is representative of the ground truth and not only rely on an error measure. It is suggested that more research must be done for 3D surface quality metrics @cite_3 . Visual quality will be assessed in the method presented here alongside quantitative results in the absence of quality metrics.\"","":""}
{"id":"2954035548","dialogue":"\"The honeynet is a promising active cyber defense mechanism. It reveals the fundamental Indicators of Compromise (IoC) by luring attackers to conduct adversarial behaviors in a controlled and monitored environment. The active interaction at the honeynet brings a high reward but also introduces high implementation costs and risks of adversarial honeynet exploitation. In this work, we apply the infinite-horizon Semi-Markov Decision Process (SMDP) to characterize the stochastic transition and sojourn time of attackers in the honeynet and quantify the reward-risk trade-off. In particular, we produce adaptive long-term engagement policies shown to be risk-averse, cost-effective, and time-efficient. Numerical results have demonstrated that our adaptive interaction policies can quickly attract attackers to the target honeypot and engage them for a sufficiently long period to obtain worthy threat information. Meanwhile, the penetration probability is kept at a low level. The results show that the expected utility is robust against attackers of a large range of persistence and intelligence. Finally, we apply reinforcement learning to SMDP to solve the curse of modeling. Under a prudent choice of the learning rate and exploration policy, we achieve a quick and robust convergence of the optimal policy and value.\"","summary":"\"SMDP generalizes MDP by considering the random sojourn time at each state, and is widely applied to machine maintenance @cite_13 , resource allocation @cite_8 , and cybersecurity @cite_9 . However, as far as we know, it is the first time that the SMDP is applied to determine the optimal attacker engagement policy and to quantify the trade-off between the investigation reward and the risks.\"","":""}
{"id":"2971620038","dialogue":"\"Learning from web data has attracted lots of research interest in recent years. However, crawled web images usually have two types of noises, label noise and background noise, which induce extra difficulties in utilizing them effectively. Most existing methods either rely on human supervision or ignore the background noise. In this paper, we propose a novel method, which is capable of handling these two types of noises together, without the supervision of clean images in the training stage. Particularly, we formulate our method under the framework of multi-instance learning by grouping ROIs (i.e., images and their region proposals) from the same category into bags. ROIs in each bag are assigned with different weights based on the representative discriminative scores of their nearest clusters, in which the clusters and their scores are obtained via our designed memory module. Our memory module could be naturally integrated with the classification module, leading to an end-to-end trainable system. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our method.\"","summary":"\"In learning classifier with web data, previous works focus on handling the label noise in three directions, removing label noise @cite_14 @cite_48 @cite_32 @cite_57 @cite_12 @cite_5 @cite_43 , building noise-robust model @cite_29 @cite_18 @cite_39 @cite_0 @cite_38 @cite_47 , and curriculum learning @cite_6 @cite_26 .\"","":""}
{"id":"2971620038","dialogue":"\"Learning from web data has attracted lots of research interest in recent years. However, crawled web images usually have two types of noises, label noise and background noise, which induce extra difficulties in utilizing them effectively. Most existing methods either rely on human supervision or ignore the background noise. In this paper, we propose a novel method, which is capable of handling these two types of noises together, without the supervision of clean images in the training stage. Particularly, we formulate our method under the framework of multi-instance learning by grouping ROIs (i.e., images and their region proposals) from the same category into bags. ROIs in each bag are assigned with different weights based on the representative discriminative scores of their nearest clusters, in which the clusters and their scores are obtained via our designed memory module. Our memory module could be naturally integrated with the classification module, leading to an end-to-end trainable system. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our method.\"","summary":"\"For label noise removal, some approaches address the label noise issue as outlier detection in an unsupervised manner. Xia al @cite_48 removes outlier images by using the reconstruction errors of an autoencoder. CleanNet @cite_32 used a fraction of manually-verified data to transfer the knowledge of label noise to other categories. For noise-robust model designing, Patrini al @cite_29 proposed to train DNN models with a loss correction framework, which is insensitive to class-dependent label noise. Sukhbaatar al @cite_11 developed an extra label flip layer which is enabled to match the noisy label distribution and absorb the noise. For curriculum learning @cite_7 , MentorNet @cite_45 designed an additional network to weight training examples and enforce the model to focus more on clean samples. CurriculumNet @cite_6 measured the distribution density of images in their feature space and ranked them for model training.\"","":""}
{"id":"2971620038","dialogue":"\"Learning from web data has attracted lots of research interest in recent years. However, crawled web images usually have two types of noises, label noise and background noise, which induce extra difficulties in utilizing them effectively. Most existing methods either rely on human supervision or ignore the background noise. In this paper, we propose a novel method, which is capable of handling these two types of noises together, without the supervision of clean images in the training stage. Particularly, we formulate our method under the framework of multi-instance learning by grouping ROIs (i.e., images and their region proposals) from the same category into bags. ROIs in each bag are assigned with different weights based on the representative discriminative scores of their nearest clusters, in which the clusters and their scores are obtained via our designed memory module. Our memory module could be naturally integrated with the classification module, leading to an end-to-end trainable system. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our method.\"","summary":"\"More recently, memory networks have been employed for one-shot learning @cite_41 @cite_2 , few-shot learning @cite_44 , and semi-supervised learning @cite_25 . Specifically, Kaiser al @cite_2 designed a memory augmented network that could do life-long one-short learning. By adding an abstracting memory module, CMN @cite_44 encoded videos into fixed-size features via a multi-saliency embedding algorithm. MA-DNN @cite_25 leveraged the assimilation-accommodation interaction in memory networks for semi-supervised learning.\"","":""}
{"id":"2956045289","dialogue":"\"CNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training.\"","summary":"\"The recent successes of deep learning in image classification @cite_21 and object recognition @cite_26 have encouraged the application of neural networks in place recognition. In early work, the pre-trained AlexNet @cite_15 network is used to produce a feature vector out of the Conv3 layer @cite_16 @cite_27 . Rather than simply using a pre-trained network, NetVLAD learns visual place recognition end-to-end. In NetVLAD, triplet loss is used to find the optimal VLAD encoding to match scenes across both viewpoint and condition variations @cite_8 . LoST uses the semantic CNN RefineNet @cite_25 to select salient keypoints within the width by height dimensions of a convolutional tensor @cite_12 . In a related work, these keypoints have been found by observing the activations out of a late convolutional layer @cite_0 . The aforementioned examples involve improving a pre-trained neural network for place recognition, either by re-training, or selecting the most useful components out of the network activations.\"","":""}
{"id":"2903071347","dialogue":"\"The main reason for the standardization of network protocols, like QUIC, is to ensure interoperability between implementations, which poses a challenging task. Manual tests are currently used to test the different existing implementations for interoperability, but given the complex nature of network protocols, it is hard to cover all possible edge cases. State-of-the-art automated software testing techniques, such as Symbolic Execution (SymEx), have proven themselves capable of analyzing complex real-world software and finding hard to detect bugs. We present a SymEx-based method for finding interoperability issues in QUIC implementations, and explore its merit in a case study that analyzes the interoperability of picoquic and QUANT. We find that, while SymEx is able to analyze deep interactions between different implementations and uncovers several bugs, in order to enable efficient interoperability testing, implementations need to provide additional information about their current protocol state.\"","summary":"\"Formal methods have long been used to analyze network protocols @cite_14 @cite_4 @cite_13 @cite_5 @cite_25 , often with a focus on security. However, even if the formal analysis of a network protocol has successfully proven a property, be it related to correctness or security, it is by no means guaranteed that this property will also hold for an implementation of said protocol.\"","":""}
{"id":"2903071347","dialogue":"\"The main reason for the standardization of network protocols, like QUIC, is to ensure interoperability between implementations, which poses a challenging task. Manual tests are currently used to test the different existing implementations for interoperability, but given the complex nature of network protocols, it is hard to cover all possible edge cases. State-of-the-art automated software testing techniques, such as Symbolic Execution (SymEx), have proven themselves capable of analyzing complex real-world software and finding hard to detect bugs. We present a SymEx-based method for finding interoperability issues in QUIC implementations, and explore its merit in a case study that analyzes the interoperability of picoquic and QUANT. We find that, while SymEx is able to analyze deep interactions between different implementations and uncovers several bugs, in order to enable efficient interoperability testing, implementations need to provide additional information about their current protocol state.\"","summary":"\"Programs have also been analyzed with formal methods, such as symex , to test for obvious problems like memory safety and assertion violations @cite_1 @cite_11 and for less easily checked properties, such as liveness violations @cite_6 and authentication bypass flaws in firmware binaries @cite_3 . One of the main problems encountered when formally analyzing real-world code is the penchant of the state-space to grow infeasibly large---a problem also known as . Many different approaches to tame the state explosion problem inherent in symex have been proposed in the past: state merging @cite_18 , targeted search strategies @cite_0 and pruning of provably equivalent paths @cite_15 , to name a few.\"","":""}
{"id":"2903071347","dialogue":"\"The main reason for the standardization of network protocols, like QUIC, is to ensure interoperability between implementations, which poses a challenging task. Manual tests are currently used to test the different existing implementations for interoperability, but given the complex nature of network protocols, it is hard to cover all possible edge cases. State-of-the-art automated software testing techniques, such as Symbolic Execution (SymEx), have proven themselves capable of analyzing complex real-world software and finding hard to detect bugs. We present a SymEx-based method for finding interoperability issues in QUIC implementations, and explore its merit in a case study that analyzes the interoperability of picoquic and QUANT. We find that, while SymEx is able to analyze deep interactions between different implementations and uncovers several bugs, in order to enable efficient interoperability testing, implementations need to provide additional information about their current protocol state.\"","summary":"\"As the the state explosion problem grows exponentially with the number of programs considered at the same time, approaches explicitly targeting distributed programs have been developed. For example, KleeNet @cite_19 @cite_10 exploits the independence of networked programs by delaying codependent path forks until messages are received at each node that require the fork to be actualized.\"","":""}
{"id":"2903071347","dialogue":"\"The main reason for the standardization of network protocols, like QUIC, is to ensure interoperability between implementations, which poses a challenging task. Manual tests are currently used to test the different existing implementations for interoperability, but given the complex nature of network protocols, it is hard to cover all possible edge cases. State-of-the-art automated software testing techniques, such as Symbolic Execution (SymEx), have proven themselves capable of analyzing complex real-world software and finding hard to detect bugs. We present a SymEx-based method for finding interoperability issues in QUIC implementations, and explore its merit in a case study that analyzes the interoperability of picoquic and QUANT. We find that, while SymEx is able to analyze deep interactions between different implementations and uncovers several bugs, in order to enable efficient interoperability testing, implementations need to provide additional information about their current protocol state.\"","summary":"\"Testing protocols and programs independently is -- however worthwhile -- not enough. To this end, approaches have been designed that test implementations for protocol compliance using many different testing and verification methodologies, ranging from fuzzing @cite_8 @cite_9 over symex @cite_23 to model checking @cite_2 @cite_25 . Validating that a given implementation fulfills a specification or standard does, however, require a formalized representation to be available, which effectively constitutes another implementation of the specification.\"","":""}
{"id":"2903071347","dialogue":"\"The main reason for the standardization of network protocols, like QUIC, is to ensure interoperability between implementations, which poses a challenging task. Manual tests are currently used to test the different existing implementations for interoperability, but given the complex nature of network protocols, it is hard to cover all possible edge cases. State-of-the-art automated software testing techniques, such as Symbolic Execution (SymEx), have proven themselves capable of analyzing complex real-world software and finding hard to detect bugs. We present a SymEx-based method for finding interoperability issues in QUIC implementations, and explore its merit in a case study that analyzes the interoperability of picoquic and QUANT. We find that, while SymEx is able to analyze deep interactions between different implementations and uncovers several bugs, in order to enable efficient interoperability testing, implementations need to provide additional information about their current protocol state.\"","summary":"\"One way to circumvent this chicken-egg problem is to exploit the fact that any relevant standard will have multiple implementations, which enables the substitution of compliance testing with that of interoperability testing. While it is possible that neither implementation is technically compliant with the standard, it becomes more and more improbable that the standard is captured incorrectly by many different people in exactly the same manner. Due to the inherent state-explosion problem of interoperability testing (multiple different, or even all possible programs are considered at once), multiple approaches to specialized @cite_22 and general @cite_17 @cite_21 interoperability testing have been proposed in the past.\"","":""}
{"id":"2913534916","dialogue":"\"Complex image processing and computer vision systems often consist of a processing pipeline of functional modules. We intend to replace parts or all of a target pipeline with deep neural networks to achieve benefits such as increased accuracy or reduced computational requirement. To acquire a large amount of labeled data necessary to train the deep neural network, we propose a workflow that leverages the target pipeline to create a significantly larger labeled training set automatically, without prior domain knowledge of the target pipeline. We show experimentally that despite the noise introduced by automated labeling and only using a very small initially labeled data set, the trained deep neural networks can achieve similar or even better performance than the components they replace, while in some cases also reducing computational requirements.\"","summary":"\"Our work can be considered an approach to @cite_16 @cite_8 , in which a target function is approximated by a surrogate that is cheaper to compute but introduces inaccuracy. In computer vision and image processing, some level of inaccuracy is often tolerable, due to the limits of human perception and the lack of a clearly delineated correct'' answer @cite_20 . Approximation can be introduced at the hardware level, such as by using approximate adder circuits (e.g. @cite_17 ), or at the software level by restructuring the algorithm.\"","":""}
{"id":"2913534916","dialogue":"\"Complex image processing and computer vision systems often consist of a processing pipeline of functional modules. We intend to replace parts or all of a target pipeline with deep neural networks to achieve benefits such as increased accuracy or reduced computational requirement. To acquire a large amount of labeled data necessary to train the deep neural network, we propose a workflow that leverages the target pipeline to create a significantly larger labeled training set automatically, without prior domain knowledge of the target pipeline. We show experimentally that despite the noise introduced by automated labeling and only using a very small initially labeled data set, the trained deep neural networks can achieve similar or even better performance than the components they replace, while in some cases also reducing computational requirements.\"","summary":"\"Our work can also be seen as an application of the semi-supervised learning paradigm @cite_15 , where the learner is given both labeled and unlabeled training data. We take a bootstrapping or self-supervised'' approach @cite_6 @cite_5 , using elements of the processing pipeline as surrogate models to label the unlabeled examples. The imputed labels will contain errors, and thus techniques for learning from noisy labels @cite_3 @cite_1 are also relevant. Several works have shown that neural networks can be trained successfully based on noisy labels (e.g. @cite_5 @cite_21 @cite_12 @cite_7 ).\"","":""}
{"id":"2902069582","dialogue":"\"We tackle one-shot visual search by example for arbitrary object categories: Given an example image of a novel reference object, find and segment all object instances of the same category within a scene. To address this problem, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We use Siamese Mask R-CNN to perform one-shot instance segmentation on MS-COCO, demonstrating that it can detect and segment objects of novel categories it was not trained on, and without using mask annotations at test time. Our results highlight challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories not used during training works very well, targeting the detection and segmentation networks towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research in this relatively unexplored field.\"","summary":"\"Object detection is a classical computer vision problem @cite_57 @cite_19 @cite_33 @cite_61 . Modern work can be split broadly into two general approaches: Single stage detectors @cite_92 @cite_70 @cite_20 @cite_43 @cite_42 are usually very fast, while multi-stage detectors @cite_34 @cite_84 @cite_11 @cite_93 perform a coarse proposal step followed by a fine-grained classification, and are usually more accurate. Most state-of-the-art systems are based on Faster R-CNN @cite_3 , a two-step object detector that generates proposals, for each of which it crops features out of the last feature map of a backbone. Feature Pyramid Networks @cite_12 are a popular extension that uses feature maps at multiple spatial resolutions to increase scale invariance.\"","":""}
{"id":"2902069582","dialogue":"\"We tackle one-shot visual search by example for arbitrary object categories: Given an example image of a novel reference object, find and segment all object instances of the same category within a scene. To address this problem, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We use Siamese Mask R-CNN to perform one-shot instance segmentation on MS-COCO, demonstrating that it can detect and segment objects of novel categories it was not trained on, and without using mask annotations at test time. Our results highlight challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories not used during training works very well, targeting the detection and segmentation networks towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research in this relatively unexplored field.\"","summary":"\"Visual search has a long history in perceptual psychology (reviewed, , by @cite_37 ), although typically with simple visual patterns, while search for arbitrary objects in real scenes has been addressed only recently @cite_45 @cite_29 , and often using a natural language cue @cite_29 .\"","":""}
{"id":"2902069582","dialogue":"\"We tackle one-shot visual search by example for arbitrary object categories: Given an example image of a novel reference object, find and segment all object instances of the same category within a scene. To address this problem, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We use Siamese Mask R-CNN to perform one-shot instance segmentation on MS-COCO, demonstrating that it can detect and segment objects of novel categories it was not trained on, and without using mask annotations at test time. Our results highlight challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories not used during training works very well, targeting the detection and segmentation networks towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research in this relatively unexplored field.\"","summary":"\"Few-Shot learning has seen great progress over the last years. A classic approach is based on metric learning using Siamese neural networks @cite_22 @cite_58 @cite_90 , which -- due to its simplicity -- is also the approach we use. The metric learning approach has seen a number of improvements in recent years @cite_56 @cite_89 @cite_51 @cite_60 @cite_15 . Other approaches are based on generative models @cite_27 @cite_68 , ideas from information retrieval @cite_32 or employ meta learning @cite_4 @cite_28 @cite_95 @cite_21 @cite_0 @cite_38 @cite_52 @cite_8 @cite_6 .\"","":""}
{"id":"2902069582","dialogue":"\"We tackle one-shot visual search by example for arbitrary object categories: Given an example image of a novel reference object, find and segment all object instances of the same category within a scene. To address this problem, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We use Siamese Mask R-CNN to perform one-shot instance segmentation on MS-COCO, demonstrating that it can detect and segment objects of novel categories it was not trained on, and without using mask annotations at test time. Our results highlight challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories not used during training works very well, targeting the detection and segmentation networks towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research in this relatively unexplored field.\"","summary":"\"There is related, but not directly comparable work on few-shot object detection. Some work focuses on settings with few (more than one) annotated training images per category @cite_16 @cite_83 , while others tackle the zero-shot setting based on only a textual description of the reference @cite_40 @cite_75 . Most closely related to our work is concurrent work based on Siamese networks for one-shot detection on an Omniglot-based dataset and for audio data @cite_86 as well as work on fine-grained bird classification and localization in ImageNet images @cite_47 , which tend to have only one or few instances per image. In contrast, we work on potentially cluttered real-world images.\"","":""}
{"id":"2903276340","dialogue":"\"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive amount of dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI sign language dataset which consists of 11,578 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting line for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from a face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieves 94.6 (60.6 , respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compare several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.\"","summary":"\"Until recently, there have been many attempts to recognize and translate sign language using deep learning (DL). @cite_33 have introduced and evaluated several architectures for CNNs to predict the 3D joint locations of a hand given a depth map. @cite_0 have developed a sign language recognition system that is robust in different video backgrounds by extracting signers using boundary and prior shape information. Then, the feature vector is constructed from the segmented signer and used as input to artificial neural network. An end-to-end sequence modelling using CNN-BLSTM architecture usually used for gesture recognition was proposed for large vocabulary sign language recognition with RWTH-PHOENIC-Weather 2014 @cite_8 .\"","":""}
{"id":"2903276340","dialogue":"\"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive amount of dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI sign language dataset which consists of 11,578 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting line for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from a face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieves 94.6 (60.6 , respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compare several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.\"","summary":"\"At the same time, one of the most interesting breakthroughs in neural machine translation or even in the entire DL was introduced under the name of sequence-to-sequence (seq2seq)' @cite_14 . The seq2seq model relies on a common framework called an encoder-decoder model with RNN cells such as LSTMs or GRUs. The seq2seq model proved its effectiveness in many sequence generation tasks by achieving almost the human-level performance @cite_14 . Despite its effectiveness, the seq2seq model still has some drawbacks such as the input sequences of varying lengths being represented in fixed-size vectors and the vanishing gradient due to the long-term dependency between distant parts.\"","":""}
{"id":"2903276340","dialogue":"\"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive amount of dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI sign language dataset which consists of 11,578 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting line for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from a face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieves 94.6 (60.6 , respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compare several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.\"","summary":"\"@cite_25 formalized a sign language translation based on the pre-existing framework of Neural Machine Translation (NMT) with word and spatial embeddings for target sequences and sign videos, respectively. The extracted non-linear frame from a sign video is converted into the spatial representation through @math CNN, and then it is tokenized. The sequence-to-sequence (seq2seq) based deep learning methods learns how to translate the spatio-temproal representation of signs into the spoken or written language. Recently, researchers developed a simple sign language recognition system based on bidirectional GRUs which just classifies a given sign language video into one of the classes that are predetermined @cite_23\"","":""}
{"id":"2955598033","dialogue":"\"In many machine learning applications, it is necessary to meaningfully aggregate, through alignment, different but related datasets. Optimal transport (OT)-based approaches pose alignment as a divergence minimization problem: the aim is to transform a source dataset to match a target dataset using the Wasserstein distance as a divergence measure. We introduce a hierarchical formulation of OT which leverages clustered structure in data to improve alignment in noisy, ambiguous, or multimodal settings. To solve this numerically, we propose a distributed ADMM algorithm that also exploits the Sinkhorn distance, thus it has an efficient computational complexity that scales quadratically with the size of the largest cluster. When the transformation between two datasets is unitary, we provide performance guarantees that describe when and how well aligned cluster correspondences can be recovered with our formulation, as well as provide worst-case dataset geometry for such a strategy. We apply this method to synthetic datasets that model data as mixtures of low-rank Gaussians and study the impact that different geometric properties of the data have on alignment. Next, we applied our approach to a neural decoding application where the goal is to predict movement directions and instantaneous velocities from populations of neurons in the macaque primary motor cortex. Our results demonstrate that when clustered structure exists in datasets, and is consistent across trials or time points, a hierarchical alignment strategy that leverages such structure can provide significant improvements in cross-domain alignment.\"","summary":"\"Various probability divergences have been proposed in the literature, such as Euclidean least-squares (when data ordering is known) @cite_33 @cite_24 @cite_17 , Kullbeck-Liebler (KL) @cite_16 , maximum mean discrepancy (MMD) @cite_40 @cite_20 @cite_14 @cite_7 , and the Wasserstein distance @cite_45 , where trade-offs are often statistical (e.g., consistency, sample complexity) versus computational. Alignment problems are ill-posed since the space of @math is large, so structure is often necessary to constrain @math based on geometric assumptions. Compact manifolds like the Grassmann or Stiefel @cite_15 @cite_18 are primary choices when little information is present, as they preserve isometry. Non-isometric transformations, though richer, demand much more structure (e.g., manifold or graph structure) @cite_38 @cite_31 @cite_19 @cite_21 @cite_45 .\"","":""}
{"id":"2955598033","dialogue":"\"In many machine learning applications, it is necessary to meaningfully aggregate, through alignment, different but related datasets. Optimal transport (OT)-based approaches pose alignment as a divergence minimization problem: the aim is to transform a source dataset to match a target dataset using the Wasserstein distance as a divergence measure. We introduce a hierarchical formulation of OT which leverages clustered structure in data to improve alignment in noisy, ambiguous, or multimodal settings. To solve this numerically, we propose a distributed ADMM algorithm that also exploits the Sinkhorn distance, thus it has an efficient computational complexity that scales quadratically with the size of the largest cluster. When the transformation between two datasets is unitary, we provide performance guarantees that describe when and how well aligned cluster correspondences can be recovered with our formulation, as well as provide worst-case dataset geometry for such a strategy. We apply this method to synthetic datasets that model data as mixtures of low-rank Gaussians and study the impact that different geometric properties of the data have on alignment. Next, we applied our approach to a neural decoding application where the goal is to predict movement directions and instantaneous velocities from populations of neurons in the macaque primary motor cortex. Our results demonstrate that when clustered structure exists in datasets, and is consistent across trials or time points, a hierarchical alignment strategy that leverages such structure can provide significant improvements in cross-domain alignment.\"","summary":"\"Principal components analysis (PCA), one of the most popular methods in data science, assumes a model where the top- @math principal components of a dataset provide the optimal rank- @math approximation under an Euclidean loss. This has been extended to robust (sparse errors) settings @cite_13 , and multi- (union of) subspaces settings where data can be partitioned into disjoint subsets where each subset of data is locally low-rank @cite_39 . Transfer learning methods based on subspace alignment @cite_43 @cite_10 @cite_26 work well with zero-mean unimodal datasets, but struggle on more complicated modalities (e.g., Gaussian mixtures or union of subspaces) due to a mixing of covariances. Related to our work, @cite_5 performs multi-subspace alignment by greedily assigning correspondences between subspaces using chordal distances; this however neglects sign ambiguities in principal directions since subspaces inadequately describe a distribution's shape.\"","":""}
{"id":"2963086661","dialogue":"\"Astronomy is well recognized as big data driven science. As the novel observation infrastructures are developed, the sky survey cycles have been shortened from a few days to a few seconds, causing data processing pressure to shift from offline to online. However, existing scientific databases focus on offline analysis of long-term historical data, not real-time and low latency analysis of large-scale newly arriving data.In this paper, a cloud based method is proposed to efficiently analyze scientific events on large-scale newly arriving data. The solution is implemented as a highly efficient system, namely Aserv. A set of compact data store and index structures are proposed to describe the proposed scientific events and a typical analysis pattern is formulized as a set of query operations. Domain aware filter, accuracy aware data partition, highly efficient index and frequently used statistical data designs are four key methods to optimize the performance of Aserv. Experimental results under the typical cloud environment show that the presented optimization mechanism can meet the low latency demand for both large data insertion and scientific event analysis. Aserv can insert 3.5 million rows of data within 3 seconds and perform the heaviest query on 6.7 billion rows of data also within 3 seconds. Furthermore, a performance model is given to help Aserv choose the right cloud resource setup to meet the guaranteed real-time performance requirement.\"","summary":"\". Real-time databases have been studied since 1980s, and the key goal is to enable as many real-time transactions as possible to meet their respective time constraints @cite_19 . Real-time databases are more concern with timeliness, not system speed @cite_33 , due to a basis hypothesis that catastrophic consequences do not happen in the real world if a transaction is finished within the deadline. Hence, many of works focus on scheduling @cite_19 @cite_4 and transaction processing @cite_10 @cite_28 . Storing and processing all the data under periodic time constraint can avoid data loss and ensure temporal data consistency maybe enough for traditional real-time databases, but the transient feature of scientific event requires that the online query latency should be as low as possible. Only in this way, we can exploit the value of scientific data. Periodic survey cycle and the unpredictability of scientific event propose the new challenge for online big scientific data analysis.\"","":""}
{"id":"2903456962","dialogue":"\"Cellular network configuration plays a critical role in network performance. In current practice, network configuration depends heavily on field experience of engineers and often remains static for a long period of time. This practice is far from optimal. To address this limitation, online-learning-based approaches have great potentials to automate and optimize network configuration. Learning-based approaches face the challenges of learning a highly complex function for each base station and balancing the fundamental exploration-exploitation tradeoff while minimizing the exploration cost. Fortunately, in cellular networks, base stations (BSs) often have similarities even though they are not identical. To leverage such similarities, we propose kernel-based multi-BS contextual bandit algorithm based on multi-task learning. In the algorithm, we leverage the similarity among different BSs defined by conditional kernel embedding. We present theoretical analysis of the proposed algorithm in terms of regret and multi-task-learning efficiency. We evaluate the effectiveness of our algorithm based on a simulator built by real traces.\"","summary":"\"Various aspects of network parameter configuration have been studied in the literature, such as pilot power configuration, spectrum, handoff threshold, etc. Traditional approaches derive analytical relationship between network configuration and its performance based on communication theory, such as @cite_13 @cite_5 @cite_20 @cite_15 . Such approaches are often prohibitively complex, involve various approximations, and require a significant amount of input information (such as the number of users, the location of each user, etc.).\"","":""}
{"id":"2903456962","dialogue":"\"Cellular network configuration plays a critical role in network performance. In current practice, network configuration depends heavily on field experience of engineers and often remains static for a long period of time. This practice is far from optimal. To address this limitation, online-learning-based approaches have great potentials to automate and optimize network configuration. Learning-based approaches face the challenges of learning a highly complex function for each base station and balancing the fundamental exploration-exploitation tradeoff while minimizing the exploration cost. Fortunately, in cellular networks, base stations (BSs) often have similarities even though they are not identical. To leverage such similarities, we propose kernel-based multi-BS contextual bandit algorithm based on multi-task learning. In the algorithm, we leverage the similarity among different BSs defined by conditional kernel embedding. We present theoretical analysis of the proposed algorithm in terms of regret and multi-task-learning efficiency. We evaluate the effectiveness of our algorithm based on a simulator built by real traces.\"","summary":"\"Recently, learning-based methods are proposed @cite_0 @cite_19 @cite_10 @cite_4 . In @cite_0 , the authors propose a tailored form of reinforcement learning to adaptively select the optimal antenna configuration in a time-varying environment. In @cite_4 , the authors use Q-learning with compact state representation for traffic offloading. In @cite_10 , the authors design a generalized global bandit algorithm to control the transmit power in the cellular coverage optimization problem. In all these papers, BS similarities are not considered, and thus require more exploration. In @cite_19 , the authors study the pilot power configuration problem and design a Gibbs-sampling-based online learning algorithm so as to maximize the throughput of users. In comparison, they make the assumption that all BSs are equal while we allow different BSs to learn different mappings.\"","":""}
{"id":"2903456962","dialogue":"\"Cellular network configuration plays a critical role in network performance. In current practice, network configuration depends heavily on field experience of engineers and often remains static for a long period of time. This practice is far from optimal. To address this limitation, online-learning-based approaches have great potentials to automate and optimize network configuration. Learning-based approaches face the challenges of learning a highly complex function for each base station and balancing the fundamental exploration-exploitation tradeoff while minimizing the exploration cost. Fortunately, in cellular networks, base stations (BSs) often have similarities even though they are not identical. To leverage such similarities, we propose kernel-based multi-BS contextual bandit algorithm based on multi-task learning. In the algorithm, we leverage the similarity among different BSs defined by conditional kernel embedding. We present theoretical analysis of the proposed algorithm in terms of regret and multi-task-learning efficiency. We evaluate the effectiveness of our algorithm based on a simulator built by real traces.\"","summary":"\"Contextual bandit @cite_16 is an extension of classic multi-armed bandit (MAB) problem @cite_8 . One type of algorithm is the UCB-type such as Lin-UCB @cite_6 , Kernel-UCB @cite_18 , in which they assume the reward is a function of the context and trade off between the exploitation and exploration based on upper confident bound of the estimation @cite_22 . The contextual bandit is also widely used in many application areas, such as news article recommendation @cite_6 , clinical trials @cite_14 .\"","":""}
{"id":"2954023930","dialogue":"\"Recently there emerge many distributed algorithms that aim at solving subgraph matching at scale. Existing algorithm-level comparisons failed to provide a systematic view to the pros and cons of each algorithm mainly due to the intertwining of strategy and optimization. In this paper, we identify four strategies and three general-purpose optimizations from representative state-of-the-art works. We implement the four strategies with the optimizations based on the common Timely dataflow system for systematic strategy-level comparison. Our implementation covers all representation algorithms. We conduct extensive experiments for both unlabelled matching and labelled matching to analyze the performance of distributed subgraph matching under various settings, which is finally summarized as a practical guide.\"","summary":"\"Isomorphism-based Subgraph Matching. In the labelled case, @cite_63 used the spanning tree of the query graph to filter infeasible results. @cite_6 observed the importance of matching order. In @cite_15 , the authors proposed to utilize the symmetry properties in the data graph to compress the results. @cite_25 proposed based on the core-forest-leaves'' matching order, and obtained performance gain by postponing the notorious cartesian product.\"","":""}
{"id":"2954023930","dialogue":"\"Recently there emerge many distributed algorithms that aim at solving subgraph matching at scale. Existing algorithm-level comparisons failed to provide a systematic view to the pros and cons of each algorithm mainly due to the intertwining of strategy and optimization. In this paper, we identify four strategies and three general-purpose optimizations from representative state-of-the-art works. We implement the four strategies with the optimizations based on the common Timely dataflow system for systematic strategy-level comparison. Our implementation covers all representation algorithms. We conduct extensive experiments for both unlabelled matching and labelled matching to analyze the performance of distributed subgraph matching under various settings, which is finally summarized as a practical guide.\"","summary":"\"The unlabelled case is also known as subgraph listing enumeration, and due to the gigantic (intermediate) results, people have been either seeking scalable algorithms in parallel, or devising techniques to compress the results. Other than the algorithms studied in this paper ( algorithms ), proposed the external-memory-based parallel algorithm DualSim @cite_46 , which maintains the data graph in blocks on the disk, and matches the query graph by swapping in out blocks of data to improve I O efficiency.\"","":""}
{"id":"2954023930","dialogue":"\"Recently there emerge many distributed algorithms that aim at solving subgraph matching at scale. Existing algorithm-level comparisons failed to provide a systematic view to the pros and cons of each algorithm mainly due to the intertwining of strategy and optimization. In this paper, we identify four strategies and three general-purpose optimizations from representative state-of-the-art works. We implement the four strategies with the optimizations based on the common Timely dataflow system for systematic strategy-level comparison. Our implementation covers all representation algorithms. We conduct extensive experiments for both unlabelled matching and labelled matching to analyze the performance of distributed subgraph matching under various settings, which is finally summarized as a practical guide.\"","summary":"\"Incremental Subgraph Matching. Computing subgraph matching in a continuous context has recently drawn a lot of attentions. @cite_9 proposed incremental algorithm that identifies a portion of the data graph affected by the update regarding the query. The authors in @cite_38 used the join scheme as algorithms ( edge-at-a-time ). The algorithm maintained a left-deep join tree for the query, with each vertex maintaining a partial query and the corresponding partial results. Then one can compute the incremental answers of each partial query in response to the update, and utilizes the join tree to re-construct the results. Graphflow @cite_4 solved incremental subgraph matching using join, in the sense that the incremental query can be transformed into @math independent joins, where @math is the number of query edges. Then they used the worst-case-optimal join algorithm to solve these joins in parallel. Most recently, proposed TurboFlux that maintains data-centric index for incremental queries, which achieves good tradeoff between performance and storage.\"","":""}
{"id":"2954023930","dialogue":"\"Recently there emerge many distributed algorithms that aim at solving subgraph matching at scale. Existing algorithm-level comparisons failed to provide a systematic view to the pros and cons of each algorithm mainly due to the intertwining of strategy and optimization. In this paper, we identify four strategies and three general-purpose optimizations from representative state-of-the-art works. We implement the four strategies with the optimizations based on the common Timely dataflow system for systematic strategy-level comparison. Our implementation covers all representation algorithms. We conduct extensive experiments for both unlabelled matching and labelled matching to analyze the performance of distributed subgraph matching under various settings, which is finally summarized as a practical guide.\"","summary":"\"Query Languages and Systems. As the increasing demand of subgraph matching in graph analysis, people start to investigate easy-use and highly expressive subgraph matching language. Neo4j introduced @cite_32 , and now people are working on standardizing the semantics of subgraph matching based on Cypher @cite_58 . Gradoop @cite_36 is a system based on Apache Hadoop that translates a Cypher query into a MapReduce job. proposed based on relational semantics for graph processing, in which they leveraged worst-case optimal join algorithm to solve subgraph matching. Arabesque @cite_1 was designed to solve graph mining (continuously computing frequent subgraphs) at scale, while it can be configured for single subgraph query.\"","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"The scene text proposal idea is mainly inspired by the success of object proposal in many object detection systems. It has advantage in locating more possible text regions to offer higher detection recall. It's often evaluated according to the recall rate as well as the number of needed proposals - typically the smaller the better at a similar recall level @cite_62 . False-positive scene text proposals are usually eliminated by either a text nontext classifier @cite_39 @cite_3 or a scene text recognition model @cite_34 @cite_11 in end-to-end scene text reading systems.","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"\"Different scene text proposal approaches have been explored. One widely adopted approach combines generic object proposal techniques with text-specific features for scene text proposal generation. For example, EdgeBoxes @cite_55 is combined with two text-specific features for scene text proposal generation @cite_11 . In another work @cite_49 , EdgeBoxes is combined with the Aggregate Channel Feature (ACF) and AdaBoost classifiers to search for text regions. In @cite_34 , Selective Search @cite_4 is combined with Maximally Stable Extremal Regions (MSER) to extract texture features for dendrogram grouping. A text-specific symmetry feature is explored in @cite_39 to search for text line proposals directly, where false text line proposals are removed by training a CNN classifier. Deep features have also been used for scene text proposal due to its superior performance in recent years. For example, inception layers are built on top of the last convolution layer of the VGG16 for generating text proposal candidates in @cite_3 . The Region Proposal Network (RPN) and Faster R-CNN structure are adopted for scene text proposal generation in @cite_63 @cite_35 .\"","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"\"Most existing scene text proposal techniques have various limitations. For example, the EdgeBoxes based technique @cite_49 is efficient but often generate a large number of false-positive proposals. The hand-crafted text-specific features rely heavily on object boundaries which are sensitive to image noise and degradation @cite_19 . Techniques using heuristic rules and parameters @cite_11 do not adapt well across datasets. The deep learning based technique @cite_3 produces a small number of proposals but the recall rate becomes unstable when the Intersection over Union (IoU) threshold increases. As a comparison, our proposed proposal technique does not leverage heuristic parameters and obtains a high recall rate with a small number of false-positive proposals.\"","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"\"A large number of scene text detection techniques have been reported in the literature. Sliding window has been widely used to search for texts in scene images @cite_51 @cite_30 @cite_9 . However, it usually has a low efficiency because it adopts an exhaustive search process by using multiple windows of different sizes and aspect ratios. Region based techniques have been proposed to overcome the low efficiency constraint. For example, the Maximal Stable External Regions (MSRE) has been widely used @cite_17 @cite_28 @cite_46 @cite_52 for scene text detection. In addition, various hand-craft text-specific features have also been extensively investigated such as Stroke Width Transform (SWT) @cite_12 , Stroke Feature Transform (SFT) @cite_14 , text edge specific features @cite_19 , Stroke End Keypoints (SEK), Stroke Bend Keypoints (SBK) @cite_29 , and deep features based regions @cite_36 @cite_58 @cite_27 . Different post-processing schemes have also been designed to remove false positives, e.g heuristic rules based classifier @cite_22 @cite_52 @cite_23 @cite_47 , graph processing @cite_51 @cite_30 , support vector regression @cite_19 , convolutional K-mean @cite_30 , distance metric learning @cite_17 , AdaBoost @cite_46 @cite_2 , random forest @cite_12 @cite_14 , convolution neural network @cite_9 @cite_28 , etc.\"","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"\"With the advance of convolutional neural network (CNN), different CNN models have been exploited for the scene text detection tasks. For example, the DeepText makes use of convolutional layers for deep features extraction and inception layers for bounding boxes predictions @cite_3 . The TextBoxes @cite_59 adopts the Single Shot Multiboxex Detector (SSD) @cite_21 to deal with multi-scale texts in scenes. Quadrilateral anchor boxes have also been proposed for detecting tighter scene text boxes @cite_40 . In addition, direct regression solution has also been proposed @cite_31 to remove the hand-crafted anchor boxes. Different CNN based detection and learning schemes have also been explored. For example, some work adopts a bottom-up approach that first detection characters and then group them to words or text lines @cite_63 @cite_57 @cite_41 . Some system instead defines a text boundary class for pixel-level scene text detection @cite_25 @cite_48 . In addition, weakly supervised and semi-supervised learning approach @cite_37 has also been studied to address the image annotation constraint @cite_7 .\"","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"\"Quite a number of CNN based end-to-end scene text reading systems have been reported in recent years. In @cite_9 @cite_45 , a CNN based character recognition model is developed where word information is extracted from text saliency map using sliding windows. The same framework has been implemented in @cite_60 , where a more robust end-to-end scene text reading system is developed by training a model handling three functions including text and non-text classification, case-insensitive characters recognition, and case-sensitive characters recognition. In @cite_59 , an advanced end-to-end scene text reading system is designed where the Single Shot Multiboxes Detector (SSD) is employed for scene text detection and a transcription model proposed in @cite_10 is adopted for recognition. End-to-end trainable scene text reading system has also been proposed which can concurrently produce texts location and text transcription @cite_61\"","":""}
{"id":"2896049206","dialogue":"\"Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.\"","summary":"Our developed end-to-end scene text reading system adopts a similar framework as presented in @cite_34 @cite_49 that exploits proposals and existing scene text recognition models. One unique feature is that it uses only around one-fifth of the number of proposals that prior proposal based end-to-end systems use thanks to our proposed pooling based proposal technique and gradient histogram based proposal ranking.","":""}
{"id":"2901872144","dialogue":"\"We develop methods for efficient amortized approximate Bayesian inference over posterior distributions of probabilistic clustering models, such as Dirichlet process mixture models. The approach is based on mapping distributed, symmetry-invariant representations of cluster arrangements into conditional probabilities. The method parallelizes easily, yields iid samples from the approximate posterior of cluster assignments with the same computational cost of a single Gibbs sampler sweep, and can easily be applied to both conjugate and non-conjugate models, as training only requires samples from the generative model.\"","summary":"\"The work @cite_7 provides an overview of deterministic clustering based on neural networks, and @cite_12 proposes a biologically inspired network for online clustering. Our work differs from previous approaches in its use of neural networks to explicitly approximate fully Bayesian inference in a probabilistic generative clustering model. Similar amortized approaches to Bayesian inference have been explored in Bayesian networks @cite_9 , sequential Monte Carlo @cite_1 , probabilistic programming @cite_13 @cite_15 and particle tracking @cite_3 . The representation of a set via a sum (or mean) of encoding vectors was also used in @cite_14 @cite_4 @cite_20 @cite_18 .\"","":""}
{"id":"2954017757","dialogue":"\"Creating a highly parallel and flexible discrete element software requires an interdisciplinary approach, where expertise from different disciplines is combined. On the one hand domain specialists provide interaction models between particles. On the other hand high-performance computing specialists optimize the code to achieve good performance on different hardware architectures. In particular, the software must be carefully crafted to achieve good scaling on massively parallel supercomputers. Combining all this in a flexible and extensible, widely usable software is a challenging task. In this article we outline the design decisions and concepts of a newly developed particle dynamics code MESA-PD that is implemented as part of the waLBerla multi-physics framework. Extensibility, flexibility, but also performance and scalability are primary design goals for the new software framework. In particular, the new modular architecture is designed such that physical models can be modified and extended by domain scientists without understanding all details of the parallel computing functionality and the underlying distributed data structures that are needed to achieve good performance on current supercomputer architectures. This goal is achieved by combining the high performance simulation framework waLBerla with code generation techniques. All code and the code generator are released as open source under GPLv3 within the publicly available waLBerla framework (this http URL).\"","summary":"\"Another approach to let the user extend the software framework is shown by some molecular dynamics packages. They use high level languages like Python or their own embedded domain specific language (DSL) to describe the simulation. In order to achieve good performance on various hardware architectures they use just-in-time compilation to generate user specific executables for various architectures. However, to support MPI or CUDA additional wrapper libraries like pyMPI and PyCUDA are needed. Many packages using this technique claim that this can be done with almost no loss in performance compared to native C++ code. Packages that provide such capabilities with a varying degree of just-in-time compilation are for example @cite_12 , @cite_13 , @cite_6 and @cite_4 .\"","":""}
{"id":"2955591928","dialogue":"\"String data are often disseminated to support applications such as location-based service provision or DNA sequence analysis. This dissemination, however, may expose sensitive patterns that model confidential knowledge (e.g., trips to mental health clinics from a string representing a user's location history). In this paper, we consider the problem of sanitizing a string by concealing the occurrences of sensitive patterns, while maintaining data utility. First, we propose a time-optimal algorithm, TFS-ALGO, to construct the shortest string preserving the order of appearance and the frequency of all non-sensitive patterns. Such a string allows accurately performing tasks based on the sequential nature and pattern frequencies of the string. Second, we propose a time-optimal algorithm, PFS-ALGO, which preserves a partial order of appearance of non-sensitive patterns but produces a much shorter string that can be analyzed more efficiently. The strings produced by either of these algorithms may reveal the location of sensitive patterns. In response, we propose a heuristic, MCSR-ALGO, which replaces letters in these strings with carefully selected letters, so that sensitive patterns are not reinstated and occurrences of spurious patterns are prevented. We implemented our sanitization approach that applies TFS-ALGO, PFS-ALGO and then MCSR-ALGO and experimentally show that it is effective and efficient.\"","summary":"\"Data sanitization ( knowledge hiding) aims at concealing patterns modeling confidential knowledge by limiting their frequency, so that they are not easily mined from the data. Existing methods are applied to: (I) a of set-valued data (transactions) @cite_4 or spatiotemporal data (trajectories) @cite_16 ; (II) a of sequences @cite_11 @cite_0 ; or (III) a sequence @cite_5 @cite_2 @cite_12 . Yet, none of these methods follows our CSD setting: Methods in category I are not applicable to string data, and those in categories II and III do not have guarantees on privacy-related constraints @cite_12 or on utility-related properties @cite_11 @cite_0 @cite_5 @cite_2 . Specifically, unlike our approach, @cite_12 cannot guarantee that all sensitive patterns are concealed (constraint C1 ), while @cite_11 @cite_0 @cite_5 @cite_2 do not guarantee the satisfaction of utility properties ( @math and P2 ).\"","":""}
{"id":"2955591928","dialogue":"\"String data are often disseminated to support applications such as location-based service provision or DNA sequence analysis. This dissemination, however, may expose sensitive patterns that model confidential knowledge (e.g., trips to mental health clinics from a string representing a user's location history). In this paper, we consider the problem of sanitizing a string by concealing the occurrences of sensitive patterns, while maintaining data utility. First, we propose a time-optimal algorithm, TFS-ALGO, to construct the shortest string preserving the order of appearance and the frequency of all non-sensitive patterns. Such a string allows accurately performing tasks based on the sequential nature and pattern frequencies of the string. Second, we propose a time-optimal algorithm, PFS-ALGO, which preserves a partial order of appearance of non-sensitive patterns but produces a much shorter string that can be analyzed more efficiently. The strings produced by either of these algorithms may reveal the location of sensitive patterns. In response, we propose a heuristic, MCSR-ALGO, which replaces letters in these strings with carefully selected letters, so that sensitive patterns are not reinstated and occurrences of spurious patterns are prevented. We implemented our sanitization approach that applies TFS-ALGO, PFS-ALGO and then MCSR-ALGO and experimentally show that it is effective and efficient.\"","summary":"Anonymization aims to prevent the disclosure of individuals' identity and or information that individuals are not willing to be associated with @cite_22 @cite_6 . Anonymization works such as @cite_22 @cite_18 @cite_14 are thus not alternatives to our work (see the appendix).","":""}
{"id":"2956057537","dialogue":"\"The area of parameterized approximation seeks to combine approximation and parameterized algorithms to obtain, e.g., (1+eps)-approximations in f(k,eps)n^ O(1) time where k is some parameter of the input. We obtain the following results on parameterized approximability: 1) In the maximum independent set of rectangles problem (MISR) we are given a collection of n axis parallel rectangles in the plane. Our goal is to select a maximum-cardinality subset of pairwise non-overlapping rectangles. This problem is NP-hard and also W[1]-hard [Marx, ESA'05]. The best-known polynomial-time approximation factor is O(loglog n) [Chalermsook and Chuzhoy, SODA'09] and it admits a QPTAS [Adamaszek and Wiese, FOCS'13; Chuzhoy and Ene, FOCS'16]. Here we present a parameterized approximation scheme (PAS) for MISR, i.e. an algorithm that, for any given constant eps>0 and integer k>0, in time f(k,eps)n^ g(eps) , either outputs a solution of size at least k (1+eps), or declares that the optimum solution has size less than k. 2) In the (2-dimensional) geometric knapsack problem (TDK) we are given an axis-aligned square knapsack and a collection of axis-aligned rectangles in the plane (items). Our goal is to translate a maximum cardinality subset of items into the knapsack so that the selected items do not overlap. In the version of TDK with rotations (TDKR), we are allowed to rotate items by 90 degrees. Both variants are NP-hard, and the best-known polynomial-time approximation factors are 558 325+eps and 4 3+eps, resp. [, FOCS'17]. These problems admit a QPTAS for polynomially bounded item sizes [Adamaszek and Wiese, SODA'15]. We show that both variants are W[1]-hard. Furthermore, we present a PAS for TDKR. For all considered problems, getting time f(k,eps)n^ O(1) , rather than f(k,eps)n^ g(eps) , would give FPT time f'(k)n^ O(1) exact algorithms using eps=1 (k+1), contradicting W[1]-hardness.\"","summary":"\"One of the first fruitful connections between parameterized complexity and approximability was observed independently by Bazgan @cite_23 and Cesati and Trevisan @cite_29 : They showed that EPTASs, i.e., @math -approximation algorithms with @math time, imply fixed-parameter tractability for the decision version. Thus, proofs for 1 -hardness of the decision version became a strong tool for ruling out improvements of PTASs, with running time @math , to EPTASs. More recently, @cite_25 improved this approach by directly proving 1 -hardness of obtaining a @math -approximation, thus bypassing the requirement of a 1 -hard decision version (see also @cite_33 ).\"","":""}
{"id":"2956057537","dialogue":"\"The area of parameterized approximation seeks to combine approximation and parameterized algorithms to obtain, e.g., (1+eps)-approximations in f(k,eps)n^ O(1) time where k is some parameter of the input. We obtain the following results on parameterized approximability: 1) In the maximum independent set of rectangles problem (MISR) we are given a collection of n axis parallel rectangles in the plane. Our goal is to select a maximum-cardinality subset of pairwise non-overlapping rectangles. This problem is NP-hard and also W[1]-hard [Marx, ESA'05]. The best-known polynomial-time approximation factor is O(loglog n) [Chalermsook and Chuzhoy, SODA'09] and it admits a QPTAS [Adamaszek and Wiese, FOCS'13; Chuzhoy and Ene, FOCS'16]. Here we present a parameterized approximation scheme (PAS) for MISR, i.e. an algorithm that, for any given constant eps>0 and integer k>0, in time f(k,eps)n^ g(eps) , either outputs a solution of size at least k (1+eps), or declares that the optimum solution has size less than k. 2) In the (2-dimensional) geometric knapsack problem (TDK) we are given an axis-aligned square knapsack and a collection of axis-aligned rectangles in the plane (items). Our goal is to translate a maximum cardinality subset of items into the knapsack so that the selected items do not overlap. In the version of TDK with rotations (TDKR), we are allowed to rotate items by 90 degrees. Both variants are NP-hard, and the best-known polynomial-time approximation factors are 558 325+eps and 4 3+eps, resp. [, FOCS'17]. These problems admit a QPTAS for polynomially bounded item sizes [Adamaszek and Wiese, SODA'15]. We show that both variants are W[1]-hard. Furthermore, we present a PAS for TDKR. For all considered problems, getting time f(k,eps)n^ O(1) , rather than f(k,eps)n^ g(eps) , would give FPT time f'(k)n^ O(1) exact algorithms using eps=1 (k+1), contradicting W[1]-hardness.\"","summary":"\"The systematic study of parameterized approximation as a field was initiated independently by three separate publications @cite_10 @cite_5 @cite_7 . A very good introduction to the area including key definitions as well as a survey of earlier results that fit into the picture was given by Marx @cite_24 . In particular, Marx also defined a so-called that, given input @math will run for @math time and return (say, for a maximization problem) a solution of value at least @math if the optimum is at least @math . As mentioned earlier, Marx pointed out that a standard FPT-approximation scheme that finds a solution of value at least @math in time @math if @math is not interesting to study: By setting @math we can decide the decision problem @math ?'' in FPT time. Thus, such a scheme is not helpful if the decision problem is 1 -hard and therefore unlikely to have an FPT-algorithm. Nevertheless, PASs can be useful in this case, as they imply standard FPT-approximation algorithms with ratio @math for each fixed @math despite 1 -hardness.\"","":""}
{"id":"2956057537","dialogue":"\"The area of parameterized approximation seeks to combine approximation and parameterized algorithms to obtain, e.g., (1+eps)-approximations in f(k,eps)n^ O(1) time where k is some parameter of the input. We obtain the following results on parameterized approximability: 1) In the maximum independent set of rectangles problem (MISR) we are given a collection of n axis parallel rectangles in the plane. Our goal is to select a maximum-cardinality subset of pairwise non-overlapping rectangles. This problem is NP-hard and also W[1]-hard [Marx, ESA'05]. The best-known polynomial-time approximation factor is O(loglog n) [Chalermsook and Chuzhoy, SODA'09] and it admits a QPTAS [Adamaszek and Wiese, FOCS'13; Chuzhoy and Ene, FOCS'16]. Here we present a parameterized approximation scheme (PAS) for MISR, i.e. an algorithm that, for any given constant eps>0 and integer k>0, in time f(k,eps)n^ g(eps) , either outputs a solution of size at least k (1+eps), or declares that the optimum solution has size less than k. 2) In the (2-dimensional) geometric knapsack problem (TDK) we are given an axis-aligned square knapsack and a collection of axis-aligned rectangles in the plane (items). Our goal is to translate a maximum cardinality subset of items into the knapsack so that the selected items do not overlap. In the version of TDK with rotations (TDKR), we are allowed to rotate items by 90 degrees. Both variants are NP-hard, and the best-known polynomial-time approximation factors are 558 325+eps and 4 3+eps, resp. [, FOCS'17]. These problems admit a QPTAS for polynomially bounded item sizes [Adamaszek and Wiese, SODA'15]. We show that both variants are W[1]-hard. Furthermore, we present a PAS for TDKR. For all considered problems, getting time f(k,eps)n^ O(1) , rather than f(k,eps)n^ g(eps) , would give FPT time f'(k)n^ O(1) exact algorithms using eps=1 (k+1), contradicting W[1]-hardness.\"","summary":"\"A central goal of parameterized approximation is to settle the status of problems like Dominating Set or Clique , which are hard to approximate and also parameterized intractable. Recently, Chen and Lin @cite_28 made important progress by showing that Dominating Set admits no constant-factor approximation with running time @math unless @math . Generally, for problems without exact FPT-algorithms, the goal is to find out whether one can beat inapproximability bounds by allowing FPT-time in some parameter; see e.g. @cite_32 @cite_9 @cite_21 @cite_13 @cite_6 @cite_17 @cite_3 @cite_4 @cite_8 ).\"","":""}
{"id":"2956057537","dialogue":"\"The area of parameterized approximation seeks to combine approximation and parameterized algorithms to obtain, e.g., (1+eps)-approximations in f(k,eps)n^ O(1) time where k is some parameter of the input. We obtain the following results on parameterized approximability: 1) In the maximum independent set of rectangles problem (MISR) we are given a collection of n axis parallel rectangles in the plane. Our goal is to select a maximum-cardinality subset of pairwise non-overlapping rectangles. This problem is NP-hard and also W[1]-hard [Marx, ESA'05]. The best-known polynomial-time approximation factor is O(loglog n) [Chalermsook and Chuzhoy, SODA'09] and it admits a QPTAS [Adamaszek and Wiese, FOCS'13; Chuzhoy and Ene, FOCS'16]. Here we present a parameterized approximation scheme (PAS) for MISR, i.e. an algorithm that, for any given constant eps>0 and integer k>0, in time f(k,eps)n^ g(eps) , either outputs a solution of size at least k (1+eps), or declares that the optimum solution has size less than k. 2) In the (2-dimensional) geometric knapsack problem (TDK) we are given an axis-aligned square knapsack and a collection of axis-aligned rectangles in the plane (items). Our goal is to translate a maximum cardinality subset of items into the knapsack so that the selected items do not overlap. In the version of TDK with rotations (TDKR), we are allowed to rotate items by 90 degrees. Both variants are NP-hard, and the best-known polynomial-time approximation factors are 558 325+eps and 4 3+eps, resp. [, FOCS'17]. These problems admit a QPTAS for polynomially bounded item sizes [Adamaszek and Wiese, SODA'15]. We show that both variants are W[1]-hard. Furthermore, we present a PAS for TDKR. For all considered problems, getting time f(k,eps)n^ O(1) , rather than f(k,eps)n^ g(eps) , would give FPT time f'(k)n^ O(1) exact algorithms using eps=1 (k+1), contradicting W[1]-hardness.\"","summary":"\"For the special case of where all input objects are squares a PTAS is known @cite_16 but there can be no EPTAS @cite_27 . Recently, @cite_1 found polynomial-time algorithms for and with approximation ratio smaller than @math (also for the weighted case). For the special case that all input objects are squares there is a PTAS @cite_19 and even an EPTAS @cite_15 .\"","":""}
{"id":"2901560048","dialogue":"\"Word sense induction (WSI), or the task of automatically discovering multiple senses or meanings of a word, has three main challenges: domain adaptability, novel sense detection, and sense granularity flexibility. While current latent variable models are known to solve the first two challenges, they are not flexible to different word sense granularities, which differ very much among words, from aardvark with one sense, to play with over 50 senses. Current models either require hyperparameter tuning or nonparametric induction of the number of senses, which we find both to be ineffective. Thus, we aim to eliminate these requirements and solve the sense granularity problem by proposing AutoSense, a latent variable model based on two observations: (1) senses are represented as a distribution over topics, and (2) senses generate pairings between the target word and its neighboring word. These observations alleviate the problem by (a) throwing garbage senses and (b) additionally inducing fine-grained word senses. Results show great improvements over the state-of-the-art models on popular WSI datasets. We also show that AutoSense is able to learn the appropriate sense granularity of a word. Finally, we apply AutoSense to the unsupervised author name disambiguation task where the sense granularity problem is more evident and show that AutoSense is evidently better than competing models. We share our data and code here: this https URL.\"","summary":"\"Previous works on WSI used context vectors and attributes @cite_8 , pretrained classification systems @cite_12 , and alignment of parallel corpus @cite_27 . In the most recent shared task on WSI @cite_7 , top models used lexical substitution method () @cite_17 and Hierarchical Dirichlet Process trained with additional instances () @cite_20 .\"","":""}
{"id":"2901560048","dialogue":"\"Word sense induction (WSI), or the task of automatically discovering multiple senses or meanings of a word, has three main challenges: domain adaptability, novel sense detection, and sense granularity flexibility. While current latent variable models are known to solve the first two challenges, they are not flexible to different word sense granularities, which differ very much among words, from aardvark with one sense, to play with over 50 senses. Current models either require hyperparameter tuning or nonparametric induction of the number of senses, which we find both to be ineffective. Thus, we aim to eliminate these requirements and solve the sense granularity problem by proposing AutoSense, a latent variable model based on two observations: (1) senses are represented as a distribution over topics, and (2) senses generate pairings between the target word and its neighboring word. These observations alleviate the problem by (a) throwing garbage senses and (b) additionally inducing fine-grained word senses. Results show great improvements over the state-of-the-art models on popular WSI datasets. We also show that AutoSense is able to learn the appropriate sense granularity of a word. Finally, we apply AutoSense to the unsupervised author name disambiguation task where the sense granularity problem is more evident and show that AutoSense is evidently better than competing models. We share our data and code here: this https URL.\"","summary":"\"Latent variable models such as LDA @cite_5 are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (, ) @cite_1 . More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (, ) @cite_0 and that topics and senses should be inferred jointly () @cite_16 . In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of target-neighbor pairs. HC was also extended to a nonparametric model () @cite_21 in order to automatically set the number of senses of a word, providing flexibility to the sense granularity @cite_29 @cite_6 @cite_20 . In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.\"","":""}
{"id":"2901560048","dialogue":"\"Word sense induction (WSI), or the task of automatically discovering multiple senses or meanings of a word, has three main challenges: domain adaptability, novel sense detection, and sense granularity flexibility. While current latent variable models are known to solve the first two challenges, they are not flexible to different word sense granularities, which differ very much among words, from aardvark with one sense, to play with over 50 senses. Current models either require hyperparameter tuning or nonparametric induction of the number of senses, which we find both to be ineffective. Thus, we aim to eliminate these requirements and solve the sense granularity problem by proposing AutoSense, a latent variable model based on two observations: (1) senses are represented as a distribution over topics, and (2) senses generate pairings between the target word and its neighboring word. These observations alleviate the problem by (a) throwing garbage senses and (b) additionally inducing fine-grained word senses. Results show great improvements over the state-of-the-art models on popular WSI datasets. We also show that AutoSense is able to learn the appropriate sense granularity of a word. Finally, we apply AutoSense to the unsupervised author name disambiguation task where the sense granularity problem is more evident and show that AutoSense is evidently better than competing models. We share our data and code here: this https URL.\"","summary":"\"In the unsupervised author name disambiguation (UAND) domain, LDA-based models have also been used @cite_22 to employ text features for the task, while non-text features such as co-authors, publication venue, year, and citations are found to be stronger features @cite_28 . In this paper, we study on how to improve the performance of text features for UAND using latent variable models, which can later be combined with non-text features in the future work.\"","":""}
{"id":"2901707509","dialogue":"\"3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10 of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark. We will release full source code and pretrained models upon acceptance of this manuscript for publication.\"","summary":"\"Detecting 2D bounding boxes in images is a widely studied problem and recent approaches are able to excel even on the most formidable datasets @cite_26 @cite_39 @cite_17 . Existing methods may broadly be divided into two main categories: detectors such as YOLO @cite_16 , SSD @cite_27 and RetinaNet @cite_40 which predict object bounding boxes directly and detectors such as Faster R-CNN @cite_1 and FPN @cite_23 which add an intermediate region proposal stage. To date the vast majority of 3D object detection methods have adopted the latter philosophy, in part due to the difficulty in mapping from fixed-sized regions in 3D space to variable-sized regions in the image space. We overcome this limitation via our OFT transform, allowing us to take advantage of the purported speed and accuracy benefits @cite_40 of a single-stage architecture.\"","":""}
{"id":"2901707509","dialogue":"\"3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10 of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark. We will release full source code and pretrained models upon acceptance of this manuscript for publication.\"","summary":"\"Obtaining 3D bounding boxes from images, meanwhile, is a much more challenging problem on account of the absence of absolute depth information. Many approaches start from 2D bounding boxes extracted using standard detectors described above, upon which they either directly regress 3D pose parameters for each region @cite_34 @cite_15 @cite_11 @cite_3 or fit 3D templates to the image @cite_35 @cite_4 @cite_13 @cite_7 . Perhaps most closely related to our work is Mono3D @cite_24 which densely spans the 3D space with 3D bounding box proposals and then scores each using a variety of image-based features. Other works which explore the idea of dense 3D proposals in the world space are 3DOP @cite_14 and Pham and Jeon @cite_33 , which rely on explicit estimates of depth using stereo geometry. A major limitation of all the above works is that each region proposal or bounding box is treated independently, precluding any joint reasoning about the 3D configuration of the scene. Our method performs a similar feature aggregation step to @cite_24 , but applies a secondary convolutional network to the resulting proposals whilst retaining their spatial configuration.\"","":""}
{"id":"2901707509","dialogue":"\"3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10 of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark. We will release full source code and pretrained models upon acceptance of this manuscript for publication.\"","summary":"\"Integral images have been fundamentally associated with object detection ever since their introduction in the seminal work of Viola and Jones @cite_22 . They have formed an important component in many contemporary 3D object detection approaches including AVOD @cite_6 , MV3D @cite_2 , Mono3D @cite_24 and 3DOP @cite_14 . In all of these cases however, integral images do not backpropagate gradients or form part of a fully end-to-end deep learning architecture. To our knowledge, the only prior work to do so is that of Kasagi al @cite_31 , which combines a convolutional layer and an average pooling layer to reduce computational cost.\"","":""}
{"id":"2901423179","dialogue":"\"Although Neural Machine Translation (NMT) models have advanced state-of-the-art performance in machine translation, they face problems like the inadequate translation. We attribute this to that the standard Maximum Likelihood Estimation (MLE) cannot judge the real translation quality due to its several limitations. In this work, we propose an adequacy-oriented learning mechanism for NMT by casting translation as a stochastic policy in Reinforcement Learning (RL), where the reward is estimated by explicitly measuring translation adequacy. Benefiting from the sequence-level training of RL strategy and a more accurate reward designed specifically for translation, our model outperforms multiple strong baselines, including (1) standard and coverage-augmented attention models with MLE-based training, and (2) advanced reinforcement and adversarial training strategies with rewards based on both word-level BLEU and character-level chrF3. Quantitative and qualitative analyses on different language pairs and NMT architectures demonstrate the effectiveness and universality of the proposed approach.\"","summary":"\"Recent work shows that maximum likelihood training could be sub-optimal due to the different conditions between training and test modes @cite_10 @cite_4 . In order to address the exposure bias and the loss which does not operate at the sequence level, Ranzato:2016:ICLR employ the REINFORCE algorithm @cite_14 to decide whether or not tokens from a sampled prediction could contribute to a high task-specific score ( BLEU). bahdanau2016actor use the actor-critic method from reinforcement learning to directly optimize a task-specific score.\"","":""}
{"id":"2901423179","dialogue":"\"Although Neural Machine Translation (NMT) models have advanced state-of-the-art performance in machine translation, they face problems like the inadequate translation. We attribute this to that the standard Maximum Likelihood Estimation (MLE) cannot judge the real translation quality due to its several limitations. In this work, we propose an adequacy-oriented learning mechanism for NMT by casting translation as a stochastic policy in Reinforcement Learning (RL), where the reward is estimated by explicitly measuring translation adequacy. Benefiting from the sequence-level training of RL strategy and a more accurate reward designed specifically for translation, our model outperforms multiple strong baselines, including (1) standard and coverage-augmented attention models with MLE-based training, and (2) advanced reinforcement and adversarial training strategies with rewards based on both word-level BLEU and character-level chrF3. Quantitative and qualitative analyses on different language pairs and NMT architectures demonstrate the effectiveness and universality of the proposed approach.\"","summary":"\"Recently, adversarial learning @cite_7 has been successfully applied to neural machine translation @cite_15 @cite_26 @cite_11 . In the adversarial framework, NMT models generally serve as the generator which defines the policy to generate the target sentence y given the source sentence x . A discriminator tries to distinguish the translation result @math from the human-generated one @math , given the source sentence @math .\"","":""}
{"id":"2901423179","dialogue":"\"Although Neural Machine Translation (NMT) models have advanced state-of-the-art performance in machine translation, they face problems like the inadequate translation. We attribute this to that the standard Maximum Likelihood Estimation (MLE) cannot judge the real translation quality due to its several limitations. In this work, we propose an adequacy-oriented learning mechanism for NMT by casting translation as a stochastic policy in Reinforcement Learning (RL), where the reward is estimated by explicitly measuring translation adequacy. Benefiting from the sequence-level training of RL strategy and a more accurate reward designed specifically for translation, our model outperforms multiple strong baselines, including (1) standard and coverage-augmented attention models with MLE-based training, and (2) advanced reinforcement and adversarial training strategies with rewards based on both word-level BLEU and character-level chrF3. Quantitative and qualitative analyses on different language pairs and NMT architectures demonstrate the effectiveness and universality of the proposed approach.\"","summary":"\"Inadequate translation problem is a commonly-cited weakness of NMT models @cite_16 . A number of recent efforts have explored ways to alleviate this problem. For example, tu2016modeling and Mi:2016:EMNLP employ coverage vector as a lexical-level indicator to indicate whether a source word is translated or not. Zheng:2018:TACL and Meng:2018:IJCAI move one step further and directly model translated and untranslated source contents by operating on the attention context vector. He:2017:NIPS use a prediction network to estimate the future cost of translating the uncovered source words. Our approach is complementary to theirs since they model the adequacy learning at the word-level inside the generator (i.e., NMT models), while we model it at the sequence-level outside the generator. We take the representative coverage mechanism @cite_16 as another stronger baseline model for its simplicity and efficiency, and experimental results show that our model can further improve performance.\"","":""}
{"id":"2901423179","dialogue":"\"Although Neural Machine Translation (NMT) models have advanced state-of-the-art performance in machine translation, they face problems like the inadequate translation. We attribute this to that the standard Maximum Likelihood Estimation (MLE) cannot judge the real translation quality due to its several limitations. In this work, we propose an adequacy-oriented learning mechanism for NMT by casting translation as a stochastic policy in Reinforcement Learning (RL), where the reward is estimated by explicitly measuring translation adequacy. Benefiting from the sequence-level training of RL strategy and a more accurate reward designed specifically for translation, our model outperforms multiple strong baselines, including (1) standard and coverage-augmented attention models with MLE-based training, and (2) advanced reinforcement and adversarial training strategies with rewards based on both word-level BLEU and character-level chrF3. Quantitative and qualitative analyses on different language pairs and NMT architectures demonstrate the effectiveness and universality of the proposed approach.\"","summary":"\"' denotes discriminator and O '' denotes orientator. MRT '' indicates minimum risk training @cite_22 , and D @math '' indicates adversarial training with a CNN-based discriminator @cite_15 . # Para.'' denotes the number of parameters, and Speed'' denotes the training speed (words second). @math '' and @math '' indicate statistically significant difference ( @math and @math respectively) from the corresponding baseline.\"","":""}
{"id":"2901254300","dialogue":"\"This paper presents methods of making using of text supervision to improve the performance of sequence-to-sequence (seq2seq) voice conversion. Compared with conventional frame-to-frame voice conversion approaches, the seq2seq acoustic modeling method proposed in our previous work achieved higher naturalness and similarity. In this paper, we further improve its performance by utilizing the text transcriptions of parallel training data. First, a multi-task learning structure is designed which adds auxiliary classifiers to the middle layers of the seq2seq model and predicts linguistic labels as a secondary task. Second, a data-augmentation method is proposed which utilizes text alignment to produce extra parallel sequences for model training. Experiments are conducted to evaluate our proposed method with training sets at different sizes. Experimental results show that the multi-task learning with linguistic labels is effective at reducing the errors of seq2seq voice conversion. The data-augmentation method can further improve the performance of seq2seq voice conversion when only 50 or 100 training utterances are available.\"","summary":"\"On image processing tasks, cropping images is common approach of data augmentation @cite_10 . In this paper, we propose to slice fragments from parallel utterances according to text alignment and use them as training samples. This technique could make use of more alignment information within the parallel utterances and is expected to reduce overfitting of the built seq2seq model.\"","":""}
{"id":"2901678097","dialogue":"\"Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ( @math ), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding @math to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.\"","summary":"\"In object classification, Incremental learning (IL) is the process of increasing the breadth of an object classifier, by training it to recognize new classes, while retaining its knowledge of the classes on which it has been trained originally. In the past couple of years, there has been considerable research efforts in this field @cite_0 @cite_3 . Moreover, there exist several subsets of this research problem which impose different constraints in terms of data storage and evaluation. We can divide existing methods based on their constraints:\"","":""}
{"id":"2901678097","dialogue":"\"Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ( @math ), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding @math to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.\"","summary":"\"In this problem, a model trained to perform object classification on specific classes of a dataset is incrementally trained to classify new unseen classes in the same dataset. Most of the existing work exploring this problem use single-headed evaluation. This makes the CI problem more difficult than the TI problem because the model can confuse the new class with a base class in the CI problem. iCaRL @cite_4 belongs to this category. In iCaRL @cite_4 , propose a technique to jointly learn feature representation and classifiers. They also introduce a strategy to select exemplars which is used in combination with the distillation loss to prevent catastrophic forgetting. In addition, a new baseline: LwF-MC is introduced in @cite_4 , which is a class incremental version of LwF @cite_3 . LwF-MC uses the distillation loss to preserve the knowledge of base classes along with a classification loss, without storing the data of base classes and is evaluated using single-headed evaluation. Another work aiming to solve the CI problem is @cite_2 , which evaluates using both single-headed and multi-headed evaluations and highlight their difference. @cite_2 introduce metrics to quantify forgetting and intransigence, and also propose an algorithm: Riemannian walk to incrementally learn classes.\"","":""}
{"id":"2901678097","dialogue":"\"Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ( @math ), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding @math to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.\"","summary":"\"Several experiments have been proposed to use a small percentage of the data of base classes while training the classifier to learn new classes. iCaRL @cite_4 uses the exemplars of base classes, while incrementally learning new classes. Similarly, @cite_2 also use a fraction of the data of base classes. @cite_2 also show that this is especially useful for alleviating intransigence, which is a problem faced in single-headed evaluation. However, storing data for base classes increases memory requirement at each incremental step, which is not feasible when the memory budget is limited.\"","":""}
{"id":"2901678097","dialogue":"\"Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ( @math ), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding @math to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.\"","summary":"\"Several TI methods described earlier (such as @cite_15 @cite_13 ) do not use the information about base classes while training the classifier to learn new classes incrementally. To the best of our knowledge, LwF-MC @cite_4 is the only CI method which does not use base class data but uses single-headed evaluation.\"","":""}
{"id":"2901678097","dialogue":"\"Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ( @math ), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding @math to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.\"","summary":"\"We intend to explore the CI problem by proposing to constrain the attention maps of the teacher and student models to be equivalent (in addition to their prediction vectors), to improve the information preserving capability of LwF-MC @cite_4 . In LwF-MC and our proposed method LwM, storing teacher models trained in previous incremental steps is not allowed since it would not be feasible to accumulate models from all the previous steps when the memory budget is limited.\"","":""}
{"id":"2901012461","dialogue":"\"Multi-object tracking (MOT) is an important and practical task related to both surveillance systems and moving camera applications, such as autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works treat tracking as a re-identification (Re-ID) task, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by appearance similarity with CNN features and intersection-over-union (IOU) with epipolar constraints to compensate camera movement between adjacent frames. Then, for every pair of two tracklets, the similarity is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups which represent individual object IDs. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieve promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.\"","summary":"\"Besides graph models, recurrent neural networks (RNN)-based tracking also plays an important role in recent years @cite_34 @cite_27 @cite_38 @cite_41 @cite_6 @cite_3 . One advantage of RNN-based tracking is the ability of online prediction. However, along with the propagation of RNN block, the relation between two faraway detections becomes very weak. Without direct connections, the performance of RNN-based methods degrades in the long run and sometimes can be easily affected by unreliable detections.\"","":""}
{"id":"2901012461","dialogue":"\"Multi-object tracking (MOT) is an important and practical task related to both surveillance systems and moving camera applications, such as autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works treat tracking as a re-identification (Re-ID) task, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by appearance similarity with CNN features and intersection-over-union (IOU) with epipolar constraints to compensate camera movement between adjacent frames. Then, for every pair of two tracklets, the similarity is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups which represent individual object IDs. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieve promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.\"","summary":"\"Features are very important in the tracking-by-detection framework. There are two types of features that are used in common, i.e., appearance features and temporal features. For appearance features, many works adopt CNN-based features from Re-ID tasks @cite_14 @cite_20 @cite_36 . However, histogram-based features, like color histograms, HOG, and LBP, are still powerful if no training data is provided @cite_33 . As for temporal features, the location, size, and motion of bounding boxes are commonly used. Given the appearance features and temporal features, the tracker can fuse them together using human defined weights @cite_20 @cite_31 @cite_33 . Although @cite_34 @cite_27 propose RNN-based networks to combine features together, it is still empirical and difficult to determine the weight of each feature.\"","":""}
{"id":"2901012461","dialogue":"\"Multi-object tracking (MOT) is an important and practical task related to both surveillance systems and moving camera applications, such as autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works treat tracking as a re-identification (Re-ID) task, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by appearance similarity with CNN features and intersection-over-union (IOU) with epipolar constraints to compensate camera movement between adjacent frames. Then, for every pair of two tracklets, the similarity is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups which represent individual object IDs. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieve promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.\"","summary":"\"Another category of tracking is based on end-to-end frameworks @cite_37 @cite_16 @cite_26 , where we input raw video sequences and output object trajectory. In other words, the detection and tracking are trained jointly in a single-stage network. One major advantage of this framework is that the errors will not be accumulated from detection to tracking. The temporal information across frames can help improve the detection performance, while reliable detections can also feedback reliable tracking. However, such a framework requires a lot of training data. Without enough training data, overfitting becomes a severe problem. Unlike detection based training, tracking annotations for video sequences are usually hard to get, which becomes the major limitation of the end-to-end tracking framework.\"","":""}
{"id":"2901632551","dialogue":"\"Approximate inference algorithm is one of the fundamental research fields in machine learning. The two dominant theoretical inference frameworks in machine learning are variational inference (VI) and Markov chain Monte Carlo (MCMC). However, because of the fundamental limitation in the theory, it is very challenging to improve existing VI and MCMC methods on both the computational scalability and statistical efficiency. To overcome this obstacle, we propose a new theoretical inference framework called ergodic Inference based on the fundamental property of ergodic transformations. The key contribution of this work is to establish the theoretical foundation of ergodic inference for the development of practical algorithms in future work.\"","summary":"\"pmlr-v70-hoffman17a proposed another hybrid method based on VI and HMC without auxiliary approximation. The idea is to use a Monte Carlo estimation of the marginal likelihood by averaging over samples from HMC chains, that are initialized by variational distribution. In a very similar framework is proposed using Metropolis-adjusted Langevin dynamics. This idea is very similar to contrastive divergence in @cite_2 . The main disadvantage of this methods is that the HMC parameters are manually pretuned. Especially, As mentioned by , No-U-turn Sampler (NUTS), an adaptive HMC, is not appliable due to engineering difficulties. @cite_6 pointed out that HMC is very sensitive to the choice of Leapfrog step size and number of leaps.\"","":""}
{"id":"2900626851","dialogue":"\"Knowledge transfer","summary":"zero-shot learning and semantic image retrieval are methods that aim at improving accuracy by utilizing semantic information","":""}
{"id":"2900626851","dialogue":"\"Knowledge transfer","summary":"zero-shot learning and semantic image retrieval are methods that aim at improving accuracy by utilizing semantic information","":""}
{"id":"2900626851","dialogue":"\"Knowledge transfer","summary":"zero-shot learning and semantic image retrieval are methods that aim at improving accuracy by utilizing semantic information","":""}
{"id":"2900626851","dialogue":"\"Knowledge transfer","summary":"zero-shot learning and semantic image retrieval are methods that aim at improving accuracy by utilizing semantic information","":""}
{"id":"2901710196","dialogue":"\"We study the emergence of communication in multiagent adversarial settings inspired by the classic Imitation game. A class of three player games is used to explore how agents based on sequence to sequence (Seq2Seq) models can learn to communicate information in adversarial settings. We propose a modeling approach, an initial set of experiments and use signaling theory to support our analysis. In addition, we describe how we operationalize the learning process of actor-critic Seq2Seq based agents in these communicational games.\"","summary":"Attention has been recently brought to the communicational aspects of multiagent systems. Recent research has explored cooperative @cite_13 @cite_15 @cite_4 and semi-cooperative scenarios such as negotiation games @cite_5 . The emergence of communication in adversarial scenarios has been explored less extensively.","":""}
{"id":"2901710196","dialogue":"\"We study the emergence of communication in multiagent adversarial settings inspired by the classic Imitation game. A class of three player games is used to explore how agents based on sequence to sequence (Seq2Seq) models can learn to communicate information in adversarial settings. We propose a modeling approach, an initial set of experiments and use signaling theory to support our analysis. In addition, we describe how we operationalize the learning process of actor-critic Seq2Seq based agents in these communicational games.\"","summary":"\"Generative adversarial networks (GANs) @cite_1 have resulted in a wide range of interesting adversarial applications. However, the extension to sequence to sequence models (Seq2Seq) @cite_20 has been difficult. Combining GAN with Seq2Seq models is challenging because discrete samples drawn from categorical distributions hinder backpropagation. In addition, alternatives on how to perform reward imputation to partial sequences @cite_3 have been proposed.\"","":""}
{"id":"2901710196","dialogue":"\"We study the emergence of communication in multiagent adversarial settings inspired by the classic Imitation game. A class of three player games is used to explore how agents based on sequence to sequence (Seq2Seq) models can learn to communicate information in adversarial settings. We propose a modeling approach, an initial set of experiments and use signaling theory to support our analysis. In addition, we describe how we operationalize the learning process of actor-critic Seq2Seq based agents in these communicational games.\"","summary":"\"With respect to backpropagating errors, reparametrization @cite_7 has been used multiple times to allow for backpropagation through stochastic nodes. In particular, Gumbel-Softmax @cite_16 has allowed categorical distributions @cite_8 in stochastic computational graphs. This technique has been shown as an alternative to reinforcement learning @cite_4 within the scope of cooperative referential games. More recently, similar ideas resulted in SeqGAN @cite_11 being proposed. Further incremental improvements have been published, such as applying actor-critic models @cite_17 or combining with proximal policy optimization (PPO) @cite_14 in order to improve learning performance.\"","":""}
{"id":"2954881165","dialogue":"\"In a disaster situation, first responders need to quickly acquire situational awareness and prioritize response based on the need, resources available and impact. Can they do this based on digital media such as Twitter alone, or newswire alone, or some combination of the two? We examine this question in the context of the 2015 Nepal Earthquakes. Because newswire articles are longer, effective summaries can be helpful in saving time yet giving key content. We evaluate the effectiveness of several unsupervised summarization techniques in capturing key content. We propose a method to link tweets written by the public and newswire articles, so that we can compare their key characteristics: timeliness, whether tweets appear earlier than their corresponding news articles, and content. A novel idea is to view relevant tweets as a summary of the matching news article and evaluate these summaries. Whenever possible, we present both quantitative and qualitative evaluations. One of our main findings is that tweets and newswire articles provide complementary perspectives that form a holistic view of the disaster situation.\"","summary":"\"Twitter for emergency applications has been studied by several researchers, e.g., @cite_35 @cite_14 @cite_7 @cite_1 @cite_28 . @cite_35 , researchers concluded that Twitter was not yet ready for first responders. However, it was helpful for civilians. These were the early days of Twitter, as we find from @cite_14 that individuals immediately posted specific information helpful to early recognition and characterization of emergency events'' in the case of the Boston marathon bombing. @cite_7 , researchers found that tangible, useful information was found in the early period before storm system Sandy and it got buried in emotional tweets as the storm actually hit. However, we think more studies are needed on this issue, since the tweets collected were rather small, approximately 27,000, using just the hashtag #sandy. A bilingual analysis of tweets obtained over 84 days overlapping the Tohoku earthquake showed, among other results, the correlation between Twitter data and earthquake events @cite_1 . A survey of this literature can be found in @cite_28 .\"","":""}
{"id":"2954881165","dialogue":"\"In a disaster situation, first responders need to quickly acquire situational awareness and prioritize response based on the need, resources available and impact. Can they do this based on digital media such as Twitter alone, or newswire alone, or some combination of the two? We examine this question in the context of the 2015 Nepal Earthquakes. Because newswire articles are longer, effective summaries can be helpful in saving time yet giving key content. We evaluate the effectiveness of several unsupervised summarization techniques in capturing key content. We propose a method to link tweets written by the public and newswire articles, so that we can compare their key characteristics: timeliness, whether tweets appear earlier than their corresponding news articles, and content. A novel idea is to view relevant tweets as a summary of the matching news article and evaluate these summaries. Whenever possible, we present both quantitative and qualitative evaluations. One of our main findings is that tweets and newswire articles provide complementary perspectives that form a holistic view of the disaster situation.\"","summary":"\"Researchers have examined the question of whether Twitter can replace newswire for breaking news @cite_20 . They studied a period of 77 days in 2011 during which 27 events occurred. The biggest disasters in this event-set are: an airplane crash with 43 deaths, and a magnitude 5.8 earthquake in Virginia that caused infrastructural damage. None of these disasters, bad as they are, rise to the level of the Nepal Earthquake(s) of 2015 in which almost 10,000 lives were lost. They collected a large dataset of tweets and news articles, but then eliminated a large collection of tweets based on clustering. More elimination of tweets led to only 97 linked tweet-news article pairs, which is a small dataset.\"","":""}
{"id":"2954881165","dialogue":"\"In a disaster situation, first responders need to quickly acquire situational awareness and prioritize response based on the need, resources available and impact. Can they do this based on digital media such as Twitter alone, or newswire alone, or some combination of the two? We examine this question in the context of the 2015 Nepal Earthquakes. Because newswire articles are longer, effective summaries can be helpful in saving time yet giving key content. We evaluate the effectiveness of several unsupervised summarization techniques in capturing key content. We propose a method to link tweets written by the public and newswire articles, so that we can compare their key characteristics: timeliness, whether tweets appear earlier than their corresponding news articles, and content. A novel idea is to view relevant tweets as a summary of the matching news article and evaluate these summaries. Whenever possible, we present both quantitative and qualitative evaluations. One of our main findings is that tweets and newswire articles provide complementary perspectives that form a holistic view of the disaster situation.\"","summary":"\"In @cite_31 , a framework for connecting news articles to Twitter conversations is proposed using Local cosine similarity, global cosine similarity, local frequency of the hashtag and global frequency of the hashtag as the classification features extracted for each article-hashtag pair. The task of linking tweets with related news articles is studied in another paper to construct user profiles @cite_10 . The authors proposed two sets of strategies to find relevant news articles to each tweet in this paper. In addition to URL-based strategies, which is similar to the idea used in @cite_25 , they also proposed several content-based strategies that include computing the similarity between hashtag-based, entity-based and bag-of-word-based representations of tweets and news articles to discover the relation between them. In addition to user modeling, the tweet-news linking task has been employed in document summarization @cite_11 , sentiment analysis @cite_17 and event extraction @cite_27\"","":""}
{"id":"2954881165","dialogue":"\"In a disaster situation, first responders need to quickly acquire situational awareness and prioritize response based on the need, resources available and impact. Can they do this based on digital media such as Twitter alone, or newswire alone, or some combination of the two? We examine this question in the context of the 2015 Nepal Earthquakes. Because newswire articles are longer, effective summaries can be helpful in saving time yet giving key content. We evaluate the effectiveness of several unsupervised summarization techniques in capturing key content. We propose a method to link tweets written by the public and newswire articles, so that we can compare their key characteristics: timeliness, whether tweets appear earlier than their corresponding news articles, and content. A novel idea is to view relevant tweets as a summary of the matching news article and evaluate these summaries. Whenever possible, we present both quantitative and qualitative evaluations. One of our main findings is that tweets and newswire articles provide complementary perspectives that form a holistic view of the disaster situation.\"","summary":"\"@cite_21 , researchers proposed two methods that leverage tweets for ranking sentences in news articles for summarization: a voting method based on tweet hit counts of sentences, and a random walk on a heterogeneous graph (HGRW) consisting of tweets and news article sentences as nodes and the edge weights are defined by weighted idf-modified-cosine scores. The best ROUGE-1 F-score @cite_6 is achieved by a version of HGRW that outputs both sentences from news articles and tweets in the summary, where the summary consists of top four sentences tweets as highlights of the article.\"","":""}
{"id":"2954881165","dialogue":"\"In a disaster situation, first responders need to quickly acquire situational awareness and prioritize response based on the need, resources available and impact. Can they do this based on digital media such as Twitter alone, or newswire alone, or some combination of the two? We examine this question in the context of the 2015 Nepal Earthquakes. Because newswire articles are longer, effective summaries can be helpful in saving time yet giving key content. We evaluate the effectiveness of several unsupervised summarization techniques in capturing key content. We propose a method to link tweets written by the public and newswire articles, so that we can compare their key characteristics: timeliness, whether tweets appear earlier than their corresponding news articles, and content. A novel idea is to view relevant tweets as a summary of the matching news article and evaluate these summaries. Whenever possible, we present both quantitative and qualitative evaluations. One of our main findings is that tweets and newswire articles provide complementary perspectives that form a holistic view of the disaster situation.\"","summary":"\"Tweet summarization has also been studied, e.g., see @cite_23 and references cited therein. Our problem is a little different, we consider tweets that are linked and found relevant (or partially relevant) to news articles from the perspective of summaries of those news articles. We then evaluate them to get an idea of how much content of the articles is captured by these tweets.\"","":""}
{"id":"2954360742","dialogue":"\"Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.\"","summary":"\"There are a wide variety of hierarchical reinforcement learning approaches. One of the most widely applied HRL framework is the framework (). An option can be thought of as an action that extends over multiple timesteps thus providing the notion of temporal abstraction or subroutines in an MDP. Each option has its own policy (which is followed if the option is selected) and the termination condition (to stop the execution of that option). Many strategies are proposed for discovering options using task-specific hierarchies, such as pre-defined sub-goals , hand-designed features , or diversity-promoting priors . These approaches do not generalize well to new tasks. @cite_1 proposed an approach to learn options in an end-to-end manner by parameterizing the intra-option policy as well as the policy and termination condition for all the options. Eigen-options use the eigenvalues of the Laplacian (for the transition graph induced by the MDP) to derive an intrinsic reward for discovering options as well as learning an intra-option policy.\"","":""}
{"id":"2952464165","dialogue":"\"State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection tracking.\"","summary":"\"Video object detection tracking is a task in ILSVRC 2017 @cite_19 , where the winning entries are optimized for accuracy rather than speed. @cite_18 adopts flow aggregation @cite_38 to improve the detection accuracy. @cite_17 combines flow-based @cite_4 and object tracking-based @cite_3 tubelet generation @cite_31 . THU-CAS @cite_19 considers flow-based tracking @cite_23 , object tracking @cite_34 and data association @cite_20 .\"","":""}
{"id":"2952464165","dialogue":"\"State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection tracking.\"","summary":"\"Nevertheless, these methods combine multiple cues (e.g., flow aggregation in detection, and flow-based and object tracking-based tubelet generation) which are complementary but time-consuming. Moreover, they apply global post-processing such as seq-NMS @cite_41 and tubelet NMS @cite_15 which greatly improve the accuracy but are not suitable for a realtime and low latency scenario.\"","":""}
{"id":"2952464165","dialogue":"\"State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection tracking.\"","summary":"\"Approaches to video object detection have been developed rapidly since the introduction of the ImageNet VID dataset @cite_25 . @cite_23 @cite_31 propose a framework that consists of per-frame proposal generation, bounding box tracking and tubelet re-scoring. @cite_22 proposes to detect frames sparsely and propagates features with optical flow. @cite_38 proposes to aggregate features in nearby frames along the motion path to improve the feature quality. Futhermore, @cite_21 proposes a high-performance approach by considering feature aggregation, partial feature updating and adaptive keyframe scheduling based on optical flow. Besides, @cite_37 proposes to learn detection and tracking using a single network with a multi-task objective. @cite_39 proposes to propagate the sparsely detected results through a space-time lattice. All the methods above focus on the accuracy of each individual frame. They either do not associate the presence of an object in different frames as a tracklet, or associate after performing object detection on each frame, which is time-consuming.\"","":""}
{"id":"2952464165","dialogue":"\"State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection tracking.\"","summary":"\"Multiple object tracking (MOT) focuses on data association: finding the set of trajectories that best explains the given detections @cite_42 . Existing approaches to MOT fall into two categories: batch and online mode. Batch mode approaches pose data association as a global optimization problem, which can be a min-cost max-flow problem @cite_26 @cite_13 , a continuous energy minimization problem @cite_16 or a graph cut problem @cite_10 @cite_36 . Contrarily, online mode approaches are only allowed to solve the data association problem with the present and past frames. @cite_14 formulates data association as a Markov decision process. @cite_8 @cite_32 employs recurrent neural networks (RNNs) for feature representation and data association.\"","":""}
{"id":"2952464165","dialogue":"\"State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection tracking.\"","summary":"\"State-of-the-art MOT approaches aim to improve the data association performance given publicly-available detections since the introduction of the MOT challenge @cite_33 . However, we focus on the sequential decision problem of detection or tracking. Although the widely-used Hungarian algorithm is adopted for simplicity and fairness in the experiments, we believe the incorporation of existing MOT approaches can further enhance the accuracy.\"","":""}
{"id":"2952464165","dialogue":"\"State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection tracking.\"","summary":"\"Researchers have proposed approaches to adaptive keyframe scheduling beyond regular frame skipping in video analytics. @cite_21 proposes to estimate the quality of optical flow, which relies on the time-consuming flow network. @cite_39 proposes an to consider the size and motion of small objects, which is hand-crafted and more importantly, it is a detect-then-schedule paradigm but cannot determine to detect or track. @cite_6 @cite_27 learn to predict the discrepancy between the segmentation map of the current frame and the keyframe, which are only applicable to segmentation tasks.\"","":""}
{"id":"2900965757","dialogue":"\"Action monitoring in a home environment provides important information for health monitoring and may serve as input into a smart home environment. Visual analysis using cameras can recognise actions in a complex scene, such as someones living room. However, although there the huge potential benefits and importance, specifically for health, cameras are not widely accepted because of privacy concerns. This paper recognises human activities using a sensor that retains privacy. The sensor is not only different by being thermal, but it is also of low resolution: 8x8 pixels. The combination of the thermal imaging, and the low spatial resolution ensures the privacy of individuals. We present an approach to recognise daily activities using this sensor based on a discrete cosine transform. We evaluate the proposed method on a state-of-the-art dataset and experimentally confirm that our approach outperforms the baseline method. We also introduce a new dataset, and evaluate the method on it. Here we show that the sensor is considered better at detecting the occurrence of falls and Activities of Daily Living. Our method achieves an overall accuracy of 87.50 across 7 activities with a fall detection sensitivity of 100 and specificity of 99.21 .\"","summary":"\"An infrared sensor array is a device composed of a small number of discrete infrared sensors. It represents the spatial distribution of temperature as a low-resolution image. Unlike colour cameras, infrared sensor arrays only capture the shape of the human body, therefore making individual identification harder. Additionally, the low spatial resolution also makes identification of individuals difficult. As this is more comfortable for users, it becomes more acceptable for installation in residential environments. Such infrared sensor arrays can be applied in many scenarios. A 4x4 sensor array has been used to recognise hand motion directions @cite_5 , although the extremely low resolution of this sensor renders it unsuitable for more complex visual tasks. A 8x8 pixel sensor array has been successfully used to detect, count and track people indoors @cite_6 . Human movements has also be inferred by using the subject's location and moving trajectory using a 16x16 sensor array @cite_10 . Most recently, a multi-sensor system has been designed for human movement detection and activity recognition @cite_1 , which our method will be compared against.\"","":""}
{"id":"2900965757","dialogue":"\"Action monitoring in a home environment provides important information for health monitoring and may serve as input into a smart home environment. Visual analysis using cameras can recognise actions in a complex scene, such as someones living room. However, although there the huge potential benefits and importance, specifically for health, cameras are not widely accepted because of privacy concerns. This paper recognises human activities using a sensor that retains privacy. The sensor is not only different by being thermal, but it is also of low resolution: 8x8 pixels. The combination of the thermal imaging, and the low spatial resolution ensures the privacy of individuals. We present an approach to recognise daily activities using this sensor based on a discrete cosine transform. We evaluate the proposed method on a state-of-the-art dataset and experimentally confirm that our approach outperforms the baseline method. We also introduce a new dataset, and evaluate the method on it. Here we show that the sensor is considered better at detecting the occurrence of falls and Activities of Daily Living. Our method achieves an overall accuracy of 87.50 across 7 activities with a fall detection sensitivity of 100 and specificity of 99.21 .\"","summary":"\"The visual trace of human activity in video forms a spatio-temporal pattern. Here the salient features are well-developed for images captured by conventional visible-light RGB cameras @cite_0 . However, the majority of well developed features, such as histogram of oriented gradients or optical flow, are not appropriate and applicable for very low resolution images such as those captured in this study, i.e. for 8x8 pixel resolution images.\"","":""}
{"id":"2900965757","dialogue":"\"Action monitoring in a home environment provides important information for health monitoring and may serve as input into a smart home environment. Visual analysis using cameras can recognise actions in a complex scene, such as someones living room. However, although there the huge potential benefits and importance, specifically for health, cameras are not widely accepted because of privacy concerns. This paper recognises human activities using a sensor that retains privacy. The sensor is not only different by being thermal, but it is also of low resolution: 8x8 pixels. The combination of the thermal imaging, and the low spatial resolution ensures the privacy of individuals. We present an approach to recognise daily activities using this sensor based on a discrete cosine transform. We evaluate the proposed method on a state-of-the-art dataset and experimentally confirm that our approach outperforms the baseline method. We also introduce a new dataset, and evaluate the method on it. Here we show that the sensor is considered better at detecting the occurrence of falls and Activities of Daily Living. Our method achieves an overall accuracy of 87.50 across 7 activities with a fall detection sensitivity of 100 and specificity of 99.21 .\"","summary":"\"Several features have been investigated specifically for low resolution infrared sensors, most notably @cite_15 . Here, connected component analysis was used to evaluate the number of individuals present in the scene, which subsequently led to motion tracking of the individuals; however this method was sensitive to background noise. A thermo-spatial sensitive histogram feature approach was able to reduce the noise from background pixels @cite_10 . Although counting and tracking of individuals is a non-trivial task, here we are concerned with the activity of each individual. Intuitively, this would appear to require finer detail, and this poses a difficult task given the low spatial resolution of the image.\"","":""}
{"id":"2900965757","dialogue":"\"Action monitoring in a home environment provides important information for health monitoring and may serve as input into a smart home environment. Visual analysis using cameras can recognise actions in a complex scene, such as someones living room. However, although there the huge potential benefits and importance, specifically for health, cameras are not widely accepted because of privacy concerns. This paper recognises human activities using a sensor that retains privacy. The sensor is not only different by being thermal, but it is also of low resolution: 8x8 pixels. The combination of the thermal imaging, and the low spatial resolution ensures the privacy of individuals. We present an approach to recognise daily activities using this sensor based on a discrete cosine transform. We evaluate the proposed method on a state-of-the-art dataset and experimentally confirm that our approach outperforms the baseline method. We also introduce a new dataset, and evaluate the method on it. Here we show that the sensor is considered better at detecting the occurrence of falls and Activities of Daily Living. Our method achieves an overall accuracy of 87.50 across 7 activities with a fall detection sensitivity of 100 and specificity of 99.21 .\"","summary":"\"A large amount of research is underway in the development of a smart sensing system to detect falls in home environments. However, the use of thermal infrared arrays for fall detection has to date not been widely investigated. Although a real-time system to recognise fall and non-fall events has been presented in @cite_13 , their study overlooks the complexity of non-fall actions, where some actions, such as sitting down and inactivity, can be confused with falling @cite_3 . Taking this into consideration, various non-fall activities are specifically incorporated in our dataset, including those most likely to be confused with falling.\"","":""}
{"id":"2963321544","dialogue":"\"We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a “choice” made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age.\"","summary":"\"There is a long history of parameter estimation in network formation processes @cite_67 @cite_19 . Probably the most studied formation model is preferential attachment @cite_52 , which relates the likelihood of new nodes connecting to a node @math to @math 's degree. More references on PA, since we claim it is the most studied? The original justification of the model was based on the degree distribution of the resulting graph qualitatively matching those of several early large-scale empirical networks. Shortly after, a number of authors proposed non-parametric methods to measure preferential attachment by counting what the degrees of the nodes getting edges are and normalizing them by the relative likelihood of those degrees @cite_41 @cite_7 @cite_73 . There are several generalizations of preferential attachment, where node @math is chosen proportional to some function @math of the degree @math of node @math (here, @math is standard preferential attachment). TODO: Need references if claiming there are several generalizations Much of this research has focused on the case when @math , i.e., attachment is preferential to some (latent) power @math of the degree @cite_17 .\"","":""}
{"id":"2963321544","dialogue":"\"We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a “choice” made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age.\"","summary":"\"Recently, @cite_56 proposed a principled method for estimating the attachment kernel. Their proposal corresponds precisely to maximum likelihood estimation of the attachment function as a conditional logit model with a parameter for every degree. Then, they find the corresponding @math using least squares, instead of doing with so maximum likelihood also. The same authors also proposed an extension of their work that includes node fitness @cite_45 . Other recent work takes a maximum likelihood approach to estimating a mixture probability @math between uniform attachment and preferential attachment @cite_51 . With their focus on characterizing individual formation models, these prior works fall short of the full potential of discrete choice modeling. For example, they don't consider the arbitrary combinations Can we take this thought one step further? Why do they fall short?\"","":""}
{"id":"2963321544","dialogue":"\"We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a “choice” made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age.\"","summary":"\"There is also a connection with the literature on link prediction in social networks @cite_9 . A network formation model implicitly makes claims about what edges are most likely to form next, and so can be evaluated by the same metrics as link prediction algorithms @cite_14 . Features like distance, common neighbors and degree have been shown to be predictive of link formation in multiple contexts @cite_9 @cite_29 . However, in that literature, the main focus of interest is usually predictive accuracy, rather than a robust understanding of the drivers of formation. While we use predictive accuracy as a measure of goodness of fit, we are more concerned with interpretability of the model and estimates, which is one of the advantages of the conditional logit model.\"","":""}
{"id":"2963321544","dialogue":"\"We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a “choice” made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age.\"","summary":"\"A related line of research studies the so-called stochastic actor-oriented model @cite_40 @cite_27 . This model combines multiple formation dynamics in a multinomial logit functional form, and develops connections between network formation and Markov chains in the space of graphs. However, they are impractical to estimate, especially for larger data sets. should increase distancing here? Need to return to the distancing here. Why is it impractical for large datasets?\"","":""}
{"id":"2963321544","dialogue":"\"We provide a framework for modeling social network formation through conditional multinomial logit models from discrete choice and random utility theory, in which each new edge is viewed as a “choice” made by a node to connect to another node, based on (generic) features of the other nodes available to make a connection. This perspective on network formation unifies existing models such as preferential attachment, triadic closure, and node fitness, which are all special cases, and thereby provides a flexible means for conceptualizing, estimating, and comparing models. The lens of discrete choice theory also provides several new tools for analyzing social network formation; for example, the significance of node features can be evaluated in a statistically rigorous manner, and mixtures of existing models can be estimated by adapting known expectation-maximization algorithms. We demonstrate the flexibility of our framework through examples that analyze a number of synthetic and real-world datasets. For example, we provide rigorous methods for estimating preferential attachment models and show how to separate the effects of preferential attachment and triadic closure. Non-parametric estimates of the importance of degree show a highly linear trend, and we expose the importance of looking carefully at nodes with degree zero. Examining the formation of a large citation graph, we find evidence for an increased role of degree when accounting for age.\"","summary":"\"Estimating the parameters that drive edge formation is different from identifying the factors that could have lead to the observed graph. There is vast literature on pursuing the latter question by estimating a logit model with maximum likelihood, called exponential random graph models (ERGMs) @cite_46 @cite_42 @cite_54 . However, these models do not consider individual edge events, are hard to estimate, and have known pathologies @cite_2 @cite_68 . Should check out TERGMs again, temporal ERGMs, and put that work in it's place here. I think Carter Butts is in that literature.\"","":""}
{"id":"2955300983","dialogue":"\"We develop new voting mechanisms for the case when voters and candidates are located in an arbitrary unknown metric space, and the goal is to choose a candidate minimizing social cost: the total distance from the voters to this candidate. Previous work has often assumed that only ordinal preferences of the voters are known (instead of their true costs), and focused on minimizing distortion: the quality of the chosen candidate as compared with the best possible candidate. In this paper, we instead assume that a (very small) amount of information is known about the voter preference strengths, not just about their ordinal preferences. We provide mechanisms with much better distortion when this extra information is known as compared to mechanisms which use only ordinal information. We quantify tradeoffs between the amount of information known about preference strengths and the achievable distortion. We further provide advice about which type of information about preference strengths seems to be the most useful. Finally, we conclude by quantifying the ideal candidate distortion, which compares the quality of the chosen outcome with the best possible candidate that could ever exist, instead of only the best candidate that is actually in the running.\"","summary":"\"Attempts to exploit preference strength information have led to various approaches for modeling, eliciting, measuring, and aggregating people's preference intensities in a variety of fields, including Likert scales, semantic differential scales, sliders, constant sum paired comparisons, graded pair comparisons, response times, willingness to pay, vote buying, and many others (see @cite_26 @cite_21 @cite_12 for summaries). In our work we specifically consider only a small amount of coarse information about preference strengths, since obtaining detailed information is extremely difficult. Intuitively, any rule used to aggregate preference strengths must ask under what circumstances an apathetic majority' should win over a more passionate minority @cite_10 , and we provide a partial answer to this question when the objective is to minimize distortion.\"","":""}
{"id":"2954276933","dialogue":"\"Recent advances in semi-supervised learning have shown tremendous potential in overcoming a major barrier to the success of modern machine learning algorithms: access to vast amounts of human-labeled training data. Algorithms based on self-ensemble learning and virtual adversarial training can harness the abundance of unlabeled data to produce impressive state-of-the-art results on a number of semi-supervised benchmarks, approaching the performance of strong supervised baselines using only a fraction of the available labeled data. However, these methods often require careful tuning of many hyper-parameters and are usually not easy to implement in practice. In this work, we present a conceptually simple yet effective semi-supervised algorithm based on self-supervised learning to combine semantic feature representations from unlabeled data. Our models are efficiently trained end-to-end for the joint, multi-task learning of labeled and unlabeled data in a single stage. Striving for simplicity and practicality, our approach requires no additional hyper-parameters to tune for optimal performance beyond the standard set for training convolutional neural networks. We conduct a comprehensive empirical evaluation of our models for semi-supervised image classification on SVHN, CIFAR-10 and CIFAR-100, and demonstrate results competitive with, and in some cases exceeding, prior state of the art. Reference code and data are available at this https URL\"","summary":"\"Self-supervised learning is similar in flavor to unsupervised learning, where the goal is to learn visual representations from large-scale unlabeled images or videos without using any human annotations. Self-supervised representations are learned by first defining a pretext task, an objective function, for the model to solve and then producing proxy labels to guide the pretext task based solely on the visual information present in unlabeled data. The simplest self-supervised task is minimizing reconstruction error in autoencoders @cite_45 to create low-dimensional feature representations, where the proxy labels are the values of the image pixels. More sophisticated self-supervised tasks such as image inpainting @cite_35 , colorizing grayscale images @cite_33 @cite_41 , and predicting image rotations @cite_4 have shown impressive results for unsupervised visual feature learning. The key to utilizing self-supervision for SSL is to learn useful features from unlabeled data through the pretext task that can be transferred and adapted to downstream supervised applications where labeled training data is scarce.\"","":""}
{"id":"2954276933","dialogue":"\"Recent advances in semi-supervised learning have shown tremendous potential in overcoming a major barrier to the success of modern machine learning algorithms: access to vast amounts of human-labeled training data. Algorithms based on self-ensemble learning and virtual adversarial training can harness the abundance of unlabeled data to produce impressive state-of-the-art results on a number of semi-supervised benchmarks, approaching the performance of strong supervised baselines using only a fraction of the available labeled data. However, these methods often require careful tuning of many hyper-parameters and are usually not easy to implement in practice. In this work, we present a conceptually simple yet effective semi-supervised algorithm based on self-supervised learning to combine semantic feature representations from unlabeled data. Our models are efficiently trained end-to-end for the joint, multi-task learning of labeled and unlabeled data in a single stage. Striving for simplicity and practicality, our approach requires no additional hyper-parameters to tune for optimal performance beyond the standard set for training convolutional neural networks. We conduct a comprehensive empirical evaluation of our models for semi-supervised image classification on SVHN, CIFAR-10 and CIFAR-100, and demonstrate results competitive with, and in some cases exceeding, prior state of the art. Reference code and data are available at this https URL\"","summary":"\"Models belonging to the self-ensembling class, such as Pseudo-Ensembles @cite_6 , Ladder networks @cite_7 , @math model @cite_13 and Mean Teacher @cite_44 , utilize the output predictions on unlabeled data as proxy labels for SSL. This class of methods considers the model as a stochastic prediction function, in which different model configurations, such as dropout @cite_34 and data augmentation, along with varying levels of noise in the input data can produce drastically different output predictions. The unsupervised objective of self-ensemble models is to minimize the mean squared error of multiple model outputs under random perturbations and data augmentation for the same training examples. The motivation behind this approach is to further regularize the model through the principle that perturbations in the input data and or data augmentation techniques should not significantly change the output of the model @cite_46 . Self-ensembling approaches are robust to random perturbations and geometric transformations, and are currently among the state of the art in SSL on several benchmark image classification datasets.\"","":""}
{"id":"2954276933","dialogue":"\"Recent advances in semi-supervised learning have shown tremendous potential in overcoming a major barrier to the success of modern machine learning algorithms: access to vast amounts of human-labeled training data. Algorithms based on self-ensemble learning and virtual adversarial training can harness the abundance of unlabeled data to produce impressive state-of-the-art results on a number of semi-supervised benchmarks, approaching the performance of strong supervised baselines using only a fraction of the available labeled data. However, these methods often require careful tuning of many hyper-parameters and are usually not easy to implement in practice. In this work, we present a conceptually simple yet effective semi-supervised algorithm based on self-supervised learning to combine semantic feature representations from unlabeled data. Our models are efficiently trained end-to-end for the joint, multi-task learning of labeled and unlabeled data in a single stage. Striving for simplicity and practicality, our approach requires no additional hyper-parameters to tune for optimal performance beyond the standard set for training convolutional neural networks. We conduct a comprehensive empirical evaluation of our models for semi-supervised image classification on SVHN, CIFAR-10 and CIFAR-100, and demonstrate results competitive with, and in some cases exceeding, prior state of the art. Reference code and data are available at this https URL\"","summary":"\"Rather than relying on the model to randomly perturb the input data by way of dropout or data augmentation, @cite_10 proposed the concept of adversarial training to approximate the perturbations in the direction that would most significantly alter the output of the model. While adversarial training requires access to ground truth labels to perform adversarial perturbations, the Virtual Adversarial Training (VAT) mechanism proposed by @cite_32 @cite_36 can be applied to unlabeled data and is thus suitable for SSL under the consistency regularization principle. Adversarial training is closely related to generative adversarial networks (GANs) @cite_20 , which have been proposed for semi-supervised learning with promising results @cite_37 @cite_43 @cite_22 . Most recently, the self-supervised GANs with auxiliary rotation loss @cite_25 have been shown to synthesize high-fidelity, diverse natural images at high resolution using only a fraction of the available labels.\"","":""}
{"id":"2950393809","dialogue":"\"Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10 relative improvement over the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.\"","summary":"\"Knowledge graph embedding learning has been an active research area with applications directly in knowledge base completion (i.e. link prediction) and relation extractions. TransE @cite_28 started this line of work by projecting both entities and relations into the same embedding vector space, with translational constraint of @math . Later works enhanced KG embedding models such as TransH @cite_5 , TransR @cite_23 , and TransD @cite_25 introduced new representations of relational translation and thus increased model complexity. These models were categorized as translational distance models @cite_21 or additive models, while DistMult @cite_6 and ComplEx @cite_0 are multiplicative models @cite_10 , due to the multiplicative score functions used for computing entity-relation-entity triplet likelihood.\"","":""}
{"id":"2950393809","dialogue":"\"Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10 relative improvement over the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.\"","summary":"\"The most recent KG embedding models are ConvE @cite_14 and ConvKB @cite_20 . ConvE was the first model using 2D convolutions over embeddings of different embedding dimensions, with the hope of extracting more feature interactions. ConvKB replaced 2D convolutions in ConvE with 1D convolutions, which constrains the convolutions to be the same embedding dimensions and keeps the translational property of TransE. ConvKB can be considered as a special case of Conv-TransE that only uses filters with width equal to @math . Although ConvKB was shown to be better than ConvE , the results on two datasets (FB15k-237 and WN18RR) were not consistent, so we leave these results out of our comparison table. The other major difference of ConvE and ConvKB is on the loss functions used in the models. ConvE used the cross-entropy loss that could be sped up with 1-N scoring in the decoder, while ConvKB used a hinge loss that was computed from positive examples and sampled negative examples. We take the decoder from ConvE because we can easily integrate the encoder of GCN and the decoder of ConvE into an end-to-end training framework, while ConvKB is not suitable for our approach.\"","":""}
{"id":"2950393809","dialogue":"\"Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10 relative improvement over the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.\"","summary":"\"GCNs were first proposed in @cite_27 where graph convolutional operations were defined in the Fourier domain. The eigendecomposition of the graph Laplacian caused intense computation. Later, smooth parametric spectral filters @cite_7 @cite_16 were introduced to achieve localization in the spatial domain and improve computational efficiency. Recently, @cite_9 simplified these spectral methods by a first-order approximation with the Chebyshev polynomials. The spatial graph convolution approaches @cite_3 define convolutions directly on graph, which sum up node features over all spatial neighbors using adjacency matrix.\"","":""}
{"id":"2950393809","dialogue":"\"Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10 relative improvement over the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.\"","summary":"\"GCN models were mostly criticized for its huge memory requirement to scale to massive graphs. However, @cite_26 developed a data efficient GCN algorithm called PinSage, which combined efficient random walks and graph convolutions to generate embeddings of nodes that incorporated both graph structure as well as node features. The experiments on Pinterest data were the largest application of deep graph embeddings to date with 3 billion nodes and 18 billion edges @cite_26 . This success paves the way for a new generation of web-scale recommender systems based on GCNs. Therefore we believe that our proposed model could take advantage of huge graph structures and high computational efficiency of Conv-TransE .\"","":""}
{"id":"2900151392","dialogue":"\"Generative Adversarial Networks have shown impressive results for the task of object translation","summary":"including face-to-face translation. A key component behind the success of recent approaches is the self-consistency loss","":""}
{"id":"2900151392","dialogue":"\"Generative Adversarial Networks have shown impressive results for the task of object translation","summary":"including face-to-face translation. A key component behind the success of recent approaches is the self-consistency loss","":""}
{"id":"2952161373","dialogue":"\"We identify a fundamental issue in the popular Stochastic Neighbour Embedding (SNE and t-SNE)","summary":"i.e.","":""}
{"id":"2952161373","dialogue":"\"We identify a fundamental issue in the popular Stochastic Neighbour Embedding (SNE and t-SNE)","summary":"i.e.","":""}
{"id":"2952161373","dialogue":"\"We identify a fundamental issue in the popular Stochastic Neighbour Embedding (SNE and t-SNE)","summary":"i.e.","":""}
{"id":"2949777943","dialogue":"\"The Log-Structured Merge-Tree (LSM-tree) has been widely adopted for use in modern NoSQL systems for its superior write performance. Despite the popularity of LSM-trees, they have been criticized for suffering from write stalls and large performance variances due to the inherent mismatch between their fast in-memory writes and slow background I O operations. In this paper, we use a simple yet effective two-phase experimental approach to evaluate write stalls for various LSM-tree designs. We further explore the design choices of LSM merge schedulers to minimize write stalls given a disk bandwidth budget. We have conducted extensive experiments in the context of the Apache AsterixDB system and we present the results here.\"","summary":"\"Recently, a large number of improvements of the original LSM-tree @cite_45 have been proposed. Chen and Carey @cite_24 survey these improvements, range from improving write performance @cite_23 @cite_26 @cite_4 @cite_21 @cite_20 , reducing the buffer cache misses due to merges @cite_16 @cite_27 , supporting automatic design tuning of LSM-trees @cite_18 @cite_34 , to optimizing LSM-based secondary indexes @cite_46 @cite_36 . However, all of these efforts focus on the throughput of LSM-trees, while performance variances and write stalls are largely ignored.\"","":""}
{"id":"2949777943","dialogue":"\"The Log-Structured Merge-Tree (LSM-tree) has been widely adopted for use in modern NoSQL systems for its superior write performance. Despite the popularity of LSM-trees, they have been criticized for suffering from write stalls and large performance variances due to the inherent mismatch between their fast in-memory writes and slow background I O operations. In this paper, we use a simple yet effective two-phase experimental approach to evaluate write stalls for various LSM-tree designs. We further explore the design choices of LSM merge schedulers to minimize write stalls given a disk bandwidth budget. We have conducted extensive experiments in the context of the Apache AsterixDB system and we present the results here.\"","summary":"\"Performance stability has long been recognized as a critical performance metric. The TPC-C benchmark @cite_9 measures not only absolute throughput, but also specifies the acceptable upper bounds for the percentile latencies of the transactions. @cite_40 applied VProfiler @cite_0 to identify major sources of variance in database transactions and proposed a variance-aware transaction scheduling algorithm. @cite_10 proposed techniques to optimize parameterized queries while balancing the average and variance of query cost. To reduce the variance of query processing, most existing proposals have either emphasized the use of table scans @cite_14 @cite_31 @cite_42 or stuck to worst-case query plans @cite_2 @cite_44 . Cao @cite_8 conducted an experimental study of the performance variance of modern storage stacks; they found that variance is common in storage stacks and heavily depends on configurations and workloads. Dean and Barroso @cite_30 discussed several engineering techniques to reduce performance variance in large-scale distributed systems at Google. Different from these efforts, in this work we focus on evaluating and minimizing the performance variances of LSM-trees due to their inherent out-of-place update design.\"","":""}
{"id":"2952773240","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.\"","summary":"\"There are several works in the literature on detecting PC-based botnets using their CnC (Command-and-control) server communication features. Bothunter @cite_24 builds a based on which three bot-specific sensors are constructed and correlation is performed between inbound intrusion scan alarms and the infection dialog model to generate a consolidated report. Spatio-temporal similarities between bots in a botnet in terms of bot-CnC coordinated activities are captured from network traffic and leveraged towards botnet detection in a local area network in Botsniffer @cite_7 . In BotMiner @cite_17 , the authors have proposed a botnet detection system which clusters similar CnC communication traffic and similar malicious activity traffic, and uses cross cluster correlation to detect bots in a monitored network.\"","":""}
{"id":"2952773240","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.\"","summary":"There has also been some research on intrusion detection and anomaly detection systems for IoT. A whitelist-based intrusion detection system for IoT devices (Heimdall) has been presented in @cite_2 . The authors in @cite_3 propose an intrusion detection model for IoT backbone networks leveraging two-layer dimension reduction and two-tier classification techniques to detect U2R (User-to-Root) and R2L (Remote-to-Local) attacks.","":""}
{"id":"2952773240","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.\"","summary":"\"Of late, there has been an interest in IoT botnet and attack detection in the research community resulting in a number of papers addressing these problems. In @cite_6 , deep-autoencoders based anomaly detection has been used to detect attacks launched from IoT botnets. A few works have focused on building normal communication profiles for IoT devices which are not expected to deviate much over a long period of time. DEFT @cite_10 has used ML algorithms at SDN controllers and access gateways to build normal device traffic fingerprints while @cite_25 proposes a tool to automatically generate MUD (Manufacturer Usage Description) profiles for a number of consumer IoT devices. In DIoT @cite_9 , the authors have proposed a method to classify typically used IoT devices into various device types and build their normal traffic profiles so that a deviation from those profiles is flagged as anomalous traffic.\"","":""}
{"id":"2952773240","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.\"","summary":"\"Our work addresses a few important gaps in the literature when it comes to distinguishing between legitimate and botnet IoT traffic. First, the works on detecting botnets using their CnC communication features @cite_0 @cite_20 @cite_7 @cite_17 are designed for PC-based botnets rather than IoT botnets which are the focus of our work. Second, we do not aim to detect botnets (networks of bots) but instead, network activity generated by individual bots. IoT botnets tend to consist of hundreds of thousands to millions of devices spread over vast geographies, hence, it is impractical to detect a whole network of IoT bots. Therefore, we do not require computationally expensive clustering algorithms as used in @cite_7 @cite_17 .\"","":""}
{"id":"2952773240","dialogue":"\"The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.\"","summary":"\"Third, unlike @cite_6 @cite_9 , we aim to detect IoT malware activity much before the actual attack, during the scanning infection phase. Finally, instead of fingerprinting the normal traffic of IoT devices @cite_10 @cite_9 and using those fingerprints towards anomaly detection, we detect the malware-induced scanning packet traffic generated by infected IoT devices. This is because the former approach suffers from limitations such as possibility of misclassification of an infected device as a legitimate device type, testing against only simple malware e.g. Mirai which may result in failure to detect other, more sophisticated malware, etc. The latter approach is not free from limitations as well, since it is not resilient against new undiscovered malware whose scanning traffic features have not been updated in the database. We advocate for a combined approach consisting of both IoT device fingerprinting anomaly detection and IoT malware scanning traffic detection.\"","":""}
{"id":"2952816888","dialogue":"\"Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations.\"","summary":"\"In literature, posteriors for Bayesian Neural Network models obtained by Hamiltonian Monte Carlo (HMC) @cite_12 are frequently used as ground truth. However, HMC scales poorly on high dimensional parameter space and large datasets @cite_0 @cite_4 . Mini-batched versions of HMC, such as Stochastic Gradient Langevin Dynamics (SGLD) @cite_0 and Stochastic Gadient HMC @cite_5 , have been introduced to address the issue of scalability. However, these methods still suffer from lower mixing rate and are not theoretically guaranteed to converge to the true posterior when model assumptions are not met (e.g. when the true model of the gradient noise is not well-estimated).\"","":""}
{"id":"2952816888","dialogue":"\"Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations.\"","summary":"\"As a result, much effort has been spent on variational methods. Mean Field Variational Bayes for BNNs were in introduced in @cite_3 , the gradient computation of which was later improved in Bayes by Backprop (BBB) . However, the fully factorized Gaussian variational family used in BBB is unable to capture correlation amongst the parameters in the posterior. In contrast, Matrix Gaussian Posteriors (MVG) @cite_2 , Multiplicative Normalizing Flows (MNF) @cite_8 , and Bayes by Hypernet (BBH) @cite_7 are explicitly designed to capture posterior correlation by imposing structured approximation families; works like Black Box @math -Divergence @cite_1 and Probabilistic Backpropagation (PBP) use a richer family of divergence measures, encouraging approximate posteriors to capture important properties of true posterior distributions.\"","":""}
{"id":"2952816888","dialogue":"\"Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations.\"","summary":"\"Finally, Dropout @cite_15 and ensemble methods @cite_10 @cite_9 @cite_14 by-pass the difficulties of performing Bayesian inference and obtain predictive uncertainty estimates through implicitly or explicitly training multiple models on the same data.\"","":""}
{"id":"2900399720","dialogue":"\"This paper presents a discovery that the length of the entities in various datasets follows a family of scale-free power law distributions. The concept of entity here broadly includes the named entity, entity mention, time expression, aspect term, and domain-specific entity that are well investigated in natural language processing and related areas. The entity length denotes the number of words in an entity. The power law distributions in entity length possess the scale-free property and have well-defined means and finite variances. We explain the phenomenon of power laws in entity length by the principle of least effort in communication and the preferential mechanism.\"","summary":"\"Our work is related to Zipf's law and the distributions of word length and sentence length. Power laws have been observed to appear in numerous natural and man-made systems @cite_62 , we here concern them in language.\"","":""}
{"id":"2900399720","dialogue":"\"This paper presents a discovery that the length of the entities in various datasets follows a family of scale-free power law distributions. The concept of entity here broadly includes the named entity, entity mention, time expression, aspect term, and domain-specific entity that are well investigated in natural language processing and related areas. The entity length denotes the number of words in an entity. The power law distributions in entity length possess the scale-free property and have well-defined means and finite variances. We explain the phenomenon of power laws in entity length by the principle of least effort in communication and the preferential mechanism.\"","summary":"\"According to the review by , first demonstrated that the word length in a corpus empirically and theoretically follows a variant of Poisson distributions. The word length of a natural corpus has been observed to follow the variants of Poisson distributions in more than 32 languages @cite_17 .\"","":""}
{"id":"2964195534","dialogue":"\"Subspace clustering aims to find groups of similar objects (clusters) that exist in lower dimensional subspaces from a high dimensional dataset. It has a wide range of applications, such as analysing high dimensional sensor data or DNA sequences. However, existing algorithms have limitations in finding clusters in non-disjoint subspaces and scaling to large data, which impinge their applicability in areas such as bioinformatics and the Internet of Things. We aim to address such limitations by proposing a subspace clustering algorithm using a bottom-up strategy. Our algorithm first searches for base clusters in low dimensional subspaces. It then forms clusters in higher-dimensional subspaces using these base clusters, which we formulate as a frequent pattern mining problem. This formulation enables efficient search for clusters in higher-dimensional subspaces, which is done using FP-trees. The proposed algorithm is evaluated against traditional bottom-up clustering algorithms and state-of-the-art subspace clustering algorithms. The experimental results show that the proposed algorithm produces clusters with high accuracy, and scales well to large volumes of data. We also demonstrate the algorithm’s performance using real-life ten genomic datasets.\"","summary":"\"From an algorithmic point of view, clustering algorithms can be classified into bottom-up algorithms and top-down algorithms @cite_9 . As our algorithm follows a bottom-up strategy, we briefly discuss the relevant algorithms of this class to highlight our contributions.\"","":""}
{"id":"2964195534","dialogue":"\"Subspace clustering aims to find groups of similar objects (clusters) that exist in lower dimensional subspaces from a high dimensional dataset. It has a wide range of applications, such as analysing high dimensional sensor data or DNA sequences. However, existing algorithms have limitations in finding clusters in non-disjoint subspaces and scaling to large data, which impinge their applicability in areas such as bioinformatics and the Internet of Things. We aim to address such limitations by proposing a subspace clustering algorithm using a bottom-up strategy. Our algorithm first searches for base clusters in low dimensional subspaces. It then forms clusters in higher-dimensional subspaces using these base clusters, which we formulate as a frequent pattern mining problem. This formulation enables efficient search for clusters in higher-dimensional subspaces, which is done using FP-trees. The proposed algorithm is evaluated against traditional bottom-up clustering algorithms and state-of-the-art subspace clustering algorithms. The experimental results show that the proposed algorithm produces clusters with high accuracy, and scales well to large volumes of data. We also demonstrate the algorithm’s performance using real-life ten genomic datasets.\"","summary":"\"Another relevant topic is co-clustering (a.k.a bi-clustering or pattern-based clustering) @cite_35 . Co-clustering can be considered as a more general class of clustering high dimensional data by simultaneously clustering rows (points) and columns (dimensions). The main point that differentiates co-clustering from subspace clustering lies in the approach to the problem, and the homogeneous methodology to find clusters in both axis-parallel and arbitrarily oriented subspaces @cite_9 . In this paper, we also compare the performance of our algorithm on gene expression data with a range of co-clustering algorithms, including SWCC @cite_23 , BBAC-S @cite_5 , ITCC @cite_18 , FFCFW @cite_34 , and HICC @cite_36 .\"","":""}
{"id":"2899821082","dialogue":"\"Real-world social networks and digital platforms are comprised of individuals (nodes) that are linked to other individuals or entities through multiple types of relationships (links). Sub-networks of such a network based on each type of link correspond to distinct views of the underlying network. In real-world applications, each node is typically linked to only a small subset of other nodes. Hence, practical approaches to problems such as node labeling have to cope with the resulting sparse networks. While low-dimensional network embeddings offer a promising approach to this problem, most of the current network embedding methods focus primarily on single view networks. We introduce a novel multi-view network embedding (MVNE) algorithm for constructing low-dimensional node embeddings from multi-view networks. MVNE adapts and extends an approach to single view network embedding (SVNE) using graph factorization clustering (GFC) to the multi-view setting using an objective function that maximizes the agreement between views based on both the local and global structure of the underlying multi-view graph. Our experiments with several benchmark real-world single view networks show that GFC-based SVNE yields network embeddings that are competitive with or superior to those produced by the state-of-the-art single view network embedding methods when the embeddings are used for labeling unlabeled nodes in the networks. Our experiments with several multi-view networks show that MVNE substantially outperforms the single view methods on integrated view and the state-of-the-art multi-view methods. We further show that even when the goal is to predict labels of nodes within a single target view, MVNE outperforms its single-view counterpart suggesting that the MVNE is able to extract the information that is useful for labeling nodes in the target view from the all of the views.\"","summary":"\"There is a growing body of recent works on multi-view learning algorithms, e.g., @cite_41 @cite_10 @cite_31 , that attempt to integrate information across the multiple views to optimize the predictive performance of the classifier (see @cite_0 @cite_12 ). Some multi-view learning methods seek to maximize the agreement between views using regularization @cite_13 @cite_20 whereas others seek to optimally selecting subsets of features from different views for each prediction task @cite_5 @cite_32 However, these methods were not designed for network embedding. Most of the existing multi-view learning algorithms are either not directly applicable to multi-view networks or are not designed to cope with high degrees of data sparsity, a key challenge in modeling real-world multi-view networks.\"","":""}
{"id":"2899821082","dialogue":"\"Real-world social networks and digital platforms are comprised of individuals (nodes) that are linked to other individuals or entities through multiple types of relationships (links). Sub-networks of such a network based on each type of link correspond to distinct views of the underlying network. In real-world applications, each node is typically linked to only a small subset of other nodes. Hence, practical approaches to problems such as node labeling have to cope with the resulting sparse networks. While low-dimensional network embeddings offer a promising approach to this problem, most of the current network embedding methods focus primarily on single view networks. We introduce a novel multi-view network embedding (MVNE) algorithm for constructing low-dimensional node embeddings from multi-view networks. MVNE adapts and extends an approach to single view network embedding (SVNE) using graph factorization clustering (GFC) to the multi-view setting using an objective function that maximizes the agreement between views based on both the local and global structure of the underlying multi-view graph. Our experiments with several benchmark real-world single view networks show that GFC-based SVNE yields network embeddings that are competitive with or superior to those produced by the state-of-the-art single view network embedding methods when the embeddings are used for labeling unlabeled nodes in the networks. Our experiments with several multi-view networks show that MVNE substantially outperforms the single view methods on integrated view and the state-of-the-art multi-view methods. We further show that even when the goal is to predict labels of nodes within a single target view, MVNE outperforms its single-view counterpart suggesting that the MVNE is able to extract the information that is useful for labeling nodes in the target view from the all of the views.\"","summary":"\"Network embedding methods aim to produce information preserving low-dimensional embeddings of nodes in large networks. State-of-the-art network embedding methods include Deepwalk @cite_3 , LINE @cite_40 and node2vec @cite_18 are limited to single view networks, i.e, networks with a single type of links. However, most real-world networks are comprised of multiple types of nodes and links @cite_25 @cite_40 @cite_38 wherein each type of link induces a view. Hence, there is a growing interest in network embedding methods for multi-view networks @cite_11 @cite_29 @cite_39 @cite_15 . Some multi-view network embedding methods use canonical correlation analysis (CCA) @cite_9 @cite_2 @cite_8 to integrate information from multiple views. Others construct multi-view embeddings by integrating embeddings obtained from the individual views. Examples include MVWE @cite_14 which uses a weighted voting mechanism to combine information from multiple views; MVE2vec @cite_21 which attempts to balance the preservation of unique information provided by specific views against information that is shared by multiple views; and DMNE @cite_28 which uses a co-regularized cost function to combine information from different views. MVWE, MVE2vec, and DMNE use deep neural network models at their core. Specifically, MVWE and MVE2vec are based on a skip-gram model and DMNE is based on an AutoEncoder.\"","":""}
{"id":"2899821082","dialogue":"\"Real-world social networks and digital platforms are comprised of individuals (nodes) that are linked to other individuals or entities through multiple types of relationships (links). Sub-networks of such a network based on each type of link correspond to distinct views of the underlying network. In real-world applications, each node is typically linked to only a small subset of other nodes. Hence, practical approaches to problems such as node labeling have to cope with the resulting sparse networks. While low-dimensional network embeddings offer a promising approach to this problem, most of the current network embedding methods focus primarily on single view networks. We introduce a novel multi-view network embedding (MVNE) algorithm for constructing low-dimensional node embeddings from multi-view networks. MVNE adapts and extends an approach to single view network embedding (SVNE) using graph factorization clustering (GFC) to the multi-view setting using an objective function that maximizes the agreement between views based on both the local and global structure of the underlying multi-view graph. Our experiments with several benchmark real-world single view networks show that GFC-based SVNE yields network embeddings that are competitive with or superior to those produced by the state-of-the-art single view network embedding methods when the embeddings are used for labeling unlabeled nodes in the networks. Our experiments with several multi-view networks show that MVNE substantially outperforms the single view methods on integrated view and the state-of-the-art multi-view methods. We further show that even when the goal is to predict labels of nodes within a single target view, MVNE outperforms its single-view counterpart suggesting that the MVNE is able to extract the information that is useful for labeling nodes in the target view from the all of the views.\"","summary":"\"In contrast to the existing multi-view network embedding methods, MVNE exploits a recently discovered connection between network adjacency matrix factorization and network embedding @cite_34 to utilize GFC @cite_7 , a graph factorization method, to perform single view network embedding. MVNE extends the resulting single view network embedding algorithm to the multi-view setting. Inspired by @cite_17 , MVNE uses a novel objective function that maximizes the agreement between views while combining information derived from the local as well as the global structure of the underlying multi-view networks. Like DMNE @cite_28 , MVNE uses a co-regularized objective function to maximize the agreement in the embedding space and to control the embedding dimension. Unlike DMNE which requires on computationally expensive training of a deep neural network, MVNE is considerably more efficient and hence scalable to large networks.\"","":""}
{"id":"2964167901","dialogue":"\"Face hallucination is a generative task to super-resolve the facial image with low resolution while human perception of face heavily relies on identity information. However, previous face hallucination approaches largely ignore facial identity recovery. This paper proposes Super-Identity Convolutional Neural Network (SICNN) to recover identity information for generating faces closed to the real identity. Specifically, we define a super-identity loss to measure the identity difference between a hallucinated face and its corresponding high-resolution face within the hypersphere identity metric space. However, directly using this loss will lead to a Dynamic Domain Divergence problem, which is caused by the large margin between the high-resolution domain and the hallucination domain. To overcome this challenge, we present a domain-integrated training approach by constructing a robust identity metric for faces from these two domains. Extensive experimental evaluations demonstrate that the proposed SICNN achieves superior visual quality over the state-of-the-art methods on a challenging task to super-resolve 12 ( ) 14 faces with an 8 ( ) upscaling factor. In addition, SICNN significantly improves the recognizability of ultra-low-resolution faces.\"","summary":"For subspace-based methods. @cite_27 employed a Principal Component Analysis (PCA) based global appearance model to hallucinate LR faces and a local non-parametric model to enhance the details. @cite_41 used multiple local exemplar patches sampled from aligned HR facial images to hallucinate LR faces. @cite_42 resolved to sparse representation on local face patches. These subspace-based methods require precisely aligned reference HR and LR facial images with the same pose and facial expression.","":""}
{"id":"2964167901","dialogue":"\"Face hallucination is a generative task to super-resolve the facial image with low resolution while human perception of face heavily relies on identity information. However, previous face hallucination approaches largely ignore facial identity recovery. This paper proposes Super-Identity Convolutional Neural Network (SICNN) to recover identity information for generating faces closed to the real identity. Specifically, we define a super-identity loss to measure the identity difference between a hallucinated face and its corresponding high-resolution face within the hypersphere identity metric space. However, directly using this loss will lead to a Dynamic Domain Divergence problem, which is caused by the large margin between the high-resolution domain and the hallucination domain. To overcome this challenge, we present a domain-integrated training approach by constructing a robust identity metric for faces from these two domains. Extensive experimental evaluations demonstrate that the proposed SICNN achieves superior visual quality over the state-of-the-art methods on a challenging task to super-resolve 12 ( ) 14 faces with an 8 ( ) upscaling factor. In addition, SICNN significantly improves the recognizability of ultra-low-resolution faces.\"","summary":"\". Recently, deep convolutional neural networks (DCNNs) achieve remarkable progresses in a variety of face analysis tasks, such as face recognition @cite_34 @cite_20 @cite_14 , face detection @cite_3 @cite_25 , facial attribute recognition @cite_8 @cite_1 @cite_43 @cite_18 . @cite_2 proposed a bichannel CNN to hallucinate blurry facial images in the wild. For un-aligned faces, @cite_4 proposed to jointly learn face hallucination and facial dense spatial correspondence field estimation. The approach of @cite_0 is a GAN-based method to generate realistic facial images. These works ignore the identity information recovery that is important for recognizability and hallucination quality. @cite_23 and @cite_38 relied on perceptual loss function closer to perceptual similarity to recover visually more convincing HR images for general image SR. In this paper we modified the perceptual loss to facilitate identity hypersphere space and propose a novel training approach to overcome the challenging while using the loss.\"","":""}
{"id":"2899689260","dialogue":"\"In this paper, we study the problem of dynamic channel allocation for URLLC traffic in a multiuser multi-channel wireless network where urgent packets have to be successfully received in a timely manner. We formulate the problem as a finite-horizon Markov Decision Process with a stochastic constraint related to the QoS requirement, defined as the packet loss rate for each user. We propose a novel weighted formulation that takes into account both the total expected reward (number of successfully decoded packets) and the risk which we define as the QoS requirement violation. First, we use the value iteration algorithm to find the optimal policy, which assumes a perfect knowledge of the controller of all the parameters, namely the channel statistics. We then propose a Q-learning algorithm where the controller learns the optimal policy without having knowledge of neither the CSI nor the channel statistics. We illustrate the performance of our algorithms with numerical studies.\"","summary":"\"The issue of deadline-constrained traffic scheduling has been investigated by several works including @cite_20 @cite_18 @cite_14 @cite_16 . For example, in @cite_16 , the authors study the problem of dynamic channel allocation in a single user multi-channel system with service costs and deadline-constrained traffic. They propose online algorithms to enable the controller to learn the optimal policy based on Thompson sampling for multi-armed bandit problems. The MDP framework and reinforcement learning approaches for downlink packet scheduling are considered in @cite_20 @cite_14 @cite_13 @cite_4 @cite_8 @cite_12 . In @cite_20 , the authors propose an MDP for deadline-constrained packet scheduling problem and use dynamic programming to find the optimal scheduling policies. The authors do not consider QoS constraints in the scheduling problem.\"","":""}
{"id":"2899689260","dialogue":"\"In this paper, we study the problem of dynamic channel allocation for URLLC traffic in a multiuser multi-channel wireless network where urgent packets have to be successfully received in a timely manner. We formulate the problem as a finite-horizon Markov Decision Process with a stochastic constraint related to the QoS requirement, defined as the packet loss rate for each user. We propose a novel weighted formulation that takes into account both the total expected reward (number of successfully decoded packets) and the risk which we define as the QoS requirement violation. First, we use the value iteration algorithm to find the optimal policy, which assumes a perfect knowledge of the controller of all the parameters, namely the channel statistics. We then propose a Q-learning algorithm where the controller learns the optimal policy without having knowledge of neither the CSI nor the channel statistics. We illustrate the performance of our algorithms with numerical studies.\"","summary":"\"Most risk-sensitive approaches consist in analyzing higher order statistics than the average metric such as the variance of the reward @cite_19 @cite_2 @cite_0 @cite_17 . For instance, a risk-sensitive reinforcement learning is studied in @cite_5 in millimeter-wave communications to optimize both the bandwidth and transmit power. The authors consider a utility (data rate) that incorporates both the average and the variance to capture the tail distribution of the rate, useful for the reliability requirement of URLLC traffic. The authors do not exploit frequency diversity.\"","":""}
{"id":"2899689260","dialogue":"\"In this paper, we study the problem of dynamic channel allocation for URLLC traffic in a multiuser multi-channel wireless network where urgent packets have to be successfully received in a timely manner. We formulate the problem as a finite-horizon Markov Decision Process with a stochastic constraint related to the QoS requirement, defined as the packet loss rate for each user. We propose a novel weighted formulation that takes into account both the total expected reward (number of successfully decoded packets) and the risk which we define as the QoS requirement violation. First, we use the value iteration algorithm to find the optimal policy, which assumes a perfect knowledge of the controller of all the parameters, namely the channel statistics. We then propose a Q-learning algorithm where the controller learns the optimal policy without having knowledge of neither the CSI nor the channel statistics. We illustrate the performance of our algorithms with numerical studies.\"","summary":"\"In this work, we consider an alternative approach to the risk which consists in minimizing the risk state visitation probability. In fact, due to the stochastic nature of the problem (time-varying channel and random arrival traffic in our context), giving a low reward to an undesirable or a risk-state may be insufficient to minimize the probability of visiting such state @cite_7 . Therefore, in addition to the maximization of the total expected reward, we propose to consider a second criterion which consists in minimizing the probability of visiting risk states where a risk state here is related to the violation of QoS requirements.\"","":""}
{"id":"2899877876","dialogue":"\"We consider the problem of learning knowledge graph (KG) embeddings for entity alignment (EA). Current methods use the embedding models mainly focusing on triple-level learning, which lacks the ability of capturing long-term dependencies existing in KGs. Consequently, the embedding-based EA methods heavily rely on the amount of prior (known) alignment, due to the identity information in the prior alignment cannot be efficiently propagated from one KG to another. In this paper, we propose RSN4EA (recurrent skipping networks for EA), which leverages biased random walk sampling for generating long paths across KGs and models the paths with a novel recurrent skipping network (RSN). RSN integrates the conventional recurrent neural network (RNN) with residual learning and can largely improve the convergence speed and performance with only a few more parameters. We evaluated RSN4EA on a series of datasets constructed from real-world KGs. Our experimental results showed that it outperformed a number of state-of-the-art embedding-based EA methods and also achieved comparable performance for KG completion.\"","summary":"\"KG representation learning has been widely studied in recent years @cite_13 . One of the most famous translational methods is TransE @cite_2 , which models a triple @math as @math . TransE works well for one-to-one relationships, but fails to model more complex relationships like one-to-many and many-to-many. TransR @cite_21 tries to solve this problem by involving a relation-specific matrix @math to project @math by @math . PTransE @cite_5 leverages path information to learn inferences among relations. For example, if there exist two triples @math , which form a path in KG, and another triple @math holds simultaneously, PTransE models the path information by learning @math , where @math denotes the operator used to merge @math . KG completion is the most prevalent task for KG representation learning, and there also exist some non-translation methods that are particularly tailored for KG completion @cite_17 @cite_1 .\"","":""}
{"id":"2899877876","dialogue":"\"We consider the problem of learning knowledge graph (KG) embeddings for entity alignment (EA). Current methods use the embedding models mainly focusing on triple-level learning, which lacks the ability of capturing long-term dependencies existing in KGs. Consequently, the embedding-based EA methods heavily rely on the amount of prior (known) alignment, due to the identity information in the prior alignment cannot be efficiently propagated from one KG to another. In this paper, we propose RSN4EA (recurrent skipping networks for EA), which leverages biased random walk sampling for generating long paths across KGs and models the paths with a novel recurrent skipping network (RSN). RSN integrates the conventional recurrent neural network (RNN) with residual learning and can largely improve the convergence speed and performance with only a few more parameters. We evaluated RSN4EA on a series of datasets constructed from real-world KGs. Our experimental results showed that it outperformed a number of state-of-the-art embedding-based EA methods and also achieved comparable performance for KG completion.\"","summary":"\"DeepWalk @cite_4 is one of the most well-known models in the network representation learning area. It uses uniform random walks to sample paths in a network, and applies Skip-Gram @cite_11 to model the generated paths. Skip-Gram learns the embedding of a node by maximizing the probabilities of its neighbors, which captures the information among the nodes. node2vec @cite_7 proposes biased random walks to refine the process of sampling paths from a network. It smoothly controls the node selection strategy to make the random walks explore neighbors in a breadth-first-search as well as a depth-first-search fashion. In this paper, the proposed EA-specific random walk sampling is inspired by node2vec, but concentrates on generating long and cross-KG paths.\"","":""}
{"id":"2950099317","dialogue":"\"Can we perform an end-to-end sound source separation (SSS) with a variable number of sources using a deep learning model? This paper presents an extension of the Wave-U-Net model which allows end-to-end monaural source separation with a non-fixed number of sources. Furthermore, we propose multiplicative conditioning with instrument labels at the bottleneck of the Wave-U-Net and show its effect on the separation results. This approach can be further extended to other types of conditioning such as audio-visual SSS and score-informed SSS.\"","summary":"\"Traditionally, people have attempted to solve audio source separation through matrix-factorization algorithms. Independent Component Analysis (ICA) @cite_16 and Non-negative Matrix Factorization (NMF) @cite_1 are two common techniques used for source separation.\"","":""}
{"id":"2950099317","dialogue":"\"Can we perform an end-to-end sound source separation (SSS) with a variable number of sources using a deep learning model? This paper presents an extension of the Wave-U-Net model which allows end-to-end monaural source separation with a non-fixed number of sources. Furthermore, we propose multiplicative conditioning with instrument labels at the bottleneck of the Wave-U-Net and show its effect on the separation results. This approach can be further extended to other types of conditioning such as audio-visual SSS and score-informed SSS.\"","summary":"\"Wave-U-Net model @cite_4 is an adaptation of the U-Net @cite_18 , a convolutional encoder-decoder network developed for image segmentation. The U-Net approach has been adapted already for singing voice separation in @cite_8 , however this model applies 2D convolutions and works with spectrograms. Instead of doing a 2D convolution, Wave-U-Net performs series of 1D convolutions, downsampling and upsampling with skip connections on a raw waveform signal. The input to this network is a single channel audio mix, and the desired output is the separated @math channels of individual audio sources, where @math is the number of sources present in the audio mix. An interesting aspect of the Wave-U-Net is that it avoids implicit zero paddings in the downsampling layers, and it performs linear interpolation as opposed to de-convolution. This means that our dimension size is not preserved, and our output results will actually become a lot shorter compared to our input. However, by doing this we can better preserve temporal continuity and avoid audio artifacts in the results.\"","":""}
{"id":"2899373513","dialogue":"\"Recently, word embedding algorithms have been applied to map the entities of recommender systems, such as users and items, to new feature spaces using textual element-context relations among them. Unlike many other domains, this approach has not achieved a desired performance in collaborative filtering problems, probably due to unavailability of appropriate textual data. In this paper we propose a new recommendation framework, called GEMRank that can be applied when the user-item matrix is the sole available souce of information. It uses the concept of profile co-occurrence for defining relations among entities and applies a factorization method for embedding the users and items. GEMRank then feeds the extracted representations to a neural network model to predict user-item like dislike relations which the final recommendations are made based on. We evaluated GEMRank in an extensive set of experiments against state of the art recommendation methods. The results show that GEMRank significantly outperforms the baseline algorithms in a variety of data sets with different degrees of density.\"","summary":"\"Traditional NCR algorithms usually suffer from sparsity of the dataset as the similarity calculation can be hard when there is not enough information about users available. Graph based methods try to solve this problem by modeling the data as a graph in order to estimate the distances more accurately when the data is sparse. These methods first construct a graph to represent data and then make recommendations by analyzing the graph. In @cite_14 different types of nodes and a multi-layer structure have been used to make context-aware recommendation through a random walk in the graph. SibRank @cite_16 uses a signed bipartite preference network for representing the data and analyzes it using a signed version of Personalized PageRank to capture users' similarities. Among more recent approaches, GRank @cite_3 is an state of the art method which uses personalized PageRank over a tripartite preference network to directly infer the total ranking of items. GRank may use unreliable paths that are inconsistent with the general idea of similarity in neighborhood collaborative ranking. ReGRank @cite_0 ranks items based on reliable recommendation paths that are in harmony with the semantics behind different approaches in neighborhood collaborative ranking.\"","":""}
{"id":"2898989428","dialogue":"\"Although neural machine translation (NMT) has achieved impressive progress recently, it is usually trained on the clean parallel data set and hence cannot work well when the input sentence is the production of the automatic speech recognition (ASR) system due to the enormous errors in the source. To solve this problem, we propose a simple but effective method to improve the robustness of NMT in the case of speech translation. We simulate the noise existing in the realistic output of the ASR system and inject them into the clean parallel data so that NMT can work under similar word distributions during training and testing. Besides, we also incorporate the Chinese Pinyin feature which is easy to get in speech translation to further improve the translation performance. Experiment results show that our method has a more stable performance and outperforms the baseline by an average of 3.12 BLEU on multiple noisy test sets, even while achieves a generalization improvement on the WMT'17 Chinese-English test set.\"","summary":"\"It is necessary to enhance the robustness of machine translation since the ASR system carries misrecognized transcriptions over into the downstream MT system in the SLT scenario. Prior work attempted to induce noise by considering the realistic ASR outputs as the source corpora used for training MT systems @cite_15 @cite_8 . Although the problem of error propagation could be alleviated by the promising end-to-end speech translation models @cite_5 @cite_11 . Unfortunately, there are few training data in the form of speech paired with text translations. In contrast, our approach utilizes the large-scale written parallel corpora. Recently, sperber2017neural adapted the NMT model to noise outputs from ASR, where they introduced artificially corrupted inputs during the training process and only achieved minor improvements on noisy input but harmed the translation quality on clean text. However, our approach not only significantly enhances the robustness of NMT on noisy test sets, but also improves the generalization performance.\"","":""}
{"id":"2898989428","dialogue":"\"Although neural machine translation (NMT) has achieved impressive progress recently, it is usually trained on the clean parallel data set and hence cannot work well when the input sentence is the production of the automatic speech recognition (ASR) system due to the enormous errors in the source. To solve this problem, we propose a simple but effective method to improve the robustness of NMT in the case of speech translation. We simulate the noise existing in the realistic output of the ASR system and inject them into the clean parallel data so that NMT can work under similar word distributions during training and testing. Besides, we also incorporate the Chinese Pinyin feature which is easy to get in speech translation to further improve the translation performance. Experiment results show that our method has a more stable performance and outperforms the baseline by an average of 3.12 BLEU on multiple noisy test sets, even while achieves a generalization improvement on the WMT'17 Chinese-English test set.\"","summary":"\"Our approach is motivated by the work of NMT incorporated with linguistic input features @cite_16 . Chinese linguistic features, such as radicals and Pinyin, have been demonstrated effective to Chinese-sourced NMT @cite_13 @cite_17 and Chinese ASR @cite_19 . We also incorporate Pinyin as an additional input feature in the robust NMT model, aiming at improving the robustness of NMT further.\"","":""}
{"id":"2898630544","dialogue":"\"Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization. Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories. This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.\"","summary":"\"Regularization in RL has been considered via several different perspectives. One line of investigation focuses on regularizing the features learned on the state space . In particular backward bootstrapping method's can be seen as regularizing in feature space based on temporal proximity @cite_2 @cite_1 @cite_7 . These approaches assume that nearby states in the state space have similar value. Other works focus on regularizing the changes in policy directly. Those approaches are often based on entropy methods . Explicit regularization in the temporal space has received much less attention. Temporal regularization in some sense may be seen as a backward'' multi-step method . The closest work to ours is possibly , where they define natural value approximator by projecting the previous states estimates by adjusting for the reward and @math . Their formulation, while sharing similarity in motivation, leads to different theory and algorithm. Convergence properties and bias induced by this class of methods were also not analyzed in .\"","":""}
{"id":"2899178888","dialogue":"\"Traffic prediction is a fundamental and vital task in Intelligence Transportation System (ITS), but it is very challenging to get high accuracy while containing low computational complexity due to the spatiotemporal characteristics of traffic flow, especially under the metropolitan circumstances. In this work, a new topological framework, called Linkage Network, is proposed to model the road networks and present the propagation patterns of traffic flow. Based on the Linkage Network model, a novel online predictor, named Graph Recurrent Neural Network (GRNN), is designed to learn the propagation patterns in the graph. It could simultaneously predict traffic flow for all road segments based on the information gathered from the whole graph, which thus reduces the computational complexity significantly from O(nm) to O(n+m), while keeping the high accuracy. Moreover, it can also predict the variations of traffic trends. Experiments based on real-world data demonstrate that the proposed method outperforms the existing prediction methods.\"","summary":"\"There are many previous works @cite_21 considering the traffic condition as a time series and predicting for different segments separately through time series analysis, like Auto-Regressive Moving Average (ARMA) based algorithms (ARIMA, SARIMA). Additionally, some research @cite_22 @cite_3 uses the methods of statistical learning such as Bayesian Network (BN), SVR and GBDT, and adds extra information to assist the training. @cite_7 compares those methods and shows their similar performances. In these approaches, the strong spatiotemporal couplings, which exist in metropolitan circumstance particularly, lead to the dilemma of choices between the computation complexity and the sufficiency of input information.\"","":""}
{"id":"2963439073","dialogue":"\"In this work we investigate the reasons why Batch Normalization (BN) improves the generalization performance of deep networks. We argue that one major reason, distinguishing it from data-independent normalization methods, is randomness of batch statistics. This randomness appears in the parameters rather than in activations and admits an interpretation as a practical Bayesian learning. We apply this idea to other (deterministic) normalization techniques that are oblivious to the batch size. We show that their generalization performance can be improved significantly by Bayesian learning of the same form. We obtain test performance comparable to BN and, at the same time, better validation losses suitable for subsequent output uncertainty estimation through approximate Bayesian posterior.\"","summary":"The improved methods that we propose are also closely related to variational drop -out @cite_0 as discussed below. We give a new interpretation to variational dropout and apply it in combination with normalization techniques.","":""}
{"id":"2899108147","dialogue":"\"Acquiring a large vocabulary is an important aspect of human intelligence. Onecommon approach for human to populating vocabulary is to learn words duringreading or listening, and then use them in writing or speaking. This ability totransfer from input to output is natural for human, but it is difficult for machines.Human spontaneously performs this knowledge transfer in complicated multimodaltasks, such as Visual Question Answering (VQA). In order to approach human-levelArtificial Intelligence, we hope to equip machines with such ability. Therefore, toaccelerate this research, we propose a newzero-shot transfer VQA(ZST-VQA)dataset by reorganizing the existing VQA v1.0 dataset in the way that duringtraining, some words appear only in one module (i.e. questions) but not in theother (i.e. answers). In this setting, an intelligent model should understand andlearn the concepts from one module (i.e. questions), and at test time, transfer themto the other (i.e. predict the concepts as answers). We conduct evaluation on thisnew dataset using three existing state-of-the-art VQA neural models. Experimentalresults show a significant drop in performance on this dataset, indicating existingmethods do not address the zero-shot transfer problem. Besides, our analysis findsthat this may be caused by the implicit bias learned during training.\"","summary":"\"VQA has improving dramatically recently @cite_18 @cite_8 @cite_10 . We briefly introduce typical VQA methods, and recommend the surveys @cite_19 @cite_16 for a more details. Depending on the attention usage, VQA methods can be roughly divided into three groups: (i) non-attention methods, (ii) visual attention methods and (iii) visual-text co-attention methods. Non-attention methods include multimodal compact bilinear networks @cite_24 , relational networks @cite_13 , and Deeper LSTM Question+Image @cite_7 . They usually produce answers through a general network architecture with attention implicitly embedded in the model. Visual attention methods, on the contrary, utilize image-question pairs to attend to the discriminative image regions to predict the answer. For example, stacked attention networks @cite_1 , ABC-CNN @cite_20 and dynamic memory networks @cite_25 all explicitly compute the visual attention by combining top-down question contexts and bottom-up image cues. Visual-text co-attention methods such as hierarchical co-attention networks @cite_22 , dual attention networks @cite_29 and compositional attention networks @cite_27 all build explicit attention on both images and questions. Tough results are encouraging, these methods don't address zero-shot transfer problem.\"","":""}
{"id":"2899108147","dialogue":"\"Acquiring a large vocabulary is an important aspect of human intelligence. Onecommon approach for human to populating vocabulary is to learn words duringreading or listening, and then use them in writing or speaking. This ability totransfer from input to output is natural for human, but it is difficult for machines.Human spontaneously performs this knowledge transfer in complicated multimodaltasks, such as Visual Question Answering (VQA). In order to approach human-levelArtificial Intelligence, we hope to equip machines with such ability. Therefore, toaccelerate this research, we propose a newzero-shot transfer VQA(ZST-VQA)dataset by reorganizing the existing VQA v1.0 dataset in the way that duringtraining, some words appear only in one module (i.e. questions) but not in theother (i.e. answers). In this setting, an intelligent model should understand andlearn the concepts from one module (i.e. questions), and at test time, transfer themto the other (i.e. predict the concepts as answers). We conduct evaluation on thisnew dataset using three existing state-of-the-art VQA neural models. Experimentalresults show a significant drop in performance on this dataset, indicating existingmethods do not address the zero-shot transfer problem. Besides, our analysis findsthat this may be caused by the implicit bias learned during training.\"","summary":"\"Zero-shot learning was early proposed by @cite_28 and soon become an interesting research problem in the cross-modal domain spanning natural language processing and computer vision @cite_15 , where no finite set of samples can cover the diversity of the real world and all datasets naturally follow a heavy-tail distribution with new classes appearing frequently after the training @cite_15 @cite_23 . Usually, zero-shot learning requires to transfer knowledge from other sources such as attributes @cite_9 , word embedding @cite_5 or the relationship to other categories @cite_21 in order to predict the novel class labels. In our dataset, novel words are embedded inside one module (i.e. questions) and we test the model's zero-shot generalization ability to the other module (i.e. answers).\"","":""}
{"id":"2899108147","dialogue":"\"Acquiring a large vocabulary is an important aspect of human intelligence. Onecommon approach for human to populating vocabulary is to learn words duringreading or listening, and then use them in writing or speaking. This ability totransfer from input to output is natural for human, but it is difficult for machines.Human spontaneously performs this knowledge transfer in complicated multimodaltasks, such as Visual Question Answering (VQA). In order to approach human-levelArtificial Intelligence, we hope to equip machines with such ability. Therefore, toaccelerate this research, we propose a newzero-shot transfer VQA(ZST-VQA)dataset by reorganizing the existing VQA v1.0 dataset in the way that duringtraining, some words appear only in one module (i.e. questions) but not in theother (i.e. answers). In this setting, an intelligent model should understand andlearn the concepts from one module (i.e. questions), and at test time, transfer themto the other (i.e. predict the concepts as answers). We conduct evaluation on thisnew dataset using three existing state-of-the-art VQA neural models. Experimentalresults show a significant drop in performance on this dataset, indicating existingmethods do not address the zero-shot transfer problem. Besides, our analysis findsthat this may be caused by the implicit bias learned during training.\"","summary":"\"There are many VQA datasets, such as @cite_26 @cite_17 for compositionality, zero-shot VQA dataset @cite_31 @cite_14 using extra resources, and more on the surveys @cite_19 @cite_16 . Different from them, our dataset focus on transfer between input and output for natural human learning.\"","":""}
{"id":"2899205526","dialogue":"\"Large scale cloud networks consist of distributed networking and computing elements that process critical information and thus security is a key requirement for any environment. Unfortunately, assessing the security state of such networks is a challenging task and the tools used in the past by security experts such as packet filtering, firewall, Intrusion Detection Systems (IDS) etc., provide a reactive security mechanism. In this paper, we introduce a Moving Target Defense (MTD) based proactive security framework for monitoring attacks which lets us identify and reason about multi-stage attacks that target software vulnerabilities present in a cloud network. We formulate the multi-stage attack scenario as a two-player zero-sum Markov Game (between the attacker and the network administrator) on attack graphs. The rewards and transition probabilities are obtained by leveraging the expert knowledge present in the Common Vulnerability Scoring System (CVSS). Our framework identifies an attacker's optimal policy and places countermeasures to ensure that this attack policy is always detected, thus forcing the attacker to use a sub-optimal policy with higher cost.\"","summary":"\"Sheyner @cite_9 present a formal analysis of attacks on a network along with cost-benefit analysis and security measures to defend against the network attacks. In @cite_13 , Chowdhary provide a polynomial time method for attack graph construction and network reconfiguration using a parallel computing approach, making it possible to leverage information for strategic reason of attacks in large-scale systems.\"","":""}
{"id":"2899205526","dialogue":"\"Large scale cloud networks consist of distributed networking and computing elements that process critical information and thus security is a key requirement for any environment. Unfortunately, assessing the security state of such networks is a challenging task and the tools used in the past by security experts such as packet filtering, firewall, Intrusion Detection Systems (IDS) etc., provide a reactive security mechanism. In this paper, we introduce a Moving Target Defense (MTD) based proactive security framework for monitoring attacks which lets us identify and reason about multi-stage attacks that target software vulnerabilities present in a cloud network. We formulate the multi-stage attack scenario as a two-player zero-sum Markov Game (between the attacker and the network administrator) on attack graphs. The rewards and transition probabilities are obtained by leveraging the expert knowledge present in the Common Vulnerability Scoring System (CVSS). Our framework identifies an attacker's optimal policy and places countermeasures to ensure that this attack policy is always detected, thus forcing the attacker to use a sub-optimal policy with higher cost.\"","summary":"\"In the context of cloud systems, Peng discusses a risk-aware MTD strategy @cite_7 where they model the attack surface as a non-decreasing probability density function and then estimate the risk of migrating a VM to a replacement node using probabilistic inference. Kampanakis @cite_8 highlight obfuscation as a possible MTD strategy in order to deal with attacks like OS fingerprinting and network reconnaissance in the SDN environment. Furthermore, they highlight that the trade-off between such random mutations, which may disrupt any active services, require analysis of cost-benefits.\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"Learning transformation-equivariant representations can trace back to the seminal work on training capsule nets @cite_19 @cite_3 @cite_32 . The transformation equivariance is characterized by the various directions of capsules, while the confidence of belonging to a particular class is captured by their lengths.\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"Many efforts have been made in literature @cite_35 @cite_13 @cite_17 on extending the conventional translation-equivariant convolutions to cover more transformations. Among them are group equivariant convolutions (G-convolution) @cite_35 that have been developed to equivary to more types of transformations. The idea of group equivariance has also been introduced to the capsule nets @cite_17 by ensuring the equivariance of output pose vectors to a group of transformations with a generic routing mechanism. However, the group equivariant convolution is restricted to discrete transformations, which limits its ability to learn the representations equivariant to generic continuous transformations.\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"Auto-Encoders and GANs. Unsupervised auto-encoders have been extensively studied in literature @cite_2 @cite_1 @cite_11 . Existing auto-encoders are trained by reconstructing input data from the outputs of encoders. A large category of auto-encoder variants have been proposed. Among them is the Variational Auto-Encoder (VAE) @cite_34 that maximizes the lower-bound of the data likelihood to train a pair of probabilistic encoder and decoder, while beta-VAE seeks to disentangle representations by introducing an adjustable hyperparameter on the capacity of latent channel to balance between the independence constraint and the reconstruction accuracy @cite_29 . Denoising auto-encoders @cite_11 attempt to reconstruct noise-corrupted data to learn robust representations, while contrastive Auto-Encoders @cite_12 encourage to learn representations invariant to small perturbations on data. Along this direction, @cite_3 propose capsule networks to explore transformation equivariance by minimizing the discrepancy between the reconstructed and target data.\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"On the other hand, Generative Adversarial Nets (GANs) have also been used to train unsupervised representations. Unlike the auto-encoders, the GANs @cite_15 and their variants @cite_24 @cite_5 @cite_21 @cite_31 generate data from the noises drawn from a simple distribution, with a discriminator trained adversarially to distinguish between real and fake data. The sampled noises can be viewed as the representation of generated data over a manifold, and one can train an encoder by inverting the generator to find the generating noise. This can be implemented by jointly training a pair of mutually inverse generator and encoder @cite_24 @cite_5 . There also exist better generalizable GANs in producing unseen data based on the Lipschitz assumption on the real data distribution @cite_21 @cite_31 , which can give rise to more powerful representations of data out of training examples @cite_24 @cite_5 @cite_25 . Compared with the Auto-Encoders, GANs do not rely on learning one-to-one reconstruction of data; instead, they aim to generate the entire distribution of data.\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"Self-Supervisory Signals. There exist many other unsupervised learning methods using different types of self-supervised signals to train deep networks. Mehdi and Favaro @cite_9 propose to solve Jigsaw puzzles to train a convolutional neural network. @cite_20 train the network by inferring the relative positions between sampled patches from an image as self-supervised information. Instead, @cite_4 count features that satisfy equivalence relations between downsampled and tiled images. @cite_6 propose to train RotNets by predicting a discrete set of image rotations, but they are unable to handle generic continuous transformations and their compositions. @cite_38 create a set of surrogate classes by applying various transformations to individual images. However, the resultant features could over-discriminate visually similar images as they always belong to different surrogate classes. Unsupervised features have also been learned from videos by estimating the self-motion of moving objects between consecutive frames @cite_10 .\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"In addition, there exist a large number of semi-supervised models in literature. Here, we particularly mention three state-of-the-art methods that will be compared in experiments. Temporal ensembling @cite_26 and mean teachers @cite_18 both use an ensemble of teachers to supervise the training of a student model. Temporal ensembling uses the exponential moving average of predictions made by past models on unlabeled data as targets to train the student model. Instead, mean teachers update the student model with the exponential moving average of the weights of past models. On the contrary, the Virtual Adversarial Training (VAT) @cite_7 seeks to minimizes the change of predictions on unlabeled examples when their output values are adversarially altered. This could result in a robust model that prefers smooth predictions over unlabeled data.\"","":""}
{"id":"2972729785","dialogue":"\"Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of translation equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.\"","summary":"\"The SAT also differs from transformation-based data augmentation in which the transformed samples and their labels are used directly as additional training examples @cite_0 . First","":""}
{"id":"2949948433","dialogue":"\"After the peace agreement of 2016 with FARC, the killings of social leaders have emerged as an important post-conflict challenge for Colombia. We present a data analysis based on official records obtained from the Colombian General Attorney's Office spanning the time period from 2012 to 2017. The results of the analysis show a drastic increase in the officially recorded number of killings of democratically elected leaders of community organizations, in particular those belonging to Juntas de Accion Comunal [Community Action Boards]. These are important entities that have been part of the Colombian democratic apparatus since 1958, and enable communities to advocate for their needs. We also describe how the data analysis guided a journalistic investigation that was motivated by the Colombian government's denial of the systematic nature of social leaders killings.\"","summary":"\"In a working paper, @cite_9 use data from the Colombian nonprofit organization to investigate the killings of social leaders. The authors hypothesise that social leaders were increasingly killed by armed groups excluded from the peace process that wanted to consolidate their power, especially in areas where they took over FARC's illegal activities. In the dataset, @cite_9 also find that the category of social leaders targeted the most since the beginning of the ceasefire are local community council leaders.\"","":""}
{"id":"2949272455","dialogue":"\"Stencil algorithms have been receiving considerable interest in HPC research for decades. The techniques used to approach multi-core stencil performance modeling and engineering span basic runtime measurements","summary":"elaborate performance models","":""}
{"id":"2951153470","dialogue":"\"Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52 FLOPs on ResNet-110 with even 2.69 relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42 FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: this https URL\"","summary":"\"Most previous works on accelerating CNNs can be roughly divided into three categories, namely, @cite_1 @cite_2 , @cite_20 @cite_32 , and . -based approaches aim to remove the unnecessary connections of the neural network @cite_8 @cite_17 . Essentially, always results in unstructured models, which makes it hard to deploy the existing efficient BLAS library, while not only reduces the storage usage on devices but also decreases computation cost to accelerate the inference. We could roughly divide the filter pruning methods into two categories by whether the training data is utilized to determine the pruned filters, that is, and filter pruning. Data independent method is more efficient than data dependent method as the ultimating of training data is computation consuming.\"","":""}
{"id":"2951153470","dialogue":"\"Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52 FLOPs on ResNet-110 with even 2.69 relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42 FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: this https URL\"","summary":"\"Many recent works @cite_8 @cite_34 @cite_31 @cite_27 @cite_25 @cite_12 @cite_10 focus on pruning fine-grained weight of filters. For example, @cite_8 proposes an iterative method to discard the small weights whose values are below the predefined threshold. @cite_25 formulates pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition.\"","":""}
{"id":"2951153470","dialogue":"\"Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52 FLOPs on ResNet-110 with even 2.69 relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42 FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: this https URL\"","summary":"\"Some filter pruning approaches @cite_33 @cite_21 @cite_0 @cite_23 @cite_4 @cite_6 @cite_30 @cite_35 are data dependent, which means the training data is utilized to determine the pruned filters. @cite_21 adopts the statistics information from the next layer to guide the filter selections. @cite_4 aims to obtain a decomposition by minimizing the reconstruction error of training set sample activations. @cite_6 proposes an inherently data driven method which use Principal Component Analysis (PCA) to specify the proportion of the energy that should be preserved. @cite_35 applies subspace clustering to feature maps to eliminate the redundancy in convolutional filters.\"","":""}
{"id":"2951153470","dialogue":"\"Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52 FLOPs on ResNet-110 with even 2.69 relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42 FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: this https URL\"","summary":"\"Concurrently with our work, some data independent filter pruning strategies @cite_17 @cite_12 @cite_3 @cite_11 have been explored. @cite_17 utilizes an @math -norm criterion to prune unimportant filters. @cite_12 proposes to select filters with an @math -norm criterion and prune those selected filters in a soft manner. @cite_3 proposes to prune models by enforcing sparsity on the scaling parameter of batch normalization layers. @cite_11 uses spectral clustering on filters to select unimportant ones.\"","":""}
{"id":"2951153470","dialogue":"\"Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52 FLOPs on ResNet-110 with even 2.69 relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42 FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: this https URL\"","summary":"\"To the best of our knowledge, only one previous work reconsiders the smaller-norm-less-important criterion @cite_3 . We would like to highlight our advantages compared to this approach as below: (1) @cite_3 pays more attention to enforce sparsity on the scaling parameter in the batch normalization operator, which is not friendly to the structure without batch normalization. On the contrary, our approach is not limited by this constraint. (2) After pruning channels selected, @cite_3 need fine-tuning to reduce the performance degradation. However, our method combines the pruning operation with normal training procedure, thus extra fine-tuning is not necessary. (3) Calculation of the gradient of scaling factor is needed for @cite_3 , thus lots of computation cost are inevitable, whereas our approach could accelerate the neural network without calculating of the gradient of scaling factor.\"","":""}
{"id":"2951153470","dialogue":"\"Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with ''relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52 FLOPs on ResNet-110 with even 2.69 relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42 FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: this https URL\"","summary":"\"Some other works @cite_29 @cite_6 @cite_4 @cite_35 @cite_11 sharing some common ideas with ours, that is, find the filters that have similar function so that the filters could be pruned, the differences exist in @cite_29 (1) We focus on acceleration of inference of neural network, while @cite_29 concentrate on the emergence of duplicate filters over training iterations; (2) We use the geometric median to select filters to prune, while @cite_29 applies cosine similarity to analyze the similarity of filters; (3) We have demonstrated our approach on large-scale datasets with sophisticated ResNet, while @cite_3 only conducted experiments on small-scale CIFAR-10 with AlexNet, @cite_6 only conducted experiments on small-scale CIFAR-10 and CIFAR-100 with AlexNet\"","":""}
{"id":"2899192577","dialogue":"\"Humanoid robots dynamically navigate an environment by interacting with it via contact wrenches exerted at intermittent contact poses. Therefore, it is important to consider dynamics when planning a contact sequence. Traditional contact planning approaches assume a quasi-static balance criterion to reduce the computational challenges of selecting a contact sequence over a rough terrain. This however limits the applicability of the approach when dynamic motions are required, such as when walking down a steep slope or crossing a wide gap. Recent methods overcome this limitation with the help of efficient mixed integer convex programming solvers capable of synthesizing dynamic contact sequences. Nevertheless, its exponential-time complexity limits its applicability to short time horizon contact sequences within small environments. In this paper, we go beyond current approaches by learning a prediction of the dynamic evolution of the robot centroidal momenta, which can then be used for quickly generating dynamically robust contact sequences for robots with arms and legs using a search-based contact planner. We demonstrate the efficiency and quality of the results of the proposed approach in a set of dynamically challenging scenarios.\"","summary":"\"Footstep planning for humanoid robot has been studied extensively @cite_15 @cite_35 @cite_12 @cite_30 @cite_24 @cite_7 @cite_21 . In these works, the planner plans a footstep sequence to avoid obstacles on the ground and remain inside the specified contact regions on a flat or piecewise-flat ground. To increase the likelihood of success, they incorporate an approximation of robot balance and kinematic reachability into the contact transition model, and do not explicitly perform balance check online. There are also works addressing contact planning in unstructured environment using both palm and foot contacts @cite_22 @cite_18 @cite_1 @cite_26 @cite_0 . However, these approaches assumes quasi-static motions, and drops solutions involving dynamic motions.\"","":""}
{"id":"2899192577","dialogue":"\"Humanoid robots dynamically navigate an environment by interacting with it via contact wrenches exerted at intermittent contact poses. Therefore, it is important to consider dynamics when planning a contact sequence. Traditional contact planning approaches assume a quasi-static balance criterion to reduce the computational challenges of selecting a contact sequence over a rough terrain. This however limits the applicability of the approach when dynamic motions are required, such as when walking down a steep slope or crossing a wide gap. Recent methods overcome this limitation with the help of efficient mixed integer convex programming solvers capable of synthesizing dynamic contact sequences. Nevertheless, its exponential-time complexity limits its applicability to short time horizon contact sequences within small environments. In this paper, we go beyond current approaches by learning a prediction of the dynamic evolution of the robot centroidal momenta, which can then be used for quickly generating dynamically robust contact sequences for robots with arms and legs using a search-based contact planner. We demonstrate the efficiency and quality of the results of the proposed approach in a set of dynamically challenging scenarios.\"","summary":"\"Approaches to synthesize dynamically feasible multi-contact motions have also been extensively studied @cite_8 @cite_14 @cite_31 @cite_40 @cite_36 . However, it is not trivial to include planning of contact poses in these approaches because contacts planning in general involves discrete or non-convex constraints for the contact poses. @cite_21 addresses the non-convexity by decomposing the environment into a set of convex regions and approximating the rotation using piecewise affine functions. The problem is then formulated as a mixed integer convex program and solved to global optimality. Although @cite_21 only uses foot contact, and does not consider dynamics, it points a direction to include contact planning in an optimization problem.\"","":""}
{"id":"2899192577","dialogue":"\"Humanoid robots dynamically navigate an environment by interacting with it via contact wrenches exerted at intermittent contact poses. Therefore, it is important to consider dynamics when planning a contact sequence. Traditional contact planning approaches assume a quasi-static balance criterion to reduce the computational challenges of selecting a contact sequence over a rough terrain. This however limits the applicability of the approach when dynamic motions are required, such as when walking down a steep slope or crossing a wide gap. Recent methods overcome this limitation with the help of efficient mixed integer convex programming solvers capable of synthesizing dynamic contact sequences. Nevertheless, its exponential-time complexity limits its applicability to short time horizon contact sequences within small environments. In this paper, we go beyond current approaches by learning a prediction of the dynamic evolution of the robot centroidal momenta, which can then be used for quickly generating dynamically robust contact sequences for robots with arms and legs using a search-based contact planner. We demonstrate the efficiency and quality of the results of the proposed approach in a set of dynamically challenging scenarios.\"","summary":"\"Extensions of @cite_21 for dynamic planning of a contact sequences are proposed in @cite_17 @cite_16 , which extend @cite_21 with the selection of contact timings or hand contacts respectively. More recent works @cite_3 @cite_9 use the same concept to plan gait sequences for quadruped robots and produce dynamically robust motions. However, mixed-integer approaches scale poorly against the number of integer decision variables. For instance, their applicability is limited to online contact generation in environments with few convex terrain regions, and short planning horizons.\"","":""}
{"id":"2899192577","dialogue":"\"Humanoid robots dynamically navigate an environment by interacting with it via contact wrenches exerted at intermittent contact poses. Therefore, it is important to consider dynamics when planning a contact sequence. Traditional contact planning approaches assume a quasi-static balance criterion to reduce the computational challenges of selecting a contact sequence over a rough terrain. This however limits the applicability of the approach when dynamic motions are required, such as when walking down a steep slope or crossing a wide gap. Recent methods overcome this limitation with the help of efficient mixed integer convex programming solvers capable of synthesizing dynamic contact sequences. Nevertheless, its exponential-time complexity limits its applicability to short time horizon contact sequences within small environments. In this paper, we go beyond current approaches by learning a prediction of the dynamic evolution of the robot centroidal momenta, which can then be used for quickly generating dynamically robust contact sequences for robots with arms and legs using a search-based contact planner. We demonstrate the efficiency and quality of the results of the proposed approach in a set of dynamically challenging scenarios.\"","summary":"\"@cite_32 proposes a kinodynamic sampling-based contact planner to plan kinodynamically feasible contact sequences. They use a simplified robot model to dynamically plan smooth center of mass (CoM) trajectories based on convex optimization and then search for kinematically feasible contact poses around it. It shows a unified planning framework to consider dynamics and kinematics constraints, but it suffers from long planning time. @cite_13 proposes an efficient dynamic feasibility check by conservatively reformulating the problem as a linear program. While the check guarantees to reject dynamically infeasible motions, they do not address dynamical robustness in the stability check. @cite_2 learns quadratic dynamics objective of humanoid walking motion, and apply this learned model to select steps in a search-based footstep planner. However, their dynamics model assumes flat contact, and does not consider palm contacts, which limits the applicability of the approach.\"","":""}
{"id":"2952634764","dialogue":"\"Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves @math accuracy, which is more than @math improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from @math to @math .\"","summary":"\"The developments of these noise injection techniques specific to the architectures are not unique to convolutional networks. In fact, similar to convolutional networks, recurrent networks require their own noise injection methods. Currently, Variational Dropout @cite_34 and ZoneOut @cite_23 are two of the most commonly used methods to inject noise to recurrent connections.\"","":""}
{"id":"2952634764","dialogue":"\"Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves @math accuracy, which is more than @math improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from @math to @math .\"","summary":"\"Our method is inspired by Cutout @cite_24 , a data augmentation method where parts of the input examples are zeroed out. DropBlock generalizes Cutout by applying Cutout at every feature map in a convolutional networks. In our experiments, having a fixed zero-out ratio for DropBlock during training is not as robust as having an increasing schedule for the ratio during training. In other words, it's better to set the DropBlock ratio to be small initially during training, and linearly increase it over time during training. This scheduling scheme is related to ScheduledDropPath @cite_6 .\"","":""}
{"id":"2899310320","dialogue":"\"Current state-of-the-art approaches to video understanding adopt temporal jittering to simulate analyzing the video at varying frame rates. However, this does not work well for multirate videos, in which actions or subactions occur at different speeds. The frame sampling rate should vary in accordance with the different motion speeds. In this work, we propose a simple yet effective strategy, termed random temporal skipping, to address this situation. This strategy effectively handles multirate videos by randomizing the sampling rate during training. It is an exhaustive approach, which can potentially cover all motion speed variations. Furthermore, due to the large temporal skipping, our network can see video clips that originally cover over 100 frames. Such a time range is enough to analyze most actions events. We also introduce an occlusion-aware optical flow learning method that generates improved motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely adopted video benchmarks.\"","summary":"\"However, compared to the CNN, the optical flow calculation is computationally expensive. It is thus the major speed bottleneck of the current two-stream approaches. There have been recent attempts to better model the temporal information. @cite_0 pre-trained a deep 3D CNN network on a large-scale dataset, and use it as a general spatiotemporal feature extractor. The features generalize well to several tasks but are inferior to two-stream approaches. @cite_23 reduced the dimension of each frame clip using a CNN and aggregated frame-level information using Long Short Term Memory (LSTM) networks. @cite_10 proposed to reduce the size of each frame and use longer clips (e.g., 60 vs 16 frames) as inputs. They managed to gain significant accuracy improvements compared to shorter clips with the same spatial size. @cite_18 experimented with sparse sampling and jointly trained on the sparsely sampled frames clips. In this way, they incorporate more temporal information while preserving the spatial resolution. Recent approaches @cite_4 @cite_26 have evolved to end-to-end learning and are currently the best at incorporating global temporal information. However, none of them handle multirate video analysis effectively.\"","":""}
{"id":"2899310320","dialogue":"\"Current state-of-the-art approaches to video understanding adopt temporal jittering to simulate analyzing the video at varying frame rates. However, this does not work well for multirate videos, in which actions or subactions occur at different speeds. The frame sampling rate should vary in accordance with the different motion speeds. In this work, we propose a simple yet effective strategy, termed random temporal skipping, to address this situation. This strategy effectively handles multirate videos by randomizing the sampling rate during training. It is an exhaustive approach, which can potentially cover all motion speed variations. Furthermore, due to the large temporal skipping, our network can see video clips that originally cover over 100 frames. Such a time range is enough to analyze most actions events. We also introduce an occlusion-aware optical flow learning method that generates improved motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely adopted video benchmarks.\"","summary":"\"To handle multirate videos, there are two widely adopted approaches. One is to train multiple models, each of them corresponding to a different fixed frame-rate. This is similar to using image pyramids to handle the multi-scale problem in image analysis. The other is to generate sliding windows of different lengths for each video (a.k.a, temporal jittering), with the hope of capturing temporal invariance. However, neither of these approaches is exhaustive, and they are both computationally intensive. @cite_27 is the most similar work to ours since they deal with motion speed variance. However, our work differs in several aspects. First, we aim to explicitly learn the transitions between frames while @cite_27 uses past and future neighboring video clips as the temporal context, and reconstruct the two temporal transitions. Their objective is considerably harder to optimize, which may lead to sub-optimal solutions. Second, our random skipping strategy is easy to implement without computational overhead whereas the image reconstruction of @cite_27 will lead to significant computational burden. Third, their proposed multirate gated recurrent unit only works in RNNs, while our strategy is generally applicable.\"","":""}
{"id":"2964301261","dialogue":"\"As automatic optimization techniques find their way into industrial applications, the behavior of many complex systems is determined by some form of planner picking the right actions to optimize a given objective function. In many cases, the mapping of plans to objective reward may change due to unforeseen events or circumstances in the real world. In those cases, the planner usually needs some additional effort to adjust to the changed situation and reach its previous level of performance. Whenever we still need to continue polling the planner even during re-planning, it oftentimes exhibits severely lacking performance. In order to improve the planner's resilience to unforeseen change, we argue that maintaining a certain level of diversity amongst the considered plans at all times should be added to the planner's objective. Effectively, we encourage the planner to keep alternative plans to its currently best solution. As an example case, we implement a diversity-aware genetic algorithm using two different metrics for diversity (differing in their generality) and show that the blow in performance due to unexpected change can be severely lessened in the average case. We also analyze the parameter settings necessary for these techniques in order to gain an intuition how they can be incorporated into larger frameworks or process models for software and systems engineering.\"","summary":"\"The author of @cite_27 describes a problem setting not unlike the one presented in this paper, i.e., the combination of maintaining diversity and searching in a changing environment. The issue of premature convergence is tackled by integrating a certain amount of random search into the genetic algorithm by performing hyper-mutation. This has since become standard procedure and is included in all genetic algorithms presented in this paper, which aims to further improve the resilience of the search process.\"","":""}
{"id":"2964301261","dialogue":"\"As automatic optimization techniques find their way into industrial applications, the behavior of many complex systems is determined by some form of planner picking the right actions to optimize a given objective function. In many cases, the mapping of plans to objective reward may change due to unforeseen events or circumstances in the real world. In those cases, the planner usually needs some additional effort to adjust to the changed situation and reach its previous level of performance. Whenever we still need to continue polling the planner even during re-planning, it oftentimes exhibits severely lacking performance. In order to improve the planner's resilience to unforeseen change, we argue that maintaining a certain level of diversity amongst the considered plans at all times should be added to the planner's objective. Effectively, we encourage the planner to keep alternative plans to its currently best solution. As an example case, we implement a diversity-aware genetic algorithm using two different metrics for diversity (differing in their generality) and show that the blow in performance due to unexpected change can be severely lessened in the average case. We also analyze the parameter settings necessary for these techniques in order to gain an intuition how they can be incorporated into larger frameworks or process models for software and systems engineering.\"","summary":"\"The preparation for unexpected or previously wrongly modeled change is an important issue for the practical application of machine learning in industry @cite_22 . From an engineer's point of view, the diversity of the population of plans can be regarded as a typical with the cost of the plan representing the functional requirement. Applying NFR engineering processes to self-adaptive systems is still a new idea and a clear canon of relevant NFRs for these new challenges has not yet been found @cite_29 @cite_43 .\"","":""}
{"id":"2913622889","dialogue":"\"In cloud computing, data processing is delegated to a remote party for efficiency and flexibility reasons. A practical user requirement usually is that the confidentiality and integrity of data processing needs to be protected. In the common scenarios of cloud computing today, this can only be achieved by assuming that the remote party does not in any form act maliciously. In this paper, we propose an approach that avoids having to trust a single entity. Our approach is based on two concepts: (1) the technical abstraction of sealed computation, i.e., a technical mechanism to confine the processing of data within a tamper-proof hardware container, and (2) the additional role of an auditing party that itself cannot add functionality to the system but is able to check whether the system (including the mechanism for sealed computation) works as expected. We discuss the abstract technical and procedural requirements of these concepts and explain how they can be applied in practice.\"","summary":"\"A trustworthy and privacy-preserving cloud may be addressed by the use of cryptographic techniques such as fully homomorphic encryption (FHE) @cite_9 . However, it is still inefficient for most computations @cite_16 . Similarly in verifiable computing @cite_8 , it was designed to enable result correctness verification but has not shown support for general purpose cloud computing yet.\"","":""}
{"id":"2950623005","dialogue":"\"The directed Physarum dynamics is known to solve positive linear programs: minimize @math subject to @math and @math for a positive cost vector @math . The directed Physarum dynamics evolves a positive vector @math according to the dynamics @math . Here @math is the solution to @math that minimizes the \"\"energy\"\" @math . In this paper","summary":"we study the non-uniform directed dynamics @math","":""}
{"id":"2950623005","dialogue":"\"The directed Physarum dynamics is known to solve positive linear programs: minimize @math subject to @math and @math for a positive cost vector @math . The directed Physarum dynamics evolves a positive vector @math according to the dynamics @math . Here @math is the solution to @math that minimizes the \"\"energy\"\" @math . In this paper","summary":"we study the non-uniform directed dynamics @math","":""}
{"id":"2950623005","dialogue":"\"The directed Physarum dynamics is known to solve positive linear programs: minimize @math subject to @math and @math for a positive cost vector @math . The directed Physarum dynamics evolves a positive vector @math according to the dynamics @math . Here @math is the solution to @math that minimizes the \"\"energy\"\" @math . In this paper","summary":"we study the non-uniform directed dynamics @math","":""}
{"id":"2949728259","dialogue":"\"Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 bansal2008wsc and WSC-09 kona2009wsc . Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.\"","summary":"\"Automated web service composition aims to loosely couple web services to fulfill a service request, without strictly obeying a pre-given abstract workflow. Instead, composition workflows are gradually built up while its component services are selected. Existing works in fully automated web service composition can be categorized into two approaches --- direct approaches and indirect approaches @cite_37 . The direct approaches represent composition solutions explicitly in the representation that displays actual execution flows of composite services, while the indirect approaches often represent composite services implicitly as permutations, which require a decoding process to build up actual execution workflows.\"","":""}
{"id":"2949728259","dialogue":"\"Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 bansal2008wsc and WSC-09 kona2009wsc . Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.\"","summary":"\"In the second category, service composition solutions are represented as permutations, which are then decoded into solutions represented as DAGs @cite_20 @cite_37 @cite_24 . PSO is utilized to find an optimized queue of services (i.e., a permutation), which can be decoded into a corresponding DAG-based composite service @cite_24 . @cite_20 extends @cite_24 to jointly optimize QoSM and QoS, where a weighted DAG is decoded, where edge weights correspond to matchmaking quality between services. These two PSO-based approaches rely on PSO to determine the weights of particle's position (that corresponding with a service) to form an ordered service queue. Optimizing QoSM and QoS simultaneously is more challenging than optimizing QoS only because the searching space has significantly increased, and it demands more effective and efficient searching techniques. Apart from that, it has been suggested that utilizing the indirect representation often contributes to a higher performance, compared to direct representation @cite_37 . It is due to that the search space is not unwittingly restricted by unconstrained random initialization of solutions and operators.\"","":""}
{"id":"2949728259","dialogue":"\"Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 bansal2008wsc and WSC-09 kona2009wsc . Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.\"","summary":"\"Memetic algorithms have drawn growing attention from researchers in recent years and achieved significant successes in many applications @cite_35 . By introducing local search, the performance of EC techniques can be improved. In the domain of service composition, to overcome the prematurity and proneness of GP, Tabu search is combined with GP to solve QoS-aware data-intensive web service composition @cite_22 . @cite_7 proposed an indirect memetic approach for QoS-aware web service composition, where a domain-dependent crossover operator is proposed to produce candidate solutions. Besides that, an exhaustive local search is applied to composite solutions represented as permutations. However, the produced neighbors are likely to be decoded into the same composite solution. Therefore, the effectiveness of this local search operator demands further improvement.\"","":""}
{"id":"2949728259","dialogue":"\"Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 bansal2008wsc and WSC-09 kona2009wsc . Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.\"","summary":"\"Recently, EDA has been used as a technique to tackle permutation-based optimization problems @cite_9 . In particular, a distribution model is learned iteratively for each population. Subsequently, new offsprings are generated based on the learned model. Moreover, domain-dependent local search operators are often introduced to enhance the performances of EDA. For example, a probability matrix that is related to the job priority permutation of a solution is learned in EDA-based flow-shop scheduling problem, and different job-based local search operators were proposed to enhance the exploitation ability of EDA @cite_4 . An Edge Histogram Matrix is applied to uncertain capacitated arc routing problems and is leaned from solutions represented by a set of routes @cite_30 . To make local improvements, different move operators, such as single insertion and swap, are also proposed.\"","":""}
{"id":"2949728259","dialogue":"\"Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 bansal2008wsc and WSC-09 kona2009wsc . Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.\"","summary":"\"The use of EDA has only been investigated for semi-automated web service composition @cite_28 @cite_34 @cite_6 . However, we recently proposed an EDA-based approach for fully automated web service composition, where candidate solutions are represented as permutations over a given service repository. The success of the proposed method strongly depends on the distribution model and the way of learning the distribution model. We employ Node Histogram Matrix (NHM) to learn the distribution of promising solutions in one population, Node Histogram-Based Sampling Algorithm (NHBSA) @cite_12 is empoloyed to produce candidate solutions. Although we started an initial study for fully automated service composition, it remains an opportunity to improve its performance further. EDA is good at global exploration, and local search operators are motivated to be introduced in EDA to enhance its capability in exploitation.\"","":""}
{"id":"2950784788","dialogue":"\"Question answering (QA) has significantly benefitted from deep learning techniques in recent years. However, domain-specific QA remains a challenge due to the significant amount of data required to train a neural network. This paper studies the answer sentence selection task in the Bible domain and answer questions by selecting relevant verses from the Bible. For this purpose, we create a new dataset BibleQA based on bible trivia questions and propose three neural network models for our task. We pre-train our models on a large-scale QA dataset, SQuAD, and investigate the effect of transferring weights on model accuracy. Furthermore, we also measure the model accuracies with different answer context lengths and different Bible translations. We affirm that transfer learning has a noticeable improvement in the model accuracy. We achieve relatively good results with shorter context lengths, whereas longer context lengths decreased model accuracy. We also find that using a more modern Bible translation in the dataset has a positive effect on the task.\"","summary":"\"From a text retrieval perspective, question answering embodies the task of finding the relevant piece of text containing the answer and subsequently extracting the answer @cite_20 . This view led to open-domain QA, which encompasses the majority of today's QA systems. In recent years, QA began incorporating machine learning, with the IBM Watson being one of the most famous systems @cite_16 . The primary approach behind Watson is extensive data, statistical and machine learning analysis. Several other neural network approaches have also been explored. used neural networks to answer quiz bowl type questions, where given a description the task is to identify the subject being discussed @cite_1 . extended simple RNN models with an attention mechanism to enable transitive reasoning and made steps towards reasoning-based QA @cite_30 . Malinowski proposed a model using both CNN and LSTM to incorporate image recognition and QA @cite_5 .\"","":""}
{"id":"2950784788","dialogue":"\"Question answering (QA) has significantly benefitted from deep learning techniques in recent years. However, domain-specific QA remains a challenge due to the significant amount of data required to train a neural network. This paper studies the answer sentence selection task in the Bible domain and answer questions by selecting relevant verses from the Bible. For this purpose, we create a new dataset BibleQA based on bible trivia questions and propose three neural network models for our task. We pre-train our models on a large-scale QA dataset, SQuAD, and investigate the effect of transferring weights on model accuracy. Furthermore, we also measure the model accuracies with different answer context lengths and different Bible translations. We affirm that transfer learning has a noticeable improvement in the model accuracy. We achieve relatively good results with shorter context lengths, whereas longer context lengths decreased model accuracy. We also find that using a more modern Bible translation in the dataset has a positive effect on the task.\"","summary":"\"Answer sentence selection is a QA task which involves selecting the sentence that is the most likely to contain the answer. Early approaches were predominantly syntactical, using the idea that the question and answer sentence should relate to each other loosely through syntactical transformations. proposed a generative model that transforms the answers to the questions @cite_0 . Wang and Manning introduced a probabilistic model that models tree-edit operations on dependency parse trees, making use of sophisticated linguistics features @cite_28 . Other similar models include using dynamic programming to find the optimal tree edit sequences @cite_27 . The main drawback of these approaches is that they require too much feature engineering and were difficult to adapt to new domains. Only recently, researchers started applying neural network models. used CNN models for answer sentence selection on the TREC benchmark @cite_33 . also proposed several CNN models for answer sentence selection task. Wang and Nyberg constructed a joint-vector based on both the question and the answer using an LSTM model @cite_18 .\"","":""}
{"id":"2898490764","dialogue":"\"Time series clustering is the process of grouping time series with respect to their similarity or characteristics. Previous approaches usually combine a specific distance measure for time series and a standard clustering method. However, these approaches do not take the similarity of the different subsequences of each time series into account, which can be used to better compare the time series objects of the dataset. In this paper, we propose a novel technique of time series clustering based on two clustering stages. In a first step, a least squares polynomial segmentation procedure is applied to each time series, which is based on a growing window technique that returns different-length segments. Then, all the segments are projected into same dimensional space, based on the coefficients of the model that approximates the segment and a set of statistical features. After mapping, a first hierarchical clustering phase is applied to all mapped segments, returning groups of segments for each time series. These clusters are used to represent all time series in the same dimensional space, after defining another specific mapping process. In a second and final clustering stage, all the time series objects are grouped. We consider internal clustering quality to automatically adjust the main parameter of the algorithm, which is an error threshold for the segmenta- tion. The results obtained on 84 datasets from the UCR Time Series Classification Archive have been compared against two state-of-the-art methods, showing that the performance of this methodology is very promising.\"","summary":"\"Many of the proposals for time series clustering are based on the combination of a distance measure and a clustering algorithm. First, we will analyse the most important distance measures proposed for time series comparison, and then we will introduce the clustering methods that can be applied based on them Further information about time series clustering can be found in @cite_37 or @cite_56 .\"","":""}
{"id":"2898290428","dialogue":"\"Speech synthesis is widely used in many practical applications. In recent years, speech synthesis technology has developed rapidly. However, one of the reasons why synthetic speech is unnatural is that it often has over-smoothness. In order to improve the naturalness of synthetic speech, we first extract the mel-spectrogram of speech and convert it into a real image, then take the over-smooth mel-spectrogram image as input, and use image-to-image translation Generative Adversarial Networks(GANs) framework to generate a more realistic mel-spectrogram. Finally, the results show that this method greatly reduces the over-smoothness of synthesized speech and is more close to the mel-spectrogram of real speech.\"","summary":"\"So far, GAN has been widely used in the field of computer vision and image processing, and has achieved many amazing results. @cite_4 @cite_6 @cite_7 @cite_8 . Pix2pixHD @cite_9 is the state-of-the-art for image-to-image translation.\"","":""}
{"id":"2898446106","dialogue":"\"An image related question defines a specific visual task that is required in order to produce an appropriate answer. The answer may depend on a minor detail in the image and require complex reasoning and use of prior knowledge. When humans perform this task, they are able to do it in a flexible and robust manner, integrating modularly any novel visual capability with diverse options for various elaborations of the task. In contrast, current approaches to solve this problem by a machine are based on casting the problem as an end-to-end learning problem, which lacks such abilities. We present a different approach, inspired by the aforementioned human capabilities. The approach is based on the compositional structure of the question. The underlying idea is that a question has an abstract representation based on its structure, which is compositional in nature. The question can consequently be answered by a composition of procedures corresponding to its substructures. The basic elements of the representation are logical patterns, which are put together to represent the question. These patterns include a parametric representation for object classes, properties and relations. Each basic pattern is mapped into a basic procedure that includes meaningful visual tasks, and the patterns are composed to produce the overall answering procedure. The UnCoRd (Understand Compose and Respond) system, based on this approach, integrates existing detection and classification schemes for a set of object classes, properties and relations. These schemes are incorporated in a modular manner, providing elaborated answers and corrections for negative answers. In addition, an external knowledge base is queried for required common-knowledge. We performed a qualitative analysis of the system, which demonstrates its representation capabilities and provide suggestions for future developments.\"","summary":"\"Visual Question answering has developed dramatically in the last few years @cite_31 @cite_27 @cite_35 . Practically all current works are based on casting the problem into a multi class classification problem, where image features, retrieved by a Convolutional Neural Network, are fused with question features (mostly extracted by Recurrent Neural Network) and used to predict one of the common training set answers, mostly short and succinct answers. These methods have the advantage of not requiring to incorporate a complicated parsing and understanding process and may present decent results when trained and tested on current existing datasets, yet they lack some important human characteristics like using a compositional process, utilizing existing and meaningful sub processes. Using meaningful sub processes allows humans to focus on different aspects and scopes according to the specific task, utilize existing abilities and modularly integrate novel ones, understand limitation and provide elaborations including suggestions of alternatives.\"","":""}
{"id":"2898446106","dialogue":"\"An image related question defines a specific visual task that is required in order to produce an appropriate answer. The answer may depend on a minor detail in the image and require complex reasoning and use of prior knowledge. When humans perform this task, they are able to do it in a flexible and robust manner, integrating modularly any novel visual capability with diverse options for various elaborations of the task. In contrast, current approaches to solve this problem by a machine are based on casting the problem as an end-to-end learning problem, which lacks such abilities. We present a different approach, inspired by the aforementioned human capabilities. The approach is based on the compositional structure of the question. The underlying idea is that a question has an abstract representation based on its structure, which is compositional in nature. The question can consequently be answered by a composition of procedures corresponding to its substructures. The basic elements of the representation are logical patterns, which are put together to represent the question. These patterns include a parametric representation for object classes, properties and relations. Each basic pattern is mapped into a basic procedure that includes meaningful visual tasks, and the patterns are composed to produce the overall answering procedure. The UnCoRd (Understand Compose and Respond) system, based on this approach, integrates existing detection and classification schemes for a set of object classes, properties and relations. These schemes are incorporated in a modular manner, providing elaborated answers and corrections for negative answers. In addition, an external knowledge base is queried for required common-knowledge. We performed a qualitative analysis of the system, which demonstrates its representation capabilities and provide suggestions for future developments.\"","summary":"Incorporating the question information is largely addressed by seeking mechanisms for image-language features fusion. A large focus in this line of works was in simplifying bilinear pulling (which is based on the outer product of the two feature vectors) by reducing dimensionality of the features @cite_29 or a low rank factorization @cite_28 @cite_0 .","":""}
{"id":"2898446106","dialogue":"\"An image related question defines a specific visual task that is required in order to produce an appropriate answer. The answer may depend on a minor detail in the image and require complex reasoning and use of prior knowledge. When humans perform this task, they are able to do it in a flexible and robust manner, integrating modularly any novel visual capability with diverse options for various elaborations of the task. In contrast, current approaches to solve this problem by a machine are based on casting the problem as an end-to-end learning problem, which lacks such abilities. We present a different approach, inspired by the aforementioned human capabilities. The approach is based on the compositional structure of the question. The underlying idea is that a question has an abstract representation based on its structure, which is compositional in nature. The question can consequently be answered by a composition of procedures corresponding to its substructures. The basic elements of the representation are logical patterns, which are put together to represent the question. These patterns include a parametric representation for object classes, properties and relations. Each basic pattern is mapped into a basic procedure that includes meaningful visual tasks, and the patterns are composed to produce the overall answering procedure. The UnCoRd (Understand Compose and Respond) system, based on this approach, integrates existing detection and classification schemes for a set of object classes, properties and relations. These schemes are incorporated in a modular manner, providing elaborated answers and corrections for negative answers. In addition, an external knowledge base is queried for required common-knowledge. We performed a qualitative analysis of the system, which demonstrates its representation capabilities and provide suggestions for future developments.\"","summary":"\"In order to extract image information that is more informative to the question and avoid the noise of irrelevant image areas, many works incorporated attention mechanisms. During the attention stage image areas, that are considered more relevant, are multiplied by higher weights and contribute more to answering the question. Attention may be stacked for multiple stages @cite_59 with the motivation of refining it for complicated questions. Extracting relevant areas was also performed by integrating regions of detected objects related to question words @cite_15 . The attention concept was also extended to include both image features and the question representation @cite_48 , where both attention types effect each other. Additional attention mechanisms utilize CRF @cite_20 , consider all word-region interactions @cite_4 , incorporate correlations between image, question and candidate answer @cite_16 and combine grid based and object detection based regions @cite_54 @cite_51 .\"","":""}
{"id":"2898446106","dialogue":"\"An image related question defines a specific visual task that is required in order to produce an appropriate answer. The answer may depend on a minor detail in the image and require complex reasoning and use of prior knowledge. When humans perform this task, they are able to do it in a flexible and robust manner, integrating modularly any novel visual capability with diverse options for various elaborations of the task. In contrast, current approaches to solve this problem by a machine are based on casting the problem as an end-to-end learning problem, which lacks such abilities. We present a different approach, inspired by the aforementioned human capabilities. The approach is based on the compositional structure of the question. The underlying idea is that a question has an abstract representation based on its structure, which is compositional in nature. The question can consequently be answered by a composition of procedures corresponding to its substructures. The basic elements of the representation are logical patterns, which are put together to represent the question. These patterns include a parametric representation for object classes, properties and relations. Each basic pattern is mapped into a basic procedure that includes meaningful visual tasks, and the patterns are composed to produce the overall answering procedure. The UnCoRd (Understand Compose and Respond) system, based on this approach, integrates existing detection and classification schemes for a set of object classes, properties and relations. These schemes are incorporated in a modular manner, providing elaborated answers and corrections for negative answers. In addition, an external knowledge base is queried for required common-knowledge. We performed a qualitative analysis of the system, which demonstrates its representation capabilities and provide suggestions for future developments.\"","summary":"Combining results of meaningful tasks (other than using pre-trained networks as visual features) such as object detection was in the focus of several additional works. One such work uses object and attribute recognition tasks for proposed regions and combines them with corresponding representations from question and candidate answer @cite_37 . The use of visual concepts (object class and attributes) of attended regions and comparing them to extracted concepts from the question was proposed as well @cite_21 . In another work concatenating pairs of vectors representing two detected objects and their properties with the encoded question was used to allow relation reasoning @cite_1 . Objects and relations between them was utilized in a work that used graph representation for both the image (synthetic images) and the question @cite_49 . For the image graph objects were the nodes and edges were the spatial relations between them and for the question graph words were the nodes and their dependencies were the edges. Representations were merged in an attention-like mechanism to fuse the features and predict the answer.","":""}
{"id":"2898389697","dialogue":"\"We introduce efficient algorithms which achieve nearly optimal regrets for the problem of stochastic online shortest path routing with end-to-end feedback. The setting is a natural application of the combinatorial stochastic bandits problem, a special case of the linear stochastic bandits problem. We show how the difficulties posed by the large scale action set can be overcome by the networked structure of the action set. Our approach presents a novel connection between bandit learning and shortest path algorithms. Our main contribution is an adaptive exploration algorithm with nearly optimal instance-dependent regret for any directed acyclic network. We then modify it so that nearly optimal worst case regret is achieved simultaneously. Driven by the carefully designed Top-Two Comparison (TTC) technique, the algorithms are efficiently implementable. We further conduct extensive numerical experiments to show that our proposed algorithms not only achieve superior regret performances, but also reduce the runtime drastically.\"","summary":"\"Stochastic multi-armed bandits is a prevalent framework for sequential decision-making. Early work on stochastic MAB problems @cite_30 @cite_17 @cite_6 tended to be more focused on asymptotic guarantees, whereas more recent work @cite_33 @cite_7 has been directed towards a non-asymptotic analysis in which regret can be bounded over a fixed time horizons @math . Two of the best-known and well-studied techniques are known as the UCB algorithm that follows the OFU principle @cite_33 and the explore then exploit algorithm @cite_15 @cite_2 . Recently, the Bayesian setting accompanied by the (TS) technique has also been thoroughly analyzed due to the ease of implementation and favorable empirical results @cite_11 .\"","":""}
{"id":"2898389697","dialogue":"\"We introduce efficient algorithms which achieve nearly optimal regrets for the problem of stochastic online shortest path routing with end-to-end feedback. The setting is a natural application of the combinatorial stochastic bandits problem, a special case of the linear stochastic bandits problem. We show how the difficulties posed by the large scale action set can be overcome by the networked structure of the action set. Our approach presents a novel connection between bandit learning and shortest path algorithms. Our main contribution is an adaptive exploration algorithm with nearly optimal instance-dependent regret for any directed acyclic network. We then modify it so that nearly optimal worst case regret is achieved simultaneously. Driven by the carefully designed Top-Two Comparison (TTC) technique, the algorithms are efficiently implementable. We further conduct extensive numerical experiments to show that our proposed algorithms not only achieve superior regret performances, but also reduce the runtime drastically.\"","summary":"\"A special case of linear bandits is combinatorial bandits where the action set is constrained to subset of @math In combinatorial stochastic bandits, it is often assumed that the reward loss vector is observed at all the coordinates sampled by the action taken. This is the so-called semi-bandit feedback setting @cite_32 . The authors of @cite_9 initiated the study of combinatorial stochastic bandits under semi-bandit feedback and a network-structured action set; while @cite_14 studied the general action set case. The authors of @cite_21 further characterized tight upper and lower bounds for this problem. Assuming the noise is independent across different coordinates, the authors of @cite_1 improved upon the results obtained in @cite_21 . For the bandit feedback case, the authors of @cite_8 gives algorithms that require brute-force search over the action space with instance dependent regret @math . For adversarial combinatorial bandits, the authors of @cite_24 presented the efficient and optimal algorithm for the semi-bandit feedback case while the authors of @cite_10 described an optimal algorithm for the bandit feedback case, but its computational complexity scales linearly with the number of actions.\"","":""}
{"id":"2896243688","dialogue":"\"Previous studies show that incorporating external information could improve the translation quality of Neural Machine Translation (NMT) systems. However, there are inevitably noises in the external information, severely reducing the benefit that the existing methods could receive from the incorporation. To tackle the problem, this study pays special attention to the discrimination of the noises during the incorporation. We argue that there exist two kinds of noise in this external information, i.e. global noise and local noise, which affect the translations for the whole sentence and for some specific words, respectively. Accordingly, we propose a general framework that learns to jointly discriminate both the global and local noises, so that the external information could be better leveraged. Our model is trained on the dataset derived from the original parallel corpus without any external labeled data or annotation. Experimental results in various real-world scenarios, language pairs, and neural architectures indicate that discriminating noises contributes to significant improvements in translation quality by being able to better incorporate the external information, even in very noisy conditions.\"","summary":"\"Besides, most of the previous methods require the presence of specific resources for training, e.g. translation of the parallel data generated by existing MT system(s) @cite_5 @cite_9 . propose approaches to use an SMT model to provide word and phrase recommendations for an attention-based NMT, where the two systems are deeply coupled. propose to train context-aware translation models by the aids of large document discourse-level data. In contrast, our training procedure is more general and simpler, which only uses word sampling from the original parallel data and requires no external resources. To leverage outside information, such as words, for a generation task, propose to use lexical constraints on decoding process to utilize correct external word translations. propose to use copying mechanism in the single-turn dialogue task, inspiring us for the basic framework. Compared to their attempts, our approach provides more robust solutions to discriminate noises.\"","":""}
{"id":"2922512839","dialogue":"\"Aspect-term sentiment analysis (ATSA) is a longstanding challenge in natural language understanding. It requires fine-grained semantical reasoning about a target entity appeared in the text. As manual annotation over the aspects is laborious and time-consuming, the amount of labeled data is limited for supervised learning. This paper proposes a semi-supervised method for the ATSA problem by using the Variational Autoencoder based on Transformer (VAET), which models the latent distribution via variational inference. By disentangling the latent representation into the aspect-specific sentiment and the lexical context, our method induces the underlying sentiment prediction for the unlabeled data, which then benefits the ATSA classifier. Our method is classifier agnostic, i.e., the classifier is an independent module and various advanced supervised models can be integrated. Experimental results are obtained on the SemEval 2014 task 4 and show that our method is effective with four classical classifiers. The proposed method outperforms two general semisupervised methods and achieves state-of-the-art performance.\"","summary":"\"Another related topic is semi-supervised learning for the text classification. A simple but efficient method is to use pre-trained modules, e.g., initializing the word embedding or bottom layers with pre-trained parameters. Although word embedding technique has been wildly used in NLP models, e.g., Glove @cite_9 and ELMo @cite_7 , other pretraining-based method is model-dependent. The ELMo @cite_7 replaces the embedding layer with the pre-trained BILSTM to capture the contextual representation. This method is complementary with the proposed method. The combination with our method may yield better performance than either of them alone, but that investigation is beyond the scope of this paper. However, other methods, e.g., the Transformer LM @cite_16 , proposed a unified semi-supervised framework to handle various tasks. This constraint prevents advanced supervised models from the semi-supervised learning.\"","":""}
{"id":"2896757367","dialogue":"\"Cloud platforms offer different types of virtual machines which ensure different guarantees in terms of availability and volatility, provisioning the same resource through multiple pricing models. For instance, in Amazon EC2 cloud, the user pays per hour for on-demand instances while spot instances are unused resources available for a lower price. Despite the monetary advantages, a spot instance can be terminated or hibernated by EC2 at any moment. Using both hibernation-prone spot instances (for cost sake) and on-demand instances, we propose in this paper a static scheduling for applications which are composed of independent tasks (bag-of-task) with deadline constraints. However, if a spot instance hibernates and it does not resume within a time which guarantees the application's deadline, a temporal failure takes place. Our scheduling, thus, aims at minimizing monetary costs of bag-of-tasks applications in EC2 cloud, respecting its deadline and avoiding temporal failures. Performance results with task execution traces, configuration of Amazon EC2 virtual machines, and EC2 market history confirms the effectiveness of our scheduling and that it tolerates temporal failures.\"","summary":"\"Bag-of-tasks on clouds are widely used not only for scientific applications but also for many commercial applications. In @cite_13 , Facebook reports that the jobs running on their own internal data centers are mostly independent tasks. Many works propose then scheduling the execution of independent tasks both on homogeneous and heterogeneous cloud environments @cite_3 . In the former, the performance and pricing of all available VMs are the same. In this case, authors usually consider either reserved VMs @cite_10 or on-demand VMs @cite_15 . For instance, @cite_15 study scheduling of applications on on-demand VMs distributed across different datacenters, focusing on the trade-offs between performance and cost while @cite_10 provides a solution that satisfies job deadlines while minimizing monetary cost. The proposed heuristics use both on-demand and reserved VMs. Works on heterogeneous cloud consider different types of VMs. For instance, in @cite_22 the authors present a heuristic algorithm for executing a bag-of-tasks applications taking into account either budget or deadline constraints. In @cite_3 , present an extensive survey and taxonomy of existing research in scheduling of bag-of-task applications on clouds.\"","":""}
{"id":"2896757367","dialogue":"\"Cloud platforms offer different types of virtual machines which ensure different guarantees in terms of availability and volatility, provisioning the same resource through multiple pricing models. For instance, in Amazon EC2 cloud, the user pays per hour for on-demand instances while spot instances are unused resources available for a lower price. Despite the monetary advantages, a spot instance can be terminated or hibernated by EC2 at any moment. Using both hibernation-prone spot instances (for cost sake) and on-demand instances, we propose in this paper a static scheduling for applications which are composed of independent tasks (bag-of-task) with deadline constraints. However, if a spot instance hibernates and it does not resume within a time which guarantees the application's deadline, a temporal failure takes place. Our scheduling, thus, aims at minimizing monetary costs of bag-of-tasks applications in EC2 cloud, respecting its deadline and avoiding temporal failures. Performance results with task execution traces, configuration of Amazon EC2 virtual machines, and EC2 market history confirms the effectiveness of our scheduling and that it tolerates temporal failures.\"","summary":"\"Some works take into account Amazon spot VMs instance features. In @cite_20 , use hybrid instances, including both on-demand instances for high priority tasks and backup, and spot instances for normal computational tasks. Authors of @cite_25 propose to switch to on-demand resources when there is no spot instance available to ensure the desired performance. Using both on-demand and spot VM instances, SpotCheck @cite_8 provides the illusion of an IaaS platform that offers always-available on-demand VMs for a cost near that of spot VMs. Also claiming performance of on-demand VMs, but at a cost near that of the spot market, the authors in @cite_14 present the SpotOn batch service computing, that uses fault-tolerance mechanism to mitigate the impact of spot revocations. To our knowledge no work studies the impact of the new hibernation feature of spot instances on scheduling algorithms.\"","":""}
{"id":"2896121525","dialogue":"\"We address the problem of decomposing a single image into reflectance and shading. The difficulty comes from the fact that the components of image---the surface albedo, the direct illumination, and the ambient illumination---are coupled heavily in observed image. We propose to infer the shading by ordering pixels by their relative brightness, without knowing the absolute values of the image components beforehand. The pairwise shading orders are estimated in two ways: brightness order and low-order fittings of local shading field. The brightness order is a non-local measure, which can be applied to any pair of pixels including those whose reflectance and shading are both different. The low-order fittings are used for pixel pairs within local regions of smooth shading. Together, they can capture both global order structure and local variations of the shading. We propose a Consistency-aware Selective Fusion (CSF) to integrate the pairwise orders into a globally consistent order. The iterative selection process solves the conflicts between the pairwise orders obtained by different estimation methods. Inconsistent or unreliable pairwise orders will be automatically excluded from the fusion to avoid polluting the global order. Experiments on the MIT Intrinsic Image dataset show that the proposed model is effective at recovering the shading including deep shadows. Our model also works well on natural images from the IIW dataset, the UIUC Shadow dataset and the NYU-Depth dataset, where the colors of direct lights and ambient lights are quite different.\"","summary":"\"Edge-based methods rely on classification of image gradients @cite_44 @cite_11 @cite_15 . The problem is that during the integration of gradients, a single misclassified edge will result in errors in a wide area of recovered reflectance @cite_40 . Our shading orders can capture much more information than the gradients, since the objects of the measurements are no longer limited to adjacent pixels. The non-local shading orders can reduce the adverse influence of misclassified edges. Further, the long range relations define the large-scale structure directly, which can avoid the accumulation of error when integrating local measurements. Some Markov random field models @cite_50 @cite_9 @cite_26 and dense Conditional random field models @cite_38 also consider the relation between distant pixels. However, their non-local smooth terms are only applicable to pixels with the same reflectance or shading. This is of particular importance for pixels whose reflectance and shading are both different. Imposing shading smoothness on these pixels ( @cite_38 ) may cause large errors.\"","":""}
{"id":"2896121525","dialogue":"\"We address the problem of decomposing a single image into reflectance and shading. The difficulty comes from the fact that the components of image---the surface albedo, the direct illumination, and the ambient illumination---are coupled heavily in observed image. We propose to infer the shading by ordering pixels by their relative brightness, without knowing the absolute values of the image components beforehand. The pairwise shading orders are estimated in two ways: brightness order and low-order fittings of local shading field. The brightness order is a non-local measure, which can be applied to any pair of pixels including those whose reflectance and shading are both different. The low-order fittings are used for pixel pairs within local regions of smooth shading. Together, they can capture both global order structure and local variations of the shading. We propose a Consistency-aware Selective Fusion (CSF) to integrate the pairwise orders into a globally consistent order. The iterative selection process solves the conflicts between the pairwise orders obtained by different estimation methods. Inconsistent or unreliable pairwise orders will be automatically excluded from the fusion to avoid polluting the global order. Experiments on the MIT Intrinsic Image dataset show that the proposed model is effective at recovering the shading including deep shadows. Our model also works well on natural images from the IIW dataset, the UIUC Shadow dataset and the NYU-Depth dataset, where the colors of direct lights and ambient lights are quite different.\"","summary":"\"Different constraints often result in quite different shading orders. How to fuse them remains an open problem. Edge-based methods classify each edge into a reflectance edge or a shading edge. Accordingly, the shading order between the two sides of the edge can be decided. In particular, Retinex classified the edges by the magnitude of gradients @cite_44 . This classification method is risky. Some shadow edges are quite strong, while the reflectance edges between similar colors are relatively weak. Extensions of Retinex introduced several new features, including texture similarity @cite_15 , classifiers over local features @cite_46 or patches @cite_17 @cite_11 , correlation between the mean luminance and luminance amplitude @cite_41 , and image sequences under different illumination directions @cite_49 @cite_22 . These features improved the accuracy of classification, but none of them are robust enough to handle all kinds of scenes. CSF faces a similar problem of selecting the optimal pairwise order from several estimates. The difference is that CSF incorporates consistency between the pairwise orders and global order into the selection criteria, which can rectify the inconsistent selections made by noisy image features.\"","":""}
{"id":"2896121525","dialogue":"\"We address the problem of decomposing a single image into reflectance and shading. The difficulty comes from the fact that the components of image---the surface albedo, the direct illumination, and the ambient illumination---are coupled heavily in observed image. We propose to infer the shading by ordering pixels by their relative brightness, without knowing the absolute values of the image components beforehand. The pairwise shading orders are estimated in two ways: brightness order and low-order fittings of local shading field. The brightness order is a non-local measure, which can be applied to any pair of pixels including those whose reflectance and shading are both different. The low-order fittings are used for pixel pairs within local regions of smooth shading. Together, they can capture both global order structure and local variations of the shading. We propose a Consistency-aware Selective Fusion (CSF) to integrate the pairwise orders into a globally consistent order. The iterative selection process solves the conflicts between the pairwise orders obtained by different estimation methods. Inconsistent or unreliable pairwise orders will be automatically excluded from the fusion to avoid polluting the global order. Experiments on the MIT Intrinsic Image dataset show that the proposed model is effective at recovering the shading including deep shadows. Our model also works well on natural images from the IIW dataset, the UIUC Shadow dataset and the NYU-Depth dataset, where the colors of direct lights and ambient lights are quite different.\"","summary":"\"Ranking elements from their pairwise comparisons has been extensively studied in many fields @cite_33 @cite_8 @cite_45 . Angular Embedding @cite_23 adopts a cosine error function, which is proven to be more robust to outliers than the traditional @math or @math errors used by Least Squares Embedding @cite_33 . Angular Synchronization (AS) also uses the angular space @cite_2 , but it does not consider the confidences of pairwise measures.\"","":""}
{"id":"2896121525","dialogue":"\"We address the problem of decomposing a single image into reflectance and shading. The difficulty comes from the fact that the components of image---the surface albedo, the direct illumination, and the ambient illumination---are coupled heavily in observed image. We propose to infer the shading by ordering pixels by their relative brightness, without knowing the absolute values of the image components beforehand. The pairwise shading orders are estimated in two ways: brightness order and low-order fittings of local shading field. The brightness order is a non-local measure, which can be applied to any pair of pixels including those whose reflectance and shading are both different. The low-order fittings are used for pixel pairs within local regions of smooth shading. Together, they can capture both global order structure and local variations of the shading. We propose a Consistency-aware Selective Fusion (CSF) to integrate the pairwise orders into a globally consistent order. The iterative selection process solves the conflicts between the pairwise orders obtained by different estimation methods. Inconsistent or unreliable pairwise orders will be automatically excluded from the fusion to avoid polluting the global order. Experiments on the MIT Intrinsic Image dataset show that the proposed model is effective at recovering the shading including deep shadows. Our model also works well on natural images from the IIW dataset, the UIUC Shadow dataset and the NYU-Depth dataset, where the colors of direct lights and ambient lights are quite different.\"","summary":"\"Many recent methods address more intrinsic components other than shading and reflectance, including specular reflection @cite_20 , shape and illumination @cite_48 , coarse-scale and detailed shading @cite_21 , direct and indirect irradiance @cite_26 @cite_43 , illuminant color and sensor characteristics @cite_37 , and texture @cite_36 . These detailed decompositions give a more comprehensive analysis of the scene, but they also make the problem much more complex. Recently, new constraints have been formed based on the geometric information of RGB-Depth images @cite_9 @cite_5 @cite_26 @cite_36 . We use the depth map to render shadow maps that can determine the positions of shading edges. More recently, the intrinsic video techniques have extended the research to videos @cite_6 @cite_3 @cite_42 .\"","":""}
{"id":"2897594638","dialogue":"\"The bottleneck distance is a natural measure of the distance between two finite point sets of equal cardinality. In this work, we consider the problem of indexing a set of @math planar point sets (of varying sizes) to create a database @math that supports nearest bottleneck distance queries: given a query point set @math of size @math , the point sets @math that are closest in terms of bottleneck distance are returned. Without loss of generality, we assume that all point sets belong to the unit box @math in the plane. The main contribution of this work is a trie -based data structure that is space efficient and supports @math -approximate nearest bottleneck queries in @math time, where @math is the minimum bottleneck distance from @math to any point set in @math . A direct consequence, of independent interest, is a simple @math time algorithm to @math -approximate @math , for any two point sets @math and @math . Finally, the querying algorithm proposed is easily adapted to support nearest subset and superset queries.\"","summary":"\"Bottleneck distance is closely related to the bipartite matching problem, which can be solved by the classic maximum flow technique of Hopcroft and Karp @cite_1 . The current best exact algorithm for planar bipartite matching is due to Efrat @cite_9 and runs in @math time for point sets of size @math .\"","":""}
{"id":"2897594638","dialogue":"\"The bottleneck distance is a natural measure of the distance between two finite point sets of equal cardinality. In this work, we consider the problem of indexing a set of @math planar point sets (of varying sizes) to create a database @math that supports nearest bottleneck distance queries: given a query point set @math of size @math , the point sets @math that are closest in terms of bottleneck distance are returned. Without loss of generality, we assume that all point sets belong to the unit box @math in the plane. The main contribution of this work is a trie -based data structure that is space efficient and supports @math -approximate nearest bottleneck queries in @math time, where @math is the minimum bottleneck distance from @math to any point set in @math . A direct consequence, of independent interest, is a simple @math time algorithm to @math -approximate @math , for any two point sets @math and @math . Finally, the querying algorithm proposed is easily adapted to support nearest subset and superset queries.\"","summary":"\"Earlier seminal work by Hefferman and Schira @cite_3 considered approximation algorithms for the more general problem in which one of the point sets is mapped by an isometry (translated, rotated, and possibly reflected) prior to being matched. In the case of just computing the bottleneck distance, their methods provide a @math time algorithm to test if @math , where the answer must be correct if @math . A key idea in @cite_3 is to check for bottleneck matchings using a maximum flow computation in graph that arises from snap-rounding'' the point sets to their nearest point in grid. Our approach uses a similar idea in which the maximum flow instance is a planar graph (not true for @cite_3 ), so a recent improved algorithm for multi-source, multi-sink maximum flow due to Borradaile @cite_8 that runs in @math time can be leveraged.\"","":""}
{"id":"2897594638","dialogue":"\"The bottleneck distance is a natural measure of the distance between two finite point sets of equal cardinality. In this work, we consider the problem of indexing a set of @math planar point sets (of varying sizes) to create a database @math that supports nearest bottleneck distance queries: given a query point set @math of size @math , the point sets @math that are closest in terms of bottleneck distance are returned. Without loss of generality, we assume that all point sets belong to the unit box @math in the plane. The main contribution of this work is a trie -based data structure that is space efficient and supports @math -approximate nearest bottleneck queries in @math time, where @math is the minimum bottleneck distance from @math to any point set in @math . A direct consequence, of independent interest, is a simple @math time algorithm to @math -approximate @math , for any two point sets @math and @math . Finally, the querying algorithm proposed is easily adapted to support nearest subset and superset queries.\"","summary":"\"Bottleneck distance arises naturally in the comparison of persistence diagrams in topological data analysis @cite_4 . @cite_0 consider the related problem of building a database of persistence diagrams that permits approximate querying in @math time ( @math is the number of point sets stored in @math ). Their approach is also based on representing point sets by snap-rounding each point to neighboring grid points at each level in a multilevel grid. All combinations of snap-roundings are considered and the resulting grid point distributions are stored in a database. Binary search is used on the hashed values to query the database and resulting matches are shown to provide a @math -approximation to the nearest point set in the database. They also observe that the approximation ratio can be improved to @math , if more snap-roundings are done ( @math per point). Rather than using a hashing scheme, it is possible to use a trie data structure (described in ) to achieve @math time queries.\"","":""}
{"id":"2897594638","dialogue":"\"The bottleneck distance is a natural measure of the distance between two finite point sets of equal cardinality. In this work, we consider the problem of indexing a set of @math planar point sets (of varying sizes) to create a database @math that supports nearest bottleneck distance queries: given a query point set @math of size @math , the point sets @math that are closest in terms of bottleneck distance are returned. Without loss of generality, we assume that all point sets belong to the unit box @math in the plane. The main contribution of this work is a trie -based data structure that is space efficient and supports @math -approximate nearest bottleneck queries in @math time, where @math is the minimum bottleneck distance from @math to any point set in @math . A direct consequence, of independent interest, is a simple @math time algorithm to @math -approximate @math , for any two point sets @math and @math . Finally, the querying algorithm proposed is easily adapted to support nearest subset and superset queries.\"","summary":"\"Approximation results are known for general bipartite matching in metric spaces; in @cite_2 the authors show that for any @math , there is an algorithm that computes a @math -approximate matching, where @math in @math time. A variation on minimum-distance bottleneck matching, with the additional constraint that the matched edges cannot cross, was recently shown to be NP-hard to approximate within a factor of less than @math @cite_7 .\"","":""}
{"id":"2949855468","dialogue":"\"Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.\"","summary":"\"Liu @cite_17 proposed a universal network for counting people in a crowd with varying density and scale. in this study the proposed network is composed of two components: a detection network (DNet) and an encoder-decoder estimation network (ENet). The input first run through DNet to detect and count individuals who can be segmented clearly. Then, ENet is utilized to estimate the density maps of the remaining areas, where the numbers of individuals cannot be detected. Modified version of Xception used as an encoder for feature extraction and a combination of dilated convolution and transposed convolution used as decoder. Authors attempted to address the variations in crowd density with two literally isolated deep networks which significantly slows down the process lacks novelty.\"","":""}
{"id":"2949855468","dialogue":"\"Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.\"","summary":"\"In another study, Mehta @cite_26 proposed independent decoding reinforcement branch as a binary classifier which helps the network converge much earlier and also enables the network to estimate density maps with high Structural Similarity Index (SSIM). A joint loss strategy, the binary cross entropy (BCE) loss and mean squared error (MSE) loss used to train the network in an end to end fashion. They have used variation of the U-net model to generate the density maps. The proposed model shows notable improvements in recreation of the crowd density maps over the existing models.\"","":""}
{"id":"2949855468","dialogue":"\"Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.\"","summary":"A study by Oh @cite_24 attempt to address the uncertainty estimation in the domain of crowd counting. This study proposed a scalable neural network framework with quantification of decomposed uncertainty using a bootstrap ensemble. The proposed method incorporates both epistemic uncertainty and aleatoric uncertainty in a neural network for crowd counting. The proposed uncertainty quantification method provides additional auxiliary insight to the crowd counting model. The proposed technique attempt to address the uncertainty issue in crowd counting. However the use of unsupervised calibration method to re-calibrate the predictions of the pre-trained network is questionable.","":""}
{"id":"2949855468","dialogue":"\"Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.\"","summary":"In another study Olmschenk @cite_18 attempt to address the inefficiency of the existing crowd density map labeling scheme for training deep neural networks. This study proposes a labeling scheme based on inverse k-nearest neighbor ( @math ) maps which does not explicitly represents the crowd density. Authors claim a single @math map provides information similar to the commonly practiced accumulation of many density maps with different Gaussian spreads.","":""}
{"id":"2949855468","dialogue":"\"Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.\"","summary":"\"A study by Idrees @cite_0 stems from the observation that crowd counting, density map estimation and localization are very interrelated and can be decomposed with respect to each other through composition loss, which can then be used to train a neural network. This study\"","":""}
{"id":"2950919816","dialogue":"\"Generating a photorealistic image with intended human pose is a promising yet challenging research topic for many applications such as smart photo editing, movie making, virtual try-on, and fashion display. In this paper, we present a novel deep generative model to transfer an image of a person from a given pose to a new pose while keeping fashion item consistent. In order to formulate the framework, we employ one generator and two discriminators for image synthesis. The generator includes an image encoder, a pose encoder and a decoder. The two encoders provide good representation of visual and geometrical context which will be utilized by the decoder in order to generate a photorealistic image. Unlike existing pose-guided image generation models, we exploit two discriminators to guide the synthesis process where one discriminator differentiates between generated image and real images (training samples), and another discriminator verifies the consistency of appearance between a target pose and a generated image. We perform end-to-end training of the network to learn the parameters through back-propagation given ground-truth images. The proposed generative model is capable of synthesizing a photorealistic image of a person given a target pose. We have demonstrated our results by conducting rigorous experiments on two data sets, both quantitatively and qualitatively.\"","summary":"\"Recently, image generative modeling has gained a lot of attention from both scientific communities and fashion industry. Generative Adversarial Networks (GANs) @cite_15 are the most popular generative models for the tasks of image synthesis and image modification. There have been some works @cite_33 @cite_27 that exploit GAN in conditional setting. In @cite_3 @cite_33 , generative models are developed conditioning upon class labels. Text @cite_28 @cite_32 and images @cite_8 @cite_27 @cite_22 @cite_11 @cite_13 have also been used as conditions to build image generative models.\"","":""}
{"id":"2950919816","dialogue":"\"Generating a photorealistic image with intended human pose is a promising yet challenging research topic for many applications such as smart photo editing, movie making, virtual try-on, and fashion display. In this paper, we present a novel deep generative model to transfer an image of a person from a given pose to a new pose while keeping fashion item consistent. In order to formulate the framework, we employ one generator and two discriminators for image synthesis. The generator includes an image encoder, a pose encoder and a decoder. The two encoders provide good representation of visual and geometrical context which will be utilized by the decoder in order to generate a photorealistic image. Unlike existing pose-guided image generation models, we exploit two discriminators to guide the synthesis process where one discriminator differentiates between generated image and real images (training samples), and another discriminator verifies the consistency of appearance between a target pose and a generated image. We perform end-to-end training of the network to learn the parameters through back-propagation given ground-truth images. The proposed generative model is capable of synthesizing a photorealistic image of a person given a target pose. We have demonstrated our results by conducting rigorous experiments on two data sets, both quantitatively and qualitatively.\"","summary":"\"Image Synthesis in Fashion. These techniques have also been applied @cite_41 @cite_32 @cite_8 to exploit image generative models in fashion technology. An image based virtual try-on network has been proposed in @cite_41 where the generative model transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. A novel approach is presented in @cite_32 for generating new clothing on a wearer through generative adversarial learning by utilizing textual information. In @cite_2 , a conditional U-Net has been used to generate image guided by shape information, and conditioned on the output of a variational autoencoder for appearance. In @cite_8 , the authors present a generative model conditioned upon pose to manipulate a person in an image to an arbitrary pose. @cite_20 study similar task with us, instead they transfer poses to target person from video in a frame-by-frame manner.\"","":""}
{"id":"2950919816","dialogue":"\"Generating a photorealistic image with intended human pose is a promising yet challenging research topic for many applications such as smart photo editing, movie making, virtual try-on, and fashion display. In this paper, we present a novel deep generative model to transfer an image of a person from a given pose to a new pose while keeping fashion item consistent. In order to formulate the framework, we employ one generator and two discriminators for image synthesis. The generator includes an image encoder, a pose encoder and a decoder. The two encoders provide good representation of visual and geometrical context which will be utilized by the decoder in order to generate a photorealistic image. Unlike existing pose-guided image generation models, we exploit two discriminators to guide the synthesis process where one discriminator differentiates between generated image and real images (training samples), and another discriminator verifies the consistency of appearance between a target pose and a generated image. We perform end-to-end training of the network to learn the parameters through back-propagation given ground-truth images. The proposed generative model is capable of synthesizing a photorealistic image of a person given a target pose. We have demonstrated our results by conducting rigorous experiments on two data sets, both quantitatively and qualitatively.\"","summary":"\"Even though we aim at solving similar problem as @cite_8 , our work differs from @cite_8 in terms of architectural choices both in generator and discriminator. Unlike most of the image generative approaches, we exploit multiple images of a fashion item as input which are usually available on e-commerce shopping platforms.\"","":""}
{"id":"2952165569","dialogue":"\"Imitation from observation is the framework of learning tasks by observing demonstrated state-only trajectories. Recently, adversarial approaches have achieved significant performance improvements over other methods for imitating complex behaviors. However, these adversarial imitation algorithms often require many demonstration examples and learning iterations to produce a policy that is successful at imitating a demonstrator's behavior. This high sample complexity often prohibits these algorithms from being deployed on physical robots. In this paper, we propose an algorithm that addresses the sample inefficiency problem by utilizing ideas from trajectory centric reinforcement learning algorithms. We test our algorithm and conduct experiments using an imitation task on a physical robot arm and its simulated version in Gazebo and will show the improvement in learning rate and efficiency.\"","summary":"\"Techniques for imitation learning differ in the way they approach the problem. Two popular approaches to imitation learning have been behavioral cloning @cite_11 and inverse reinforcement learning (IRL) @cite_17 @cite_7 . Behavioral cloning views the imitation learning problem as a supervised learning problem that attempts to learn a direct mapping from states to actions. On the other hand, inverse reinforcement learning works to find a cost function under which the expert demonstrator is optimal. One approach of this type is guided cost learning @cite_8 which builds on maximum entropy IRL @cite_1 and guided policy search algorithm @cite_19 and achieves impressive results on physical robots. Later, in ho2016generative , ho2016generative used generative adversarial networks to imitate policies when both states and actions are available using a technique called generative adversarial imitation learning (GAIL) @cite_28 . One imitator network attempts to imitate the policy while another attempts to discriminate between the imitation and provided demonstration data @cite_9 . Several follow-up works have improved upon this approach on different aspects @cite_20 @cite_21 and recently, there has been efforts to address sample efficiency of this algorithm by proposing approaches for unbiasing rewards and deriving an off-policy formulation of adversarial imitation learning algorithms @cite_4 .\"","":""}
{"id":"2952165569","dialogue":"\"Imitation from observation is the framework of learning tasks by observing demonstrated state-only trajectories. Recently, adversarial approaches have achieved significant performance improvements over other methods for imitating complex behaviors. However, these adversarial imitation algorithms often require many demonstration examples and learning iterations to produce a policy that is successful at imitating a demonstrator's behavior. This high sample complexity often prohibits these algorithms from being deployed on physical robots. In this paper, we propose an algorithm that addresses the sample inefficiency problem by utilizing ideas from trajectory centric reinforcement learning algorithms. We test our algorithm and conduct experiments using an imitation task on a physical robot arm and its simulated version in Gazebo and will show the improvement in learning rate and efficiency.\"","summary":"\"On the other hand, in reinforcement learning policy learning through environment-provided reward functions only direct policy search in a large state-action space requires numerous samples and often can fall into poor local optima. Guided policy search (GPS) is a method to improve the sample efficiency of direct policy search and guide learning in a large space away from poor local optima @cite_18 . The basis of GPS is to use trajectory optimization to focus policy learning on high-reward actions.\"","":""}
{"id":"2952165569","dialogue":"\"Imitation from observation is the framework of learning tasks by observing demonstrated state-only trajectories. Recently, adversarial approaches have achieved significant performance improvements over other methods for imitating complex behaviors. However, these adversarial imitation algorithms often require many demonstration examples and learning iterations to produce a policy that is successful at imitating a demonstrator's behavior. This high sample complexity often prohibits these algorithms from being deployed on physical robots. In this paper, we propose an algorithm that addresses the sample inefficiency problem by utilizing ideas from trajectory centric reinforcement learning algorithms. We test our algorithm and conduct experiments using an imitation task on a physical robot arm and its simulated version in Gazebo and will show the improvement in learning rate and efficiency.\"","summary":"\"In guided policy search under unknown dynamics, time-varying linear Gaussian models of the dynamics for a small set of specific tasks are first trained to fit a small set of sample data through LQR @cite_19 . These Gaussian controllers are then sampled to generate samples to optimize a general policy for a model with thousands of parameters that would typically require much more training data. Specifically, samples in regions of trajectories that have been found to lead to higher reward are generated, guiding the policy learning.\"","":""}
{"id":"2894886314","dialogue":"\"Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models. In this paper, we propose a novel RNN model for NLI and empirically evaluate the effect of applying dropout at different layers in the model. We also investigate the impact of varying dropout rates at these layers. Our empirical evaluation on a large (Stanford Natural Language Inference (SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward connection severely affects the model accuracy at increasing dropout rates. We also show that regularizing the embedding layer is efficient for SNLI whereas regularizing the recurrent layer improves the accuracy for SciTail. Our model achieved an accuracy (86.14 ) on the SNLI dataset and (77.05 ) on SciTail.\"","summary":"Different NLI models apply dropout at different layers in general NLI architecture. NLI models proposed by @cite_8 and @cite_3 apply dropout to each feed-forward layer in the network whereas others have applied dropout only to the final classifier layer @cite_15 . @cite_13 apply dropout only to the input and output of sentence encoding layers.The models proposed by @cite_9 and @cite_6 applied dropout to the output of embedding layer and to the input and output of classifier layer. @cite_5 and @cite_11 use dropout but they do not elaborate on the location.","":""}
{"id":"2894886314","dialogue":"\"Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models. In this paper, we propose a novel RNN model for NLI and empirically evaluate the effect of applying dropout at different layers in the model. We also investigate the impact of varying dropout rates at these layers. Our empirical evaluation on a large (Stanford Natural Language Inference (SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward connection severely affects the model accuracy at increasing dropout rates. We also show that regularizing the embedding layer is efficient for SNLI whereas regularizing the recurrent layer improves the accuracy for SciTail. Our model achieved an accuracy (86.14 ) on the SNLI dataset and (77.05 ) on SciTail.\"","summary":"Dropout rates are also crucial for the NLI models @cite_12 . Even the models which apply dropout at the same locations vary dropout rates.","":""}
{"id":"2894886314","dialogue":"\"Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models. In this paper, we propose a novel RNN model for NLI and empirically evaluate the effect of applying dropout at different layers in the model. We also investigate the impact of varying dropout rates at these layers. Our empirical evaluation on a large (Stanford Natural Language Inference (SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward connection severely affects the model accuracy at increasing dropout rates. We also show that regularizing the embedding layer is efficient for SNLI whereas regularizing the recurrent layer improves the accuracy for SciTail. Our model achieved an accuracy (86.14 ) on the SNLI dataset and (77.05 ) on SciTail.\"","summary":"\"Previous research on dropout for RNNs on the applications such as neural language models @cite_16 , handwriting recognition @cite_14 and machine translation @cite_19 have established that recurrent connection dropout should not be applied to RNNs as it affects the long term dependencies in sequential data.\"","":""}
{"id":"2894886314","dialogue":"\"Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models. In this paper, we propose a novel RNN model for NLI and empirically evaluate the effect of applying dropout at different layers in the model. We also investigate the impact of varying dropout rates at these layers. Our empirical evaluation on a large (Stanford Natural Language Inference (SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward connection severely affects the model accuracy at increasing dropout rates. We also show that regularizing the embedding layer is efficient for SNLI whereas regularizing the recurrent layer improves the accuracy for SciTail. Our model achieved an accuracy (86.14 ) on the SNLI dataset and (77.05 ) on SciTail.\"","summary":"\"@cite_4 studied dropout at different places with respect to the LSTM units in the network proposed by @cite_14 for handwriting recognition. The results show that significant performance difference is observed when dropout is applied to distinct places. They concluded that applying dropout only after recurrent layers (as applied by @cite_14 ) or between every feed-forward layer (as done by @cite_19 ) does not always yield good results. @cite_10 , investigated the effect of applying dropout in LSTMs. They randomly switch off the outputs of various gates of LSTM, achieving an optimal word error rate when dropout is applied to output, forget and input gates of the LSTM.\"","":""}
{"id":"2894886314","dialogue":"\"Dropout is a crucial regularization technique for the Recurrent Neural Network (RNN) models of Natural Language Inference (NLI). However, dropout has not been evaluated for the effectiveness at different layers and dropout rates in NLI models. In this paper, we propose a novel RNN model for NLI and empirically evaluate the effect of applying dropout at different layers in the model. We also investigate the impact of varying dropout rates at these layers. Our empirical evaluation on a large (Stanford Natural Language Inference (SNLI)) and a small (SciTail) dataset suggest that dropout at each feed-forward connection severely affects the model accuracy at increasing dropout rates. We also show that regularizing the embedding layer is efficient for SNLI whereas regularizing the recurrent layer improves the accuracy for SciTail. Our model achieved an accuracy (86.14 ) on the SNLI dataset and (77.05 ) on SciTail.\"","summary":"\"Evaluations in previous research were conducted on datasets with fewer samples. We evaluate the RNN model on a large, SNLI dataset (570,000 data sample) as well as on a smaller SciTail dataset (27,000 data samples). Furthermore, previous studies concentrate only on the location of dropout in the network with fixed dropout rate.We further investigate the effect of varying dropout rates. We focus on the application of widely used conventional dropout @cite_18 to non-recurrent connection in RNNs.\"","":""}
{"id":"2951836492","dialogue":"\"This paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where ConvNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.\"","summary":"\"Applying machine learning techniques has proven very effective in optical flow estimation problem @cite_31 @cite_30 @cite_29 which is closely related to finding pixel correspondence task. Recently proposed methods, PWC-Net @cite_29 and FlowNet2 @cite_31 , utilize a correlation layer to predict image similarities in some neighborhood around the center pixel in a coarse-to-fine manner. While such a spatially constrained correlation layer leads to state-of-the-art results in optical flow, it performs poorly for very strong geometric transformations that we consider in this work. Rocco al @cite_8 proposed a CNN-based approach for determining correspondences between two images and applying it to instance-level and category-level tasks. In contrast to optical flow methods @cite_31 @cite_29 , it comprises a matching layer calculating the correlation between target and reference feature maps without any spatial constraint. The method casts finding pixel correspondences task as a regression problem and consisting of two independent Siamese CNNs trained separately and directly predicting affine and TPS geometric transformations parametrizing 6-element and 18-element vectors. On the contrary, we propose a more general approach handling more diverse transformations and operating in an end-to-end fashion.\"","":""}
{"id":"2900464026","dialogue":"\"Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.\"","summary":"\"Most state-of-the-art approaches for inferring road maps from aerial imagery apply convolutional neural networks (CNNs) to segment the imagery for road'' and non-road'' pixels, and then post-process the segmentation output to extract a road network graph. develop a cascaded CNN architecture with two jointly trained components, where the first component detects pixels on the road, and the second focuses on pixels close to the road centerline @cite_17 . They then threshold and thin the centerline segmentation output to extract a graph.\"","":""}
{"id":"2900464026","dialogue":"\"Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.\"","summary":"\"propose improving the segmentation output by using a conditional generative adversarial network @cite_13 . They train the segmentation CNN not only to output the ground truth labels (with a mean-squared-error loss), but also to fool a discriminator CNN that is trained to distinguish between the ground truth labels and the segmentation CNN outputs.\"","":""}
{"id":"2900464026","dialogue":"\"Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.\"","summary":"\"DeepRoadMapper adds an additional post-processing step to infer missing connections in the initial extracted road network @cite_10 . Candidate missing connections are generated by performing a shortest path search on a graph defined by the segmentation probabilities. Then, a separate CNN is trained to identify correct missing connections.\"","":""}
{"id":"2900464026","dialogue":"\"Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.\"","summary":"\"Rather than segmenting the imagery, RoadTracer @cite_8 and IDL @cite_18 employ an iterative graph construction (IGC) approach that extracts roads via a series of steps in a search process. On each step, a CNN is queried to determine what direction to move in the search, and a road segment is added to a partial road network graph in that direction. Although IGC methods improve accuracy, they are an order of magnitude slower in execution time than segmentation approaches and thus not suitable in interactive settings.\"","":""}
{"id":"2897323328","dialogue":"\"Stringent latency requirements in advanced Internet of Things (IoT) applications as well as an increased load on cloud data centers have prompted a move towards a more decentralized approach, bringing storage and processing of IoT data closer to the end-devices through the deployment of multi-purpose IoT gateways. However, the resource constrained nature and diversity of these gateways pose a challenge in developing applications that can be deployed widely. This challenge can be overcome with containerization, a form of lightweight virtualization, bringing support for a wide range of hardware architectures and operating system agnostic deployment of applications on IoT gateways. This paper discusses the architectural aspects of containerization, and studies the suitability of available containerization tools for multi-container deployment in the context of IoT gateways. We present containerization in the context of AGILE, a multi-container and micro-service based open source framework for IoT gateways, developed as part of a Horizon 2020 project. Our study of containerized services to perform common gateway functions like device discovery, data management and cloud integration among others, reveal the advantages of having a containerized environment for IoT gateways with regard to use of base image hierarchies and image layering for in-container and cross-container performance optimizations. We illustrate these results in a set of benchmark experiments in this paper.\"","summary":"\"The growth in both the number of IoT devices and IoT applications in a wide array of domains has brought about new requirements to the IoT ecosystem which include location awareness, geo-distribution of processing nodes and low latency in device-cloud communication. This has led to a burden on the traditional resources for IoT including networking, storage and processing resources. Consequently, considerable amount of literature has been published on the use of Single Board Computers (SBCs) like Raspberry Pi as an intermediate processing layer @cite_16 @cite_7 and an enabler for various IoT applications @cite_15 @cite_14 . Moreover, the suitability of networking resource virtualization for IoT including Software Defined Networks (SDNs) and Network Virtualization among others @cite_10 @cite_13 have also been discussed in existing work. Our study on the existing literature focuses on virtualization of storage and processing resources at the OS-level in the form of containerization. We study two aspects of containerization in the IoT context, first, the existing studies on the suitability and performance of containerization on resource-constrained devices and second, the application of containerization to different use-cases of IoT.\"","":""}
{"id":"2897323328","dialogue":"\"Stringent latency requirements in advanced Internet of Things (IoT) applications as well as an increased load on cloud data centers have prompted a move towards a more decentralized approach, bringing storage and processing of IoT data closer to the end-devices through the deployment of multi-purpose IoT gateways. However, the resource constrained nature and diversity of these gateways pose a challenge in developing applications that can be deployed widely. This challenge can be overcome with containerization, a form of lightweight virtualization, bringing support for a wide range of hardware architectures and operating system agnostic deployment of applications on IoT gateways. This paper discusses the architectural aspects of containerization, and studies the suitability of available containerization tools for multi-container deployment in the context of IoT gateways. We present containerization in the context of AGILE, a multi-container and micro-service based open source framework for IoT gateways, developed as part of a Horizon 2020 project. Our study of containerized services to perform common gateway functions like device discovery, data management and cloud integration among others, reveal the advantages of having a containerized environment for IoT gateways with regard to use of base image hierarchies and image layering for in-container and cross-container performance optimizations. We illustrate these results in a set of benchmark experiments in this paper.\"","summary":"\"Previous studies have focused on the tradeoffs in applying hypervisor based virtualization and lightweight containerization to edge devices. The authors of @cite_5 illustrate the advantages of containerization over hypervisor based hardware virtualization in terms of size of the resources, flexibility and portability. The authors state that hypervisor based virtualization is more suited for Infrastructure-as-a-Service on the cloud than containerization, which offers a portable runtime, easier deployability on multiple servers and interconnectivity among containers. These advantages, on the other hand, make containerization more suitable for the edge layer in a Platform-as-a-Service scenario @cite_0 . Pahl et. al @cite_16 leverages the resources of Raspberry Pi devices to further build a cluster of containers running on multiple devices in the PaaS context. The cluster is designed to perform computationally intensive tasks including cluster and data management, overcoming the resource-constrained nature of each device.\"","":""}
{"id":"2897323328","dialogue":"\"Stringent latency requirements in advanced Internet of Things (IoT) applications as well as an increased load on cloud data centers have prompted a move towards a more decentralized approach, bringing storage and processing of IoT data closer to the end-devices through the deployment of multi-purpose IoT gateways. However, the resource constrained nature and diversity of these gateways pose a challenge in developing applications that can be deployed widely. This challenge can be overcome with containerization, a form of lightweight virtualization, bringing support for a wide range of hardware architectures and operating system agnostic deployment of applications on IoT gateways. This paper discusses the architectural aspects of containerization, and studies the suitability of available containerization tools for multi-container deployment in the context of IoT gateways. We present containerization in the context of AGILE, a multi-container and micro-service based open source framework for IoT gateways, developed as part of a Horizon 2020 project. Our study of containerized services to perform common gateway functions like device discovery, data management and cloud integration among others, reveal the advantages of having a containerized environment for IoT gateways with regard to use of base image hierarchies and image layering for in-container and cross-container performance optimizations. We illustrate these results in a set of benchmark experiments in this paper.\"","summary":"\"Several articles in existing literature present applications of IoT based on containerization in different use cases. The authors of @cite_11 demonstrate a distributed and layered architecture for Industrial IoT based applications with deployment of docker containers on end devices, the gateway and the cloud. Chesov et. al @cite_6 present a multi-tier approach to containerize different functionalities in the smart cities context like data aggregation, business analytics and user interaction with data and deploy the containers on the cloud. Kovatsch et. al simplify programmability of IoT applications by exposing scripts and configurations using a RESTful CoAP interface from a deployed container @cite_1 .\"","":""}
{"id":"2952898185","dialogue":"\"This paper addresses the problem of very large-scale image retrieval, focusing on improving its accuracy and robustness. We target enhanced robustness of search to factors, such as variations in illumination, object appearance and scale, partial occlusions, and cluttered backgrounds—particularly important when a search is performed across very large datasets with significant variability. We propose a novel CNN-based global descriptor, called REMAP, which learns and aggregates a hierarchy of deep features from multiple CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly learns discriminative features which are mutually supportive and complementary at various semantic levels of visual abstraction. These dense local features are max-pooled spatially at each layer, within multi-scale overlapping regions, before aggregation into a single image-level descriptor. To identify the semantically useful regions and layers for retrieval, we propose to measure the information gain of each region and layer using KL-divergence. Our system effectively learns during training how useful various regions and layers are and weights them accordingly. We show that such relative entropy-guided aggregation outperforms classical CNN-based aggregation controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays, Oxford, and MPEG, the REMAP descriptor achieves mAP of 95.5 , 91.5 , and 80.1 , respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to the Google Landmark Retrieval Challenge on Kaggle.\"","summary":"\"Early approaches typically involve extracting multiple local descriptors (usually hand-crafted), and combining them into a fixed length image-level representation for fast matching. Local descriptors may be scale-invariant and centered on image feature points, such as for SIFT @cite_6 , or extracted on regular, dense grids, possibly at multiple scales independently of the image content @cite_20 . An impressive number of local descriptors have been developed over years, each claiming superiority, making it difficult to select the best one for the job - an attempt at comparative study can be found in @cite_18 . It should be noted that descriptor dimension (for hand-crafted features) is typically between 32 and 192, which is an order or two less than the number of deep features available for each image region.\"","":""}
{"id":"2952898185","dialogue":"\"This paper addresses the problem of very large-scale image retrieval, focusing on improving its accuracy and robustness. We target enhanced robustness of search to factors, such as variations in illumination, object appearance and scale, partial occlusions, and cluttered backgrounds—particularly important when a search is performed across very large datasets with significant variability. We propose a novel CNN-based global descriptor, called REMAP, which learns and aggregates a hierarchy of deep features from multiple CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly learns discriminative features which are mutually supportive and complementary at various semantic levels of visual abstraction. These dense local features are max-pooled spatially at each layer, within multi-scale overlapping regions, before aggregation into a single image-level descriptor. To identify the semantically useful regions and layers for retrieval, we propose to measure the information gain of each region and layer using KL-divergence. Our system effectively learns during training how useful various regions and layers are and weights them accordingly. We show that such relative entropy-guided aggregation outperforms classical CNN-based aggregation controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays, Oxford, and MPEG, the REMAP descriptor achieves mAP of 95.5 , 91.5 , and 80.1 , respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to the Google Landmark Retrieval Challenge on Kaggle.\"","summary":"\"Virtually all aggregation schemes rely on clustering in feature space, with varying degree of sophistication: Bag-of-Words (BOW) @cite_14 , Vector of Locally Aggregated Descriptors (VLAD) @cite_28 , Fisher Vector (FV) @cite_21 , and Robust Visual Descriptor (RVD) @cite_7 . BOW is effectively a fixed length histogram with descriptors assigned to the closest visual word; VLAD additionally encodes the positions of local descriptors within each voronoi region by computing their residuals; the Fisher Vector (FV) aggregates local descriptors using the Fisher Kernel framework (second order statistics), and RVD combines rank-based multi-assignment with robust accumulation to reduce the impact of outliers.\"","":""}
{"id":"2952898185","dialogue":"\"This paper addresses the problem of very large-scale image retrieval, focusing on improving its accuracy and robustness. We target enhanced robustness of search to factors, such as variations in illumination, object appearance and scale, partial occlusions, and cluttered backgrounds—particularly important when a search is performed across very large datasets with significant variability. We propose a novel CNN-based global descriptor, called REMAP, which learns and aggregates a hierarchy of deep features from multiple CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly learns discriminative features which are mutually supportive and complementary at various semantic levels of visual abstraction. These dense local features are max-pooled spatially at each layer, within multi-scale overlapping regions, before aggregation into a single image-level descriptor. To identify the semantically useful regions and layers for retrieval, we propose to measure the information gain of each region and layer using KL-divergence. Our system effectively learns during training how useful various regions and layers are and weights them accordingly. We show that such relative entropy-guided aggregation outperforms classical CNN-based aggregation controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays, Oxford, and MPEG, the REMAP descriptor achieves mAP of 95.5 , 91.5 , and 80.1 , respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to the Google Landmark Retrieval Challenge on Kaggle.\"","summary":"\"All of the aforementioned approaches use fixed pre-trained CNNs. However, these CNNs were trained for the purpose of image classification (e.g. 1000 classes of ImageNet), in a fashion blind to the aggregation method, and hence likely to perform sub-optimally in the task of image retrieval. To tackle this, @cite_23 , proposed to fine-tune MAC representation using the Flickr Landmarks dataset @cite_15 . More precisely, the MAC layer is added to the last convolutional layer of VGG or ResNet. The resultant network is then trained with a siamese architecture @cite_23 , minimizing the contrastive loss. In @cite_17 , the MAC layer is replaced by trainable Generalized-Mean (GEM) pooling layer which significantly boosts retrieval accuracy. In @cite_27 , trained a siamese architecture with ranking loss to enhance the RMAC representation. The recent NetVLAD @cite_13 consists of a standard CNN followed by a Vector of Locally Aggregated Descriptors (VLAD) layer that aggregates the last convolutional features into a fixed dimensional signature and its parameters are trainable via back-propagation. @cite_11 proposed SIAM-FV: an end-to-end architecture which aggregates deep descriptors using Fisher Vector Pooling.\"","":""}
{"id":"2897282582","dialogue":"\"Artificial sound event detection (SED) aims to mimic the human ability to perceive and understand what is happening in the surroundings. Nowadays, deep learning offers valuable techniques for this goal, such as convolutional neural networks (CNNs). The capsule neural network (CapsNet) architecture has been recently introduced in the image processing field with the intent to overcome some of the known limitations of CNNs, specifically regarding the scarce robustness to affine transformations (i.e., perspective, size, and orientation) and the detection of overlapped images. This motivated the authors to employ CapsNets to deal with the polyphonic SED task, in which multiple sound events occur simultaneously. Specifically, we propose to exploit the capsule units to represent a set of distinctive properties for each individual sound event. Capsule units are connected through a so-called dynamic routing that encourages learning part-whole relationships and improves the detection performance in a polyphonic context. This paper reports extensive evaluations carried out on three publicly available datasets, showing how the CapsNet-based algorithm not only outperforms standard CNNs but also achieves the best results with respect to the state-of-the-art algorithms.\"","summary":"\"The use of deep learning models has been motivated by the increased availability of datasets and computational resources and resulted in significant performance improvements. The methods based on CNNs and RNNs have established the new state-of-the-art performance on the SED task, thanks to the capabilities to learn the non-linear relationship between time-frequency features of the audio signal and a target vector representing sound events. In @cite_3 , the authors show how local'' patterns can be learned by a CNN and can be exploited to improve the performance of detection and classification of non-speech acoustic events occurring in conversation scenes, in particular compared to a FNN-based system which processes multiple resolution spectrograms in parallel.\"","":""}
{"id":"2897282582","dialogue":"\"Artificial sound event detection (SED) aims to mimic the human ability to perceive and understand what is happening in the surroundings. Nowadays, deep learning offers valuable techniques for this goal, such as convolutional neural networks (CNNs). The capsule neural network (CapsNet) architecture has been recently introduced in the image processing field with the intent to overcome some of the known limitations of CNNs, specifically regarding the scarce robustness to affine transformations (i.e., perspective, size, and orientation) and the detection of overlapped images. This motivated the authors to employ CapsNets to deal with the polyphonic SED task, in which multiple sound events occur simultaneously. Specifically, we propose to exploit the capsule units to represent a set of distinctive properties for each individual sound event. Capsule units are connected through a so-called dynamic routing that encourages learning part-whole relationships and improves the detection performance in a polyphonic context. This paper reports extensive evaluations carried out on three publicly available datasets, showing how the CapsNet-based algorithm not only outperforms standard CNNs but also achieves the best results with respect to the state-of-the-art algorithms.\"","summary":"\"The combination of the CNN structure with recurrent units has increased the detection performance by taking advantage of the characteristics of each architecture. This is the case of convolutional recurrent neural networks (CRNNs) @cite_27 , which provided state-of-the-art performance especially in the case of polyphonic SED. CRNNs consolidate the CNN property of local shift invariance with the capability to model short and long term temporal dependencies provided by the RNN layers. This architecture has been also employed in almost all of the most performing algorithms proposed in the recent editions of research challenges such as the IEEE Audio and Acoustic Signal Processing (AASP) Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) @cite_31 . On the other hand, if the datasets are not sufficiently large, problems such as overfitting can be encountered with these models, which typically are composed of a considerable number of free-parameters (i.e., more than 1M).\"","":""}
{"id":"2897282582","dialogue":"\"Artificial sound event detection (SED) aims to mimic the human ability to perceive and understand what is happening in the surroundings. Nowadays, deep learning offers valuable techniques for this goal, such as convolutional neural networks (CNNs). The capsule neural network (CapsNet) architecture has been recently introduced in the image processing field with the intent to overcome some of the known limitations of CNNs, specifically regarding the scarce robustness to affine transformations (i.e., perspective, size, and orientation) and the detection of overlapped images. This motivated the authors to employ CapsNets to deal with the polyphonic SED task, in which multiple sound events occur simultaneously. Specifically, we propose to exploit the capsule units to represent a set of distinctive properties for each individual sound event. Capsule units are connected through a so-called dynamic routing that encourages learning part-whole relationships and improves the detection performance in a polyphonic context. This paper reports extensive evaluations carried out on three publicly available datasets, showing how the CapsNet-based algorithm not only outperforms standard CNNs but also achieves the best results with respect to the state-of-the-art algorithms.\"","summary":"\"The authors of @cite_21 show that CapsNets outperform state-of-the-art approaches based on CNNs for digit recognition in the MNIST dataset case study. They designed the CapsNet to learn how to assign the suited partial information to the entities that the neural network has to predict in the final classification. This property should overcome the limitations of solutions such as max-pooling, currently employed in CNNs to provide local translation invariance, but often reported to cause an excessive information loss. Theoretically, the introduction of the dynamic routing can supply invariances for any property captured by a capsule, allowing also to adequately train the model without requiring extensive data augmentation or dedicated domain adaptation procedures.\"","":""}
{"id":"2949660381","dialogue":"\"Barcodes are used in many commercial applications, thus fast and robust reading is important. There are many different types of barcodes, some of them look similar while others are completely different. In this paper we introduce new fast and robust deep learning detector based on semantic segmentation approach. It is capable of detecting barcodes of any type simultaneously both in the document scans and in the wild by means of a single model. The detector achieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset with detection rate 0.995. Moreover, developed detector can deal with more complicated object shapes like very long but narrow or very small barcodes. The proposed approach can also identify types of detected barcodes and performs at real-time speed on CPU environment being much faster than previous state-of-the-art approaches.\"","summary":"The early work in the domain of barcode detection from 2D images was motivated by the wide spread of mobile phones with cameras. @cite_4 proposes a method for finding 2D barcodes via corner detection and 1D barcodes through spiral scanning. @cite_5 introduces another method for 1D barcode detection based on decoding. Both approaches however require certain guidance from the user.","":""}
{"id":"2949660381","dialogue":"\"Barcodes are used in many commercial applications, thus fast and robust reading is important. There are many different types of barcodes, some of them look similar while others are completely different. In this paper we introduce new fast and robust deep learning detector based on semantic segmentation approach. It is capable of detecting barcodes of any type simultaneously both in the document scans and in the wild by means of a single model. The detector achieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset with detection rate 0.995. Moreover, developed detector can deal with more complicated object shapes like very long but narrow or very small barcodes. The proposed approach can also identify types of detected barcodes and performs at real-time speed on CPU environment being much faster than previous state-of-the-art approaches.\"","summary":"\"In more recent papers authors pay more attention on developing solutions which can be done automatically with less user guidance. @cite_8 finds regions with high difference between x and y derivatives","":""}
{"id":"2949660381","dialogue":"\"Barcodes are used in many commercial applications, thus fast and robust reading is important. There are many different types of barcodes, some of them look similar while others are completely different. In this paper we introduce new fast and robust deep learning detector based on semantic segmentation approach. It is capable of detecting barcodes of any type simultaneously both in the document scans and in the wild by means of a single model. The detector achieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset with detection rate 0.995. Moreover, developed detector can deal with more complicated object shapes like very long but narrow or very small barcodes. The proposed approach can also identify types of detected barcodes and performs at real-time speed on CPU environment being much faster than previous state-of-the-art approaches.\"","summary":"\"The work of Cresot , 2015 @cite_0 is a solid baseline for 1D barcode detection. They evaluated their approach on Muenster and on extended ArTe-Lab 1D Medium barcode database (Artelab) provided by Zamberletti @cite_3 outperforming him on both datasets. The solution in @cite_0 seems to outperform @cite_1 despite it is hard to compare as they were evaluated on different datasets using slightly different metrics. Cresot's algorithm detects dark bars of barcodes using Maximal Stable Extremal Regions (MSER) followed by finding imaginary perpendicular to bars center line in Hough space. In 2016 Cresot came with a new paper @cite_11 improving previous results using a new variant of Line Segment Detector instead of MSER, which they called Parallel Segment Detector. @cite_9 proposes another bars detection method for 1D barcode detection, which is reported to be absolutely precise in real-time applications.\"","":""}
{"id":"2952137661","dialogue":"\"We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.\"","summary":"\"The problem of speaker naming in movies has been explored by the computer vision and the speech communities. In the computer vision community, the speaker naming problem is usually considered as a face person naming problem, in which names are assigned to their corresponding faces on the screen @cite_31 @cite_20 @cite_38 @cite_11 @cite_35 . On the other hand, the speech community considered the problem as a speaker identification problem, which focuses on recognizing and clustering speakers rather than naming them @cite_33 @cite_21 . In this work, we aim to solve the problem of speaker naming in movies, in which we label each segment of the subtitles with its corresponding speaker name whether the speaker's face appeared on in the video or not.\"","":""}
{"id":"2952137661","dialogue":"\"We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.\"","summary":"\"In @cite_20 @cite_11 , the authors proposed a weakly supervised model depending on subtitles and a character list. They extracted textual cues from the dialog: first, second, and third person references, such as I'm Jack'', Hey, Jack!'', and Jack left''. Using a character list from IMDB, they mapped these references onto true names using minimum edit distance, and then they ascribed the references to face tracks. Other work removed the dependency on a true character list by determining all names through coreference resolution. However, this work also depended on the availability of scripts @cite_2 . In our model, we removed the dependency on both the true cast list and the script, which makes it easier to apply our model to other movies and TV shows.\"","":""}
{"id":"2952137661","dialogue":"\"We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.\"","summary":"\"On the other hand, talking faces have been used to improve speaker recognition and diarization in TV shows @cite_28 @cite_17 @cite_29 . In the case of @cite_10 , they modeled the problem of speaker naming as facial recognition to identify speakers in news broadcasts. This work leveraged optical character recognition to read the broadcasters' names that were displayed on screen, requiring the faces to already be annotated.\"","":""}
{"id":"2897622729","dialogue":"\"We present Nesterov-type acceleration techniques for Alternating Least Squares (ALS) methods applied to canonical tensor decomposition. While Nesterov acceleration turns gradient descent into an optimal first-order method for convex problems by adding a momentum term with a specific weight sequence, a direct application of this method and weight sequence to ALS results in erratic convergence behaviour or divergence. This is so because the tensor decomposition problem is non-convex and ALS is accelerated instead of gradient descent. We investigate how line search or restart mechanisms can be used to obtain effective acceleration. We first consider a cubic line search (LS) strategy for determining the momentum weight, showing numerically that the combined Nesterov-ALS-LS approach is competitive with or superior to other recently developed nonlinear acceleration techniques for ALS, including acceleration by nonlinear conjugate gradients (NCG) and LBFGS. As an alternative, we consider various restarting techniques, some of which are inspired by previously proposed restarting mechanisms for Nesterov's accelerated gradient method. We study how two key parameters, the momentum weight and the restart condition, should be set. Our extensive empirical results show that the Nesterov-accelerated ALS methods with restart can be dramatically more efficient than the stand-alone ALS or Nesterov accelerated gradient method, when problems are ill-conditioned or accurate solutions are required. The resulting methods perform competitively with or superior to existing acceleration methods for ALS, and additionally enjoy the benefit of being much simpler and easier to implement. On a large and ill-conditioned 71 x 1000 x 900 tensor consisting of readings from chemical sensors used for tracking hazardous gases, the restarted Nesterov-ALS method outperforms any of the existing methods by a large factor.\"","summary":"\"Nesterov's technique has also been used to accelerate non-gradient based methods. In @cite_12 it was used to accelerate ADMM, and @cite_13 used it to accelerate an approximate Newton method.\"","":""}
{"id":"2897622729","dialogue":"\"We present Nesterov-type acceleration techniques for Alternating Least Squares (ALS) methods applied to canonical tensor decomposition. While Nesterov acceleration turns gradient descent into an optimal first-order method for convex problems by adding a momentum term with a specific weight sequence, a direct application of this method and weight sequence to ALS results in erratic convergence behaviour or divergence. This is so because the tensor decomposition problem is non-convex and ALS is accelerated instead of gradient descent. We investigate how line search or restart mechanisms can be used to obtain effective acceleration. We first consider a cubic line search (LS) strategy for determining the momentum weight, showing numerically that the combined Nesterov-ALS-LS approach is competitive with or superior to other recently developed nonlinear acceleration techniques for ALS, including acceleration by nonlinear conjugate gradients (NCG) and LBFGS. As an alternative, we consider various restarting techniques, some of which are inspired by previously proposed restarting mechanisms for Nesterov's accelerated gradient method. We study how two key parameters, the momentum weight and the restart condition, should be set. Our extensive empirical results show that the Nesterov-accelerated ALS methods with restart can be dramatically more efficient than the stand-alone ALS or Nesterov accelerated gradient method, when problems are ill-conditioned or accurate solutions are required. The resulting methods perform competitively with or superior to existing acceleration methods for ALS, and additionally enjoy the benefit of being much simpler and easier to implement. On a large and ill-conditioned 71 x 1000 x 900 tensor consisting of readings from chemical sensors used for tracking hazardous gases, the restarted Nesterov-ALS method outperforms any of the existing methods by a large factor.\"","summary":"\"Nesterov's accelerated gradient method is known to exhibit oscillatory behavior on convex problems. An interesting discussion on this is provided in @cite_5 which formulates an ODE as the continuous time analogue of Nesterov's method. Such oscillatory behavior happens when the method approaches convergence, and can be alleviated by restarting the algorithm using the current iterate as the initial solution, usually resetting the sequence of momentum weights to its initial state close to 0. In @cite_5 an explanation is provided of why resetting the momentum weight to a small value is effective using the ODE formulation of Nesterov's accelerated gradient descent. In @cite_17 the use of adaptive restarting was explored for convex problems, and @cite_9 explored the use of adaptive restarting and adaptive momentum weight for nonlinear systems of equations resulting from finite element approximation of PDEs. Our work is the first study of a general Nesterov-accelerated ALS scheme.\"","":""}
{"id":"2897622729","dialogue":"\"We present Nesterov-type acceleration techniques for Alternating Least Squares (ALS) methods applied to canonical tensor decomposition. While Nesterov acceleration turns gradient descent into an optimal first-order method for convex problems by adding a momentum term with a specific weight sequence, a direct application of this method and weight sequence to ALS results in erratic convergence behaviour or divergence. This is so because the tensor decomposition problem is non-convex and ALS is accelerated instead of gradient descent. We investigate how line search or restart mechanisms can be used to obtain effective acceleration. We first consider a cubic line search (LS) strategy for determining the momentum weight, showing numerically that the combined Nesterov-ALS-LS approach is competitive with or superior to other recently developed nonlinear acceleration techniques for ALS, including acceleration by nonlinear conjugate gradients (NCG) and LBFGS. As an alternative, we consider various restarting techniques, some of which are inspired by previously proposed restarting mechanisms for Nesterov's accelerated gradient method. We study how two key parameters, the momentum weight and the restart condition, should be set. Our extensive empirical results show that the Nesterov-accelerated ALS methods with restart can be dramatically more efficient than the stand-alone ALS or Nesterov accelerated gradient method, when problems are ill-conditioned or accurate solutions are required. The resulting methods perform competitively with or superior to existing acceleration methods for ALS, and additionally enjoy the benefit of being much simpler and easier to implement. On a large and ill-conditioned 71 x 1000 x 900 tensor consisting of readings from chemical sensors used for tracking hazardous gases, the restarted Nesterov-ALS method outperforms any of the existing methods by a large factor.\"","summary":"\"Several ALS-specific nonlinear acceleration techniques have been developed recently as discussed in the introduction @cite_11 @cite_8 @cite_7 . These algorithms often have complex forms and incur significant computational overhead. Our Nesterov-ALS scheme is simple and straightforward to implement, and only incurs a small amount of computational overhead.\"","":""}
{"id":"2897826647","dialogue":"\"Text simplification (TS) can be viewed as monolingual translation task, translating between text variations within a single language. Recent neural TS models draw on insights from neural machine translation to learn lexical simplification and content reduction using encoder-decoder model. But different from neural machine translation, we cannot obtain enough ordinary and simplified sentence pairs for TS, which are expensive and time-consuming to build. Target-side simplified sentences plays an important role in boosting fluency for statistical TS, and we investigate the use of simplified sentences to train, with no changes to the network architecture. We propose to pair simple training sentence with a synthetic ordinary sentence via back-translation, and treating this synthetic data as additional training data. We train encoder-decoder model using synthetic sentence pairs and original sentence pairs, which can obtain substantial improvements on the available WikiLarge data and WikiSmall data compared with the state-of-the-art methods.\"","summary":"\"Compared with SMT, neural machine translation (NMT) has shown to produce state-of-the-art results @cite_1 @cite_11 . The central approach of NMT is an encoder-decoder architecture implemented by recurrent neural networks, which can represent the input sequence as a vector, and then decode that vector into an output sequence. Therefore, NMT models were used for text simplification task, and achieved good results @cite_9 @cite_10 @cite_2 . The main limitation of the aforementioned NMT models for text simplification depended on the parallel ordinary-simplified sentence pairs. Because ordinary-simplified sentence pairs are expensive and time-consuming to build, the available largest data is EW-SEW that only have 296,402 sentence pairs. The dataset is insufficiency for NMT model if we want to NMT model can obtain the best parameters. Considering simplified data plays an important role in boosting fluency for phrase-based text simplification, and we investigate the use of simplified data for text simplification. We are the first to show that we can effectively adapt neural translation models for text simplifiation with simplified corpora.\"","":""}
{"id":"2896778177","dialogue":"\"Distance metric learning (DML) aims to find an appropriate way to reveal the underlying data relationship. It is critical in many machine learning, pattern recognition and data mining algorithms, and usually require large amount of label information (such as class labels or pair triplet constraints) to achieve satisfactory performance. However, the label information may be insufficient in real-world applications due to the high-labeling cost, and DML may fail in this case. Transfer metric learning (TML) is able to mitigate this issue for DML in the domain of interest (target domain) by leveraging knowledge information from other related domains (source domains). Although achieved a certain level of development, TML has limited success in various aspects such as selective transfer, theoretical understanding, handling complex data, big data and extreme cases. In this survey, we present a systematic review of the TML literature. In particular, we group TML into different categories according to different settings and metric transfer strategies, such as direct metric approximation, subspace approximation, distance approximation, and distribution approximation. A summarization and insightful discussion of the various TML approaches and their applications will be presented. Finally, we indicate some challenges and provide possible future directions.\"","summary":"\"TML is quite related to transfer subspace learning (TSL) @cite_26 @cite_111 or transfer feature learning (TFL) @cite_19 . An early work on TSL is presented in @cite_26 that finds a low-dimensional latent space, where the distribution difference between the source and target domain is minimized. This algorithm is conducted in a transductive manner and not convenient to derive a representation for new samples. This issue is tackled by @cite_41 , where a generic regularization framework is proposed for TSL based on Bregman divergence @cite_60 . A low-rank TSL (LTSL) framework is proposed in @cite_125 @cite_113 , where the subspace is found by reconstructing the projected target data using the projected source data under the low-rank representation @cite_88 @cite_43 theme. The main advantage of the framework is that only relevant source data are utilized to find the subspace and noisy information can be filtered out. That is, it can avoid negative transfer. The framework is further extended in @cite_67 to help recover missing modality in the target domain and improved in @cite_63 by exploiting both low-rank and sparse structures on the reconstruction matrix.\"","":""}
{"id":"2896778177","dialogue":"\"Distance metric learning (DML) aims to find an appropriate way to reveal the underlying data relationship. It is critical in many machine learning, pattern recognition and data mining algorithms, and usually require large amount of label information (such as class labels or pair triplet constraints) to achieve satisfactory performance. However, the label information may be insufficient in real-world applications due to the high-labeling cost, and DML may fail in this case. Transfer metric learning (TML) is able to mitigate this issue for DML in the domain of interest (target domain) by leveraging knowledge information from other related domains (source domains). Although achieved a certain level of development, TML has limited success in various aspects such as selective transfer, theoretical understanding, handling complex data, big data and extreme cases. In this survey, we present a systematic review of the TML literature. In particular, we group TML into different categories according to different settings and metric transfer strategies, such as direct metric approximation, subspace approximation, distance approximation, and distribution approximation. A summarization and insightful discussion of the various TML approaches and their applications will be presented. Finally, we indicate some challenges and provide possible future directions.\"","summary":"\"TFL is very similar to TSL and a representative method is presented in @cite_19 , where the typical MMD is modified to take both the marginal and class-conditional distributions into consideration. More recent works on TFL are built upon the powerful deep feature learning. For example, considering that the features in deep neural networks are usually general in the first layers and task-specific in higher layers, @cite_84 propose the deep adaptation networks (DAN), which frozes the general layers in convolutional neural networks (CNN) @cite_17 and only conduct adaption in the task-specific layers. Besides, multi-kernel MMD (MK-MMD) @cite_38 is employed to improve kernel selection in MMD. In DAN, only the marginal distribution difference between the source and target domains is exploited. This is improved by the joint adaptation networks (JAN) @cite_0 , which is able to reduce the joint distribution divergence using a proposed joint MMD (JMMD). The JMMD can involve both the input features and output labels in domain adaptation. The constrained deep TSL @cite_120 method can also exploit the joint distribution and the target domain knowledge is incorporated gradually during a progressive transfer procedure.\"","":""}
{"id":"2966206672","dialogue":"\"After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I O bandwidth and the energy consumption is dominated by I O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4× relative to uncompressed data and a gain of 60 over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.\"","summary":"\"There are several methods out there describing hardware accelerators which exploit feature map sparsity to reduce computation: Cnvlutin @cite_3 , SCNN @cite_29 , Cambricon-X @cite_28 , NullHop @cite_13 , Eyeriss @cite_8 , EIE @cite_26 . Their focus is on power gating or skipping some of the operations and memory accesses. While this automatically entails defining a scheme to feed the data into the system, minimizing the bandwidth was not the primary objective of any of them. They all use one of three methods: Zero-RLE (used in SCNN): A simple run-length encoding for the zero values, i.e. a single prefix bit followed by the number of zero-values or the non-zero value. Zero-free neuron array format (ZFNAf) (used in Cnvlutin): Similarly to the widely-used compressed sparse row (CSR) format, non-zero elements are encoded with an offset and their value. Compressed column storage (CCS) format (e.g. used in EIE): Similar to ZFNAf, but the offsets are stored in relative form, thus requiring less bits to store them. Few bits are sufficient, and in case they are all exhausted, a zero-value can be encoded as if it was non-zero.\"","":""}
{"id":"2966206672","dialogue":"\"After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I O bandwidth and the energy consumption is dominated by I O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4× relative to uncompressed data and a gain of 60 over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.\"","summary":"\"Most compression methods are focusing on minimizing the model size. Most of them are very complex (area) to implement in hardware and need large dictionaries. One such method, deep compression @cite_30 , combines pruning, trained clustering-based quantization, and Huffman coding. Most of these steps involved cannot be applied to the intermediate feature map, which change for every inference as opposed to the weights which are static and can be optimized off-line. Furthermore, applying Huffman coding---while being optimal---implies storing a large dictionary (typically several MB). Similar issues arise when using Lempel-Ziv-Welch (LZW) coding @cite_20 @cite_25 as present in e.g. the ZIP compression scheme, where the dictionary is encoded in the compressed data stream. This makes it unsuitable for a lightweight and energy-efficient VLSI implementation @cite_1 @cite_7 .\"","":""}
{"id":"2966206672","dialogue":"\"After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I O bandwidth and the energy consumption is dominated by I O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4× relative to uncompressed data and a gain of 60 over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.\"","summary":"\"The most directly comparable approach, cDMA @cite_14 , describes a hardware-friendly compression scheme to reduce the data size of intermediate feature maps. Their target application differs in that their main goal is to allow faster offloading of the feature maps from GPU to CPU memory through the PCIe bandwidth bottleneck during training, thereby enabling larger batch sizes and deeper and wider networks without sacrificing performance. They propose to use , which takes a block of 32 activation values, and generates a 32-bit mask where only the bits to the non-zero values are set. The non-zero values are transferred after the mask. This provides the main advantage over Zero-RLE that the resulting data volume is independent of how the values of the feature maps are serialized while also providing small compression ratio advantages. Note that this is a special case of Zero-RLE with a maximum zero burst length of 1.\"","":""}
{"id":"2966206672","dialogue":"\"After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I O bandwidth and the energy consumption is dominated by I O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4× relative to uncompressed data and a gain of 60 over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.\"","summary":"\"For this work, we build on a method known in the area of texture compression for GPUs, @cite_27 , fuse it with sparsity-focused compression methods, and evaluate the resulting compression algorithm on intermediate feature maps to show compression ratios of 4.4 and 2.8 for 8 ,bit AlexNet and SqueezeNet, respectively.\"","":""}
{"id":"2897082611","dialogue":"\"Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges. From the viewpoint of automated systems, the urgent questions are: (a) How can models of moral scenarios and blameworthiness be extracted and learnt automatically from data? (b) How can judgements be computed tractably, given the split-second decision points faced by the system? By building on deep tractable probabilistic learning, we propose a learning regime for inducing models of such scenarios automatically from data and reasoning tractably from them. We report on experiments that compare our system with human judgement in three illustrative domains: lung cancer staging, teamwork management, and trolley problems.\"","summary":"\"As mentioned before, we do not motivate new definitions for moral responsibility here, but draw on HK, which, in turn, is based upon @cite_20 and the work on causality in @cite_7 . Their framework is also related to the intentions model in @cite_6 which considers predictions about the moral permissibility of actions via influence diagrams, though there is no emphasis on learning or tractability. In fact, the use of tractable architectures for decision-making itself is recent (see, e.g. @cite_10 @cite_17 ). The authors in @cite_4 learn PSDDs over preference rankings (as opposed to decision-making scenarios more generally), though their approach does not take account of different preferences in different contexts.\"","":""}
{"id":"2897082611","dialogue":"\"Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges. From the viewpoint of automated systems, the urgent questions are: (a) How can models of moral scenarios and blameworthiness be extracted and learnt automatically from data? (b) How can judgements be computed tractably, given the split-second decision points faced by the system? By building on deep tractable probabilistic learning, we propose a learning regime for inducing models of such scenarios automatically from data and reasoning tractably from them. We report on experiments that compare our system with human judgement in three illustrative domains: lung cancer staging, teamwork management, and trolley problems.\"","summary":"\"An important part of learning a model of moral decision-making is in learning a utility function. This is often referred to as (IRL) @cite_23 or @cite_29 . Our current implementation considers a simple approach for learning utilities (similar to @cite_12 ), but more involved paradigms such as those above could indeed have been used.\"","":""}
{"id":"2897082611","dialogue":"\"Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges. From the viewpoint of automated systems, the urgent questions are: (a) How can models of moral scenarios and blameworthiness be extracted and learnt automatically from data? (b) How can judgements be computed tractably, given the split-second decision points faced by the system? By building on deep tractable probabilistic learning, we propose a learning regime for inducing models of such scenarios automatically from data and reasoning tractably from them. We report on experiments that compare our system with human judgement in three illustrative domains: lung cancer staging, teamwork management, and trolley problems.\"","summary":"\"Our contributions here are related to the body of work surrounding MIT's Moral Machine website @cite_3 . For example, @cite_8 build on the theory of @cite_30 by developing a computational model of moral decision-making whose predictions they test against Moral Machine data. Their focus is on learning abstract moral principles via hierarchical Bayesian inference, and although our framework can be used to these ends, it is also flexible with respect to different contexts, and allows constraints on learnt models. @cite_13 develop a method of aggregating the preferences of all participants (again, a secondary feature of our system) in order to make a given decision. However, due to the large numbers of such preference orderings, tractability issues arise and so sampling must be used.\"","":""}
{"id":"2894597154","dialogue":"\"This paper focuses on density-based clustering, particularly the Density Peak (DP) algorithm and the one based on density-connectivity DBSCAN; and proposes a new method which takes advantage of the individual strengths of these two methods to yield a density-based hierarchical clustering algorithm. Our investigation begins with formally defining the types of clusters DP and DBSCAN are designed to detect; and then identifies the kinds of distributions that DP and DBSCAN individually fail to detect all clusters in a dataset. These identified weaknesses inspire us to formally define a new kind of clusters and propose a new method called DC-HDP to overcome these weaknesses to identify clusters with arbitrary shapes and varied densities. In addition, the new method produces a richer clustering result in terms of hierarchy or dendrogram for better cluster structures understanding. Our empirical evaluation results show that DC-HDP produces the best clustering results on 14 datasets in comparison with 7 state-of-the-art clustering algorithms.\"","summary":"\"Many variants of DBSCAN have been attempted to overcome the weakness of detecting clusters with varied densities. OPTICS @cite_21 draws a reachability'' plot based on the @math -nearest neighbour distance. In the @math -axis of the plot, adjacent points follow close to each other such that point @math is the closest to @math in terms of the reachability distance'' The reachability-distance'' of object @math to object @math is the greater one between the core distance'' of @math and the distance between @math and @math . The core distance'' of @math is the minimum @math that makes @math a core'' object (the distance to its @math -nearest neighbour, @math ). . The reachability distance for each point is shown in @math -axis. Since clusters centre normally has a higher density or lower reachability distance than the cluster boundaries, each cluster is visible as a valley'' in this plot. Then a hierarchical method can be used to extract different clusters. The overall clustering performance depends on the hierarchical method employed on the reachability plot.\"","":""}
{"id":"2895590610","dialogue":"\"We propose two novel samplers to produce high-quality samples from a given (un-normalized) probability density. The sampling is achieved by transforming a reference distribution to the target distribution with neural networks, which are trained separately by minimizing two kinds of Stein Discrepancies, and hence our method is named as Stein neural sampler. Theoretical and empirical results suggest that, compared with traditional sampling schemes, our samplers share the following three advantages: 1. Being asymptotically correct; 2. Experiencing less convergence issue in practice; 3. Generating samples instantaneously.\"","summary":"\"The fusion of deep learning and sampling is not new. @cite_0 proposed A-NICE-MC, where the proposal distribution in MCMC is, instead of domain-agnostic, adversarially trained using neural networks. Stein GAN also proposes to train a neural network to draw samples from given target distributions for probabilistic inference. Their method is by iteratively adjusting the weights according to the SVGD updates. From a GAN perspective, in each iteration of Stein GAN, the discriminator is performing a two sample test between the currently generated samples and the one-step updated samples by SVGD. This method generalized SVGD to training neural networks and it is minimizing the Kullback-Leibler divergence between the sampling distribution and the target inside a RKHS.\"","":""}
{"id":"2951285883","dialogue":"\"In this paper, we consider a class of constrained clustering problems of points in @math , where @math could be rather high. A common feature of these problems is that their optimal clusterings no longer have the locality property (due to the additional constraints), which is a key property required by many algorithms for their unconstrained counterparts. To overcome the difficulty caused by the loss of locality, we present in this paper a unified framework, called Peeling-and-Enclosing (PnE) , to iteratively solve two variants of the constrained clustering problems, constrained @math -means clustering ( @math -CMeans) and constrained @math -median clustering ( @math -CMedian). Our framework is based on two standalone geometric techniques, called Simplex Lemma and Weaker Simplex Lemma , for @math -CMeans and @math -CMedian, respectively. The simplex lemma (or weaker simplex lemma) enables us to efficiently approximate the mean (or median) point of an unknown set of points by searching a small-size grid, independent of the dimensionality of the space, in a simplex (or the surrounding region of a simplex), and thus can be used to handle high dimensional data. If @math and @math are fixed numbers, our framework generates, in nearly linear time ( i.e., @math ), @math @math -tuple candidates for the @math mean or median points, and one of them induces a @math -approximation for @math -CMeans or @math -CMedian, where @math is the number of points. Combining this unified framework with a problem-specific selection algorithm (which determines the best @math -tuple candidate), we obtain a @math -approximation for each of the constrained clustering problems. We expect that our technique will be applicable to other constrained clustering problems without locality.\"","summary":"\"As for the hardness of the problem, Dasgupta @cite_3 showed that it is NP-hard for @math -means clustering in high dimensional space even if @math ; @cite_45 proved that there is no PTAS for @math -means clustering if both @math and @math are large, unless @math . Guruswami and Indyk @cite_16 showed that it is NP-hard to obtain any PTAS for @math -median clustering if @math is not a constant and @math is @math .\"","":""}
{"id":"2951285883","dialogue":"\"In this paper, we consider a class of constrained clustering problems of points in @math , where @math could be rather high. A common feature of these problems is that their optimal clusterings no longer have the locality property (due to the additional constraints), which is a key property required by many algorithms for their unconstrained counterparts. To overcome the difficulty caused by the loss of locality, we present in this paper a unified framework, called Peeling-and-Enclosing (PnE) , to iteratively solve two variants of the constrained clustering problems, constrained @math -means clustering ( @math -CMeans) and constrained @math -median clustering ( @math -CMedian). Our framework is based on two standalone geometric techniques, called Simplex Lemma and Weaker Simplex Lemma , for @math -CMeans and @math -CMedian, respectively. The simplex lemma (or weaker simplex lemma) enables us to efficiently approximate the mean (or median) point of an unknown set of points by searching a small-size grid, independent of the dimensionality of the space, in a simplex (or the surrounding region of a simplex), and thus can be used to handle high dimensional data. If @math and @math are fixed numbers, our framework generates, in nearly linear time ( i.e., @math ), @math @math -tuple candidates for the @math mean or median points, and one of them induces a @math -approximation for @math -CMeans or @math -CMedian, where @math is the number of points. Combining this unified framework with a problem-specific selection algorithm (which determines the best @math -tuple candidate), we obtain a @math -approximation for each of the constrained clustering problems. We expect that our technique will be applicable to other constrained clustering problems without locality.\"","summary":"\"Besides the traditional clustering models, Balcan considered the problem of finding the clustering with small difference from the unknown ground truth @cite_29 @cite_33 .\"","":""}
{"id":"2892472836","dialogue":"\"Open story generation is the problem of automatically creating a story for any domain without retraining. Neural language models can be trained on large corpora across many domains and then used to generate stories. However, stories generated via language models tend to lack direction and coherence. We introduce a policy gradient reinforcement learning approach to open story generation that learns to achieve a given narrative goal state. In this work, the goal is for a story to end with a specific type of event, given in advance. However, a reward based on achieving the given goal is too sparse for effective learning. We use reward shaping to provide the reinforcement learner with a partial reward at every step. We show that our technique can train a model that generates a story that reaches the goal 94 of the time and reduces model perplexity. A human subject evaluation shows that stories generated by our technique are perceived to have significantly higher plausible event ordering and plot coherence over a baseline language modeling technique without perceived degradation of overall quality, enjoyability, or local causality.\"","summary":"\"Early story and plot generation systems relied on symbolic planning @cite_19 @cite_8 @cite_11 @cite_0 @cite_12 @cite_3 or case-based reasoning @cite_22 @cite_13 . These techniques only generated stories for predetermined, well-defined domains, conflating the robustness of manually-engineered knowledge with algorithm suitability. Regardless, symbolic planners in particular are able to provide long-term causal coherence. Early machine learning story generation techniques include textual case-based reasoning trained on blogs @cite_6 and probabilistic graphical models learned from crowdsourced example stories @cite_23 .\"","":""}
{"id":"2892472836","dialogue":"\"Open story generation is the problem of automatically creating a story for any domain without retraining. Neural language models can be trained on large corpora across many domains and then used to generate stories. However, stories generated via language models tend to lack direction and coherence. We introduce a policy gradient reinforcement learning approach to open story generation that learns to achieve a given narrative goal state. In this work, the goal is for a story to end with a specific type of event, given in advance. However, a reward based on achieving the given goal is too sparse for effective learning. We use reward shaping to provide the reinforcement learner with a partial reward at every step. We show that our technique can train a model that generates a story that reaches the goal 94 of the time and reduces model perplexity. A human subject evaluation shows that stories generated by our technique are perceived to have significantly higher plausible event ordering and plot coherence over a baseline language modeling technique without perceived degradation of overall quality, enjoyability, or local causality.\"","summary":"\"Reinforcement learning (RL) addresses some of the issues of preserving coherence for text generation when sampling from a neural language model. Additionally, it provides the ability to specify a goal. Reinforcement learning @cite_10 is a technique that is used to solve a Markov decision process (MDP). An MDP is a tuple @math where @math is the set of possible world states, @math is the set of possible actions, @math is a transition function @math , @math is a reward function @math , and @math is a discount factor @math . The result of reinforcement learning is a policy @math , which defines which actions should be taken in each state in order to maximize the expected future reward. The policy gradient learning approach to reinforcement learning directly optimizes the parameters of a policy model, which is represented as a neural network. One model-free policy gradient approach, REINFORCE @cite_16 , learns a policy by sampling from the current policy and backpropagating any reward received through the weights of the policy model.\"","":""}
{"id":"2951622944","dialogue":"\"Sampling-based Motion Planners (SMPs) have become increasingly popular as they provide collision-free path solutions regardless of obstacle geometry in a given environment. However, their computational complexity increases significantly with the dimensionality of the motion planning problem. Adaptive sampling is one of the ways to speed up SMPs by sampling a particular region of a configuration space that is more likely to contain an optimal path solution. Although there are a wide variety of algorithms for adaptive sampling, they rely on hand-crafted heuristics; furthermore, their performance decreases significantly in high-dimensional spaces. In this paper, we present a neural network-based adaptive sampler for motion planning called Deep Sampling-based Motion Planner (DeepSMP). DeepSMP generates samples for SMPs and enhances their overall speed significantly while exhibiting efficient scalability to higher-dimensional problems. DeepSMP's neural architecture comprises of a Contractive AutoEncoder which encodes given workspaces directly from a raw point cloud data, and a Dropout-based stochastic deep feedforward neural network which takes the workspace encoding, start and goal configuration, and iteratively generates feasible samples for SMPs to compute end-to-end collision-free optimal paths. DeepSMP is not only consistently computationally efficient in all tested environments but has also shown remarkable generalization to completely unseen environments. We evaluate DeepSMP on multiple planning problems including planning of a point-mass robot, rigid-body, 6-link robotic manipulator in various 2D and 3D environments. The results show that on average our method is at least 7 times faster in point-mass and rigid-body case and about 28 times faster in 6-link robot case than the existing state-of-the-art.\"","summary":"\"RRT* @cite_6 extends RRTs to guarantee asymptotic optimality by incrementally rewiring the RRT graph connections such that the shortest path is asymptotically guaranteed @cite_6 . However, to determine an @math -near optimal path in @math dimensions, roughly @math samples are required, which makes RRT* no better than grid search methods @cite_9 . Likewise, experiments in @cite_0 @cite_3 also confirmed that RRT* exhibits slow convergence rates to optimal path solution in higher-dimensional spaces. The following sections discusses various existing biased adaptive sampling methods to speed up the convergence rate of SMPs to compute optimal near-optimal path solution.\"","":""}
{"id":"2951622944","dialogue":"\"Sampling-based Motion Planners (SMPs) have become increasingly popular as they provide collision-free path solutions regardless of obstacle geometry in a given environment. However, their computational complexity increases significantly with the dimensionality of the motion planning problem. Adaptive sampling is one of the ways to speed up SMPs by sampling a particular region of a configuration space that is more likely to contain an optimal path solution. Although there are a wide variety of algorithms for adaptive sampling, they rely on hand-crafted heuristics; furthermore, their performance decreases significantly in high-dimensional spaces. In this paper, we present a neural network-based adaptive sampler for motion planning called Deep Sampling-based Motion Planner (DeepSMP). DeepSMP generates samples for SMPs and enhances their overall speed significantly while exhibiting efficient scalability to higher-dimensional problems. DeepSMP's neural architecture comprises of a Contractive AutoEncoder which encodes given workspaces directly from a raw point cloud data, and a Dropout-based stochastic deep feedforward neural network which takes the workspace encoding, start and goal configuration, and iteratively generates feasible samples for SMPs to compute end-to-end collision-free optimal paths. DeepSMP is not only consistently computationally efficient in all tested environments but has also shown remarkable generalization to completely unseen environments. We evaluate DeepSMP on multiple planning problems including planning of a point-mass robot, rigid-body, 6-link robotic manipulator in various 2D and 3D environments. The results show that on average our method is at least 7 times faster in point-mass and rigid-body case and about 28 times faster in 6-link robot case than the existing state-of-the-art.\"","summary":"\"@cite_3 proposed the Informed-RRT* algorithm which takes an initial solution from RRT* algorithm to define an ellipsoidal region from which new samples are drawn to minimize the initial solution for a given cost function. Although Informed-RRT* demonstrated enhanced convergence towards an optimal solution, this method suffers in situations where finding an initial path solution takes most of the computation time. To address this limitation, proposed Batch Informed Trees (BIT*) @cite_16 . BIT* is an incremental graph search technique where an ellipsoidal subset, containing configurations to update the graph, is incrementally enlarged. BIT* is shown empirically to outperform prior methods such as RRT* and Informed-RRT*. However, confining a graph search to ellipsoidal region slows down the performance of an algorithm in maze-like scenarios especially where the start and goal configurations are very close to each other, but the path among them traverses a complicated maze stretching waypoints far away from the goal. Furthermore, such a method would not translate to non-stationary environments or unseen environments.\"","":""}
{"id":"2951622944","dialogue":"\"Sampling-based Motion Planners (SMPs) have become increasingly popular as they provide collision-free path solutions regardless of obstacle geometry in a given environment. However, their computational complexity increases significantly with the dimensionality of the motion planning problem. Adaptive sampling is one of the ways to speed up SMPs by sampling a particular region of a configuration space that is more likely to contain an optimal path solution. Although there are a wide variety of algorithms for adaptive sampling, they rely on hand-crafted heuristics; furthermore, their performance decreases significantly in high-dimensional spaces. In this paper, we present a neural network-based adaptive sampler for motion planning called Deep Sampling-based Motion Planner (DeepSMP). DeepSMP generates samples for SMPs and enhances their overall speed significantly while exhibiting efficient scalability to higher-dimensional problems. DeepSMP's neural architecture comprises of a Contractive AutoEncoder which encodes given workspaces directly from a raw point cloud data, and a Dropout-based stochastic deep feedforward neural network which takes the workspace encoding, start and goal configuration, and iteratively generates feasible samples for SMPs to compute end-to-end collision-free optimal paths. DeepSMP is not only consistently computationally efficient in all tested environments but has also shown remarkable generalization to completely unseen environments. We evaluate DeepSMP on multiple planning problems including planning of a point-mass robot, rigid-body, 6-link robotic manipulator in various 2D and 3D environments. The results show that on average our method is at least 7 times faster in point-mass and rigid-body case and about 28 times faster in 6-link robot case than the existing state-of-the-art.\"","summary":"\"Many approaches exist that use learning to improve classical SMPs computationally. A recent method called a Lightning Framework @cite_17 stored paths into a lookup table and used a learned heuristic to write new paths as well as to read and repair old paths. Another similar framework by @cite_10 is an experience-based strategy to cache experiences in a graph instead of individual trajectories. Although these approaches exhibit superior performance in higher-dimensional spaces when compared to conventional planning methods, lookup tables are memory inefficient and incapable of generalizing well to new planning problems. @cite_13 proposed a reinforcement learning-based method to bias samples in discretized workspaces. However, reinforcement learning-based approaches are known for their slow convergence as they require a large number of interactive experiences.\"","":""}
{"id":"2951122667","dialogue":"\"Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Class-agnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.\"","summary":"\"It aims to classify and locate objects in an image. Generally, the methods of object detection can be divided into two main classes: two-stage methods and one-stage methods. Two-stage methods firstly extract some candidate object proposals from an image and then classify these candidate proposals into the specific object categories. R-CNN @cite_9 and its variants (e.g., Fast RCNN @cite_14 and Faster RCNN @cite_17 ) are the most representative frameworks among the two-stage methods. Based on R-CNN series, researchers have done many improvements @cite_21 @cite_22 @cite_12 . To accelerate detection speed, Dai @cite_21 proposed R-FCN which uses position-sensitive feature maps for proposal classification and bounding box regression. To output multi-scale feature maps with strong semantics, Lin @cite_22 proposed feature pyramid network (FPN) based on skip-layer connection and top-down pathway. Recently, Cai @cite_12 trained a sequence of object detectors with increasing IoU thresholds to improve detection quality.\"","":""}
{"id":"2951122667","dialogue":"\"Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Class-agnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.\"","summary":"\"One-stage methods directly predict object class and bounding box in a single network. YOLO @cite_44 and SSD @cite_16 are two of the earliest proposed one-stage methods. After that, many variants are proposed @cite_32 @cite_29 @cite_20 @cite_19 . DSSD @cite_32 and RON @cite_29 use the encoder-decoder network to add context information for multi-scale object detection. To train object detector from scratch, DSOD @cite_20 uses dense layer-wise connections on SSD for deep supervision. Instead of using in-network feature maps of different resolutions for multi-scale object detection, STDN @cite_19 uses scale-transferrable module to generate different high-resolution feature maps from last feature map. To solve class imbalance in the training stage, RetinaNet @cite_23 introduces focal loss to downweight the contribution of easy samples.\"","":""}
{"id":"2951122667","dialogue":"\"Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Class-agnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.\"","summary":"\"It aims to predict the semantic label of each pixel in an image, which has achieved significant progress based on fully convolutional networks (i.e., FCN @cite_47 ). Generally, the methods of semantic segmentation can be also divided into two main classes: encoder-decoder methods and spatial pyramid methods. Encoder-decoder methods contain two subnetworks: an encoder subnetwork and a decoder subnetwork. The encoder subnetwork extracts strong semantic features and reduces spatial resolution of feature maps, which is usually based on the classical CNN models (e.g., VGG @cite_2 , ResNet @cite_45 , DenseNet @cite_28 ) pre-trained on ImageNet @cite_8 . The decoder subnetwork gradually upsamples the feature maps of encoder subnetwork. DeconvNet @cite_25 and SegNet @cite_31 use max-pooling indices of the encoder subnetwork to upsample the feature maps. To extract context information, some methods @cite_13 @cite_33 @cite_3 adopt skip-layer connection to combine the feature maps from the encoder and decoder subnetworks.\"","":""}
{"id":"2951122667","dialogue":"\"Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Class-agnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.\"","summary":"\"Spatial pyramid methods adopt the idea of spatial pyramid pooling @cite_39 to extract multi-scale information from the last output feature maps. Chen @cite_46 @cite_1 @cite_42 @cite_40 proposed to use multiple convolutional layers of different atrous rates in parallel (called ASPP) to extract multi-scale features. Instead of using convolutional layers of different atrous rates, Zhao @cite_30 proposed pyramid pooling module (called PSPnet), which downsamples and upsamples the feature maps in parallel. Yang @cite_40 proposed to use dense connection to cover object scale range densely.\"","":""}
{"id":"2951122667","dialogue":"\"Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Class-agnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.\"","summary":"\"It aims to simultaneously detect objects and predict pixel semantic labels by a single network. Recently, researchers have done some attempts. Yao @cite_36 proposed to use the graphical model to holistic scene understanding. Teichmann @cite_11 proposed to join object detection and semantic segmentation by sharing the encoder subnetwork. Kokkinos @cite_4 also proposed to integrate multiple computer vision tasks together. Mao @cite_18 found that joint semantic segmentation and pedestrian detection can help improve performance of pedestrian detection. The similar conclusion is also demonstrated by SDS-RCNN @cite_0 . Meanwhile, joint instance semantic segmentation and object detection is also proposed @cite_41 . Recently, Dvornik @cite_41 proposed a real-time framework (called BlitzNet) for joint object detection and semantic segmentation. It is based on the encoder-decoder network, where each layer of the decoder is used to detect objects of different scales and multi-scale fused layer is used for semantic segmentation.\"","":""}
{"id":"2950669295","dialogue":"\"Procedural content generation via Machine Learning (PCGML) is the umbrella term for approaches that generate content for games via machine learning. One of the benefits of PCGML is that, unlike search or grammar-based PCG, it does not require hand authoring of initial content or rules. Instead, PCGML relies on existing content and black box models, which can be difficult to tune or tweak without expert knowledge. This is especially problematic when a human designer needs to understand how to manipulate their data or models to achieve desired results. We present an approach to Explainable PCGML via Design Patterns in which the design patterns act as a vocabulary and mode of interaction between user and model. We demonstrate that our technique outperforms non-explainable versions of our system in interactions with five expert designers, four of whom lack any machine learning expertise.\"","summary":"\"Procedural content generation via Machine Learning @cite_10 is a relatively new field, focused on generating content through machine learning methods. The majority of PCGML approaches represent black box methods, without any prior approach focused on explainability or co-creativity. We note some discussion in the survey paper on potential collaborative approaches. Summerville explored adapting levels to players, but no work to our knowledge looks at adapting models to individual designers.\"","":""}
{"id":"2950669295","dialogue":"\"Procedural content generation via Machine Learning (PCGML) is the umbrella term for approaches that generate content for games via machine learning. One of the benefits of PCGML is that, unlike search or grammar-based PCG, it does not require hand authoring of initial content or rules. Instead, PCGML relies on existing content and black box models, which can be difficult to tune or tweak without expert knowledge. This is especially problematic when a human designer needs to understand how to manipulate their data or models to achieve desired results. We present an approach to Explainable PCGML via Design Patterns in which the design patterns act as a vocabulary and mode of interaction between user and model. We demonstrate that our technique outperforms non-explainable versions of our system in interactions with five expert designers, four of whom lack any machine learning expertise.\"","summary":"\"Super Mario Bros. (SMB) represents a common area of research into PCGML @cite_24 @cite_14 @cite_8 @cite_17 . Beyond explainability, our approach differs from prior SMB PCGML approaches in terms of representation quality and the size of generated content. We focus on the generation of individual level sections instead of entire levels in order to better afford collaborative level building @cite_15 . Second, prior approaches have abstracted away the possible level components into higher order groups. For example, treating all enemy types as equivalent and ignoring decorative elements. We make use of a rich representation of all possible level components and an ordering that allows our approach to place decorative elements appropriately.\"","":""}
{"id":"2950669295","dialogue":"\"Procedural content generation via Machine Learning (PCGML) is the umbrella term for approaches that generate content for games via machine learning. One of the benefits of PCGML is that, unlike search or grammar-based PCG, it does not require hand authoring of initial content or rules. Instead, PCGML relies on existing content and black box models, which can be difficult to tune or tweak without expert knowledge. This is especially problematic when a human designer needs to understand how to manipulate their data or models to achieve desired results. We present an approach to Explainable PCGML via Design Patterns in which the design patterns act as a vocabulary and mode of interaction between user and model. We demonstrate that our technique outperforms non-explainable versions of our system in interactions with five expert designers, four of whom lack any machine learning expertise.\"","summary":"\"Design patterns represent a well-researched approach to game design @cite_3 . In theory, game design patterns describe general solutions to game design problems that occur across many different games. Game Design patterns have been used as heuristics in evolutionary PCG systems including in the domain of Super Mario Bros. @cite_24 . Researchers tend to derive game design patterns through either rigorous, cross-domain analysis @cite_21 or based upon their subjective interpretation of game structure. We embrace this subjectivity in our work by having designers create a language of game design patterns unique to themselves with which to interact with a PCGML system.\"","":""}
{"id":"2891804603","dialogue":"\"In many fields of social and industrial sciences, simulation is crucial in comprehending a target system. A major task in simulation is the estimation of optimal parameters to express the observed data need to directly elucidate the properties of the target system as a modeling based on the expert's domain knowledge. However, skilled human experts struggle to obtain the desired parameters. Data assimilation therefore becomes an unavoidable task to reduce the cost of simulator optimization. Another necessary task is extrapolation; in many practical cases, predictions based on simulation results will be often outside of the dominant range of a given data area, and this is referred to as the covariate shift. This paper focuses on a regression problem with covariate shift. While the parameter estimation for the covariate shift has been studied thoroughly in parametric and nonparametric settings, conventional statistical methods of parameter searching are not applicable in the data assimilation of the simulation owing to the properties of the likelihood function: intractable or nondifferentiable. Hence, we propose a novel framework of Bayesian inference based on kernel mean embedding. This framework allows for predictions in covariate shift situations, and its effectiveness is evaluated in both synthetic numerical experiments and a widely used production simulator reproducing real-world manufacturing factories.\"","summary":"\"A series of studies exist for covariate shift assuming that a regression function is an analytical function @cite_4 @cite_2 @cite_1 , such as kernel ridge regression @cite_0 . In our problem, however, we assume that the functional relation of the regression model @math is only given as a nonanalytical function: a simulation. The difference between kernel ridge regression and the proposed method with formulation is presented in .\"","":""}
{"id":"2891804603","dialogue":"\"In many fields of social and industrial sciences, simulation is crucial in comprehending a target system. A major task in simulation is the estimation of optimal parameters to express the observed data need to directly elucidate the properties of the target system as a modeling based on the expert's domain knowledge. However, skilled human experts struggle to obtain the desired parameters. Data assimilation therefore becomes an unavoidable task to reduce the cost of simulator optimization. Another necessary task is extrapolation; in many practical cases, predictions based on simulation results will be often outside of the dominant range of a given data area, and this is referred to as the covariate shift. This paper focuses on a regression problem with covariate shift. While the parameter estimation for the covariate shift has been studied thoroughly in parametric and nonparametric settings, conventional statistical methods of parameter searching are not applicable in the data assimilation of the simulation owing to the properties of the likelihood function: intractable or nondifferentiable. Hence, we propose a novel framework of Bayesian inference based on kernel mean embedding. This framework allows for predictions in covariate shift situations, and its effectiveness is evaluated in both synthetic numerical experiments and a widely used production simulator reproducing real-world manufacturing factories.\"","summary":"\"Kernel mean embedding is a framework to map distributions into a reproducing kernel Hilbert space (RKHS) @math as a feature space @cite_7 . Figure shows a schematic illustration of the kernel mean embedding. In this section, we briefly review three applications of kernel mean embedding: kernel ABC, kernel sum rule, and kernel herding. The detailed formulations of these methods are described in with the proposed method.\"","":""}
{"id":"2890241187","dialogue":"\"Recent progress in logic programming (e.g., the development of the Answer Set Programming paradigm) has made it possible to teach it to general undergraduate and even middle high school students. Given the limited exposure of these students to computer science, the complexity of downloading, installing and using tools for writing logic programs could be a major barrier for logic programming to reach a much wider audience. We developed onlineSPARC, an online answer set programming environment with a self contained file system and a simple interface. It allows users to type edit logic programs and perform several tasks over programs, including asking a query to a program, getting the answer sets of a program, and producing a drawing animation based on the answer sets of a program.\"","summary":"\"As ASP has been applied to more and more problems, the importance of ASP software development tools has been realized by the community. Some integrated development environment (IDE) tools, e.g., APE @cite_12 , ASPIDE @cite_20 , iGROM @cite_16 and SeaLion @cite_15 have previously been developed. They provide a graphical user interface for users to carry out a sequence of tasks from editing an ASP program to debugging that program, easing the use of ASP significantly. However, the target audience of these tools is experienced software developers. Compared with the existing environments, our environment is online, self contained (i.e., fully independent of the users' local computers) and provides a very simple interface, focusing on teaching only. The interface is operable by any person who is able to use a typical web site and traverse a file system.\"","":""}
{"id":"2890241187","dialogue":"\"Recent progress in logic programming (e.g., the development of the Answer Set Programming paradigm) has made it possible to teach it to general undergraduate and even middle high school students. Given the limited exposure of these students to computer science, the complexity of downloading, installing and using tools for writing logic programs could be a major barrier for logic programming to reach a much wider audience. We developed onlineSPARC, an online answer set programming environment with a self contained file system and a simple interface. It allows users to type edit logic programs and perform several tasks over programs, including asking a query to a program, getting the answer sets of a program, and producing a drawing animation based on the answer sets of a program.\"","summary":"\"As for online systems, in addition to IDP mentioned earlier, there are several others. Both DLV and Clingo offer online environments ( http: asptut.gibbi.com and http: potassco.sourceforge.net clingo.html respectively) which provide an editor and a window to show the direct output of the execution of DLV and Clingo command, but provide no other functionalities. We also noted SWISH http: lpsdemo.interprolog.com which offers an online environment for Prolog and a more recent computer language Logic-based Production Systems @cite_10 . A recent online system LoIDE @cite_21 allows a user to edit ASP programs and find answer sets of the programs. LoIDE allows a programmer to highlight names in answer sets.\"","":""}
{"id":"2892371805","dialogue":"\"Collaborative filtering (CF) has been successfully employed by many modern recommender systems. Conventional CF-based methods use the user-item interaction data as the sole information source to recommend items to users. However","summary":"CF-based methods are known for suffering from cold start problems and data sparsity problems. Hybrid models that utilize auxiliary information on top of interaction data have increasingly gained attention. A few \"\"collaborative learning\"\"-based models","":""}
{"id":"2892371805","dialogue":"\"Collaborative filtering (CF) has been successfully employed by many modern recommender systems. Conventional CF-based methods use the user-item interaction data as the sole information source to recommend items to users. However","summary":"CF-based methods are known for suffering from cold start problems and data sparsity problems. Hybrid models that utilize auxiliary information on top of interaction data have increasingly gained attention. A few \"\"collaborative learning\"\"-based models","":""}
{"id":"2949302783","dialogue":"\"This research attempts to construct a network that can convert online and offline handwritten characters to each other. The proposed network consists of two Variational Auto-Encoders (VAEs) with a shared latent space. The VAEs are trained to generate online and offline handwritten Latin characters simultaneously. In this way, we create a cross-modal VAE (Cross-VAE). During training, the proposed Cross-VAE is trained to minimize the reconstruction loss of the two modalities, the distribution loss of the two VAEs, and a novel third loss called the space sharing loss. This third, space sharing loss is used to encourage the modalities to share the same latent space by calculating the distance between the latent variables. Through the proposed method mutual conversion of online and offline handwritten characters is possible. In this paper, we demonstrate the performance of the Cross-VAE through qualitative and quantitative analysis.\"","summary":"\"Recently, there are two common approaches that have become popular which use neural networks to learn latent representations, Encoder-Decoders and Generative Adversarial Networks (GAN) @cite_4 . Encoder-Decoders, such as an Autoencoder @cite_6 , compress data by encoding the inputs into a latent vector which is then uncompressed by the decoder. The Autoencoder is trained by minimizes the difference between the input and the output of the decoder. GANs take the opposite approach and use a generator, similar to an encoder, then uses a discriminator to maximize the authenticity of the generated data. Where Encoder-Decoders learn the latent representations directly, GANs learn to construct data from random latent representations.\"","":""}
{"id":"2949302783","dialogue":"\"This research attempts to construct a network that can convert online and offline handwritten characters to each other. The proposed network consists of two Variational Auto-Encoders (VAEs) with a shared latent space. The VAEs are trained to generate online and offline handwritten Latin characters simultaneously. In this way, we create a cross-modal VAE (Cross-VAE). During training, the proposed Cross-VAE is trained to minimize the reconstruction loss of the two modalities, the distribution loss of the two VAEs, and a novel third loss called the space sharing loss. This third, space sharing loss is used to encourage the modalities to share the same latent space by calculating the distance between the latent variables. Through the proposed method mutual conversion of online and offline handwritten characters is possible. In this paper, we demonstrate the performance of the Cross-VAE through qualitative and quantitative analysis.\"","summary":"\"For offline and online handwriting conversion, it has traditionally been done using classical feature-based methods @cite_17 but there has been some recent work using neural networks. @cite_9 used a CNN and RNN-based Encoder-Decoder network for handwriting trajectory recovery. Attempts were also made using neural networks to identify graph features @cite_20 and for sequential stroke prediction using regression CNNs @cite_21 .\"","":""}
{"id":"2949479297","dialogue":"\"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction ( 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.\"","summary":"\"@cite_10 introduced RNN into language modeling to handle arbitrary-length sequences in computing conditional probability @math . They demonstrated that the RNN language model outperformed the Kneser-Ney smoothed 5-gram language model @cite_3 , which is a sophisticated @math -gram language model.\"","":""}
{"id":"2949479297","dialogue":"\"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction ( 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.\"","summary":"\"@cite_9 drastically improved the performance of language modeling by applying LSTM and the dropout technique @cite_27 . @cite_9 applied dropout to all the connections except for recurrent connections but @cite_39 proposed variational inference based dropout to regularize recurrent connections. @cite_19 demonstrated that the standard LSTM can achieve superior performance by selecting appropriate hyperparameters. Finally, @cite_26 introduced DropConnect @cite_13 and averaged SGD @cite_7 into the LSTM language model and achieved state-of-the-art perplexities on PTB and WT2. For WT103, @cite_17 found that QRNN @cite_40 , which is a faster architecture than LSTM, achieved the best perplexity. Our experimental results show that the proposed char @math -MS-vec improved the performance of these state-of-the-art language models.\"","":""}
{"id":"2949479297","dialogue":"\"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction ( 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.\"","summary":"\"@cite_31 introduced character information into RNN language models. They applied CNN to character embeddings for word embedding construction. Their proposed method achieved perplexity competitive with the basic LSTM language model @cite_9 even though its parameter size is small. @cite_35 also applied CNN to construct word embeddings from character embeddings. They indicated that CNN also positively affected the LSTM language model in a huge corpus. @cite_33 proposed a method concatenating character embeddings with a word embedding to use character information. In contrast to these methods, we used character @math -gram embeddings to construct word embeddings. To compare the proposed method to these methods, we combined the CNN proposed by @cite_31 with the state-of-the-art LSTM language model (AWD-LSTM) @cite_26 . Our experimental results indicate that the proposed method outperformed the method using character embeddings (charCNN in Table ).\"","":""}
{"id":"2949479297","dialogue":"\"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction ( 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.\"","summary":"\"Some previous studies focused on boosting the performance of language models during testing @cite_16 @cite_34 . For example, @cite_34 proposed dynamic evaluation that updates model parameters based on the given correct sequence during evaluation. Although these methods might further improve our proposed language model, we omitted these methods since it is unreasonable to obtain correct outputs in applications such as machine translation.\"","":""}
{"id":"2949479297","dialogue":"\"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction ( 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.\"","summary":"\"Previous studies proposed various methods to construct word embeddings. @cite_0 applied Recursive Neural Networks to construct word embeddings from morphemic embeddings. @cite_45 applied bidirectional LSTMs to character embeddings for word embedding construction. On the other hand, @cite_12 and @cite_44 focused on character @math -gram. They demonstrated that the sum of character @math -gram embeddings outperformed ordinary word embeddings. In addition, @cite_44 found that the sum of character @math -gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM.\"","":""}
{"id":"2949479297","dialogue":"\"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction ( 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.\"","summary":"\"As an encoder, previous studies argued that additive composition, which computes the (weighted) sum of embeddings, is a suitable method theoretically @cite_32 and empirically @cite_30 @cite_24 . In this paper, we used multi-dimensional self-attention to construct word embeddings because it can be interpreted as an element-wise weighted sum. Through experiments, we indicated that multi-dimensional self-attention is superior to the summation and standard self-attention as an encoder.\"","":""}
{"id":"2952193948","dialogue":"\"We present a meta-learning approach based on learning an adaptive, high-dimensional loss function that can generalize across multiple tasks and different model architectures. We develop a fully differentiable pipeline for learning a loss function targeted at maximizing the performance of an optimizee trained using this loss function. We observe that the loss landscape produced by our learned loss significantly improves upon the original task-specific loss. We evaluate our method on supervised and reinforcement learning tasks. Furthermore, we show that our pipeline is able to operate in sparse reward and self-supervised reinforcement learning scenarios.\"","summary":"\"Meta-learning originates in the concept of learning to learn @cite_4 @cite_16 @cite_31 . Recently, there has a been a wide interest in finding ways to improve learning speeds and generalization to new tasks through meta-learning. The main directions of the research in this area can be divided into learning representations that can be easily adapted to new tasks @cite_11 , learning unsupervised rules that can be transferred between tasks @cite_3 @cite_23 , learning optimizer policies that transform policy updates with respect to known loss or reward functions @cite_2 @cite_24 @cite_29 @cite_8 , or learning loss reward landscapes @cite_13 @cite_22 .\"","":""}
{"id":"2952193948","dialogue":"\"We present a meta-learning approach based on learning an adaptive, high-dimensional loss function that can generalize across multiple tasks and different model architectures. We develop a fully differentiable pipeline for learning a loss function targeted at maximizing the performance of an optimizee trained using this loss function. We observe that the loss landscape produced by our learned loss significantly improves upon the original task-specific loss. We evaluate our method on supervised and reinforcement learning tasks. Furthermore, we show that our pipeline is able to operate in sparse reward and self-supervised reinforcement learning scenarios.\"","summary":"\"Our framework falls into the category of learning loss landscapes; similar to @cite_2 , we aim at learning a separate optimization procedure that can be applied to various optimizee models. However, in contrast to @cite_2 and @cite_1 , our framework does not require a specific recurrent architecture of the optimizer and can operate without an explicit external loss or reward function during test time. Furthermore, as our learned loss functions are independent of the models to be optimized, they can be easily transferred to other optimizee models, in contrast to @cite_11 , where the learned representation can not be separated from the original model of the optimizee.\"","":""}
{"id":"2952193948","dialogue":"\"We present a meta-learning approach based on learning an adaptive, high-dimensional loss function that can generalize across multiple tasks and different model architectures. We develop a fully differentiable pipeline for learning a loss function targeted at maximizing the performance of an optimizee trained using this loss function. We observe that the loss landscape produced by our learned loss significantly improves upon the original task-specific loss. We evaluate our method on supervised and reinforcement learning tasks. Furthermore, we show that our pipeline is able to operate in sparse reward and self-supervised reinforcement learning scenarios.\"","summary":"\"A range of recent works demonstrate advantages of meta-learning for improving exploration strategies in RL settings, especially in the presence of sparse rewards. @cite_27 , an agent is trained to mimic expert demonstrations while only having access to a sparse reward signal during test time. @cite_26 and @cite_21 , a structured latent exploration space is learned from prior experience, which enables fast exploration in novel tasks. @cite_5 proposes a method for automatically learning potential-based reward shaping by learning the Q-function parameters during the meta-training phase, such that at meta-test time the Q-function can adapt quickly to new tasks. In our work, we also demonstrate that we can significantly improve the RL sample efficiency by training our meta-loss to optimize an actor policy, even when providing only limited or no reward information to the learned loss function at test time.\"","":""}
{"id":"2952193948","dialogue":"\"We present a meta-learning approach based on learning an adaptive, high-dimensional loss function that can generalize across multiple tasks and different model architectures. We develop a fully differentiable pipeline for learning a loss function targeted at maximizing the performance of an optimizee trained using this loss function. We observe that the loss landscape produced by our learned loss significantly improves upon the original task-specific loss. We evaluate our method on supervised and reinforcement learning tasks. Furthermore, we show that our pipeline is able to operate in sparse reward and self-supervised reinforcement learning scenarios.\"","summary":"\"Closest to our method are the works on @cite_30 , @cite_14 and @cite_13 . In contrast to using an evolutionary approach as in @cite_30 , we design a differentiable framework and describe a way to optimize the loss function with gradient descent in both supervised and reinforcement learning settings. In @cite_14 , instead of learning a differentiable loss function directly, a teacher network is trained to predict parameters of a manually designed loss function, whereas each new loss function class requires a new teacher network design and training. Our method does not require manual design of the loss function parameterization as our loss functions are learned entirely from data. Finally, in @cite_13 a is learned to provide a value function conditioned on a task, used to train an actor policy. Although training a meta-critic in the supervised setting reduces to learning a loss function similar to our work, in the reinforcement learning setting we show that it is possible to use learned loss functions to optimize policies directly with gradient descent.\"","":""}
{"id":"2952771416","dialogue":"\"In this paper, we propose a deep, globally normalized topic model that incorporates structural relationships connecting documents in socially generated corpora, such as online forums. Our model (1) captures discursive interactions along observed reply links in addition to traditional topic information, and (2) incorporates latent distributed representations arranged in a deep architecture, which enables a GPU-based mean-field inference procedure that scales efficiently to large data. We apply our model to a new social media dataset consisting of 13M comments mined from the popular internet forum Reddit, a domain that poses significant challenges to models that do not account for relationships connecting user comments. We evaluate against existing methods across multiple metrics including perplexity and metadata prediction, and qualitatively analyze the learned interaction patterns.\"","summary":"\"Many topic models such as LDA @cite_7 treat documents as independent mixtures, yet this approach fails to model how comments interact with one another throughout a larger discourse if such connections exist in the data. Other work has considered modeling hierarchy in topics @cite_2 . These models form hierarchical representations of topics themselves, but still treat documents as independent. While this approach can succeed in learning topics of various granularities, it does not explicitly track how topics interact in the context of a nested conversation.\"","":""}
{"id":"2952771416","dialogue":"\"In this paper, we propose a deep, globally normalized topic model that incorporates structural relationships connecting documents in socially generated corpora, such as online forums. Our model (1) captures discursive interactions along observed reply links in addition to traditional topic information, and (2) incorporates latent distributed representations arranged in a deep architecture, which enables a GPU-based mean-field inference procedure that scales efficiently to large data. We apply our model to a new social media dataset consisting of 13M comments mined from the popular internet forum Reddit, a domain that poses significant challenges to models that do not account for relationships connecting user comments. We evaluate against existing methods across multiple metrics including perplexity and metadata prediction, and qualitatively analyze the learned interaction patterns.\"","summary":"\"Some approaches such as Pairwise-Link-LDA and Link-PSLA-LDA @cite_6 attempt to model interactions among documents in an arbitrary graph, albeit with important drawbacks. The former models every possible pairwise link between comments, and the latter models links as a bipartite graph, limiting its ability to scale to large tree-structured threads. Similar work on Topic-Link LDA @cite_9 models link probabilities conditioned on both topic similarity and an authorship model, yet this approach is poorly suited to high volume, semi-anonymous online domains. Other studies have leveraged reply-structures on Reddit in the context of predicting persuasion , but DDTM differs in its generative, unsupervised approach.\"","":""}
{"id":"2952771416","dialogue":"\"In this paper, we propose a deep, globally normalized topic model that incorporates structural relationships connecting documents in socially generated corpora, such as online forums. Our model (1) captures discursive interactions along observed reply links in addition to traditional topic information, and (2) incorporates latent distributed representations arranged in a deep architecture, which enables a GPU-based mean-field inference procedure that scales efficiently to large data. We apply our model to a new social media dataset consisting of 13M comments mined from the popular internet forum Reddit, a domain that poses significant challenges to models that do not account for relationships connecting user comments. We evaluate against existing methods across multiple metrics including perplexity and metadata prediction, and qualitatively analyze the learned interaction patterns.\"","summary":"\"DDTM's emission potentials are similar to those of Replicated Softmax @cite_5 , an undirected model based on a Restricted Boltzmann Machine. Unlike LDA-style models, RS does not assign a topic to each word, but instead builds a distributed representation. In this setting, a single word can be likely under two different topics, both of which are present, and lend probability mass to that word. LDA-style models by contrast would require the topics to compete for the word.\"","":""}
{"id":"2950546634","dialogue":"\"Navigating in search and rescue environments is challenging, since a variety of terrains has to be considered. Hybrid driving-stepping locomotion, as provided by our robot Momaro, is a promising approach. Similar to other locomotion methods, it incorporates many degrees of freedom---offering high flexibility but making planning computationally expensive for larger environments. We propose a navigation planning method, which unifies different levels of representation in a single planner. In the vicinity of the robot, it provides plans with a fine resolution and a high robot state dimensionality. With increasing distance from the robot, plans become coarser and the robot state dimensionality decreases. We compensate this loss of information by enriching coarser representations with additional semantics. Experiments show that the proposed planner provides plans for large, challenging scenarios in feasible time.\"","summary":"\"Planning for systems with high-dimensional motion flexibility quickly reaches its limits for larger environments since the search space grows exponentially. Similar to multiresolution planning, several approaches utilize multiple representations with different planning dimensionalities to decrease planning complexity. @cite_5 generate an initial plan in a low-dimensional search space and replan in the high-dimensional search space by only considering those states that are part of the low-dimensional plan. @cite_4 plan a path in a low-dimensional search space and only switch to high-dimensional planning in those areas where low-dimensional planning cannot find a solution. Similarly, @cite_9 plan in 2D and switch to high-dimensional planning in the robot vicinity and at key points. As described for multiresolution planning, planning with multiple robot configuration dimensionalities might lead to wrong or bad plans, since a low-dimensional robot representation might assess challenging situations wrongly.\"","":""}
{"id":"2950546634","dialogue":"\"Navigating in search and rescue environments is challenging, since a variety of terrains has to be considered. Hybrid driving-stepping locomotion, as provided by our robot Momaro, is a promising approach. Similar to other locomotion methods, it incorporates many degrees of freedom---offering high flexibility but making planning computationally expensive for larger environments. We propose a navigation planning method, which unifies different levels of representation in a single planner. In the vicinity of the robot, it provides plans with a fine resolution and a high robot state dimensionality. With increasing distance from the robot, plans become coarser and the robot state dimensionality decreases. We compensate this loss of information by enriching coarser representations with additional semantics. Experiments show that the proposed planner provides plans for large, challenging scenarios in feasible time.\"","summary":"\"To achieve further planning acceleration, it is an obvious idea to combine multiresolution and multidimensional planning. However, only few works, such as by @cite_17 address this. Different planning dimensionalities and resolutions are applied by using different sets of motion primitives. A fine resolution is only considered close to the start and goal pose and close to obstacles. A high planning dimensionality is considered for states which will be reached within a given time interval. This allows the planner to provide detailed plans close to the robot while planning times stay feasible. The drawbacks of both, multiresolutional and multidimensional planning also apply to this work.\"","":""}
{"id":"2889976821","dialogue":"\"Abstract This paper introduces SensoGraph, a novel approach for fast sensory evaluation using two-dimensional geometric techniques. In the tasting sessions, the assessors follow their own criteria to place samples on a tablecloth, according to the similarity between samples. In order to analyse the data collected, first a geometric clustering is performed to each tablecloth, extracting connections between the samples. Then, these connections are used to construct a global similarity matrix. Finally, a graph drawing algorithm is used to obtain a 2D consensus graphic, which reflects the global opinion of the panel by (1) positioning closer those samples that have been globally perceived as similar and (2) showing the strength of the connections between samples. The proposal is validated by performing four tasting sessions, with three types of panels tasting different wines, and by developing a new software to implement the proposed techniques. The results obtained show that the graphics provide similar positionings of the samples as the consensus maps obtained by multiple factor analysis (MFA), further providing extra information about connections between samples, not present in any previous method. The main conclusion is that the use of geometric techniques provides information complementary to MFA, and of a different type. Finally, the method proposed is computationally able to manage a significantly larger number of assessors than MFA, which can be useful for the comparison of pictures by a huge number of consumers, via the Internet.\"","summary":"\"Thus, several alternative methods have arisen in the last years , aiming to provide a fast sensory positioning of a set of products by assessors who are not necessarily trained. Skipping the need to train the panellists allows to elude the need of waiting a long time before obtaining results, as well as the need of agreeing on particular attributes, which may become difficult when working with experts like wine professionals or chefs . Introduced by @cite_2 @cite_10 , Projective Mapping asks the assessors to position the presented samples on a two-dimensional space, usually a blank sheet of paper as tablecloth, following their own criteria: The more similar they perceive two samples, the closer they should position them, and vice versa . In those seminal works, the data were analysed by generalized procrustes analysis (GPA) and principal component analysis (PCA) , using the RV coefficient to compare the method with conventional profiling.\"","":""}
{"id":"2889976821","dialogue":"\"Abstract This paper introduces SensoGraph, a novel approach for fast sensory evaluation using two-dimensional geometric techniques. In the tasting sessions, the assessors follow their own criteria to place samples on a tablecloth, according to the similarity between samples. In order to analyse the data collected, first a geometric clustering is performed to each tablecloth, extracting connections between the samples. Then, these connections are used to construct a global similarity matrix. Finally, a graph drawing algorithm is used to obtain a 2D consensus graphic, which reflects the global opinion of the panel by (1) positioning closer those samples that have been globally perceived as similar and (2) showing the strength of the connections between samples. The proposal is validated by performing four tasting sessions, with three types of panels tasting different wines, and by developing a new software to implement the proposed techniques. The results obtained show that the graphics provide similar positionings of the samples as the consensus maps obtained by multiple factor analysis (MFA), further providing extra information about connections between samples, not present in any previous method. The main conclusion is that the use of geometric techniques provides information complementary to MFA, and of a different type. Finally, the method proposed is computationally able to manage a significantly larger number of assessors than MFA, which can be useful for the comparison of pictures by a huge number of consumers, via the Internet.\"","summary":"\"Projective Mapping has been successfully used with many different kinds of products, among which the application to wine stands out . Other examples of beverages analysed by these methods are beers , citrus juices , drinking waters high alcohol products , hot beverages , lemon iced teas , powdered juices , or smoothies . The book by @cite_6 details more products to which consumer based descriptive methodologies have been applied.\"","":""}
{"id":"2889818188","dialogue":"\"Despite its simplicity, bag-of-n-grams sen- tence representation has been found to excel in some NLP tasks. However, it has not re- ceived much attention in recent years and fur- ther analysis on its properties is necessary. We propose a framework to investigate the amount and type of information captured in a general- purposed bag-of-n-grams sentence represen- tation. We first use sentence reconstruction as a tool to obtain bag-of-n-grams representa- tion that contains general information of the sentence. We then run prediction tasks (sen- tence length, word content, phrase content and word order) using the obtained representation to look into the specific type of information captured in the representation. Our analysis demonstrates that bag-of-n-grams representa- tion does contain sentence structure level in- formation. However, incorporating n-grams with higher order n empirically helps little with encoding more information in general, except for phrase content information.\"","summary":"\"Studies such as @cite_0 show that bag-of-n-grams model is usually considered deficient in dealing with data sparsity and poor generalization. In particular, Le shows that CBOW and bigram models perform poorly in encoding paragraph information, and bigram representation generally outperforms unigram. This induces the question whether this result can be generalized to sentence representation and whether a bigger n leads to even better performance. However, a more detailed and systematic analysis has not been done on the properties of bag-of-n-grams embeddings.\"","":""}
{"id":"2951229489","dialogue":"\"We propose an efficient transfer learning method for adapting ImageNet pre-trained Convolutional Neural Network (CNN) to fine-grained image classification task. Conventional transfer learning methods typically face the trade-off between training time and accuracy. By adding \"\"attention module\"\" to each convolutional filters of the pre-trained network","summary":"we are able to rank and adjust the importance of each convolutional signal in an end-to-end pipeline. In this report","":""}
{"id":"2951229489","dialogue":"\"We propose an efficient transfer learning method for adapting ImageNet pre-trained Convolutional Neural Network (CNN) to fine-grained image classification task. Conventional transfer learning methods typically face the trade-off between training time and accuracy. By adding \"\"attention module\"\" to each convolutional filters of the pre-trained network","summary":"we are able to rank and adjust the importance of each convolutional signal in an end-to-end pipeline. In this report","":""}
{"id":"2891427406","dialogue":"\"Robotic grasp detection for novel objects is a challenging task, but for the last few years, deep learning based approaches have achieved remarkable performance improvements, up to 96.1 accuracy, with RGB-D data. In this paper, we propose fully convolutional neural network (FCNN) based methods for robotic grasp detection. Our methods also achieved state-of-the-art detection accuracy (up to 96.6 ) with state-of- the-art real-time computation time for high-resolution images (6-20ms per 360x360 image) on Cornell dataset. Due to FCNN, our proposed method can be applied to images with any size for detecting multigrasps on multiobjects. Proposed methods were evaluated using 4-axis robot arm with small parallel gripper and RGB-D camera for grasping challenging small, novel objects. With accurate vision-robot coordinate calibration through our proposed learning-based, fully automatic approach, our proposed method yielded 90 success rate.\"","summary":"\"Data-driven robotic grasp detection for novel object has been investigated extensively @cite_14 . Saxena proposed a machine learning based method to rank the best graspable location for all candidate image patches from different locations @cite_12 . Jiang proposed a 5D robotic grasp representation and further improved the work of Saxena by proposing a machine learning method to rank the best graspable image patch whose representation includes orientation and gripper distance among all candidates @cite_17 . The work of Jiang achieved the prediction accuracy of 60.5 time of 50 sec (50,000 ms) per image.\"","":""}
{"id":"2891427406","dialogue":"\"Robotic grasp detection for novel objects is a challenging task, but for the last few years, deep learning based approaches have achieved remarkable performance improvements, up to 96.1 accuracy, with RGB-D data. In this paper, we propose fully convolutional neural network (FCNN) based methods for robotic grasp detection. Our methods also achieved state-of-the-art detection accuracy (up to 96.6 ) with state-of- the-art real-time computation time for high-resolution images (6-20ms per 360x360 image) on Cornell dataset. Due to FCNN, our proposed method can be applied to images with any size for detecting multigrasps on multiobjects. Proposed methods were evaluated using 4-axis robot arm with small parallel gripper and RGB-D camera for grasping challenging small, novel objects. With accurate vision-robot coordinate calibration through our proposed learning-based, fully automatic approach, our proposed method yielded 90 success rate.\"","summary":"\"Redmon proposed a deep learning regressor based robotic grasp detection method based on the AlexNet @cite_19 that that yielded 84.4 with fast computation time (76 ms per image) @cite_11 . When performing robotic grasp regression and object classification together, image-wise prediction accuracy of 85.5 Kumra also proposed a real-time regression based grasp detection method using ResNet @cite_16 especially for multimodal information (RGB-D). Their method yielded up to 89.2 with fast computation time (103 ms per image) @cite_23 .\"","":""}
{"id":"2892244049","dialogue":"\"We study the problem of visibility-based exploration, reconstruction and surveillance in the context of supervised learning. Using a level set representation of data and information, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. We present realistic simulations on 2D and 3D urban environments.\"","summary":"\"The surveillance problem is related to the art gallery problem in computational geometry, where the task is to determine the minimum set of guards who can together observe a polygonal gallery. Vertex guards must be stationed at the vertices of the polygon, while point guards can be anywhere in the interior. For simply-connected polygonal scenes, Chv 'atal showed that @math vertex guards, where @math is the number of vertices, are sometimes necessary and always sufficient @cite_21 . For polygonal scenes with @math holes, @math point guards are sufficient @cite_27 @cite_15 . However, determining the optimal set of observers is NP-complete @cite_1 @cite_17 @cite_29 .\"","":""}
{"id":"2892244049","dialogue":"\"We study the problem of visibility-based exploration, reconstruction and surveillance in the context of supervised learning. Using a level set representation of data and information, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. We present realistic simulations on 2D and 3D urban environments.\"","summary":"propose an alternating minimization scheme for optimizing the visibility of @math observers @cite_25 . use a system of differential equations to optimize the location and orientation of @math sensors to maximize surveillance @cite_6 . Both works assume the number of sensors is given.","":""}
{"id":"2892244049","dialogue":"\"We study the problem of visibility-based exploration, reconstruction and surveillance in the context of supervised learning. Using a level set representation of data and information, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. We present realistic simulations on 2D and 3D urban environments.\"","summary":"\"For the exploration problem, a class of approaches pick new vantage points along shadow boundaries (aka frontiers), the boundary between free and occluded regions @cite_22 . propose a frontier-based approach for 2D polygonal environments which requires @math views, where @math is the number of reflex angles @cite_10 . For general 2D environments, @cite_3 @cite_23 @cite_32 use high order ENO interpolation to estimate curvature, which is then used to determine how far past the horizon to step. However, it is not necessarily optimal to pick only points along the shadow boundary, e.g. when the map is a star-shaped polygon @cite_10 .\"","":""}
{"id":"2892244049","dialogue":"\"We study the problem of visibility-based exploration, reconstruction and surveillance in the context of supervised learning. Using a level set representation of data and information, we train a convolutional neural network to determine vantage points that maximize visibility. We show that this method drastically reduces the on-line computational cost and determines a small set of vantage points that solve the problem. This enables us to efficiently produce highly-resolved and topologically accurate maps of complex 3D environments. We present realistic simulations on 2D and 3D urban environments.\"","summary":"\"There has been some attempts to incorporate deep learning into the exploration problem, but they are myopic and focus on navigation rather than exploration. The approach of @cite_7 terminates when there is no occlusion within view of the agent, even if the global map is still incomplete. Tai and Liu @cite_4 @cite_24 @cite_18 train agents to learn obstacle avoidance.\"","":""}
{"id":"2952029609","dialogue":"\"Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.\"","summary":"\"While graph structures are central tools for various learning tasks ( semi-supervised learning in @cite_15 @cite_13 ), how to design efficient graph convolution networks has become a popular research topic. Graph convolutional approaches are often categorized into spectral and non-spectral classes @cite_24 . The spectral approach first proposed by @cite_21 defines the convolution operation in Fourier domain. Later, @cite_12 enables localized filtering by applying efficient spectral filters, and @cite_19 employs Chebyshev expansion of the graph Laplacian to avoid the eigendecomposition. Recently, GCN is proposed in @cite_13 to simplify previous methods with first-order expansion and re-parameterization trick. Non-spectral approaches define convolution on graph by using the spatial connections directly. For instance, @cite_7 learns a weight matrix for each node degree, the work by @cite_20 defines multiple-hop neighborhoods by using the powers series of a transition matrix, and other authors @cite_23 extracted normalized neighborhoods that contain a fixed number of nodes.\"","":""}
{"id":"2952029609","dialogue":"\"Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.\"","summary":"\"A recent line of research is to generalize convolutions by making use of the patch operation @cite_14 and self-attention @cite_24 . As opposed to GCNs, these methods implicitly assign different importance weights to nodes of a same neighborhood, thus enabling a leap in model capacity. Particularly, Monti al @cite_14 presents mixture model CNNs to build CNN architectures on graphs using the patch operation, while the graph attention networks @cite_24 compute the hidden representations of each node on graph by attending over its neighbors following a self-attention strategy.\"","":""}
{"id":"2952029609","dialogue":"\"Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.\"","summary":"\"More recently, two kinds of sampling-based methods including GraphSAGE @cite_8 and FastGCN @cite_10 were developed for fast representation learning on graphs. To be specific, GraphSAGE computes node representations by sampling neighborhoods of each node and then performing a specific aggregator for information fusion. The FastGCN model interprets graph convolutions as integral transforms of embedding functions and samples the nodes in each layer independently. While our method is closely related to these methods, we develop a different sampling strategy in this paper. Compared to GraphSAGE that is node-wise, our method is based on layer-wise sampling as all neighborhoods are sampled as altogether, and thus can allow neighborhood sharing as illustrated in Figure . In contrast to FastGCN that constructs each layer independently, our model is capable of capturing the between-layer connections as the lower layer is sampled conditionally on the top one. We detail the comparisons in . Another related work is the control-variate-based method by @cite_5 . However, the sampling process of this method is node-wise, and the historical activations of nodes are required.\"","":""}
{"id":"2890571755","dialogue":"\"Deep generative models are tremendously successful in learning low-dimensional latent representations that well-describe the data. These representations, however, tend to much distort relationships between points, i.e. pairwise distances tend to not reflect semantic similarities well. This renders unsupervised tasks, such as clustering, difficult when working with the latent representations. We demonstrate that taking the geometry of the generative model into account is sufficient to make simple clustering algorithms work well over latent representations. Leaning on the recent finding that deep generative models constitute stochastically immersed Riemannian manifolds, we propose an efficient algorithm for computing geodesics (shortest paths) and computing distances in the latent space, while taking its distortion into account. We further propose a new architecture for modeling uncertainty in variational autoencoders, which is essential for understanding the geometry of deep generative models. Experiments show that the geodesic distance is very likely to reflect the internal structure of the data.\"","summary":"\", is based on a recent observation that deep generative models immerse random Riemannian manifolds @cite_9 . This implies a change in the way distances are measured in the latent space, which reveals a clustering structure. Unfortunately, practical algorithms for actually computing such distances are missing, and it is the main focus of the present paper. With such an algorithm in hand, clustering can be performed with high accuracy in the latent space of an off-the-shelf VAE.\"","":""}
{"id":"2949394659","dialogue":"\"Using a neural network architecture for depth map inference from monocular stabilized videos with application to UAV videos in rigid scenes, we propose a multi-range architecture for unconstrained UAV flight, leveraging flight data from sensors to make accurate depth maps for uncluttered outdoor environment. We try our algorithm on both synthetic scenes and real UAV flight data. Quantitative results are given for synthetic scenes with a slightly noisy orientation, and show that our multi-range architecture improves depth inference. Along with this article is a video that present our results more thoroughly.\"","summary":"\"Depth from vision is one of the problems studied with neural network, and has been addressed with a wide range of training solution. Some datasets @cite_16 @cite_4 allow a neural network to learn end-to-end depth or disparity @cite_22 @cite_7 @cite_9 . Reprojection error has also been used for unsupervised training for depth from a single image @cite_14 @cite_17 or for disparity between two frames of a stereo rig @cite_19 @cite_0 .\"","":""}
{"id":"2949394659","dialogue":"\"Using a neural network architecture for depth map inference from monocular stabilized videos with application to UAV videos in rigid scenes, we propose a multi-range architecture for unconstrained UAV flight, leveraging flight data from sensors to make accurate depth maps for uncluttered outdoor environment. We try our algorithm on both synthetic scenes and real UAV flight data. Quantitative results are given for synthetic scenes with a slightly noisy orientation, and show that our multi-range architecture improves depth inference. Along with this article is a video that present our results more thoroughly.\"","summary":"\"For depth from more complex movement from a monocular camera, current state of the art methods tend to use motion, and especially structure from motion, and most algorithm do not rely on deep learning @cite_20 @cite_3 @cite_12 . Prior knowledge w.r.t. scene is used to infer a sparse depth map with its density usually growing over time. These techniques also called SLAM are typically used with unstructured movement (translation and rotation with varying magnitudes), produce very sparse point-cloud based 3D maps and require heavy calculation to keep track of the scene structure and align newly detected 3D points to the existing ones.\"","":""}
{"id":"2952395293","dialogue":"\"Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on large-scale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.\"","summary":"\"Tracking-by-detection is becoming the most popular strategy for multi-object tracking. @cite_20 associated tracklets with detection in different ways according to their confidence values. Sanchez- @cite_12 exploited multiple detectors to improve tracking performance. They collected outputs from multiple detectors, during the so called over-detection process. Combining results from multiple detectors can improve the tracking performance but is not efficient for real-time applications. In contrast, our tracking framework needs only one detector and generates candidates from existing tracks. @cite_18 used a binary classifier and single object tracker for online multi-object tracking. They shared the feature maps for classification but still had a high computation complicity.\"","":""}
{"id":"2950839960","dialogue":"\"We present Semantic WordRank (SWR), an unsupervised method for generating an extractive summary of a single document. Built on a weighted word graph with semantic and co-occurrence edges, SWR scores sentences using an article-structure-biased PageRank algorithm with a Softplus function adjustment, and promotes topic diversity using spectral subtopic clustering under the Word-Movers-Distance metric. We evaluate SWR on the DUC-02 and SummBank datasets and show that SWR produces better summaries than the state-of-the-art algorithms over DUC-02 under common ROUGE measures. We then show that, under the same measures over SummBank, SWR outperforms each of the three human annotators (aka. judges) and compares favorably with the combined performance of all judges.\"","summary":"\"UniformLink @cite_6 builds a sentence graph on a set of similar documents, where a sentence's score is computed based on both with-in document score and cross-document score. URank @cite_24 uses a unified graph-based framework to study both single-document and multi-document summarizations.\"","":""}
{"id":"2950839960","dialogue":"\"We present Semantic WordRank (SWR), an unsupervised method for generating an extractive summary of a single document. Built on a weighted word graph with semantic and co-occurrence edges, SWR scores sentences using an article-structure-biased PageRank algorithm with a Softplus function adjustment, and promotes topic diversity using spectral subtopic clustering under the Word-Movers-Distance metric. We evaluate SWR on the DUC-02 and SummBank datasets and show that SWR produces better summaries than the state-of-the-art algorithms over DUC-02 under common ROUGE measures. We then show that, under the same measures over SummBank, SWR outperforms each of the three human annotators (aka. judges) and compares favorably with the combined performance of all judges.\"","summary":"\"@math @cite_1 , as well as @math @cite_2 , use a bipartite graph to represent a document and a different algorithm, Hyperlink-Induced Topic Search (HITS) @cite_23 , is used to score sentences. They both treat the summarization problem as an ILP problem, which maximizes the sentence importance, non-redundancy, and coherence at the same time. However, since ILP is an NP-hard problem, obtaining an exact solution to an ILP problem is intractable.\"","":""}
{"id":"2950839960","dialogue":"\"We present Semantic WordRank (SWR), an unsupervised method for generating an extractive summary of a single document. Built on a weighted word graph with semantic and co-occurrence edges, SWR scores sentences using an article-structure-biased PageRank algorithm with a Softplus function adjustment, and promotes topic diversity using spectral subtopic clustering under the Word-Movers-Distance metric. We evaluate SWR on the DUC-02 and SummBank datasets and show that SWR produces better summaries than the state-of-the-art algorithms over DUC-02 under common ROUGE measures. We then show that, under the same measures over SummBank, SWR outperforms each of the three human annotators (aka. judges) and compares favorably with the combined performance of all judges.\"","summary":"Submodularity optimization @cite_19 and Latent Semantic Analysis @cite_18 are two other widely used unsupervised techniques for extractive summarizations.","":""}
{"id":"2950839960","dialogue":"\"We present Semantic WordRank (SWR), an unsupervised method for generating an extractive summary of a single document. Built on a weighted word graph with semantic and co-occurrence edges, SWR scores sentences using an article-structure-biased PageRank algorithm with a Softplus function adjustment, and promotes topic diversity using spectral subtopic clustering under the Word-Movers-Distance metric. We evaluate SWR on the DUC-02 and SummBank datasets and show that SWR produces better summaries than the state-of-the-art algorithms over DUC-02 under common ROUGE measures. We then show that, under the same measures over SummBank, SWR outperforms each of the three human annotators (aka. judges) and compares favorably with the combined performance of all judges.\"","summary":"\"Deep learning methods, able to learn sentence or document representations automatically, have recently been used to score sentences. For example, R2N2 @cite_12 uses a recursive neural network for both word level and sentence level scoring, followed by an ILP optimization strategy for selecting sentences. CNN-W2V @cite_21 is another example, which modifies a convolutional-neural-network (CNN) model of sentence classification @cite_7 to rank sentences. SummaRuNNer @cite_22 , on the other hand, treats summarizations as sequence classifications and uses a two-layer bi-directional recurrent neural network (RNN) model to extract sentences, where the first layer RNN is for words and the second layer is for sentences. Unlike unsupervised methods, the state-of-the-art deep learning approaches require a larger dataset and a significantly longer time to train a model, yet with a much lower ROUGE-1 score when evaluating on DUC dataset.\"","":""}
{"id":"2889866549","dialogue":"\"Although neural network approaches achieve remarkable success on a variety of NLP tasks, many of them struggle to answer questions that require commonsense knowledge. We believe the main reason is the lack of commonsense connections between concepts. To remedy this, we provide a simple and effective method that leverages external commonsense knowledge base such as ConceptNet. We pre-train direct and indirect relational functions between concepts, and show that these pre-trained functions could be easily added to existing neural network models. Results show that incorporating commonsense-based function improves the baseline on three question answering tasks that require commonsense reasoning. Further analysis shows that our system discovers and leverages useful evidence from an external commonsense knowledge base, which is missing in existing neural network models and help derive the correct answer.\"","summary":"\"Our work relates to the field of model pretraining in NLP and computer vision fields @cite_15 . In the NLP community, works on model pretraining can be divided into unstructured text-based and structured knowledge-based ones. Both word embedding learning algorithms @cite_23 and contextual embedding learning algorithms @cite_19 @cite_16 belong to the text-based direction. Compared with these methods, which aim to learn a representation for a continuous sequence of words, our goal is to model the concept relatedness with graph structure in the knowledge base. Previous works on knowledge-based pretraining are typically validated on knowledge base completion or link prediction task @cite_24 @cite_11 . Our work belongs to the second line. We pre-train models from the commonsense knowledge base and apply the approach to the question answering task. We believe that combining both structured knowledge graphs and unstructured texts to do model pretraining is very attractive, and we leave this for future work.\"","":""}
{"id":"2890375627","dialogue":"\"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.\"","summary":"\"Initially most behavior planners were handcrafted state machines, made up by a variety of modules to handle different tasks of driving. During the DARPA Urban Challenge Boss (CMU) for example used five different modules to conduct on road driving. The responsibilities of the modules ranged from lane selection, merge planning to distance keeping @cite_10 . Other participants such as Odin (Virginia Tech) or Talos (MIT) developed very similar behavior generators @cite_1 @cite_27 .\"","":""}
{"id":"2890375627","dialogue":"\"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.\"","summary":"\"Due to the limitations of state machines, current research has expanded on the initial efforts by creating more complex and formal models: A mixture of POMDP, stochastic non-linear MPC and domain knowledge can be used to generate lane change decisions in a variety of traffic scenarios @cite_33 . Capturing the mutual dependency of maneuver decisions between different agents, planning can be conducted with foresight @cite_13 @cite_18 . While @cite_13 plans only the next maneuver focusing on the reduction of collision probabilities between all traffic participants, @cite_34 explicitly addresses longer planning horizons and the replanning capabilities of others.\"","":""}
{"id":"2890375627","dialogue":"\"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.\"","summary":"\"In recent years, deep reinforcement learning (DRL) has been successfully used to learn policies for various challenges. used DRL in conjunction with supervised learning on human game data to train the policy networks of their program AlphaGo @cite_26 ; @cite_23 @cite_31 present an overview of RL and DRL respectively. In @cite_7 and @cite_25 their agents achieve superhuman performance in their respective domains solely using a self-play reinforcement learning algorithm which utilizes Monte Carlo Tree Search (MCTS) to accelerate the training. proposed their deep Q-network (DQN) @cite_14 @cite_15 which was able to learn policies for a plethora of different Atari 2600 games and reach or surpass human level of performance. The DQN approach offers a high generalizability and versatility in tasks with high dimensional state spaces and has been extended in various work @cite_6 @cite_29 @cite_8 . Especially actor-critic approaches have shown huge success in learning complex policies and are also able to learn behavior policies in domains with a continuous action space @cite_2 @cite_11 .\"","":""}
{"id":"2890375627","dialogue":"\"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.\"","summary":"\"In the domain of autonomous driving, DRL has been used to directly control the movements of simulated vehicles to solve tasks like lane-keeping @cite_20 @cite_4 . In these approaches, the agents receive sensor input from the respective simulation environments and are trained to determine a suitable steering angle to keep the vehicle within its driving lane. Thereby, the focus lies mostly on low-level control.\"","":""}
{"id":"2890375627","dialogue":"\"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.\"","summary":"\"Since it can be problematic to model multi-agent scenarios as a Markov Decision Process (MDP) due to the unpredictable behavior of other agents, one possibility is to decompose the problem into learning a cost function for driving trajectories @cite_12 . To make learning faster and more data efficient, expert knowledge can be incorporated by restricting certain actions during the training process @cite_22 . Additionally, there is the option to handle task and motion planning by learning low-level controls for lateral as well as longitudinal maneuvers from a predefined set and a high-level maneuver policy @cite_0 .\"","":""}
{"id":"2949556825","dialogue":"\"Urban traffic scenarios often require a high degree of cooperation between traffic participants to ensure safety and efficiency. Observing the behavior of others, humans infer whether or not others are cooperating. This work aims to extend the capabilities of automated vehicles, enabling them to cooperate implicitly in heterogeneous environments. Continuous actions allow for arbitrary trajectories and hence are applicable to a much wider class of problems than existing cooperative approaches with discrete action spaces. Based on cooperative modeling of other agents, Monte Carlo Tree Search (MCTS) in conjunction with Decoupled-UCT evaluates the action-values of each agent in a cooperative and decentralized way, respecting the interdependence of actions among traffic participants. The extension to continuous action spaces is addressed by incorporating novel MCTS-specific enhancements for efficient search space exploration. The proposed algorithm is evaluated under different scenarios, showing that the algorithm is able to achieve effective cooperative planning and generate solutions egocentric planning fails to identify.\"","summary":"\"Other approaches are not explicitly cooperative, however they do capture the interdependencies of actions as they evaluate the threat resulting from different maneuver combinations, and hence predict the future motions of vehicles @cite_4 and are able to generate proactive cooperative driving actions @cite_26 .\"","":""}
{"id":"2949556825","dialogue":"\"Urban traffic scenarios often require a high degree of cooperation between traffic participants to ensure safety and efficiency. Observing the behavior of others, humans infer whether or not others are cooperating. This work aims to extend the capabilities of automated vehicles, enabling them to cooperate implicitly in heterogeneous environments. Continuous actions allow for arbitrary trajectories and hence are applicable to a much wider class of problems than existing cooperative approaches with discrete action spaces. Based on cooperative modeling of other agents, Monte Carlo Tree Search (MCTS) in conjunction with Decoupled-UCT evaluates the action-values of each agent in a cooperative and decentralized way, respecting the interdependence of actions among traffic participants. The extension to continuous action spaces is addressed by incorporating novel MCTS-specific enhancements for efficient search space exploration. The proposed algorithm is evaluated under different scenarios, showing that the algorithm is able to achieve effective cooperative planning and generate solutions egocentric planning fails to identify.\"","summary":"\"Furthermore, off-line calculated maneuver templates can be used to devise cooperative plans. First, the maneuver template needs to match a given traffic scene with specific initial constraints, then the maneuver described in the template is checked for feasibility. If multiple templates are feasible given a specific traffic scene the maneuver template with the lowest cost specifies the cooperative maneuver @cite_18 .\"","":""}
{"id":"2891313905","dialogue":"\"This work investigates the emergence of oscillations in one of the simplest cellular signaling networks exhibiting oscillations, namely, the dual-site phosphorylation and dephosphorylation network (futile cycle), in which the mechanism for phosphorylation is processive while the one for dephosphorylation is distributive (or vice-versa). The fact that this network yields oscillations was shown recently by Suwanmajo and Krishnan. Our results, which significantly extend their analyses, are as follows. First, in the three-dimensional space of total amounts, the border between systems with a stable versus unstable steady state is a surface defined by the vanishing of a single Hurwitz determinant. Second, this surface consists generically of simple Hopf bifurcations. Next, simulations suggest that when the steady state is unstable, oscillations are the norm. Finally, the emergence of oscillations via a Hopf bifurcation is enabled by the catalytic and association constants of the distributive part of the mechanism: if these rate constants satisfy two inequalities, then the system generically admits a Hopf bifurcation. Our proofs are enabled by the Routh-Hurwitz criterion, a Hopf-bifurcation criterion due to Yang, and a monomial parametrization of steady states.\"","summary":"\"Our work joins a growing number of works that harness steady-state parametrizations. Such results include criteria for when such parametrizations exist @cite_19 @cite_37 and methods for using them to determine whether a network is multistationary @cite_20 @cite_23 @cite_27 @cite_2 . Going further, steady-state parametrizations can also be used to find a witness to multistationarity or even the precise parameter regions that yield multistationarity @cite_13 @cite_39 . In this work, we use a steady-state parametrization in a novel way: to study oscillations via Hopf bifurcations. (Our approach is similar in spirit to using Clarke's convex parameters together with a Hopf-bifurcation criterion @cite_31 @cite_15 @cite_22 @cite_10 ).\"","":""}
{"id":"2891313905","dialogue":"\"This work investigates the emergence of oscillations in one of the simplest cellular signaling networks exhibiting oscillations, namely, the dual-site phosphorylation and dephosphorylation network (futile cycle), in which the mechanism for phosphorylation is processive while the one for dephosphorylation is distributive (or vice-versa). The fact that this network yields oscillations was shown recently by Suwanmajo and Krishnan. Our results, which significantly extend their analyses, are as follows. First, in the three-dimensional space of total amounts, the border between systems with a stable versus unstable steady state is a surface defined by the vanishing of a single Hurwitz determinant. Second, this surface consists generically of simple Hopf bifurcations. Next, simulations suggest that when the steady state is unstable, oscillations are the norm. Finally, the emergence of oscillations via a Hopf bifurcation is enabled by the catalytic and association constants of the distributive part of the mechanism: if these rate constants satisfy two inequalities, then the system generically admits a Hopf bifurcation. Our proofs are enabled by the Routh-Hurwitz criterion, a Hopf-bifurcation criterion due to Yang, and a monomial parametrization of steady states.\"","summary":"\"As mentioned earlier, there has been much interest in the dynamics of phosphorylation systems @cite_28 . The mixed-mechanism network fits into the related literature as follows. The mixed network is a dual-site network situated between two extremes: the fully processive dual-site network -- in which the phosphorylation and dephosphorylation mechanisms are both processive -- and the fully distributive dual-site network. One might therefore expect the dynamics of the mixed-mechanism network to straddle those of the two networks. This is indeed the case. As summarized in Table , and reviewed in @cite_28 , fully processive networks are globally convergent to a unique steady state @cite_26 @cite_12 @cite_9 , while mixed-mechanism networks admit oscillations but not bistability @cite_3 , and fully distributive networks admit bistability @cite_16 (and the question of oscillations is open @cite_28 ).\"","":""}
{"id":"2891313905","dialogue":"\"This work investigates the emergence of oscillations in one of the simplest cellular signaling networks exhibiting oscillations, namely, the dual-site phosphorylation and dephosphorylation network (futile cycle), in which the mechanism for phosphorylation is processive while the one for dephosphorylation is distributive (or vice-versa). The fact that this network yields oscillations was shown recently by Suwanmajo and Krishnan. Our results, which significantly extend their analyses, are as follows. First, in the three-dimensional space of total amounts, the border between systems with a stable versus unstable steady state is a surface defined by the vanishing of a single Hurwitz determinant. Second, this surface consists generically of simple Hopf bifurcations. Next, simulations suggest that when the steady state is unstable, oscillations are the norm. Finally, the emergence of oscillations via a Hopf bifurcation is enabled by the catalytic and association constants of the distributive part of the mechanism: if these rate constants satisfy two inequalities, then the system generically admits a Hopf bifurcation. Our proofs are enabled by the Routh-Hurwitz criterion, a Hopf-bifurcation criterion due to Yang, and a monomial parametrization of steady states.\"","summary":"\"Finally, we revisit Suwanmajo and Krishnan's claim mentioned earlier that the mixed-mechanism network is among the simplest enzymatic mechanisms with oscillations. In support of this claim, Tung proved that the simpler system obtained from the mixed-mechanism network by taking its (two-dimensional) Michaelis-Menten approximation, is not oscillatory @cite_25 . Moreover, Rao showed that this approximation is globally convergent to a unique steady state @cite_30 . The validity of the Michaelis-Menten approximation for phosphorylation systems has been called into question @cite_24 , and what we know about the mixed-mechanism system concurs: this system is oscillatory, but its Michaelis-Menten approximation is not.\"","":""}
{"id":"2890965997","dialogue":"\"Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.\"","summary":"\"(ER), first introduced in @cite_18 , is a crucial component to stabilize convergence in off-policy deep RL networks, as demonstrated by the successful Deep Q-Network @cite_8 algorithm. ER dissects episodes into experience quadruples of the form @math , where the elements represent current state, action, next state and reward respectively. The experiences are stored in a collective database termed as , from where they undergo uniform random sampling to form minibatches for training the RL networks. In addition to allowing for temporal independence in the training set, this approach enables the reuse of past experiences, as any single experience may be sampled multiple times, and hence increases sample-efficiency.\"","":""}
{"id":"2890965997","dialogue":"\"Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.\"","summary":"\"Subsequent research aimed to find more effective sampling strategies and compositions of the replay buffer. Prioritized Experience Replay (PER) @cite_6 achieves higher sample efficiency by selecting experiences from the replay buffer according to a frequency distribution prioritizing the importance of each individual transition. @cite_20 investigates the effect of size of the replay buffer on performance and proposes Combined Experience Replay (CER) to alleviate the liability of a large replay buffer. Hindsight Experience Replay (HER) @cite_11 , described in greater detail in the following section, strategically augments the replay buffer by reformulating unsuccessful episodes as successful transitions accomplishing a different goal. The resulting balance in successful and unsuccessful experiences in the replay buffer overcomes the disadvantage of sparse rewards.\"","":""}
{"id":"2890965997","dialogue":"\"Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.\"","summary":"\"Hindsight bias was first documented by Fischhoff @cite_9","":""}
{"id":"2890965997","dialogue":"\"Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.\"","summary":"\"Parallel methods to improve sample-efficiency of deep RL include learning hierarchical abstractions @cite_14 @cite_31 , reducing variance in policy gradients @cite_25 , model-based algorithms @cite_22 and effective exploration @cite_0 @cite_19 .\"","":""}
{"id":"2890623477","dialogue":"\"Neural Machine Translation (NMT) in low-resource settings and of morphologically rich languages is made difficult in part by data sparsity of vocabulary words. Several methods have been used to help reduce this sparsity, notably Byte-Pair Encoding (BPE) and a character-based CNN layer (charCNN). However, the charCNN has largely been neglected, possibly because it has only been compared to BPE rather than combined with it. We argue for a reconsideration of the charCNN, based on cross-lingual improvements on low-resource data. We translate from 8 languages into English, using a multi-way parallel collection of TED transcripts. We find that in most cases, using both BPE and a charCNN performs best, while in Hebrew, using a charCNN over words is best.\"","summary":"\"A few papers have worked with the charCNN and analyzed its performance. @cite_3 show that the hidden states of the charCNN can classify morphology better than standard seq2seq models at the word level. @cite_10 show that charCNNs are sensitive to various types of noise. Our analysis complements this line of work, supporting the conclusion that the charCNN is learning morphology better than standard seq2seq models and explores additionally how its strengths interact with BPE.\"","":""}
{"id":"2889775105","dialogue":"\"We introduce a method for learning adversarial perturbations targeted to individual images or videos. The learned perturbations are found to be sparse while at the same time containing a high level of feature detail. Thus, the extracted perturbations allow a form of object or action recognition and provide insights into what features the studied deep neural network models consider important when reaching their classification decisions. From an adversarial point of view, the sparse perturbations successfully confused the models into misclassifying, although the perturbed samples still belonged to the same original class by visual examination. This is discussed in terms of a prospective data augmentation scheme. The sparse yet high-quality perturbations may also be leveraged for image or video compression.\"","summary":"\"Soon after the release of the first widely successful deep learning models for image classification, perturbation schemes were proposed for understanding them @cite_16 or for exposing some of their shortcomings through the generation of adversarial perturbations that confuse the models into misclassifying @cite_11 . These two areas of study using perturbation methods have seen considerable progress over the years.\"","":""}
{"id":"2889775105","dialogue":"\"We introduce a method for learning adversarial perturbations targeted to individual images or videos. The learned perturbations are found to be sparse while at the same time containing a high level of feature detail. Thus, the extracted perturbations allow a form of object or action recognition and provide insights into what features the studied deep neural network models consider important when reaching their classification decisions. From an adversarial point of view, the sparse perturbations successfully confused the models into misclassifying, although the perturbed samples still belonged to the same original class by visual examination. This is discussed in terms of a prospective data augmentation scheme. The sparse yet high-quality perturbations may also be leveraged for image or video compression.\"","summary":"\"Following the seminal work of Szegedy et. al. @cite_11 on adversarial perturbations, the linear nature of convolutional layers (before activation functions) was exploited for the rapid generation of adversarial examples @cite_35 . The existence of (untainted) real-world images that also cause deep learning models to misclassify has also been reported @cite_44 . These and other findings triggered a wave of introspection resulting on a series of comprehensive studies such as an evaluation of the robustness of neural networks @cite_12 @cite_4 , model generalization assessments @cite_40 @cite_4 , and a proof of the existence of universal adversarial perturbations that can be applied to any image @cite_6 . An impressive consequence of the latter result may be found in the work of Baluja and Fisher @cite_24 who were able to train adversarial models on a full dataset of images with the explicit goal of confusing a target model into believing that all of the samples belong to an arbitrarily chosen class.\"","":""}
{"id":"2889775105","dialogue":"\"We introduce a method for learning adversarial perturbations targeted to individual images or videos. The learned perturbations are found to be sparse while at the same time containing a high level of feature detail. Thus, the extracted perturbations allow a form of object or action recognition and provide insights into what features the studied deep neural network models consider important when reaching their classification decisions. From an adversarial point of view, the sparse perturbations successfully confused the models into misclassifying, although the perturbed samples still belonged to the same original class by visual examination. This is discussed in terms of a prospective data augmentation scheme. The sparse yet high-quality perturbations may also be leveraged for image or video compression.\"","summary":"\"The only previous work that we are aware of regarding perturbations of video samples for the study of deep neural network models was recently published by Wei et. al. @cite_50 . In this study, video pixels are modified to reduce the class score assigned to the original video while attempting to keep the perturbed video as close as possible to the original one. Modifications to the original video are confined to a few video frames by the use in the temporal direction of a norm penalty during the optimization and by the introduction of a mask that explicitly prevents modifications to some frames. The authors were able to produce sparse adversarial perturbations that considerably reduced the original score assigned by the model.\"","":""}
{"id":"2889775105","dialogue":"\"We introduce a method for learning adversarial perturbations targeted to individual images or videos. The learned perturbations are found to be sparse while at the same time containing a high level of feature detail. Thus, the extracted perturbations allow a form of object or action recognition and provide insights into what features the studied deep neural network models consider important when reaching their classification decisions. From an adversarial point of view, the sparse perturbations successfully confused the models into misclassifying, although the perturbed samples still belonged to the same original class by visual examination. This is discussed in terms of a prospective data augmentation scheme. The sparse yet high-quality perturbations may also be leveraged for image or video compression.\"","summary":"\"Due to the large number of parameters present in deep neural networks, generally a very large number of training samples is necessary to prevent overfitting. This fact was recognized early on in the development of modern convnet architectures @cite_0 and prompted the development of data augmentation techniques. Early image data augmentation was achieved by shifting, rotating, cropping, and flipping the images @cite_0 @cite_17 . More recently, other heuristics have been used for data augmentation such as color casting, vignetting and lens distortion @cite_48 leading to the training of more robust image classification models. In general, any transformation that does not change the class of a sample can be exploited for data augmentation. Thus, adversarial image perturbations generated by a learned perturbative mask @cite_11 @cite_35 @cite_33 or through the training of generative adversarial networks (GANs) @cite_32 have been proposed for data augmentation. GANs have been successfully applied to the augmentation of biomedical images @cite_18 field in which the number of available samples is limited.\"","":""}
{"id":"2891523355","dialogue":"\"This paper explores the use of deep reinforcement learning agents to transfer knowledge from one environment to another. More specifically, the method takes advantage of asynchronous advantage actor critic (A3C) architecture to generalize a target game using an agent trained on a source game in Atari. Instead of fine-tuning a pre-trained model for the target game, we propose a learning approach to update the model using multiple agents trained in parallel with different representations of the target game. Visual mapping between video sequences of transfer pairs is used to derive new representations of the target game; training on these visual representations of the target game improves model updates in terms of performance, data efficiency and stability. In order to demonstrate the functionality of the architecture, Atari games Pong-v0 and Breakout-v0 are being used from the OpenAI gym environment; as the source and target environment.\"","summary":"\"Domain adaption aims to generate a shared representation for distinct domains @cite_7 . @cite_0 use multi-task and transfer learning to act in a new domain by using previous knowledge. The multi-task learning method employed","":""}
{"id":"2891414023","dialogue":"\"Consensus in decentralized systems that asynchronously receive events and which are subject to Byzantine faults is a common problem with many real-life applications. Advances in decentralized systems, such as distributed ledger (i.e., blockchain) technology, has only increased the importance of finding performant and secure solutions to consensus of state machine replication in decentralized systems. YAC is a practical decentralized consensus algorithm, that solves the problems of inefficient message passing and strong leaders that occur in classical Byzantine fault tolerant consensus algorithms. The algorithm is open source and currently is used to provide Byzantine fault tolerant consensus for the Hyperledger Iroha blockchain project. We provide proofs of safety and liveness, as well as empirical results showing that our algorithm can scale to dozens of validating peers.\"","summary":"\"Consensus @cite_0 should guarantee of the system, , and (consistency) of data stored in the ledger. Liveness means that the system should never stop and should be able to recover from errors. Security means that non-faulty peers should not accept false data. Consistency means that all non-faulty peers should maintain or converge to the same global ordering and state. Various consensus algorithms have been proposed for different situations. Consensus algorithms are often discussed in terms of some weak synchrony assumption, and, as shown by @cite_11 , no distributed consensus algorithm can give a deterministic solution in a fully asynchronous network.\"","":""}
{"id":"2891414023","dialogue":"\"Consensus in decentralized systems that asynchronously receive events and which are subject to Byzantine faults is a common problem with many real-life applications. Advances in decentralized systems, such as distributed ledger (i.e., blockchain) technology, has only increased the importance of finding performant and secure solutions to consensus of state machine replication in decentralized systems. YAC is a practical decentralized consensus algorithm, that solves the problems of inefficient message passing and strong leaders that occur in classical Byzantine fault tolerant consensus algorithms. The algorithm is open source and currently is used to provide Byzantine fault tolerant consensus for the Hyperledger Iroha blockchain project. We provide proofs of safety and liveness, as well as empirical results showing that our algorithm can scale to dozens of validating peers.\"","summary":"\"Zyzzyva @cite_5 extended PBFT, avoiding the expensive three-phase commit protocol, utilizing fast track and actively involving the client into the consensus process. In the best case scenario, the client sends the request to the leader, who is broadcasts it to all other replicas, and the client waits for replies from all the peers. If the client receives 3 @math +1 replies, it commits the transaction(s). In the case when the client receives between 2 @math +1 to 3 @math messages, a regular consensus algorithm is used. The client is a major player, who is responsible for checking the integrity of replicas.\"","":""}
{"id":"2891414023","dialogue":"\"Consensus in decentralized systems that asynchronously receive events and which are subject to Byzantine faults is a common problem with many real-life applications. Advances in decentralized systems, such as distributed ledger (i.e., blockchain) technology, has only increased the importance of finding performant and secure solutions to consensus of state machine replication in decentralized systems. YAC is a practical decentralized consensus algorithm, that solves the problems of inefficient message passing and strong leaders that occur in classical Byzantine fault tolerant consensus algorithms. The algorithm is open source and currently is used to provide Byzantine fault tolerant consensus for the Hyperledger Iroha blockchain project. We provide proofs of safety and liveness, as well as empirical results showing that our algorithm can scale to dozens of validating peers.\"","summary":"\"@cite_18 argued that current weak synchronous consensus algorithms rely heavily on timing assumptions. They showed that an adversary can attack PBFT, causing it to either stop making any progress for consensus, or significantly slowing the protocol.\"","":""}
{"id":"2889831774","dialogue":"\"Moments capture a huge part of our lives. Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments. Action recognition refers to the act of classifying the desired action activity present in a given video. In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips. We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset. Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.\"","summary":"\"Traditional approaches in activity recognition deals were performed on untrimmed videos. Thus, being able to localize the action in the video stream is important. The candidate temporal or spatio-temporal regions are referred as action proposals. Action proposals don't provide information about the class of an action but provides localize regions which are most likely to contain an action as mentioned in this extensive review @cite_14 paper . Action proposals (or non-action shots) prevent going through an exhaustive search space and helps detect temporal or spatio-temporal locations of actions.\"","":""}
{"id":"2889831774","dialogue":"\"Moments capture a huge part of our lives. Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments. Action recognition refers to the act of classifying the desired action activity present in a given video. In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips. We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset. Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.\"","summary":"\"Paper @cite_2 describes that the models trained on Kinetics dataset serves as a better pre-trained model as compared with the ImageNet model. The incorporate temporal information using 3D models and long temporal information using temporal segment networks. Further, their approach also explored multi-modal cues such as audio cues and body pose estimation. Paper @cite_8 mentions\"","":""}
{"id":"2889831774","dialogue":"\"Moments capture a huge part of our lives. Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments. Action recognition refers to the act of classifying the desired action activity present in a given video. In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips. We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset. Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.\"","summary":"In @cite_3 optical flow features between consecutive frames are used to model temporal aspects of activities depicted in the videos. It was shown by @cite_16 optical flow features capture crucial information for the task of activity classification. @cite_3 use the features obtained in the fc6 layer activations in the CNN pre-trained on the UCF101 video dataset to represent the activity for the task of activity classification. Deep convolutional networks can also be employed to capture the complementary information contained in still frames and motion between frames. In order to incorporate spatial and temporal features @cite_18 propose a two-stream ConvNet architecture and demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance despite having limited training data. They also we show that applying multitask learning to two different action classification datasets can be used to increase the amount of training data and improve the performance on both. UCF101 dataset and Thumos dataset built from web videos have become benchmarks for video classification.","":""}
{"id":"2891168223","dialogue":"\"We present a machine learning based method for noise classification using a low-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients for audio feature extraction and supervised classification algorithms (that is, support vector machine and k-nearest neighbors) for noise classification. We evaluate our approach experimentally with a dataset of about 3000 sound samples grouped in eight sound classes (such as, car horn, jackhammer, or street music). We explore the parameter space of support vector machine and k-nearest neighbors algorithms to estimate the optimal parameter values for classification of sound samples in the dataset under study. We achieve a noise classification accuracy in the range 85 -- 100 . Training and testing of our k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than a second for a dataset with features of more than 3000 sound samples.\"","summary":"\"@cite_2 provide an overview of a platform for distributed urban noise measurement, which is part of an ongoing German research project called . A wireless distributed network of audio sensors based on quad-core ARM BCM2837 SoC was employed to receive urban noise signals, pre-process the obtained audio data and send it to a central unit for data storage and performing higher-level audio processing. A final stage of web application was used for visualization and administration of both processed and unprocessed audio data. The authors in @cite_16 used Ameba RTL 8195AM and Ameba 8170AF as IoT platforms to implement a distributed sensing system for visualization of the noise pollution. In @cite_10 , two hardware alternatives, Raspberry Pi platform and Tmote-Invent nodes, were evaluated in terms of their cost and feasibility for analyzing urban noise and measuring the psycho-acoustic metrics according to the Zwicker's annoyance model. In contrast to related work, our approach is not concerned with measuring the noise level in dB using IoT, but with determining the type of noise (for instance, a jackhammer or gun shot).\"","":""}
{"id":"2949240260","dialogue":"\"Mobile edge caching enables content delivery within the radio access network, which effectively alleviates the backhaul burden and reduces response time. To fully exploit edge storage resources, the most popular contents should be identified and cached. Observing that user demands on certain contents vary greatly at different locations, this paper devises location-customized caching schemes to maximize the total content hit rate. Specifically, a linear model is used to estimate the future content hit rate. For the case where the model noise is zero-mean, a ridge regression based online algorithm with positive perturbation is proposed. Regret analysis indicates that the proposed algorithm asymptotically approaches the optimal caching strategy in the long run. When the noise structure is unknown, an @math filter based online algorithm is further proposed by taking a prescribed threshold as input, which guarantees prediction accuracy even under the worst-case noise process. Both online algorithms require no training phases, and hence are robust to the time-varying user demands. The underlying causes of estimation errors of both algorithms are numerically analyzed. Moreover, extensive experiments on real world dataset are conducted to validate the applicability of the proposed algorithms. It is demonstrated that those algorithms can be applied to scenarios with different noise features, and are able to make adaptive caching decisions, achieving content hit rate that is comparable to that via the hindsight optimal strategy.\"","summary":"\"Mobile user's capacity is greatly augmented in the era of MEC. As a result, mobile service provisioning is expected to have further improved quality of experience (QoE) @cite_21 . To this end, various mobile edge architectures have been proposed. Tandom proposed to deploy edge resources within radio access networks. They characterized the relationship between latency and caching size, as well as latency and fronthaul capacity, from an information-theoretic perspective @cite_12 . Yang introduced an edge resource provisioning architecture based on cloud radio access network (C-RAN), and devised a cloud-edge interoperation scheme via software defined networking techniques @cite_16 . Tong designed a hierarchical edge architecture, aiming at making efficient use of edge resources when serving the peak loads from mobile users @cite_1 . As the 5G wireless network is expected to incorporate diverse access technologies, in this paper, we consider edge caching in the context of heterogeneous networks. Potential EN deployment can be capacity-augmented base stations, WiFi access points and other devices with excess resources.\"","":""}
{"id":"2949240260","dialogue":"\"Mobile edge caching enables content delivery within the radio access network, which effectively alleviates the backhaul burden and reduces response time. To fully exploit edge storage resources, the most popular contents should be identified and cached. Observing that user demands on certain contents vary greatly at different locations, this paper devises location-customized caching schemes to maximize the total content hit rate. Specifically, a linear model is used to estimate the future content hit rate. For the case where the model noise is zero-mean, a ridge regression based online algorithm with positive perturbation is proposed. Regret analysis indicates that the proposed algorithm asymptotically approaches the optimal caching strategy in the long run. When the noise structure is unknown, an @math filter based online algorithm is further proposed by taking a prescribed threshold as input, which guarantees prediction accuracy even under the worst-case noise process. Both online algorithms require no training phases, and hence are robust to the time-varying user demands. The underlying causes of estimation errors of both algorithms are numerically analyzed. Moreover, extensive experiments on real world dataset are conducted to validate the applicability of the proposed algorithms. It is demonstrated that those algorithms can be applied to scenarios with different noise features, and are able to make adaptive caching decisions, achieving content hit rate that is comparable to that via the hindsight optimal strategy.\"","summary":"\"Unfortunately","":""}
{"id":"2949240260","dialogue":"\"Mobile edge caching enables content delivery within the radio access network, which effectively alleviates the backhaul burden and reduces response time. To fully exploit edge storage resources, the most popular contents should be identified and cached. Observing that user demands on certain contents vary greatly at different locations, this paper devises location-customized caching schemes to maximize the total content hit rate. Specifically, a linear model is used to estimate the future content hit rate. For the case where the model noise is zero-mean, a ridge regression based online algorithm with positive perturbation is proposed. Regret analysis indicates that the proposed algorithm asymptotically approaches the optimal caching strategy in the long run. When the noise structure is unknown, an @math filter based online algorithm is further proposed by taking a prescribed threshold as input, which guarantees prediction accuracy even under the worst-case noise process. Both online algorithms require no training phases, and hence are robust to the time-varying user demands. The underlying causes of estimation errors of both algorithms are numerically analyzed. Moreover, extensive experiments on real world dataset are conducted to validate the applicability of the proposed algorithms. It is demonstrated that those algorithms can be applied to scenarios with different noise features, and are able to make adaptive caching decisions, achieving content hit rate that is comparable to that via the hindsight optimal strategy.\"","summary":"\"In this paper, we exploit locational features for context differentiation. Locational information can be easily obtained, for example, users attached to different ENs are naturally divided into geographical groups. Based on which we investigate the location-aware caching problem with unknown and time-varying content popularity profile. By modeling user demand as linear combination of location features and content attributes, our previous work has addressed the content popularity prediction problem with the assumption that the model noise is zero-mean @cite_25 . As an extension, this paper additionally considers the practical scenario, where noise structure is unknown . Specifically, a robust prediction algorithm is proposed with detailed theoretical caching performance analysis. The proposed algorithm is robust and practical as it guarantees prediction accuracy regardless of the noise statistics. Additionally, numerical analysis and comparison on the root causes of estimation errors of both algorithms are presented. Much extensive experiments are conducted to validate the performance of the proposed algorithms.\"","":""}
{"id":"2949240260","dialogue":"\"Mobile edge caching enables content delivery within the radio access network, which effectively alleviates the backhaul burden and reduces response time. To fully exploit edge storage resources, the most popular contents should be identified and cached. Observing that user demands on certain contents vary greatly at different locations, this paper devises location-customized caching schemes to maximize the total content hit rate. Specifically, a linear model is used to estimate the future content hit rate. For the case where the model noise is zero-mean, a ridge regression based online algorithm with positive perturbation is proposed. Regret analysis indicates that the proposed algorithm asymptotically approaches the optimal caching strategy in the long run. When the noise structure is unknown, an @math filter based online algorithm is further proposed by taking a prescribed threshold as input, which guarantees prediction accuracy even under the worst-case noise process. Both online algorithms require no training phases, and hence are robust to the time-varying user demands. The underlying causes of estimation errors of both algorithms are numerically analyzed. Moreover, extensive experiments on real world dataset are conducted to validate the applicability of the proposed algorithms. It is demonstrated that those algorithms can be applied to scenarios with different noise features, and are able to make adaptive caching decisions, achieving content hit rate that is comparable to that via the hindsight optimal strategy.\"","summary":"\"It is worth noting that, in the mobile context, fetching content from the edge cache significantly reduces the delay, compared with that from conventional content distribution network (CDN). Moreover, existing content pushing strategies in CDN do not consider the fine-grained popularity differentiation in neighbouring Wi-Fi APs and cellular base stations @cite_4 . With the consideration of location awareness, this paper further models and predicts the dynamics of content popularity, which is constantly varying with time.\"","":""}
{"id":"2889010694","dialogue":"\"The use of spreadsheets in industry is widespread. Companies base decisions on information coming from spreadsheets. Unfortunately, spreadsheets are error-prone and this increases the risk that companies base their decisions on inaccurate information, which can lead to incorrect decisions and loss of money. In general, spreadsheet research is aimed to reduce the error-proneness of spreadsheets. Most research is concentrated on the use of formulas. However, there are other constructions in spreadsheets, like charts, pivot tables, and array formulas, that are also used to present decision support information to the user. There is almost no research about how these constructions are used. To improve spreadsheet quality it is important to understand how spreadsheets are used and to obtain a complete understanding, the use of charts, pivot tables, and array formulas should be included in research. In this paper, we analyze two popular spreadsheet corpora: Enron and EUSES on the use of the aforementioned constructions.\"","summary":"\"Most related to our work are the papers introducing the corpora that we have analyzed, Euses @cite_6 and Enron @cite_10 . While the paper introducing EUSES describes statistics on charts too the paper on Enron does not. Another spreadsheet corpus is FUSE @cite_14 . This corpus consists of almost 250,000 spreadsheets that were extracted from a public web archive with over 26 billion pages. We preferred the Enron corpus over the FUSE corpus because the Enron spreadsheets were used in an industrial setting.\"","":""}
{"id":"2950381197","dialogue":"\"Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset @math has two other equivalent but literally different forms @math and @math . Pattern matching is known to provide a handy tool set to treat such data types. Although many studies on pattern matching and implementations for practical programming languages have been proposed so far, we observe that none of these studies satisfy all the criteria of practical pattern matching, which are as follows: i) efficiency of the backtracking algorithm for non-linear patterns, ii) extensibility of matching process, and iii) polymorphism in patterns. This paper aims to design a new pattern-matching-oriented programming language that satisfies all the above three criteria. The proposed language features clean Scheme-like syntax and efficient and extensible pattern matching semantics. This programming language is especially useful for the processing of complex non-free data types that not only include multisets and sets but also graphs and symbolic mathematical expressions. We discuss the importance of our criteria of practical pattern matching and how our language design naturally arises from the criteria. The proposed language has been already implemented and open-sourced as the Egison programming language.\"","summary":"\"Miranda laws @cite_10 @cite_5 @cite_22 and Wadler's views @cite_14 @cite_23 are seminal work. These proposals provide methods to decompose data with multiple representations by explicitly declaring transformations between each representation. These are the earliest studies that allow users to customize the execution process of pattern matching. However, the pattern-matching systems in these proposals treat neither multiple pattern matching results nor non-linear patterns. Also, these studies demand a canonical form for each representation.\"","":""}
{"id":"2950381197","dialogue":"\"Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset @math has two other equivalent but literally different forms @math and @math . Pattern matching is known to provide a handy tool set to treat such data types. Although many studies on pattern matching and implementations for practical programming languages have been proposed so far, we observe that none of these studies satisfy all the criteria of practical pattern matching, which are as follows: i) efficiency of the backtracking algorithm for non-linear patterns, ii) extensibility of matching process, and iii) polymorphism in patterns. This paper aims to design a new pattern-matching-oriented programming language that satisfies all the above three criteria. The proposed language features clean Scheme-like syntax and efficient and extensible pattern matching semantics. This programming language is especially useful for the processing of complex non-free data types that not only include multisets and sets but also graphs and symbolic mathematical expressions. We discuss the importance of our criteria of practical pattern matching and how our language design naturally arises from the criteria. The proposed language has been already implemented and open-sourced as the Egison programming language.\"","summary":"\"Active patterns @cite_13 @cite_12 provides a method to decompose non-free data. In active patterns, users define a for each pattern to specify how to decompose non-free data. For example, insert for multisets is defined as a match function in @cite_13 . An example of pattern matching against graphs using matching function is also shown in @cite_1 . One limitation of active patterns is that it does not support backtracking in the pattern matching process. In active patterns, the values bound to pattern variables are fixed in order from the left to right of a pattern. Therefore, we cannot write non-linear patterns that requires backtracking such as a pattern that matches with a collection (like sets or multisets) that contains two identical elements. (The pattern matching fails if we unfortunately pick an element that appears more than twice at the first choice.)\"","":""}
{"id":"2950381197","dialogue":"\"Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset @math has two other equivalent but literally different forms @math and @math . Pattern matching is known to provide a handy tool set to treat such data types. Although many studies on pattern matching and implementations for practical programming languages have been proposed so far, we observe that none of these studies satisfy all the criteria of practical pattern matching, which are as follows: i) efficiency of the backtracking algorithm for non-linear patterns, ii) extensibility of matching process, and iii) polymorphism in patterns. This paper aims to design a new pattern-matching-oriented programming language that satisfies all the above three criteria. The proposed language features clean Scheme-like syntax and efficient and extensible pattern matching semantics. This programming language is especially useful for the processing of complex non-free data types that not only include multisets and sets but also graphs and symbolic mathematical expressions. We discuss the importance of our criteria of practical pattern matching and how our language design naturally arises from the criteria. The proposed language has been already implemented and open-sourced as the Egison programming language.\"","summary":"\"First-class patterns @cite_16 is a sophisticated system that treats patterns as first-class objects. The essence of this study is a that defines how to decompose data with each data constructor. First-class patterns can deal with pattern matching that generates multiple results. To generate multiple results, a pattern function returns a list. A critical limitation of this proposal is that first-class patterns do not support non-linear pattern matching.\"","":""}
{"id":"2950381197","dialogue":"\"Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset @math has two other equivalent but literally different forms @math and @math . Pattern matching is known to provide a handy tool set to treat such data types. Although many studies on pattern matching and implementations for practical programming languages have been proposed so far, we observe that none of these studies satisfy all the criteria of practical pattern matching, which are as follows: i) efficiency of the backtracking algorithm for non-linear patterns, ii) extensibility of matching process, and iii) polymorphism in patterns. This paper aims to design a new pattern-matching-oriented programming language that satisfies all the above three criteria. The proposed language features clean Scheme-like syntax and efficient and extensible pattern matching semantics. This programming language is especially useful for the processing of complex non-free data types that not only include multisets and sets but also graphs and symbolic mathematical expressions. We discuss the importance of our criteria of practical pattern matching and how our language design naturally arises from the criteria. The proposed language has been already implemented and open-sourced as the Egison programming language.\"","summary":"\"Functional logic programming @cite_4 is an approach towards this integration. It allows both of non-linear patterns and multiple pattern-matching results. The key difference between the functional logic programming and our approach is in the method for defining pattern-matching algorithms. In functional logic programming, we describe the pattern-matching algorithm for each pattern in the logic-programming style. A function that describes such an algorithm is called a . A pattern constructor takes decomposed values as its arguments and returns the target data. On the other hand, in our proposal, pattern constructors are defined in the functional-programming style: pattern constructors take a target datum as an argument and returns the decomposed values. This enables direct description of algorithms.\"","":""}
{"id":"2889181072","dialogue":"\"Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity.\"","summary":"\"In this work, we use Learning from Demonstration framework to train an agent to learn the task of driving using high-dimensional observations in form of images. Learning from Demonstration along with deep function approximators have been used to tackle a lot of problems in robotics like indoor mobile robot navigation , quad-rotor control in forest trials , robot-arm manipulation among others. The closest to our work are the works of @cite_7 who show autonomous lane following using a single trained network, @cite_2 who demonstrate autonomous driving in CARLA using an additional conditional input from a high-level planner, @cite_3 who compare various contemporary networks for autonomous driving tasks and @cite_4 who demonstrate multi-task and multi-modal behavior for autonomous driving.\"","":""}
{"id":"2889181072","dialogue":"\"Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity.\"","summary":"\"Multi-task learning (MTL) research shows the joint training of auxiliary related side-tasks along with the main task enhances the training performance . MTL in neural networks has been successfully demonstrated in many tasks previously including text-to-speech conversion , natural language processing , speech processing and computer vision . In the field of sequential decision making, @cite_0 demonstrate MTL for 3D game playing, @cite_8 and @cite_1 demonstrate MTL in 3D maze navigation task whereas @cite_4 utilize the MTL framework for autonomous driving. Instead of employing future control outputs as auxiliary tasks as shown by @cite_4 , in this work we employ action and visual abstractions to guide the driving behavior.\"","":""}
{"id":"2889181072","dialogue":"\"Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity.\"","summary":"\"Supervised learning of visual affordances for autonomous driving was introduced by @cite_5 , though they use the predicted affordances to plan using a set of fixed rules whereas our network uses visual affordances as auxiliary tasks for the main task of driving. Action primitives can be inferred as sub-policies for the desired task. Learning hierarchical policies via demonstration is an active area of research and research in developmental psychology has also found evidence of hierarchical task decomposition during imitation in young children . Our work decomposes the main task of driving into sub-policies which are used as auxiliary supervision to derive the final control commands.\"","":""}
{"id":"2952367671","dialogue":"\"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.\"","summary":"\"Our definition of a scaffold task includes stand-alone methods for estimating word embeddings @cite_37 @cite_40 @cite_3 . After training word embeddings, the tasks implied by models like the skip-gram or ELMo's language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of @math through a multitask objective.\"","":""}
{"id":"2952367671","dialogue":"\"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.\"","summary":"\"Neural architectures have often yielded performance gains when trained for multiple tasks together @cite_44 @cite_26 @cite_38 @cite_41 . In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks @cite_29 @cite_32 @cite_9 . Contemporaneously with this work, proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics @cite_15 . Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task.\"","":""}
{"id":"2889225139","dialogue":"\"In this study, we investigate the limits of the current state of the art AI system for detecting buffer overflows and compare it with current static analysis tools. To do so, we developed a code generator, s-bAbI, capable of producing an arbitrarily large number of code samples of controlled complexity. We found that the static analysis engines we examined have good precision, but poor recall on this dataset, except for a sound static analyzer that has good precision and recall. We found that the state of the art AI system, a memory network modeled after [1], can achieve similar performance to the static analysis engines, but requires an exhaustive amount of training data in order to do so. Our work points towards future approaches that may solve these problems; namely, using representations of code that can capture appropriate scope information and using deep learning methods that are able to perform arithmetic operations.\"","summary":"\"There is much recent interest in applying artificial intelligence and machine learning techniques to a variety of software engineering tasks; see @cite_10 for a comprehensive review. The two papers closest to our work are @cite_8 , which attempts to identify vulnerabilities at the function level, and @cite_26 . which attempts to identify vulnerabilities at the line level.\"","":""}
{"id":"2889225139","dialogue":"\"In this study, we investigate the limits of the current state of the art AI system for detecting buffer overflows and compare it with current static analysis tools. To do so, we developed a code generator, s-bAbI, capable of producing an arbitrarily large number of code samples of controlled complexity. We found that the static analysis engines we examined have good precision, but poor recall on this dataset, except for a sound static analyzer that has good precision and recall. We found that the state of the art AI system, a memory network modeled after [1], can achieve similar performance to the static analysis engines, but requires an exhaustive amount of training data in order to do so. Our work points towards future approaches that may solve these problems; namely, using representations of code that can capture appropriate scope information and using deep learning methods that are able to perform arithmetic operations.\"","summary":"\"Our work, in contrast, is more modest: we attempt to find the minimal code complexity necessary to break the state of the art AI system @cite_26 . In doing so, we find that a memory network can learn how to identify buffer overflows in synthetic code, but the it appears as though the memory network needs a nearly exhaustive amount of training data in order to do so.\"","":""}
{"id":"2952157924","dialogue":"\"Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.\"","summary":"\"There are other works that employ various types of turn-taking interaction to learn models for language grounding. Some of these use a restricted vocabulary @cite_16 @cite_34 , or additional knowledge of predicates (for example that red'' is a color) @cite_9 . Others do not use active learning @cite_15 @cite_13 @cite_2 @cite_32 , or do not learn a policy that guides the interaction @cite_24 @cite_19 @cite_4 .\"","":""}
{"id":"2952157924","dialogue":"\"Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.\"","summary":"\"Also related to our work is the use of reinforcement learning in dialog tasks, such as slot-filling and recommendation @cite_31 @cite_25 , understanding natural language instructions or commands @cite_11 @cite_36 , and open domain conversation @cite_23 @cite_6 . These typically do not use active learning. In our task, the policy needs to trade-off model improvement against task completion.\"","":""}
{"id":"2888844929","dialogue":"\"Traditional survival models such as the Cox proportional hazards model are typically based on scalar or categorical clinical features. With the advent of increasingly large image datasets, it has become feasible to incorporate quantitative image features into survival prediction. So far, this kind of analysis is mostly based on radiomics features, i.e. a fixed set of features that is mathematically defined a priori. To capture highly abstract information, it is desirable to learn the feature extraction using convolutional neural networks. However, for tomographic medical images, model training is difficult because on the one hand, only few samples of 3D image data fit into one batch at once and on the other hand, survival loss functions are essentially ordering measures that require large batch sizes. In this work, we show that by simplifying survival analysis to median survival classification, convolutional neural networks can be trained with small batch sizes and learn features that predict survival equally well as end-to-end hazard prediction networks. Our approach outperforms the previous state of the art in a publicly available lung cancer dataset.\"","summary":"\"We aim to address this issue by transferring features learned by a classification problem to survival analysis without losing performance. Moreover, we propose a method to combine radiomics and learned CNN features that enforce the CNN to learn features that are both discriminative and not covered by the radiomics feature set. All methods are evaluated on a publicly available dataset of computed tomography (CT) images of non-small-cell lung cancer (NSCLC) patients and corresponding survival labels. We show that our method can outperform the previous state-of-the-art presented in @cite_3 .\"","":""}
{"id":"2952444318","dialogue":"\"An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set.\"","summary":"\"Back-translation (BT) is an alternative to leverage monolingual data. BT is simple and easy to apply as it does not require modification to the MT training algorithms. It requires training a target-to-source system in order to generate additional synthetic parallel data from the monolingual target data. This data complements human bitext to train the desired source-to-target system. BT has been applied earlier to phrase-base systems . For these systems, BT has also been successful in leveraging monolingual data for domain adaptation @cite_8 @cite_4 . Recently, BT has been shown beneficial for NMT . It has been found to be particularly useful when parallel data is scarce .\"","":""}
{"id":"2889519671","dialogue":"\"Recent advances in sequence-to-sequence learning reveal a purely data-driven approach to the response generation task. Despite its diverse applications, existing neural models are prone to producing short and generic replies, making it infeasible to tackle open-domain challenges. In this research, we analyze this critical issue in light of the model's optimization goal and the specific characteristics of the human-to-human dialog corpus. By decomposing the black box into parts, a detailed analysis of the probability limit was conducted to reveal the reason behind these universal replies. Based on these analyses, we propose a max-margin ranking regularization term to avoid the models leaning to these replies. Finally, empirical experiments on case studies and benchmarks with several metrics validate this approach.\"","summary":"\"Conversation dialog has been accumulating in the online communities making the data-driven dialog model possible @cite_10 . In literature, query-response pairs are modeled by seq2seq model with attention mechanism @cite_24 @cite_17 @cite_19 . And the essence of the neural response generation model are designed by maximizing the likelihood of target response given source query in the training procedure. As various response are reasonable to reply to a query, the information included in the query is limited to constrain the model inference, which makes the NRG models prefer universal replies with minimum risk @cite_8 @cite_2 .\"","":""}
{"id":"2889519671","dialogue":"\"Recent advances in sequence-to-sequence learning reveal a purely data-driven approach to the response generation task. Despite its diverse applications, existing neural models are prone to producing short and generic replies, making it infeasible to tackle open-domain challenges. In this research, we analyze this critical issue in light of the model's optimization goal and the specific characteristics of the human-to-human dialog corpus. By decomposing the black box into parts, a detailed analysis of the probability limit was conducted to reveal the reason behind these universal replies. Based on these analyses, we propose a max-margin ranking regularization term to avoid the models leaning to these replies. Finally, empirical experiments on case studies and benchmarks with several metrics validate this approach.\"","summary":"\"To address this issue, various works are conducted on bringing more information to influence these models. propose to utilize keywords to constrain the topic of responses, and incorporates the replies with topic information. By contrary, some researcher believed in diverse responses are just buried by the greedy beam-search rules @cite_7 , so they focus on involving more punishment or randomness in the inference stages. To illustrate, constrain the search space using mutual information with the query and randomly chose candidate words from top beams to consist short phrases. These existing works mainly focus on the generation strategies during inference, in contrast, the model's architecture and loss function have rarely been explored.\"","":""}
{"id":"2888833318","dialogue":"\"When might human input help (or not) when assessing risk in fairness domains? Dressel and Farid (2018) asked Mechanical Turk workers to evaluate a subset of defendants in the ProPublica COMPAS data for risk of recidivism, and concluded that COMPAS predictions were no more accurate or fair than predictions made by humans. We delve deeper into this claim to explore differences in human and algorithmic decision making. We construct a Human Risk Score based on the predictions made by multiple Turk workers, characterize the features that determine agreement and disagreement between COMPAS and Human Scores, and construct hybrid Human+Machine models to predict recidivism. Our key finding is that on this data set, Human and COMPAS decision making differed, but not in ways that could be leveraged to significantly improve ground-truth prediction. We present the results of our analyses and suggestions for data collection best practices to leverage complementary strengths of human and machines in the fairness domain.\"","summary":"\"In addition to the work mentioned in the Introduction, Lakkaraju et. al., ( lakkaraju2017selective ) showed that analyses of recidivism based on human decisions are further complicated by the selective labels'' problem, where observability of outcomes are affected by judges' release decisions. Other work studied how humans perceive different features as fair or not @cite_8 .\"","":""}
{"id":"2950945984","dialogue":"\"We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.\"","summary":"\"Fully-supervised approaches based on neural networks have achieved impressive results on fine-grained sentiment classification @cite_11 @cite_27 . More recently, (MIL) models have been proposed that use freely available review ratings to train segment-level predictors. and train sentence-level predictors under a MIL objective, while our previous work introduced , a hierarchical model that is trained end-to-end on document labels and produces polarity-based opinion summaries of single reviews. Here, we use to predict the sentiment polarity of individual opinions.\"","":""}
{"id":"2950945984","dialogue":"\"We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.\"","summary":"\"A few extractive neural models have been recently applied to generic multi-document summarization. train a recursive neural network using a ranking objective to identify salient sentences, while follow-up work @cite_0 employs a multi-task objective to improve sentence extraction, an idea we adapted to our task. propose a graph convolution network to represent sentence relations and estimate sentence salience. Our summarization method is tailored to the opinion extraction task, it identifies aspect-specific and salient units, while minimizing the redundancy of the final summary with a greedy selection algorithm @cite_16 @cite_26 . Redundancy is also addressed in who propose a graph-based framework for abstractive summarization. introduce an encoder-decoder neural method for extractive opinion summarization. Their approach requires direct supervision via gold-standard extractive summaries for training, in contrast to our weakly supervised formulation.\"","":""}
{"id":"2950615878","dialogue":"\"We propose a large margin criterion for training neural language models. Conventionally","summary":"neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However","":""}
{"id":"2950615878","dialogue":"\"We propose a large margin criterion for training neural language models. Conventionally","summary":"neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However","":""}
{"id":"2950615878","dialogue":"\"We propose a large margin criterion for training neural language models. Conventionally","summary":"neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However","":""}
{"id":"2948856152","dialogue":"\"Sentence-level representations are necessary for various NLP tasks. Recurrent neural networks have proven to be very effective in learning distributed representations and can be trained efficiently on natural language inference tasks. We build on top of one such model and propose a hierarchy of BiLSTM and max pooling layers that implements an iterative refinement strategy and yields state of the art results on the SciTail dataset as well as strong results for SNLI and MultiNLI. We can show that the sentence embeddings learned in this way can be utilized in a wide variety of transfer learning tasks, outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks. Furthermore, our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences.\"","summary":"\"There is a wide variety of approaches to sentence-level representations that can be used in natural language inference. @cite_3 and @cite_0 explore RNN and LSTM architectures, @cite_2 convolutional neural networks and @cite_5 GRUs, to name a few. The basic idea behind these approaches is to encode the premise and hypothesis sentences separately and then combine those using a neural network classifier.\"","":""}
{"id":"2948856152","dialogue":"\"Sentence-level representations are necessary for various NLP tasks. Recurrent neural networks have proven to be very effective in learning distributed representations and can be trained efficiently on natural language inference tasks. We build on top of one such model and propose a hierarchy of BiLSTM and max pooling layers that implements an iterative refinement strategy and yields state of the art results on the SciTail dataset as well as strong results for SNLI and MultiNLI. We can show that the sentence embeddings learned in this way can be utilized in a wide variety of transfer learning tasks, outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks. Furthermore, our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences.\"","summary":"\"@cite_4 explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks. They show that, out of these models, BiLSTM with max pooling achieves the strongest results not only in NLI but also in many other NLP tasks requiring sentence level meaning representations. They also show that their model trained on NLI data achieves strong performance on various transfer learning tasks.\"","":""}
{"id":"2888436843","dialogue":"\"In the past few years, various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks (GANs). GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer. In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data. Attempts have been made for utilizing GANs with word embeddings for text generation. This work presents an approach to text generation using Skip-Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures. The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used.\"","summary":"Supervised learning with deep neural networks in the framework of encoder-decoder models has become the state-of-the-art methods for approaching NLP problems @cite_23 . Recent text generation models use a wide variety of GANs such as gradient policy based sequence generation framework @cite_10 and an actor-critic conditional GAN to fill missing text conditioned on surrounding text @cite_1 for performing natural language generation tasks. Other architectures such as those proposed in @cite_11 with RNN and variational autoencoder generator with CNN discriminator and in @cite_28 with leaky discriminator to guide generator through high-level extracted features have also shown great results.","":""}
{"id":"2933982025","dialogue":"\"Visual exploration of large multidimensional datasets has seen tremendous progress in recent years, allowing users to express rich data queries that produce informative visual summaries, all in real time. Techniques based on data cubes are some of the most promising approaches. However, these techniques usually require a large memory footprint for large datasets. To tackle this problem, we present NeuralCubes: neural networks that predict results for aggregate queries, similar to data cubes. NeuralCubes learns a function that takes as input a given query, for instance, a geographic region and temporal interval, and outputs the result of the query. The learned function serves as a real-time, low-memory approximator for aggregation queries. NeuralCubes models are small enough to be sent to the client side (e.g. the web browser for a web-based application) for evaluation, enabling data exploration of large datasets without database network connection. We demonstrate the effectiveness of NeuralCubes through extensive experiments on a variety of datasets and discuss how NeuralCubes opens up opportunities for new types of visualization and interaction.\"","summary":"\"Our approach is inspired by the recent success of applying deep neural networks to a variety of domains, including image recognition @cite_2 , machine translation @cite_19 , and speech recognition @cite_20 . These techniques are solely focused on prediction, and our method is similar, in that we are focused on training deep networks for the purposes of query prediction. Yet, we differ in that prediction is not the only goal, rather we want to perform learning in a manner that provides the user a fast and low-memory-cost way to visually explore data. The query prediction task at hand can be viewed as a means to realize these goals.\"","":""}
{"id":"2933982025","dialogue":"\"Visual exploration of large multidimensional datasets has seen tremendous progress in recent years, allowing users to express rich data queries that produce informative visual summaries, all in real time. Techniques based on data cubes are some of the most promising approaches. However, these techniques usually require a large memory footprint for large datasets. To tackle this problem, we present NeuralCubes: neural networks that predict results for aggregate queries, similar to data cubes. NeuralCubes learns a function that takes as input a given query, for instance, a geographic region and temporal interval, and outputs the result of the query. The learned function serves as a real-time, low-memory approximator for aggregation queries. NeuralCubes models are small enough to be sent to the client side (e.g. the web browser for a web-based application) for evaluation, enabling data exploration of large datasets without database network connection. We demonstrate the effectiveness of NeuralCubes through extensive experiments on a variety of datasets and discuss how NeuralCubes opens up opportunities for new types of visualization and interaction.\"","summary":"\"Our approach to training neural networks for data exploration can be viewed as a form of making deep networks more interpretable. In the visual analytics (VA) literature there has been much recent work devoted to the interpretability of deep networks, namely with respect to the training process, the learned features of the network, the input domain, and the network's output space. @cite_33 visualize convolutional neural networks (CNNs) by clustering activations within each layer given a set of inputs, and visualize prominent flows between clustered activations. Other VA approaches to interpreting CNNs have considered visualizing correlations between classes during the training process @cite_0 , and visualizing per-layer convolution filters and their relationships, in order to understand filters that are important to training @cite_25 . Visualizing and understanding recurrent neural networks (RNNs) has also received much attention, through understanding the training process @cite_24 , as well as understanding hidden states dynamics @cite_47 . All of these approaches seek to provide interpretability for deep neural networks that were never designed to be interpretable. In contrast, our approach into the network, such that the user can take advantage of different aspects of the learned network to help their exploration.\"","":""}
{"id":"2933982025","dialogue":"\"Visual exploration of large multidimensional datasets has seen tremendous progress in recent years, allowing users to express rich data queries that produce informative visual summaries, all in real time. Techniques based on data cubes are some of the most promising approaches. However, these techniques usually require a large memory footprint for large datasets. To tackle this problem, we present NeuralCubes: neural networks that predict results for aggregate queries, similar to data cubes. NeuralCubes learns a function that takes as input a given query, for instance, a geographic region and temporal interval, and outputs the result of the query. The learned function serves as a real-time, low-memory approximator for aggregation queries. NeuralCubes models are small enough to be sent to the client side (e.g. the web browser for a web-based application) for evaluation, enabling data exploration of large datasets without database network connection. We demonstrate the effectiveness of NeuralCubes through extensive experiments on a variety of datasets and discuss how NeuralCubes opens up opportunities for new types of visualization and interaction.\"","summary":"\"In this context, our method for learning features of aggregation queries can be viewed as a form of unsupervised learning, where treating query prediction as pretext, the features that we learn along the way can be used for other purposes -- in our case exploratory purposes. This is similar to recent techniques in computer vision that learn features using different forms of self supervision, for instance learning to predict spatial context @cite_42 @cite_11 , temporal context @cite_17 , and perhaps more pertinent to our work, learning to count visual primitives in a scene @cite_21 . These techniques solve certain types of relevant visual tasks that do not require human supervision, but then extract the learned features for supervised learning. Our approach is similar: our training data does not require human intervention, since it is built from existing data cubes techniques, yet the features that we learn from this task can be used to help with visual data exploration.\"","":""}
{"id":"2933982025","dialogue":"\"Visual exploration of large multidimensional datasets has seen tremendous progress in recent years, allowing users to express rich data queries that produce informative visual summaries, all in real time. Techniques based on data cubes are some of the most promising approaches. However, these techniques usually require a large memory footprint for large datasets. To tackle this problem, we present NeuralCubes: neural networks that predict results for aggregate queries, similar to data cubes. NeuralCubes learns a function that takes as input a given query, for instance, a geographic region and temporal interval, and outputs the result of the query. The learned function serves as a real-time, low-memory approximator for aggregation queries. NeuralCubes models are small enough to be sent to the client side (e.g. the web browser for a web-based application) for evaluation, enabling data exploration of large datasets without database network connection. We demonstrate the effectiveness of NeuralCubes through extensive experiments on a variety of datasets and discuss how NeuralCubes opens up opportunities for new types of visualization and interaction.\"","summary":"\"We also note that there is some very recent work that seeks to combine databases with neural networks. @cite_16 make the connection between indexing, such as b-trees or hashes, and models, and show that such indexing schemes can be learned using neural networks. Mitzenmacher @cite_29 consider similar learning techniques for Bloom filters. These methods are concerned with using neural networks to speed up computation and minimize memory storage. Although we demonstrate that our method can attain these benefits, the primary focus of our method is in using a neural network as an integral component to visual exploration, i.e. is not trying to predict queries that a database can answer.\"","":""}
{"id":"2964139713","dialogue":"\"The functional programming language Erlang is well-suited for concurrent and distributed applications, but numerical computing is not seen as one of its strengths. Yet, the recent introduction of Federated Learning, which leverages client devices for decentralized machine learning tasks, while a central server updates and distributes a global model, motivated us to explore how well Erlang is suited to that problem. We present the Federated Learning framework ffl-erl and evaluate it in two scenarios: one in which the entire system has been written in Erlang, and another in which Erlang is relegated to coordinating client processes that rely on performing numerical computations in the programming language C. There is a concurrent as well as a distributed implementation of each case. We show that Erlang incurs a performance penalty, but for certain use cases this may not be detrimental, considering the trade-off between speed of development (Erlang) versus performance (C). Thus, Erlang may be a viable alternative to C for some practical machine learning tasks.\"","summary":"\"There has been some preceding work in academia related to using functional programming languages for tackling machine learning tasks. About a decade ago, Allison explored using Haskell for defining various machine learning and statistical learning models @cite_15 . Yet, that work was of a theoretical nature. Going back even further, Yu and Clack presented a system for polymorphic genetic programming in Haskell @cite_3 . Likewise, this was from a theoretical perspective. More recently, Sher @cite_8 did extensive work on modeling evolutionary computations. A central part of his contribution is an ANN implemented in Erlang. However, his fairly complex system could only have been used as the starting point of our work with substantial modifications. One key difference is that individual nodes of the ANN are modeled as independent processes, and so are sensors and actuators. A related ANN implementation in Erlang is yanni , The corresponding code repository is located at https: bitbucket.org nato yanni (accessed on August 6, 2018). which follows Sher's approach of using message passing, albeit only between layers.\"","":""}
{"id":"2952114694","dialogue":"\"In this paper, we propose an offline counterfactual policy estimation framework called Genie to optimize Sponsored Search Marketplace. Genie employs an open box simulation engine with click calibration model to compute the KPI impact of any modification to the system. From the experimental results on Bing traffic, we showed that Genie performs better than existing observational approaches that employs randomized experiments for traffic slices that have frequent policy updates. We also show that Genie can be used to tune completely new policies efficiently without creating risky randomized experiments due to cold start problem. As time of today, Genie hosts more than 10000 optimization jobs yearly which runs more than 30 Million processing node hours of big data jobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of the major platforms to optimize Bing Ads Marketplace due to its reliability under frequent policy changes and its efficiency to minimize risks in real experiments.\"","summary":"\"Counterfactual reasoning and estimation systems @cite_19 @cite_21 @cite_14 answer questions like \"\"how would the system performance had been changed if the modification had been applied to the system during log collection?\"\". The idea for these systems is to use online event logs to train models and predict KPIs for new modifications.\"","":""}
{"id":"2952114694","dialogue":"\"In this paper, we propose an offline counterfactual policy estimation framework called Genie to optimize Sponsored Search Marketplace. Genie employs an open box simulation engine with click calibration model to compute the KPI impact of any modification to the system. From the experimental results on Bing traffic, we showed that Genie performs better than existing observational approaches that employs randomized experiments for traffic slices that have frequent policy updates. We also show that Genie can be used to tune completely new policies efficiently without creating risky randomized experiments due to cold start problem. As time of today, Genie hosts more than 10000 optimization jobs yearly which runs more than 30 Million processing node hours of big data jobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of the major platforms to optimize Bing Ads Marketplace due to its reliability under frequent policy changes and its efficiency to minimize risks in real experiments.\"","summary":"\"Our focus in this paper is to propose efficient counterfactual estimator for practical tuning problems within the context of Sponsored Search System. In particular, we target on tuning scenarios where @math testing and existing observational approaches can not work efficiently. We split the causal graph for the sponsored search problem into input and system layer and apply different methodologies to predict outcomes. While machine learning methods gives very good results for predicting user click behaviors @cite_3 @cite_12 @cite_16 , we used log replay based to estimate the outcome of the sponsored search system. The reader could think of applying similar approaches for policy optimization problem when training data is very noisy due to frequent system changes or when it is not practical to create online experiment with the proposed modification.\"","":""}
{"id":"2888396831","dialogue":"\"With the development of information technology, there is an explosive growth in the number of online comment concerning news, blogs and so on. The massive comments are overloaded, and often contain some misleading and unwelcome information. Therefore, it is necessary to identify high-quality comments and filter out low-quality comments. In this work, we introduce a novel task: high-quality comment identification (HQCI), which aims to automatically assess the quality of online comments. First, we construct a news comment corpus, which consists of news, comments, and the corresponding quality label. Second, we analyze the dataset, and find the quality of comments can be measured in three aspects: informativeness, consistency, and novelty. Finally, we propose a novel multi-target text matching model, which can measure three aspects by referring to the news and surrounding comments. Experimental results show that our method can outperform various baselines by a large margin on the news dataset.\"","summary":"\"There is previous work regarding rating the academic paper @cite_7 , while this work is about rating the news comments, which is different from them.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Remote control may be accomplished through the use of a wide variety of interfaces @cite_91 . These have changed considerably with the advent of Human-Centered Design (HCD) and have evolved from joystick-and-button black boxes to force-feedback devices @cite_21 , smart gloves @cite_33 , body-suits @cite_132 , verbal instructions @cite_9 , and vision-based analysis of natural body motions @cite_36 @cite_44 @cite_126 @cite_2 . While attractive, an immediate complication in using these methods is obtaining and learning to use the required equipment. In addition, mixed results from immersive teleoperation have been reported, resulting in efforts to reduce the cognitive load of the operators @cite_38 . In @cite_116 a clear trade-off was detected: greater control and situational awareness came at the cost of possible cognitive overload and impaired performance.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"On the other hand, there has been a lot of effort in developing methods for reducing the cognitive load on the user in 2D-GUIs. In @cite_77 , simple constraining of motions was effective in offloading some of the cognitive effort. The familiarity and widespread use of smartphones and tablets has also brought attention to touch-based control. In @cite_78 , a simple touch-based interface was aimed at reducing operator fatigue.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Interfaces based on Monitor, Mouse, and Keyboard are a simple and cheap alternative that has been applied to a wide variety of robotics applications like robot navigation @cite_26 , grasping @cite_66 @cite_56 @cite_113 , or object manipulation @cite_16 @cite_53 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Work on human perception points to the fact that we process 3D objects as arrangements of 2D views @cite_97 @cite_109 . In addition, several tasks involving shape understanding only need 2D views, like recognition and detection @cite_59 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Display type may play a part on the ability to perform different tasks. In the work done by St @cite_124 a distinction is made between scene understanding and specific tasks involving the precise judgment of relative position. They compared pure 2D interface without projective effects to 3D displays: 2D views of 3D scenes with perspective effects. They reason that, while 3D displays seem compelling, integrated and natural, they can cause ambiguity or distortion by the nature of their presentation in a 2D display. In particular, for tasks requiring the precise relative position of two objects, 2D views proved superior.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Informative can be obtained from users in several ways. One possibility is to add tags and qualifiers to a dataset after it has been acquired. This can range from intensive human labeling in post-production to methods that require minimal markings. Full image segments are drawn by humans and used as ground truth in @cite_98 , whereas in @cite_48 , the segmentation algorithm needs only an initial seed point provided by a human. In 3D environments, human-segmented data is often used for training and validation @cite_22 . Simple hints have also been used in these environments to seed automatic segmentation algorithms @cite_52 . Annotations can also be used for tracking objects in 3D from an initial seed labeling @cite_14 . In @cite_131 , an annotated dataset is constructed for robot navigation.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Learning from Demonstration (LfD) offers an alternative to the active addition of annotations in post-processing. In LfD, the parameters of actuation are implicitly refined through the observation of examples provided by humans. This happens because under this strategy, perception-action systems are built specifically to follow human actions and replicate them within their own circumstances (like different kinematic structures or safety constraints). See @cite_87 for a thorough review.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"A third approach is , where an operator indicates appropriate motions by physically guiding the robot while it records the event @cite_125 . Alternatively, the robot may be guided virtually, aided with a variety of feedback options @cite_102 @cite_10 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"In one sense, annotation can be obtained without human supervision by automatically processing the data and extracting useful intelligence from the collected information. Metadata can be analyzed as a form of annotation for the payload it describes. Often, the input data itself comes from data sources rich with explicit or implicit human annotation. In @cite_63 , automatic classification methods are trained using existing human-interaction datasets that have been previously grouped by activity. While effective in its own right, this type of unsupervised learning needs to be validated before being put to used in any context where safety, preference, or informed judgment is required. One problem with unintentional annotation is that it is unstructured and noisy by nature, making the extraction of usable intelligence a considerably more difficult proposition @cite_62 . When possible, it is preferable to use annotations that are directly aimed at enhancing the understanding of a situation or task that one intends to refine.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Annotation of Perceptual Data There are different mediums for annotation that the robot may provide or use as input for operation. When using kinesthetic learning, the system focuses on the recording of changes in its joint space with respect to a given objective @cite_125 . While useful, it is usually more powerful to also have a way of capturing the surrounding environment. A common approach is to use visual perception of some kind. This is due to the enormous amount of relevant information that may be extracted from this approach. It may capture color, changes in spatial and temporal relations, and importantly, it's a perceptual stream that we as humans generally favor to analyze our environments. Annotation of images has a long tradition in robotics. For traditional image stills or streams of images (2D), large databases have been constructed for segmentation @cite_98 and object recognition @cite_94 @cite_128 . More recently, stereo vision and depth sensors have extended the perceptual stream to 3D. In this realm, several annotated databases have been constructed @cite_18 @cite_17 . The volumetric and color information has been used for object recognition @cite_24 , tracking @cite_14 , or shape reconstruction @cite_52 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Off-the-shelf sensors like the Microsoft Kinect give easy access to these input streams, and have made the 3D scanning of environments and objects ubiquitous. Several algorithms have been developed and refined to allow the integration of depth sensing onto unified models of the scenes they capture @cite_30 . The Kinect Fusion algorithm @cite_39 , and its many variants @cite_6 , allow the tracking of a scene and integration of captured depth-frames into a coherent volumetric representation. Some variants have even accomplished deformation and part-tracking @cite_90 . One important aspect of the models that these approaches create is their sensitivity to the sensor and the context. The Kinect has a resolution of about 2 mm and has trouble capturing thin surfaces, transparent or shiny objects and requires careful scanning or large amounts of computation for dealing with noise. While various techniques exist to deal with these problems, when these appear in conjunction to context-dependent constraints, human annotations become highly valuable. For an in-depth review of automatic and semi-automatic methods of reconstruction from point clouds, see @cite_123 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Annotations that help solve particular instances of problems can be gathered into a knowledge-base that can, in turn, be studied to gain insights into the solving of more general situations. One way to gather a large amount of usable annotations is to take advantage of the growing infrastructure for tasks, like Amazon's Mechanical Turk @cite_25 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"One important aspect of GUI design is to consider the user and the objectives. Several studies have looked at what humans can and can't do in these robotics tasks, what interaction exists between an individual's abilities, and what the interface can do @cite_38 @cite_106 . In addition, studies involving tasks in 3D space usually consider human natural abilities or previous experience in similar environments @cite_16 . Indeed, studies suggest that spatial reasoning ability not only varies between individuals, but that different tasks might require different abilities within the realm of spatial reasoning @cite_81 @cite_41 . One common precept in HCD is to avoid visual clutter in order to lower cognitive load. Studies have found that novice users get better results on interfaces that hide extraneous features @cite_46 , and where they only focus on the high-level tasks while the robot controls the low level details @cite_15 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Object reconstruction refers to the recovery of a real object's structure in digital form from some stream of perceptual data. Significant progress has been made in automatic reconstruction. One of the general methods consists of using multiple 2D views of an object to obtain its shape @cite_11 @cite_88 . While this approach allows the use of monocular cameras, depth-annotated images have become easily accessible and allow for accurate pose estimation and refinement of models @cite_12 . The work of @cite_39 used the Microsoft Kinect sensor to integrate depth-augmented images into a single volumetric model of the scene. This approach was very successful and has been used and refined extensively @cite_6 @cite_90 @cite_80 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"The main problem with automatic reconstruction is its sensitivity to input resolution and any noise or artifacts the sensor might cause. A well known problem has to do with the natural difficulties in dealing with thin structures, noisy scans, or shiny or transparent surfaces @cite_29 . Several algorithms attempt to combat these problems by imposing correction mechanisms to preserve shape or inferred structure @cite_95 @cite_105 @cite_50 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"A third option is semi-automatic reconstruction. Here, the idea is to use human cues to seed or refine automatic methods. These can work on 2D images, like the work of @cite_89 , where simple human marks on 2D images may result in the creation of 3D objects. The work of @cite_104 integrates simple annotations on 2D images with 3D object models extracted from a repository to extract complex objects from 2D scenes. Another approach is to work directly with 3D scans of a scene. This can either be on the depth-based point clouds or the reconstructed mesh. In @cite_115 , simple cues help snap planar primitives on top of point cloud sections in order to build simple architectural models. In the work of @cite_52 , annotations are made on reconstructed 3D models of the whole environment to generate segmented 3D scenes. In the work of @cite_95 , topology-aware reconstruction is first attempted automatically, and then iteratively corrected with human input. A refined interface showing a similar approach is shown in the work by @cite_118 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"One important distinction between methods is the form these annotations take. In @cite_52 @cite_89 simple lines and sketches are interpreted as instructions and combined with the section of the images they are used on. The results are either 3D models of the 2D-segmented shapes, or a segmented 3D space, where each part of the scene may be labeled with the object categories. Another typical objective is the creation of new object models from existing ones. One approach is to deform them into new shapes. In the work of @cite_40 @cite_86 , cuboid proxies are used to guide shape deformations.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"The idea of using shape proxies for future actions over the base shape is, in a sense, a method of annotation. In the work of @cite_110 , a slice-based proxy was developed for representing 3D meshes. These were placed by humans and later used as training input for an automatic planar-proxy creation algorithm. The potential applications range from printing simplified 3D objects to the annotation and recovery of 3D shapes. In a follow-up work, @cite_57 used these planar proxies, in conjunction with crowdsourcing to study user abilities regarding estimation of surface normals in commonly occurring 3D object models.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"In the work of @cite_121 , a is extracted from 3D object meshes and later used to deform them by performing simple modifications of the underlying proxy. This approach highlights the power of simplified representations for object analysis and modification. The work of @cite_108 shows the use of these simple sweep-based proxies to fit 2D sketches and transform them to 3D objects.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"@cite_118 , developed an annotation scheme that uses simple gestures and the structure of object point clouds to infer a set of underlying part proxies that can be modified to reconstruct the shape. This gesture-based reconstruction approach can help deal with incomplete scans and was designed specifically to offer the best trade-off between user-effort and shape quality. The inference of underlying shape is carried out automatically, with some parameter tweaking done by the operator. One disadvantage is that this automatic reconstruction has trouble dealing with multiple small holes or with topologies that contain cavities, like mugs or bowls. A similar approach has been used to segment meshes into generalized cylinders @cite_19 . Two typical topology-related problems are objects missing cavities, like a mug without a cavity @cite_118 or unexpected topologies like mugs with various holes @cite_110 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Some studies avoid using object models and use the perceptual input directly to extract grasp affordances @cite_119 . In @cite_65 , local surface features are used to propose grasps. Some techniques use a multitude of heuristics to generate grasp hypotheses from the sensed point clouds of the scene @cite_96 . These approaches are, in a way, model-less and allow the grasping of objects by their current appearance. An alternative approach uses object models to store and seed the inference of grasps. In the work of @cite_55 @cite_60 , , develop a technique called where low-dimensional grasp subspaces are computed that match the object shape. These in turn may be used for seeding interactive grasping approaches. Some approaches are denominated hybrid, since they use appearance features or 3D-model detection depending on the situation @cite_4 . While automatic grasping is a promising field, context and functional requirements might place additional conditions on the choosing fo a specific grasp.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"A slightly less independent'' procedure is to match and apply grasping templates to the sensed input @cite_51 . For graping based on existing knowledge-bases, one option is to detect known objects followed by executing a specific grasp linked to that particular instance or that category of objects @cite_23 @cite_49 . In @cite_85 , shape primitives are linked to the target object and used to seed appropriate contact-level grasping.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Databases of grasping examples may be used as a starting point for automatically generating a grasp hypotheses @cite_92 . While these approaches are not fully automatic, they already contain important semantic information that might be germane to a multitude of context-dependent grasping tasks. In @cite_23 @cite_69 , shape and objective constraints are used to specify appropriate grasps.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Humans are excellent at quickly arriving at functional grasps, even for novel objects. We have the advantage of context and experience to help guide our choice. Whether for direct grasp suggestions, or for creating labeled model data, human annotations have been widely used. Human action may be observed and used under the LfD strategy and then processed to generate robot grasps @cite_130 . Alternatively, human input can serve to label grasping databases or to interactively choose grasps @cite_60 . In summary, a range of automatic and interactive methods exist that can use either raw perception and automation or existing models to obtain grasps. For a survey on data-driven grasp synthesis, see @cite_122 .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Significant effort has been placed on obtaining a taxonomy of types of grasps and to judge their physical qualities @cite_84 @cite_93 @cite_111 . These studies draw inspiration from early analysis of human grasping @cite_61 or robot hands @cite_35 . One widely used approach for judging grasp quality is that of the metric developed by @cite_27 . It uses the grasp contact points to construct a convex hull over the set of wrenches that can be applied with the given grasp. This is called the Grasp Wrench Space (GWS). Two measurements that can be used are the volume of the computed convex hull and the radius of the maximum sphere contained in it, which we call the epsilon-distance, or @math -dist. These indicate the robustness of a grasp and the versatility of the wrenches applicable for the given grasp. One popular tool used in the literature to evaluate grasp quality and generate grasp candidates is GraspIt! @cite_66 . It uses the GWS volume and @math metrics to rank grasps that have been generated using a variety of grasp-planning techniques including the above mentioned .\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"While several methods abound that use existing 3D mesh objects or that detect them from 2D-views or point clouds, less effort has been paid to linking grasps to simpler shape representations or proxies like the ones obtained in @cite_121 @cite_110 . As mentioned previously, these proxies are compact versions of objects, and can be used to represent categories as well as instances, and help indicate shape, as well as topology. Linking grasps to these might work as a sort of look-up table for grasps.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"Grasping and manipulation are closely related. For some grasps finding the final contacts of the gripper is insufficient. In several tasks, the approach to accomplish the grasp, as well as the task-dependent manipulation that will be performed affect the generated grasp hypotheses. Some studies have looked at shape category and the physical context to determine manipulation constraints @cite_69 , while others have used human experience to seed approach vectors @cite_33 . In @cite_23 , a grasp planning algorithm is used in conjunction to a set of task-dependent semantic constraints to choose grasps, which in turn may inform the approach vector. These semantic constraints are extracted from example grasps and then used to construct a semantic affordance map which directly relates the object class (obtained from object depth information) to the approach vectors and different task-appropriate grasps. This provides evidence that relating object instance or category models to manipulation annotations is of great utility.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"One popular tool for motion planning is Moveit! @cite_113 . This tool can be used within the larger Robot Operating System (ROS) to perform automatic motion planning with collision avoidance. This may also be used in conjunction with a manipulation interface that humans can use to indicate motions. In the work by , @cite_16 , this interface was used to analyze manipulation strategies for indicating grasps and grasp approach methodologies. They compared continuous teleoperation against three other strategies with varying degrees of autonomy. They showed than a combination of manual annotation and automatic processing resulted in the greatest success rate. This is supported by the work done by Hertkorn @cite_129 , where the importance of shared workload is thoroughly analyzed in the context of grasping and manipulation.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"We based our object modeling usability study on similar studies involving interactive shape reconstruction from point clouds @cite_95 @cite_40 @cite_115 . These constitute the state-of-the-art on this type of interactive approaches @cite_32 @cite_123 . In these studies, user interactions with their software are logged and timed, and later analyzed for precision errors. In @cite_95 @cite_40 , an expert user illustrates the possibilities of their interface, while in @cite_115 , five users are tested. Similarly to these studies, we recorded action choices and timing and evaluated shape quality. We also chose to include an expert user to show the possibilities of the interface if enough time is devoted to learning it.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"In the area of object handling, the work by Hertkorn @cite_129 has one study with a similar objective (evaluating the effect a particular grasping assistance technique). In this study, @math participants were were asked to complete @math grasp trials each. Basic shapes were chosen to attempt to eliminate the effects of shape complexity on the interface's effectiveness in assisted grasping. Similarly, we chose to use simple and or familiar basic shapes.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"@cite_8 uses a database of preplanned grasps, as well as an online planner to help a user find appropriate grasps. Time and success rate was computed for five subjects attempting three tasks. While five subjects is a small number, results indicate trends and allow the research to hone in on the appropriate refinements for a more in-depth evaluation.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"The work by @cite_16 is designed to compare different interaction strategies for attaining valid grasps using the PR2 robot. In this study, @math participants were asked to grasp as many objects as possible (between @math and @math ) in three rounds of trials. Participants were shown a tutorial before the start of the trial.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"In the work of @cite_56 , several tasks and trials were tested using crowdsourcing. Here, the annotation tasks were simple in order to allow testing the approach, and later integrated with the ultimate objective of completing pick-and-place tasks. They found that in many cases a single average worker'' produced poor results, but that by cleaning and averaging @math of them, high accuracy was obtained.\"","":""}
{"id":"2888691113","dialogue":"\"We present and evaluate an approach for human-in-the-loop specification of shape reconstruction with annotations for basic robot-object interactions. Our method is based on the idea of model annotation: the addition of simple cues to an underlying object model to specify shape and delineate a simple task. The goal is to explore reducing the complexity of CAD-like interfaces so that novice users can quickly recover an object's shape and describe a manipulation task that is then carried out by a robot. The object modeling and interaction annotation capabilities are tested with a user study and compared against results obtained using existing approaches. The approach has been analyzed using a variety of shape comparison, grasping, and manipulation metrics, and tested with the PR2 robot platform, where it was shown to be successful.\"","summary":"\"The work by @cite_67 uses a simple protocol that involves input and output surveys, a tutorial, and a challenge section. While they were focused on evaluating different methods of directing robot motion and attention, we used the same protocol structure for our own annotation tests.\"","":""}
{"id":"2888353881","dialogue":"\"Identification of the most frequent sense of a polysemous word is an important semantic task. We introduce two concepts that can benefit MFS detection: companions, which are the most frequently co-occurring words, and the most frequent translation in a bitext. We present two novel methods that incorporate these new concepts, and show that they advance the state of the art on MFS detection.\"","summary":"\"@cite_24 lay the groundwork for by analyzing the relevance of GermaNet synsets to specific domains. @cite_2 build upon this, showing that WSD performance can be improved by performing on a corpus of the same domain as the testing data. @cite_3 present an method based on a published thesaurus, which they use to induce a coarse-grained sense inventory. This separates their method from other related work, which typically uses WordNet as the sense inventory for WSD and .\"","":""}
{"id":"2888353881","dialogue":"\"Identification of the most frequent sense of a polysemous word is an important semantic task. We introduce two concepts that can benefit MFS detection: companions, which are the most frequently co-occurring words, and the most frequent translation in a bitext. We present two novel methods that incorporate these new concepts, and show that they advance the state of the art on MFS detection.\"","summary":"\"@cite_20 present a method for based on a thesaurus constructed from a parsed corpus. This thesaurus is used to induce a word similarity function, which they use to assess the prevalence of each sense of a given target word. They perform both intrinsic and extrinsic evaluations; we compare to their reported results to the extent their experimental setup allows. This method was subsequently applied to a WSD shared task @cite_23 , and to the identification of infrequent word senses @cite_12 . An extended analysis of the method was presented by @cite_5 . @cite_39 adapt this method to Japanese using only the glosses of words, excluding the use of semantic networks such as WordNet.\"","":""}
{"id":"2888353881","dialogue":"\"Identification of the most frequent sense of a polysemous word is an important semantic task. We introduce two concepts that can benefit MFS detection: companions, which are the most frequently co-occurring words, and the most frequent translation in a bitext. We present two novel methods that incorporate these new concepts, and show that they advance the state of the art on MFS detection.\"","summary":"\"@cite_21 present the first method based on automatically learned vector word embeddings. They test their method on English and a private Hindi dataset. This is the most recent work we are aware of which considers the exact same task as we do, in the same setting; given its recency relative to other such works, we consider this to be the state-of-the-art for . We re-implement this method, and compare to it directly in our experimental evaluation.\"","":""}
{"id":"2888353881","dialogue":"\"Identification of the most frequent sense of a polysemous word is an important semantic task. We introduce two concepts that can benefit MFS detection: companions, which are the most frequently co-occurring words, and the most frequent translation in a bitext. We present two novel methods that incorporate these new concepts, and show that they advance the state of the art on MFS detection.\"","summary":"\"As part of one of our methods, we induce vector representations not only of words, but also of individual senses. Our method differs from recent prior work on constructing embeddings using sense information @cite_7 @cite_33 @cite_34 @cite_4 ; instead, we build upon the methods of @cite_13 and @cite_21 . The resulting vectors are easier to create with fewer resources, more interpretable, and easier to extend and compare to other types of vectors such as those we construct in Sections and . We are particularly interested in maintaining the ability to perform semantic comparisons across different languages, as demonstrated by prior work @cite_35 , motivating our decision to work with vectors known to have this property.\"","":""}
{"id":"2888353881","dialogue":"\"Identification of the most frequent sense of a polysemous word is an important semantic task. We introduce two concepts that can benefit MFS detection: companions, which are the most frequently co-occurring words, and the most frequent translation in a bitext. We present two novel methods that incorporate these new concepts, and show that they advance the state of the art on MFS detection.\"","summary":"\"Our methods leverage cross-lingual information and contextually related words. These concepts have previously been used to improve WSD -- @cite_9 @cite_27 @cite_19 @cite_16 , and others -- but our usage of these concepts for is novel.\"","":""}
{"id":"2886884222","dialogue":"\"We consider the problem of inferring the directed, causal graph from observational data, assuming no hidden confounders. We take an information theoretic approach, and make three main contributions. First, we show how through algorithmic information theory we can obtain SCI, a highly robust, effective and computationally efficient test for conditional independence---and show it outperforms the state of the art when applied in constraint-based inference methods such as stable PC. Second, building upon on SCI, we show how to tell apart the parents and children of a given node based on the algorithmic Markov condition. We give the Climb algorithm to efficiently discover the directed, causal Markov blanket---and show it is at least as accurate as inferring the global network, while being much more efficient. Last, but not least, we detail how we can use the Climb score to direct those edges that state of the art causal discovery algorithms based on PC or GES leave undirected---and show this improves their precision, recall and F1 scores by up to 20 .\"","summary":"\"Many methods for discovering Markov blankets and causal networks rely on conditional independence tests @cite_15 @cite_4 ---which, by conditioning on the empty set, can also be used to measure association. For discrete data, the state of the art commonly relies on either the @math test or conditional mutual information (CMI) @cite_15 @cite_29 @cite_14 . Both measures, however, have drawbacks. The @math test is biased to independence when only limited data is available, whereas CMI has the opposite problem. In particular, it holds that @math for any @math , even if @math . For both @math and CMI it is necessary to define a significance threshold or cut-off, which can be arbitrary. Our proposed method based on algorithmic independence overcomes these limitations.\"","":""}
{"id":"2886884222","dialogue":"\"We consider the problem of inferring the directed, causal graph from observational data, assuming no hidden confounders. We take an information theoretic approach, and make three main contributions. First, we show how through algorithmic information theory we can obtain SCI, a highly robust, effective and computationally efficient test for conditional independence---and show it outperforms the state of the art when applied in constraint-based inference methods such as stable PC. Second, building upon on SCI, we show how to tell apart the parents and children of a given node based on the algorithmic Markov condition. We give the Climb algorithm to efficiently discover the directed, causal Markov blanket---and show it is at least as accurate as inferring the global network, while being much more efficient. Last, but not least, we detail how we can use the Climb score to direct those edges that state of the art causal discovery algorithms based on PC or GES leave undirected---and show this improves their precision, recall and F1 scores by up to 20 .\"","summary":"\"The discovery of Markov blankets is important in two regards. First, it represents the optimal set of variables for feature selection @cite_15 and second, for investigating the local structure around a target it is much faster than discovering the whole Bayesian network @cite_2 . The idea of first discovering the neighbourhood of a node, instead of the full Bayesian network got more common with the grow and shrink (GS) algorithm @cite_8 . It consists of two sub routines. First, it discovers the potential parents and children in a bottom up approach. Then, it finds the spouses based on the parents and children from the previous step.\"","":""}
{"id":"2886884222","dialogue":"\"We consider the problem of inferring the directed, causal graph from observational data, assuming no hidden confounders. We take an information theoretic approach, and make three main contributions. First, we show how through algorithmic information theory we can obtain SCI, a highly robust, effective and computationally efficient test for conditional independence---and show it outperforms the state of the art when applied in constraint-based inference methods such as stable PC. Second, building upon on SCI, we show how to tell apart the parents and children of a given node based on the algorithmic Markov condition. We give the Climb algorithm to efficiently discover the directed, causal Markov blanket---and show it is at least as accurate as inferring the global network, while being much more efficient. Last, but not least, we detail how we can use the Climb score to direct those edges that state of the art causal discovery algorithms based on PC or GES leave undirected---and show this improves their precision, recall and F1 scores by up to 20 .\"","summary":"\"To the best of our knowledge, there exists no algorithm that directly discovers the directed, causal Markov blanket, i.e. that can tell apart parents, children and spouses given only the Markov blanket of a target. In , we first discover the Markov blanket, and then orient the edges. To discover the blanket, we build upon and extend the state of the art @cite_17 and @cite_24 algorithms. Both follow the general structure of the GS algorithm, with employing fast symmetry correction to exclude children of children. Zhu and Yang @cite_13 proposed to speed up by pre-filtering based on mutual information, whereas @cite_14 discover Markov blankets based on conditional mutual information. Unlike our approach, these approaches require the user to set a cut-off value and a scaling parameter @math . For an in depth overview of related algorithms, we refer to the following articles @cite_15 @cite_25 .\"","":""}
{"id":"2885143803","dialogue":"\"Probabilistic graphical models compactly represent joint distributions by decomposing them into factors over subsets of random variables. In Bayesian networks, the factors are conditional probability distributions. For many problems, common information exists among those factors. Adding similarity restrictions can be viewed as imposing prior knowledge for model regularization. With proper restrictions, learned models usually generalize better. In this work, we study methods that exploit such high-level similarities to regularize the learning process and apply them to the task of modeling the wave propagation in inhomogeneous media. We propose a novel distribution-based penalization approach that encourages similar conditional probability distribution rather than force the parameters to be similar explicitly. We show in experiment that our proposed algorithm solves the modeling wave propagation problem, which other baseline methods are not able to solve.\"","summary":"\"Multi-Task Learning (MTL) is a learning paradigm in machine learning and its target is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. Consider the wave propagation problem, we can formulize it as finding the distribution @math , where @math denotes the state of all nodes at time @math . The assumption is that the state of each node at time @math is independent of other nodes at time @math given the states of its neighbors at time @math . So the distribution can be decomposed into @math (assume we have @math nodes in total) conditional distributions. Rather than using @math neural networks to approximate those distributions, MTL uses one neural network with @math outputs. The first few hidden layers are shared among nodes, while the following layers are node-specific. @cite_9 shows that such setting greatly reduces the risk of overfitting. This makes sense intuitively: The more factors we are learning simultaneously, the more our model has to find a representation that captures all of the factors and the less is the chance of overfitting. We implement a MTL model as the opponent of our distribution-based penalization model.\"","":""}
{"id":"2886849295","dialogue":"\"Although stochastic gradient descent (SGD) method and its variants (e.g., stochastic momentum methods, AdaGrad) are the choice of algorithms for solving non-convex problems (especially deep learning), there still remain big gaps between the theory and the practice with many questions unresolved. For example, there is still a lack of theories of convergence for SGD and its variants that use stagewise step size and return an averaged solution in practice. In addition, theoretical insights of why adaptive step size of AdaGrad could improve non-adaptive step size of is still missing for non-convex optimization. This paper aims to address these questions and fill the gap between theory and practice. We propose a universal stagewise optimization framework for a broad family of non-smooth non-convex (namely weakly convex) problems with the following key features: (i) at each stage any suitable stochastic convex optimization algorithms (e.g., SGD or AdaGrad) that return an averaged solution can be employed for minimizing a regularized convex problem; (ii) the step size is decreased in a stagewise manner; (iii) an averaged solution is returned as the final solution that is selected from all stagewise averaged solutions with sampling probabilities increasing as the stage number. Our theoretical results of stagewise AdaGrad exhibit its adaptive convergence, therefore shed insights on its faster convergence for problems with sparse stochastic gradients than stagewise SGD. To the best of our knowledge, these new results are the first of their kind for addressing the unresolved issues of existing theories mentioned earlier. Besides theoretical contributions, our empirical studies show that our stagewise SGD and ADAGRAD improve the generalization performance of existing variants implementations of SGD and ADAGRAD.\"","summary":"\"Finally, we refer readers to several recent papers for other algorithms for weakly convex problems @cite_12 @cite_45 . For example, Drusvyatskiy and Paquette @cite_45 studied a subclass of weakly convex problems whose objective consists of a composition of a convex function and a smooth map, and proposed a prox-linear method that could enjoy a lower iteration complexity than @math by smoothing the objective of each subproblem. Davis and Drusvyatskiy @cite_12 studied a more general algorithm that successively minimizes a proximal regularized stochastic model of the objective function. When the objective function is smooth and has a finite form, variance-reduction based methods are also studied @cite_20 @cite_19 @cite_38 @cite_22 @cite_26 , which have provable faster convergence than in terms of @math . However, in all of these studies the convergence is provided on an impractical solution, which is either a solution that gives the minimum value of the (proximal) subgradient's norm @cite_45 or on a uniformly sampled solution from all iterations @cite_20 @cite_19 @cite_38 @cite_22 .\"","":""}
{"id":"2887894061","dialogue":"\"When neural networks process images which do not resemble the distribution seen during training, so called out-of-distribution images, they often make wrong predictions, and do so too confidently. The capability to detect out-of-distribution images is therefore crucial for many real-world applications. We divide out-of-distribution detection between novelty detection ---images of classes which are not in the training set but are related to those---, and anomaly detection ---images with classes which are unrelated to the training set. By related we mean they contain the same type of objects, like digits in MNIST and SVHN. Most existing work has focused on anomaly detection, and has addressed this problem considering networks trained with the cross-entropy loss. Differently from them, we propose to use metric learning which does not have the drawback of the softmax layer (inherent to cross-entropy methods), which forces the network to divide its prediction power over the learned classes. We perform extensive experiments and evaluate both novelty and anomaly detection, even in a relevant application such as traffic sign recognition, obtaining comparable or better results than previous works.\"","summary":"\"Anomaly and novelty detection. Also known as out-of-distribution detection, it aims at identifying inputs that are completely different from or unknown to the original data distribution used for training @cite_14 . @cite_18 , they perform novelty detection by learning a distance in an embedding. It proposes a Kernel Null Foley-Sammon transform that aims at projecting all the samples of each in-distribution class into a single point in a certain space. Consequently, novelty detection can be performed by thresholding the distance of a test sample to the nearest of the collapsed class representations. However, they employ handcrafted features, thus optimizing only the transform parameters and not the representation, like in the presently dominating paradigm of deep learning.\"","":""}
{"id":"2887894061","dialogue":"\"When neural networks process images which do not resemble the distribution seen during training, so called out-of-distribution images, they often make wrong predictions, and do so too confidently. The capability to detect out-of-distribution images is therefore crucial for many real-world applications. We divide out-of-distribution detection between novelty detection ---images of classes which are not in the training set but are related to those---, and anomaly detection ---images with classes which are unrelated to the training set. By related we mean they contain the same type of objects, like digits in MNIST and SVHN. Most existing work has focused on anomaly detection, and has addressed this problem considering networks trained with the cross-entropy loss. Differently from them, we propose to use metric learning which does not have the drawback of the softmax layer (inherent to cross-entropy methods), which forces the network to divide its prediction power over the learned classes. We perform extensive experiments and evaluate both novelty and anomaly detection, even in a relevant application such as traffic sign recognition, obtaining comparable or better results than previous works.\"","summary":"\"Metric Learning. Several computer vision tasks such as retrieval, matching, verification, even multi-class classification, share the need of being able to measure the similarity between pairs of images. Deriving such a measure from data samples is known as metric learning @cite_31 . Two often cited seminal works on this subject through neural networks are @cite_11 @cite_6 , where the Siamese architecture was proposed for this purpose. Differently from classification networks, the goal is to learn rather than a representation amenable for classification, one for measuring how similar two instances are in terms of the Euclidean distance. Another popular architecture is triplet networks @cite_27 . For both of them many authors have realized that mining the samples of the training set in order to find out or challenging pairs or triplets is important in order to converge faster or to better minima @cite_32 @cite_9 @cite_23 . Like them, we have also resorted to a mining strategy in order to obtain good results in the task of out-of-distribution detection.\"","":""}
{"id":"2951863869","dialogue":"\"Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.\"","summary":"\"6D pose estimation using only RGB information has been widely studied @cite_27 @cite_12 @cite_10 @cite_0 . Since this work concentrates on using point cloud inputs, which contain depth information, we mainly review works that also consider depth information. We also review how depth information can be represented.\"","":""}
{"id":"2951863869","dialogue":"\"Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.\"","summary":"\"Voting-based methods attempt to infer the pose of an object by accumulating evidence from local or global features of image patches. One example is the Latent-Class Hough Forest @cite_15 @cite_2 which adapts the template feature from @cite_32 for generating training data. During inference stage, a random set of patches is sampled from the input image. The patches are used in Hough voting to obtain pose hypotheses for verification.\"","":""}
{"id":"2951863869","dialogue":"\"Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.\"","summary":"\"3D object coordinates and object instance probabilities are learned using a Decision Forest in @cite_33 . The 6D pose estimation is then formulated as an energy optimization problem which compares synthetic images rendered with the estimated pose with observed depth values. 3D object coordinates are also used in @cite_22 @cite_23 . However, those approaches tend to be very computationally intensive due to generation and verification of hypotheses @cite_22 .\"","":""}
{"id":"2951863869","dialogue":"\"Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.\"","summary":"\"Most recent approaches rely on convolutional neural networks (CNNs). @cite_1 , the work in @cite_33 is extended by adding a CNN to describe the posterior density of an object pose. A combination of using a CNN for object segmentation and geometry-based pose estimation is proposed in @cite_8 . PoseCNN @cite_7 uses a similar two-stage network, in which the first stage extracts feature maps from RGB input and the second stage uses the generated maps for object segmentation, 3D translation estimation and 3D rotation regression in quaternion format. Depth data and ICP are used for pose refinement. @cite_14 propose a three-stage, instance-aware approach for 6D object pose estimation. An instance segmentation network is first applied, followed by an encoder-decoder network which estimates the 3D object coordinates for each segment. The 6D pose is recovered with a geometric pose optimization step similar to @cite_33 . The approaches @cite_1 @cite_14 @cite_7 do not directly use CNN to predict the pose. Instead, they provide segmentation and other intermediate information, which are used to infer the object pose.\"","":""}
{"id":"2951863869","dialogue":"\"Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from raw point cloud segments using a convolutional neural network. Experimental results show that our method can potentially achieve competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.\"","summary":"\"Depth information in deep learning systems can be represented with, e.g., voxel grids @cite_30 @cite_18 , truncated signed distance functions (TSDF) @cite_9 , or point clouds @cite_5 . Voxel grids are simple to generate and use. Because of their regular grid structure, voxel grids can be directly used as inputs to 3D CNNs. However, voxel grids are inefficient since they also have to explicitly represent empty space. They also suffer from discretization artifacts. TSDF tries to alleviate these problems by storing the shortest distance to the surface represented in each voxel. This allows a more faithful representation of the 3D information. In comparison to other depth data representations, a point cloud has a simple representation without redundancy, yet contains rich geometric information. Recently, PointNet @cite_5 has allowed to use raw point clouds directly as an input of a CNN.\"","":""}
{"id":"2951865326","dialogue":"\"In this paper we propose an edge-direct visual odometry algorithm that efficiently utilizes edge pixels to find the relative pose that minimizes the photometric error between images. Prior work on exploiting edge pixels instead treats edges as features and employ various techniques to match edge lines or pixels, which adds unnecessary complexity. Direct methods typically operate on all pixel intensities, which proves to be highly redundant. In contrast our method builds on direct visual odometry methods naturally with minimal added computation. It is not only more efficient than direct dense methods since we iterate with a fraction of the pixels, but also more accurate. We achieve high accuracy and efficiency by extracting edges from only one image, and utilize robust Gauss-Newton to minimize the photometric error of these edge pixels. This simultaneously finds the edge pixels in the reference image, as well as the relative camera pose that minimizes the photometric error. We test various edge detectors, including learned edges, and determine that the optimal edge detector for this method is the Canny edge detection algorithm using automatic thresholding. We highlight key differences between our edge direct method and direct dense methods, in particular how higher levels of image pyramids can lead to significant aliasing effects and result in incorrect solution convergence. We show experimentally that reducing the photometric error of edge pixels also reduces the photometric error of all pixels, and we show through an ablation study the increase in accuracy obtained by optimizing edge pixels only. We evaluate our method on the RGB-D TUM benchmark on which we achieve state-of-the-art performance.\"","summary":"\"Due to the high level of inaccuracies present in feature extraction and matching, such algorithms must compute the fundamental matrix or homography in a RANSAC loop. While feature-based methods have achieved accurate results, they remain computationally wasteful due to their reliance on RANSAC for robust estimation of such parameters. Several examples of such systems that use indirect methods are ORB-SLAM, ORB-SLAM2 @cite_4 @cite_23 and Parallel Tracking and Mapping (PTAM) @cite_5 . Alternatively, direct methods directly use the sensor inputs, such as image intensities, to optimize an error function to determine relative camera pose.\"","":""}
{"id":"2951865326","dialogue":"\"In this paper we propose an edge-direct visual odometry algorithm that efficiently utilizes edge pixels to find the relative pose that minimizes the photometric error between images. Prior work on exploiting edge pixels instead treats edges as features and employ various techniques to match edge lines or pixels, which adds unnecessary complexity. Direct methods typically operate on all pixel intensities, which proves to be highly redundant. In contrast our method builds on direct visual odometry methods naturally with minimal added computation. It is not only more efficient than direct dense methods since we iterate with a fraction of the pixels, but also more accurate. We achieve high accuracy and efficiency by extracting edges from only one image, and utilize robust Gauss-Newton to minimize the photometric error of these edge pixels. This simultaneously finds the edge pixels in the reference image, as well as the relative camera pose that minimizes the photometric error. We test various edge detectors, including learned edges, and determine that the optimal edge detector for this method is the Canny edge detection algorithm using automatic thresholding. We highlight key differences between our edge direct method and direct dense methods, in particular how higher levels of image pyramids can lead to significant aliasing effects and result in incorrect solution convergence. We show experimentally that reducing the photometric error of edge pixels also reduces the photometric error of all pixels, and we show through an ablation study the increase in accuracy obtained by optimizing edge pixels only. We evaluate our method on the RGB-D TUM benchmark on which we achieve state-of-the-art performance.\"","summary":"\"There have been many iterations of direct dense methods such as direct dense VO in @cite_1 , RGB-D SLAM @cite_22 , and LSD-SLAM @cite_25 . Even using dense methods, these systems achieve real-time performance on modern CPUs due to the highly efficient nature of these types of algorithms. More recent advances highlight the fact that the information contained in image intensities are highly redundant, and attempt to minimize the photometric error only over sparse random points in the image in order to increase efficiency and thus speed @cite_10 . Another direct method that has been used with success is the iterative closest point (ICP) algorithm, which is used in systems such as @cite_8 @cite_19 . These systems minimize the difference between point alignment in contrast to image intensities.\"","":""}
{"id":"2951865326","dialogue":"\"In this paper we propose an edge-direct visual odometry algorithm that efficiently utilizes edge pixels to find the relative pose that minimizes the photometric error between images. Prior work on exploiting edge pixels instead treats edges as features and employ various techniques to match edge lines or pixels, which adds unnecessary complexity. Direct methods typically operate on all pixel intensities, which proves to be highly redundant. In contrast our method builds on direct visual odometry methods naturally with minimal added computation. It is not only more efficient than direct dense methods since we iterate with a fraction of the pixels, but also more accurate. We achieve high accuracy and efficiency by extracting edges from only one image, and utilize robust Gauss-Newton to minimize the photometric error of these edge pixels. This simultaneously finds the edge pixels in the reference image, as well as the relative camera pose that minimizes the photometric error. We test various edge detectors, including learned edges, and determine that the optimal edge detector for this method is the Canny edge detection algorithm using automatic thresholding. We highlight key differences between our edge direct method and direct dense methods, in particular how higher levels of image pyramids can lead to significant aliasing effects and result in incorrect solution convergence. We show experimentally that reducing the photometric error of edge pixels also reduces the photometric error of all pixels, and we show through an ablation study the increase in accuracy obtained by optimizing edge pixels only. We evaluate our method on the RGB-D TUM benchmark on which we achieve state-of-the-art performance.\"","summary":"\"The extension of direct methods using edge pixels is a logical direction, yet to the best of our knowledge no work has solely used edge pixels in a direct method minimizing the photometric error. @cite_18 the authors reduce a Euclidean geometric error using the distance transform on edges which does not utilize all information available in the scene. @cite_0 the authors minimize a joint error function combining photometric error over all pixels along with geometric error over edge pixels. Minimizing a joint error function always suffers from the decision on how best to weight each function, and the weighting can have significant effect on the final converged solution. @cite_10 , the authors threshold by gradients, which does not guarantee edges due to noise. They additionally select texture-less regions as well.\"","":""}
{"id":"2951865326","dialogue":"\"In this paper we propose an edge-direct visual odometry algorithm that efficiently utilizes edge pixels to find the relative pose that minimizes the photometric error between images. Prior work on exploiting edge pixels instead treats edges as features and employ various techniques to match edge lines or pixels, which adds unnecessary complexity. Direct methods typically operate on all pixel intensities, which proves to be highly redundant. In contrast our method builds on direct visual odometry methods naturally with minimal added computation. It is not only more efficient than direct dense methods since we iterate with a fraction of the pixels, but also more accurate. We achieve high accuracy and efficiency by extracting edges from only one image, and utilize robust Gauss-Newton to minimize the photometric error of these edge pixels. This simultaneously finds the edge pixels in the reference image, as well as the relative camera pose that minimizes the photometric error. We test various edge detectors, including learned edges, and determine that the optimal edge detector for this method is the Canny edge detection algorithm using automatic thresholding. We highlight key differences between our edge direct method and direct dense methods, in particular how higher levels of image pyramids can lead to significant aliasing effects and result in incorrect solution convergence. We show experimentally that reducing the photometric error of edge pixels also reduces the photometric error of all pixels, and we show through an ablation study the increase in accuracy obtained by optimizing edge pixels only. We evaluate our method on the RGB-D TUM benchmark on which we achieve state-of-the-art performance.\"","summary":"\"Any system that extracts edges must choose between several edge extraction algorithms. The most prominent are Canny edges @cite_6 , followed by edges extracted from Laplacian of Gaussian (LoG) filters which are efficiently implemented using Difference of Gaussians (DoG). Another type of edge that is not as popular but is very simple are Sobel edges. More recently, there has been research involving the learning of edge features. In @cite_20 the authors utilize structured forests, and in @cite_15 the authors utilize deep learning. Instead of selecting one, we test various edge extraction algorithms with our system select the optimal edge extraction algorithm. Note that @cite_15 requires the use of a GPU and is far from real-time, so we do not consider this method.\"","":""}
{"id":"2954829969","dialogue":"\"Determining the optimal location of control cabinet components requires the exploration of a large configuration space. For real-world control cabinets it is impractical to evaluate all possible cabinet configurations. Therefore, we need to apply methods for intelligent exploration of cabinet configuration space that enable to find a near-optimal configuration without evaluation of all possible configurations. In this paper, we describe an approach for multi-objective optimization of control cabinet layout that is based on Pareto Simulated Annealing. Optimization aims at minimizing the total wire length used for interconnection of components and the heat convection within the cabinet. We simulate heat convection to study the warm air flow within the control cabinet and determine the optimal position of components that generate heat during the operation. We evaluate and demonstrate the effectiveness of our approach empirically for various control cabinet sizes and usage scenarios.\"","summary":"\"@cite_4 propose an approach that is based on Bayesian networks for automatic generation of residential building layouts in the context of computer graphics applications (such as, computer games). Authors define a cost function that aims at avoiding layout anomalies, such as, ill-formed rooms or incompatibilities between floors.\"","":""}
{"id":"2954829969","dialogue":"\"Determining the optimal location of control cabinet components requires the exploration of a large configuration space. For real-world control cabinets it is impractical to evaluate all possible cabinet configurations. Therefore, we need to apply methods for intelligent exploration of cabinet configuration space that enable to find a near-optimal configuration without evaluation of all possible configurations. In this paper, we describe an approach for multi-objective optimization of control cabinet layout that is based on Pareto Simulated Annealing. Optimization aims at minimizing the total wire length used for interconnection of components and the heat convection within the cabinet. We simulate heat convection to study the warm air flow within the control cabinet and determine the optimal position of components that generate heat during the operation. We evaluate and demonstrate the effectiveness of our approach empirically for various control cabinet sizes and usage scenarios.\"","summary":"\"@cite_8 surveyed various methods (such as, Simulated Annealing) for decentralized scheduling in Grid computing environments. Grid scheduling involves mapping of a collection of tasks to resources with the aim of minimizing the total execution time of all considered tasks. Compared to a game theory scheduling method, the average scheduling time per task of Simulated Annealing was lower.\"","":""}
{"id":"2954829969","dialogue":"\"Determining the optimal location of control cabinet components requires the exploration of a large configuration space. For real-world control cabinets it is impractical to evaluate all possible cabinet configurations. Therefore, we need to apply methods for intelligent exploration of cabinet configuration space that enable to find a near-optimal configuration without evaluation of all possible configurations. In this paper, we describe an approach for multi-objective optimization of control cabinet layout that is based on Pareto Simulated Annealing. Optimization aims at minimizing the total wire length used for interconnection of components and the heat convection within the cabinet. We simulate heat convection to study the warm air flow within the control cabinet and determine the optimal position of components that generate heat during the operation. We evaluate and demonstrate the effectiveness of our approach empirically for various control cabinet sizes and usage scenarios.\"","summary":"\"@cite_12 @cite_2 use Simulated Annealing for optimization of DNA sequence analysis on heterogeneous computing systems that comprise a host with multi-core processors and one or more many-core devices. The optimization procedure aims at determining the number of threads, thread affinities, and DNA sequence fractions for host and device, such that the overall execution time of DNA sequence analysis is minimized.\"","":""}
{"id":"2954829969","dialogue":"\"Determining the optimal location of control cabinet components requires the exploration of a large configuration space. For real-world control cabinets it is impractical to evaluate all possible cabinet configurations. Therefore, we need to apply methods for intelligent exploration of cabinet configuration space that enable to find a near-optimal configuration without evaluation of all possible configurations. In this paper, we describe an approach for multi-objective optimization of control cabinet layout that is based on Pareto Simulated Annealing. Optimization aims at minimizing the total wire length used for interconnection of components and the heat convection within the cabinet. We simulate heat convection to study the warm air flow within the control cabinet and determine the optimal position of components that generate heat during the operation. We evaluate and demonstrate the effectiveness of our approach empirically for various control cabinet sizes and usage scenarios.\"","summary":"\"Drexl and Nikulin @cite_17 use Pareto Simulated Annealing for solving the gate assignment problem in the context of an airport. Various aspects of airport operation are considered, such as, the total passenger walking distance, open flights, connection time, and gate assignment preferences.\"","":""}
{"id":"2954829969","dialogue":"\"Determining the optimal location of control cabinet components requires the exploration of a large configuration space. For real-world control cabinets it is impractical to evaluate all possible cabinet configurations. Therefore, we need to apply methods for intelligent exploration of cabinet configuration space that enable to find a near-optimal configuration without evaluation of all possible configurations. In this paper, we describe an approach for multi-objective optimization of control cabinet layout that is based on Pareto Simulated Annealing. Optimization aims at minimizing the total wire length used for interconnection of components and the heat convection within the cabinet. We simulate heat convection to study the warm air flow within the control cabinet and determine the optimal position of components that generate heat during the operation. We evaluate and demonstrate the effectiveness of our approach empirically for various control cabinet sizes and usage scenarios.\"","summary":"\"@cite_16 propose to use Simulated Annealing for solving the hybrid vehicle routing problem. The aim is to minimize the total travel cost for hybrid vehicles that use fuel and electricity while considering the time limit, electric capacity, fuel capacity, locations of fuel stations and electricity charging stations.\"","":""}
{"id":"2964249268","dialogue":"\"The evolving explosion in high data rate services and applications will soon require the use of untapped, abundant unregulated spectrum of the visible light for communications to adequately meet the demands of the fifth-generation (5G) mobile technologies. Radio-frequency (RF) networks are proving to be too scarce to cover the escalation in data rate services. Visible light communication (VLC) has emerged as a great potential solution, either in replacement of, or a complement to, existing RF networks, to support the projected traffic demands. Despite the prolific advantages of VLC networks, VLC faces many challenges that must be resolved in the near future to achieve full standardization and to be integrated to future wireless systems. Here, we review the emerging research in the field of VLC networks and lay out the challenges, technological solutions, and future work predictions. Specifically, we first review the VLC channel capacity derivation and discuss the performance metrics and the associated variables. The optimization of VLC networks are also discussed, including resources and power allocation techniques, user-to-access point (AP) association and APs-to-clustered-users-association, APs coordination techniques, non-orthogonal multiple access (NOMA) VLC networks, simultaneous energy harvesting and information transmission using the visible light, and the security issues in VLC networks. Finally, we propose several open research problems to optimize the various VLC networks by maximizing either the sum rate, fairness, energy efficiency, secrecy rate, or harvested energy.\"","summary":"\"Some review articles focused on specific aspects of VLC such as VLC channel modeling methods @cite_47 , noise optical sources and noise mitigation mechanisms @cite_57 , VLC-based positioning techniques for indoor and outdoor applications @cite_18 , and the pertinent issues associated with the outdoor usage of VLC in vehicular communication @cite_177 . They generally identified emerging challenges and proposed future research directions.\"","":""}
{"id":"2964249268","dialogue":"\"The evolving explosion in high data rate services and applications will soon require the use of untapped, abundant unregulated spectrum of the visible light for communications to adequately meet the demands of the fifth-generation (5G) mobile technologies. Radio-frequency (RF) networks are proving to be too scarce to cover the escalation in data rate services. Visible light communication (VLC) has emerged as a great potential solution, either in replacement of, or a complement to, existing RF networks, to support the projected traffic demands. Despite the prolific advantages of VLC networks, VLC faces many challenges that must be resolved in the near future to achieve full standardization and to be integrated to future wireless systems. Here, we review the emerging research in the field of VLC networks and lay out the challenges, technological solutions, and future work predictions. Specifically, we first review the VLC channel capacity derivation and discuss the performance metrics and the associated variables. The optimization of VLC networks are also discussed, including resources and power allocation techniques, user-to-access point (AP) association and APs-to-clustered-users-association, APs coordination techniques, non-orthogonal multiple access (NOMA) VLC networks, simultaneous energy harvesting and information transmission using the visible light, and the security issues in VLC networks. Finally, we propose several open research problems to optimize the various VLC networks by maximizing either the sum rate, fairness, energy efficiency, secrecy rate, or harvested energy.\"","summary":"\"This paper provides, in Section II, an overview of VLC technology, defines and discusses the objectives and constraints that must be taken into account when optimizing VLC networks. Special emphasis is placed on channel capacity derivations, and the unique properties of VLC. We also discuss the variables, parameters, and constraints having an impact on the performance of VLC networks. All optimization techniques are reviewed in Section III, including power and resource allocation, users-to-APs association, cell formation, and AP cooperation used for mitigating the disadvantages of VLC networks to improve performance. This important topic was previously investigated by Li @cite_202 . However, their study was focused on the difference between user-centric and network-centric cell formations, and the interference reduction techniques, whereas in this paper, we place our attention on the techniques, used in RF VLC and in VLC standalone networks, that are aimed at alleviating the limitations in VLC networks. In other words, we show how to formulate optimization problems, what are the techniques used for solving these optimization problems, how the different objectives, limitations, constraints are evaluated, and how added RF APs can remove stand-alone VLC network limitations.\"","":""}
{"id":"2949220746","dialogue":"\"The Authentication and Authorization for Constrained Environments (ACE) framework provides fine-grained access control in the Internet of Things, where devices are resource-constrained and with limited connectivity. The ACE framework defines separate profiles to specify how exactly entities interact and what security and communication protocols to use. This paper presents the novel ACE IPsec profile, which specifies how a client establishes a secure IPsec channel with a resource server, contextually using the ACE framework to enforce authorized access to remote resources. The profile makes it possible to establish IPsec Security Associations, either through their direct provisioning or through the standard IKEv2 protocol. We provide the first Open Source implementation of the ACE IPsec profile for the Contiki OS and test it on the resource-constrained Zolertia Firefly platform. Our experimental performance evaluation confirms that the IPsec profile and its operating modes are affordable and deployable also on constrained IoT platforms.\"","summary":"\"Finally, Sciancalepore propose a different authorization framework for the IoT @cite_24 , also based on OAuth 2.0 and other standard protocols. In particular, it provides access control through an intermediary gateway acting as mediator between IoT networks and non-constrained Internet segments. However, unlike the ACE framework, @cite_24 displays a considerably higher level of complexity and requires the intermediary gateway to be fully trusted.\"","":""}
{"id":"2950895666","dialogue":"\"Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.\"","summary":"\"The game @cite_23 has been a testbed for various vision-and-language tasks over the past years, including object retrieval @cite_19 @cite_15 @cite_32 @cite_34 @cite_11 @cite_44 , semantic image segmentation @cite_28 @cite_1 , and generating referring descriptions @cite_15 @cite_11 @cite_32 . To tackle object retrieval, @cite_19 @cite_15 @cite_44 extract additional visual features such as relative object locations and @cite_32 @cite_11 use reinforcement learning to iteratively train the object retrieval and description generation models. Closer to our work, @cite_25 @cite_34 use the full image and the object crop to locate the correct object. While some previous work relies on task-specific modules @cite_15 @cite_44 , our approach is general and can be easily extended to other vision-and-language tasks.\"","":""}
{"id":"2950895666","dialogue":"\"Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.\"","summary":"\"The game @cite_22 can be seen as a dialogue version of the game, one which additionally draws on visual question answering ability. @cite_52 @cite_45 @cite_49 make headway on the dialogue generation task via reinforcement learning. However, these approaches are bottlenecked by the accuracy of Oracle and Guesser models, despite existing modeling advances @cite_34 @cite_2 ; accurate Oracle and Guesser models are crucial for providing a meaningful learning signal for dialogue generation models, so we believe the architecture will facilitate high quality dialogue generation as well.\"","":""}
{"id":"2950895666","dialogue":"\"Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.\"","summary":"\"A special case of Feature-wise Linear Modulation was first successfully applied to image style transfer @cite_27 , whose approach modulates image features according to some image style (i.e. @, cubism or impressionism). @cite_2 extended this approach to vision-and-language tasks, injecting FiLM-like layers along the entire visual pipeline of a pre-trained ResNet. @cite_4 demonstrates that a convolutional network with FiLM layers achieves strong performance on CLEVR @cite_43 , a task that focuses on answering reasoning-oriented, multi-step questions about synthetic images. Subsequent work has demonstrated that FiLM and variants thereof are effective for video object segmentation where the conditioning input is the first image's segmentation (instead of language) @cite_36 and language-guided image segmentation @cite_29 . Even more broadly, @cite_50 overviews the strength of FiLM-related methods across machine learning domains, ranging from reinforcement learning to generative modeling to domain adaptation.\"","":""}
{"id":"2887561040","dialogue":"\"In this paper, we propose the task of live comment generation. Live comments are a new form of comments on videos, which can be regarded as a mixture of comments and chats. A high-quality live comment should be not only relevant to the video, but also interactive with other users. In this work, we first construct a new dataset for live comment generation. Then, we propose a novel end-to-end model to generate the human-like live comments by referring to the video and the other users' comments. Finally, we evaluate our model on the constructed dataset. Experimental results show that our method can significantly outperform the baselines.\"","summary":"\"One task that is similar to live comment generation is image caption generation, which is an area that has been studied for a long time. tried to generate descriptions of an image by retrieving from a big sentence pool. proposed to generate descriptions based on the parsing result of the image with a simple language model. These systems are often applied in a pipeline fashion, and the generated description is not creative. More recent work is to use stepwise merging network to improve the performance @cite_2 .\"","":""}
{"id":"2887561040","dialogue":"\"In this paper, we propose the task of live comment generation. Live comments are a new form of comments on videos, which can be regarded as a mixture of comments and chats. A high-quality live comment should be not only relevant to the video, but also interactive with other users. In this work, we first construct a new dataset for live comment generation. Then, we propose a novel end-to-end model to generate the human-like live comments by referring to the video and the other users' comments. Finally, we evaluate our model on the constructed dataset. Experimental results show that our method can significantly outperform the baselines.\"","summary":"\"We cast this problem as a natural language generation problem, and we are also inspired by the recent related work of natural language generation models with the text inputs @cite_5 @cite_6 @cite_8 @cite_3 .\"","":""}
{"id":"2887150966","dialogue":"\"In this paper, we investigate the potential of the Boyer-Moore waterfall model for the automation of inductive proofs within a modern proof assistant. We analyze the basic concepts and methodology underlying this 30-year-old model and implement a new, fully integrated tool in the theorem prover HOL Light that can be invoked as a tactic. We also describe several extensions and enhancements to the model. These include the integration of existing HOL Light proof procedures and the addition of state-of-the-art generalization techniques into the waterfall. Various features, such as proof feedback and heuristics dealing with non-termination, that are needed to make this automated tool useful within our interactive setting are also discussed. Finally, we present a thorough evaluation of the approach using a set of 150 theorems, and discuss the effectiveness of our additions and relevance of the model in light of our results.\"","summary":"\"One of the main heuristic tools in Proof Planning is Rippling @cite_3 . Rippling gives a direction to the rewriting process. The idea is based on the observation that throughout the process, on each side of the equality being proved, there is an unchanging part (skeleton) and a changing part (wave-front). In principle the proof is guided towards rewrite rules that move the wave-front upwards in the syntax tree of the term. Rippling has proved to be a powerful heuristic for inductive proofs. However, there is still a possibility that the proof will block, thus requiring a patch'' or critic''. Critics include lemma speculations and generalizations, some of which can be produced automatically in the modern Proof Planning systems.\"","":""}
{"id":"2963408210","dialogue":"\"Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the significance and qualities of the dataset needed for this. Having identified both a problem domain and characteristics of an appropriate dataset, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.\"","summary":"\"Perhaps it is precisely because music is so often perceived as a profoundly human endeavour that there has also been, in parallel, an ongoing fascination with automating its creation. This fascination long predates notions such as the Turing test (ostensibly for discriminating automation of the most human behaviour), and has spawned a range of efforts: from attempts at the formalization of unambiguously strict rules of composition to incorporation of complete random chance into scores and performances. The use of rules exemplifies the algorithmic (and largely deterministic) approach to music generation, one that is interesting and outside the scope of the current work; for background on this we refer the reader, for example, to the text by Nierhaus @cite_1 . Our present work, on the other hand, lies in a part of the spectrum that incorporates probability and sampling.\"","":""}
{"id":"2963408210","dialogue":"\"Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the significance and qualities of the dataset needed for this. Having identified both a problem domain and characteristics of an appropriate dataset, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.\"","summary":"\"Two centuries later, as the foundations of AI were being set, the notion of automatically understanding (and therefore generating) music was among the earliest applications to capture the imagination of researchers, with papers on computational approaches to perception, interpretation and generation of music by Simon, Longuet-Higgins and others @cite_4 @cite_12 @cite_30 @cite_28 @cite_6 . Since then, many interesting efforts were made @cite_42 @cite_18 @cite_22 @cite_2 @cite_11 @cite_17 , and it is clear that in recent years both interest and progress in score generation has continued to advance, e.g. @cite_38 , Boulanger- @cite_32 , @cite_0 , @cite_41 , @cite_19 , Sturm @cite_5 , to name only a few. @cite_20 provide a survey of generative music models that involve machine learning. @cite_43 provide a comprehensive survey and satisfying taxonomy of music generation systems. McDonald @cite_3 gives an overview highlighting some key examples of such work.\"","":""}
{"id":"2963408210","dialogue":"\"Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the significance and qualities of the dataset needed for this. Having identified both a problem domain and characteristics of an appropriate dataset, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.\"","summary":"\"@cite_15 , observe that many previous performance rendering systems often consist of many heuristic rules and tend to be complex. It makes [it] difficult to generate and select the useful rules, or perform the optimization of parameters in the rules.'' They thus present a method that uses Gaussian Processes to achieve this, where some parameters can be learned. In their ostensibly simpler system, for each single note, three outputs and corresponding thirteen input features are defined, and three functions each of which returns one of three outputs and receive the thirteen input features, are independently learned''. However, some of these features, too, depend on certain information, e.g. they compute the differences between successive pitches, and this only works in compositions where the voice leading is absolutely clear; in the majority of classical piano repertoire, this is not the case. In Laminae @cite_34 , systematize a set of context-dependent models, building a decision tree which allows rendering a performance by combining contextual information.\"","":""}
{"id":"2963408210","dialogue":"\"Music generation has generally been focused on either creating scores or interpreting them. We discuss differences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the significance and qualities of the dataset needed for this. Having identified both a problem domain and characteristics of an appropriate dataset, we show an LSTM-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.\"","summary":"\"Moulieras and Pachet @cite_27 use a maximum entropy model to generate expressive music, but their focus is again monophonic plus simple harmonic information. They also explicitly assume that musical expression consists in local texture, rather than long-range correlations''. While this is fairly reasonable at this point, and indeed it is hard to say how much long-range correlation is captured by our model, we wished to choose a model which, at least in principle, allowed the possibility of modeling long-range correlation: ultimately, we believe that these correlations are of fundamental importance. Malik and Ek @cite_7 use a neural network to learn to predict the dynamic levels of individual notes while assuming quantized and steady timing.\"","":""}
{"id":"2950506508","dialogue":"\"We propose a framework called inverse augmented reality (IAR) which describes the scenario that a virtual agent living in the virtual world can observe both virtual objects and real objects. This is different from the traditional augmented reality. The traditional virtual reality, mixed reality and augmented reality are all generated for humans, i.e., they are human-centered frameworks. On the contrary, the proposed inverse augmented reality is a virtual agent-centered framework, which represents and analyzes the reality from a virtual agent's perspective. In this paper, we elaborate the framework of inverse augmented reality to argue the equivalence of the virtual world and the physical world regarding the whole physical structure.\"","summary":"\"In the past, a lot of related researches about augmented reality have been presented. Before IAR, some novel styles of reality have been proposed. For example, Lifton al @cite_4 proposed the dual reality'' system to make the virtual world and the physical world be corresponding to each other. Roo al @cite_0 proposed the one reality'' system, which contained a 6-level mixture of virtual and real contents ranging from purely physical to purely virtual world. But they all describe the mixed reality from the perspective of humans, ignoring the view from the virtual world.\"","":""}
{"id":"2950506508","dialogue":"\"We propose a framework called inverse augmented reality (IAR) which describes the scenario that a virtual agent living in the virtual world can observe both virtual objects and real objects. This is different from the traditional augmented reality. The traditional virtual reality, mixed reality and augmented reality are all generated for humans, i.e., they are human-centered frameworks. On the contrary, the proposed inverse augmented reality is a virtual agent-centered framework, which represents and analyzes the reality from a virtual agent's perspective. In this paper, we elaborate the framework of inverse augmented reality to argue the equivalence of the virtual world and the physical world regarding the whole physical structure.\"","summary":"\"To make the virtual world intelligent, Taylor al @cite_7 discussed the possibility of making a virtual world evolve by itself. The evolution of the virtual world took advantage of the principle of biological evolution in the physical world. Though the self-learning is not simple, there are still many learning frameworks that can be used to obtain the self-learning ability, such as evolutionary computation @cite_2 , reinforcement learning @cite_6 and deep learning @cite_9 .\"","":""}
{"id":"2887133168","dialogue":"\"Shamir's celebrated secret sharing scheme provides an efficient method for encoding a secret of arbitrary length @math among any @math players such that for a threshold parameter @math , (i) the knowledge of any @math shares does not reveal any information about the secret and, (ii) any choice of @math shares fully reveals the secret. It is known that any such threshold secret sharing scheme necessarily requires shares of length @math , and in this sense Shamir's scheme is optimal. The more general notion of ramp schemes requires the reconstruction of secret from any @math shares, for a positive integer gap parameter @math . Ramp secret sharing scheme necessarily requires shares of length @math . Other than the bound related to secret length @math , the share lengths of ramp schemes can not go below a quantity that depends only on the gap ratio @math . In this work, we study secret sharing in the extremal case of bit-long shares and arbitrarily small gap ratio @math , where standard ramp secret sharing becomes impossible. We show, however, that a slightly relaxed but equally effective notion of semantic security for the secret, and negligible reconstruction error probability, eliminate the impossibility. Moreover, we provide explicit constructions of such schemes. One of the consequences of our relaxation is that, unlike standard ramp schemes with perfect secrecy, adaptive and non-adaptive adversaries need different analysis and construction. For non-adaptive adversaries, we explicitly construct secret sharing schemes that provide secrecy against any @math fraction of observed shares, and reconstruction from any @math fraction of shares, for any choices of @math . Our construction achieves secret length @math , which we show to be optimal. For adaptive adversaries, we construct explicit schemes attaining a secret length @math .\"","summary":"\"In @cite_23 @cite_17 , the secret is one bit. In @cite_20 , secrets of length equal to a fraction of @math (number of players) is considered. This time binary secret sharing with adaptive and non-adaptive adversaries similar to the model we consider in this work is defined. However the paper considers only a privacy threshold @math , and reconstruction is from the full share set ( @math always). Their goal is to achieve large secrets @math over binary shares with large privacy parameter @math , which is also similar to ours. They have an additional goal of keeping the computational complexity of the reconstruction algorithm within AC @math , which we do not consider in this work. Their large privacy parameter @math is with a @math much smaller than @math , which means that the relative threshold gap @math can not be arbitrarily small.\"","":""}
{"id":"2887133168","dialogue":"\"Shamir's celebrated secret sharing scheme provides an efficient method for encoding a secret of arbitrary length @math among any @math players such that for a threshold parameter @math , (i) the knowledge of any @math shares does not reveal any information about the secret and, (ii) any choice of @math shares fully reveals the secret. It is known that any such threshold secret sharing scheme necessarily requires shares of length @math , and in this sense Shamir's scheme is optimal. The more general notion of ramp schemes requires the reconstruction of secret from any @math shares, for a positive integer gap parameter @math . Ramp secret sharing scheme necessarily requires shares of length @math . Other than the bound related to secret length @math , the share lengths of ramp schemes can not go below a quantity that depends only on the gap ratio @math . In this work, we study secret sharing in the extremal case of bit-long shares and arbitrarily small gap ratio @math , where standard ramp secret sharing becomes impossible. We show, however, that a slightly relaxed but equally effective notion of semantic security for the secret, and negligible reconstruction error probability, eliminate the impossibility. Moreover, we provide explicit constructions of such schemes. One of the consequences of our relaxation is that, unlike standard ramp schemes with perfect secrecy, adaptive and non-adaptive adversaries need different analysis and construction. For non-adaptive adversaries, we explicitly construct secret sharing schemes that provide secrecy against any @math fraction of observed shares, and reconstruction from any @math fraction of shares, for any choices of @math . Our construction achieves secret length @math , which we show to be optimal. For adaptive adversaries, we construct explicit schemes attaining a secret length @math .\"","summary":"\"The notion of secrecy in wiretap codes has evolved over years. More recently the notion of semantic security for wiretap model has been introduced @cite_6 , which allows arbitrary message distribution and is shown to be equivalent to negligible leakage with respect to statistical distance. There remains one last distinction between semantically secure wiretap model and secret sharing with fixed share size. That is the nature of the main and wiretapper channels are typically stochastic (e.g., the erasure channel with random i.i.d. erasures), whereas for secret sharing a worst-case guarantee for the erasure patterns is required. Namely, in secret sharing, reconstruction with overwhelming probability is required for every choice of @math or more shares, and privacy of the secret is required for every (adaptive or non-adaptive) choice of the @math shares observed by the adversary.\"","":""}
{"id":"2752721426","dialogue":"\"With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms. We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1 degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point. To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.\"","summary":"\"To the best of our knowledge, our work is the first to examine the impact of numeric representations on the accuracy-efficiency trade-offs on large-scale, deployed DNNs with over half a million neurons (GoogLeNet, VGG, AlexNet), whereas prior work has only reported results on much smaller networks such as CIFARNET and LeNet-5 @cite_16 @cite_15 @cite_10 @cite_17 @cite_20 @cite_11 . Many of these works focused on fixed-point computation due to the fixed-point representation working well on small-scale neural networks. We find very different conclusions when considering production-ready DNNs.\"","":""}
{"id":"2949760630","dialogue":"\"Domain Adaptation arises when we aim at learning from source domain a model that can per- form acceptably well on a different target domain. It is especially crucial for Natural Language Generation (NLG) in Spoken Dialogue Systems when there are sufficient annotated data in the source domain, but there is a limited labeled data in the target domain. How to effectively utilize as much of existing abilities from source domains is a crucial issue in domain adaptation. In this paper, we propose an adversarial training procedure to train a Variational encoder-decoder based language generator via multiple adaptation steps. In this procedure, a model is first trained on a source domain data and then fine-tuned on a small set of target domain utterances under the guidance of two proposed critics. Experimental results show that the proposed method can effec- tively leverage the existing knowledge in the source domain to adapt to another related domain by using only a small amount of in-domain data.\"","summary":"\"Generally, Domain Adaptation involves two different types of datasets, one from a source domain and the other from a target domain. The source domain typically contains a sufficient amount of annotated data such that a model can be efficiently built, while there is often little or no labeled data in the target domain. Domain adaptation for NLG have been less studied despite its important role in developing multi-domain SDS. proposed a SPoT-based generator to address domain adaptation problems. Subsequently, a system focused on tailoring user preferences @cite_19 , and controlling user perceptions of linguistic style @cite_3 . Moreover, a phrase-based statistical generator @cite_13 using graphical models and active learning, and a multi-domain procedure @cite_21 via data counterfeiting and discriminative training.\"","":""}
{"id":"2949760630","dialogue":"\"Domain Adaptation arises when we aim at learning from source domain a model that can per- form acceptably well on a different target domain. It is especially crucial for Natural Language Generation (NLG) in Spoken Dialogue Systems when there are sufficient annotated data in the source domain, but there is a limited labeled data in the target domain. How to effectively utilize as much of existing abilities from source domains is a crucial issue in domain adaptation. In this paper, we propose an adversarial training procedure to train a Variational encoder-decoder based language generator via multiple adaptation steps. In this procedure, a model is first trained on a source domain data and then fine-tuned on a small set of target domain utterances under the guidance of two proposed critics. Experimental results show that the proposed method can effec- tively leverage the existing knowledge in the source domain to adapt to another related domain by using only a small amount of in-domain data.\"","summary":"Neural variational framework for generative models of text have been studied longitudinally. proposed a recurrent latent variable model VRNN for sequential data by integrating latent random variables into hidden state of a RNN model. A hierarchical multi scale recurrent neural networks was proposed to learn both hierarchical and temporal representation @cite_23 . introduced a variational neural machine translation that incorporated a continuous latent variable to model underlying semantics of sentence pairs. presented a variational autoencoder for unsupervised generative language model.","":""}
{"id":"2949760630","dialogue":"\"Domain Adaptation arises when we aim at learning from source domain a model that can per- form acceptably well on a different target domain. It is especially crucial for Natural Language Generation (NLG) in Spoken Dialogue Systems when there are sufficient annotated data in the source domain, but there is a limited labeled data in the target domain. How to effectively utilize as much of existing abilities from source domains is a crucial issue in domain adaptation. In this paper, we propose an adversarial training procedure to train a Variational encoder-decoder based language generator via multiple adaptation steps. In this procedure, a model is first trained on a source domain data and then fine-tuned on a small set of target domain utterances under the guidance of two proposed critics. Experimental results show that the proposed method can effec- tively leverage the existing knowledge in the source domain to adapt to another related domain by using only a small amount of in-domain data.\"","summary":"\"Adversarial adaptation methods have shown promising improvement in many machine learning applications despite the presence of domain shift or dataset bias, which reduce the difference between the training and test domain distributions, and thus improve generalization performance. proposed an improved unsupervised domain adaptation method to learn a discriminative mapping of target images to the source feature space by fooling a domain discriminator that tries to differentiate the encoded target images from source examples. We borrowed the idea of @cite_9 , where a domain-adversarial neural network are proposed to learn features that are discriminative for the main learning task on the source domain, and indiscriminate with respect to the shift between domains.\"","":""}
{"id":"2949402865","dialogue":"\"In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Extensive experiments on public VQA datasets validate the effectiveness of QGHC.\"","summary":"\"The attention mechanisms @cite_13 @cite_34 are originally proposed for solving language-related tasks @cite_5 . Xu @cite_13 introduce an attention mechanism for image captioning, which shows that the attention maps could be adaptively generated for predicting captioning words. Based on @cite_13 , Yang @cite_1 propose to stack multiple attention layers so that each layer can focus on different regions adaptively. In @cite_32 , a co-attention mechanism is proposed. The model generates question attention and spatial attention masks so that salient words and regions could be jointly selected for more effective feature fusion. Similarly, Lu @cite_10 employ a co-attention mechanism to simultaneously learn free-form and detection-based image regions related to the input question. In MCB @cite_33 , MLB @cite_28 , and MUTAN @cite_19 , attention mechanisms are adopted to partially recover the spatial information from the input image. Question-guided attention methods @cite_29 @cite_13 are proposed to generate attention maps from the question.\"","":""}
{"id":"2949402865","dialogue":"\"In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Extensive experiments on public VQA datasets validate the effectiveness of QGHC.\"","summary":"\"Network parameters could be dynamically predicted across different modalities. Our approach is mostly related to methods in this direction. In @cite_31 , language are used to predict parameters of a fully-connected (FC) layer for learning visual features. However, the predicted fully-connected layer cannot capture spatial information of the image. To avoid introducing too many parameters, they predict only a small portion of parameters using a hashing function. However, this strategy introduces redundancy because the FC parameters only contain a small amount of training parameters. In @cite_23 , language is used to modulate the mean and variance parameters of the Batch Normalization layers in the visual CNN. However, learning the interactions between two modalities by predicting the BN parameters has limited learning capacity. We conduct comparisons with @cite_31 and @cite_23 . Our proposed method shows favorable performance. We notice that @cite_6 use language-guided convolution for object tracking. However, they predict all the parameters which is difficult to train.\"","":""}
{"id":"2949402865","dialogue":"\"In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Extensive experiments on public VQA datasets validate the effectiveness of QGHC.\"","summary":"\"Recent research found that the combination of depth-wise convolution and channel shuffle with group convolution could reduce the number of parameters in CNN without hindering the final performance. Motivated by Xception @cite_39 , ResNeXt @cite_26 , and ShuffleNet @cite_22 , we decompose the visual CNN kernels into several groups. By shuffling parameters among different groups, our model can reduce the number of predicted parameters and improve the answering accuracy simultaneously. Note that for existing CNN methods with group convolution, the convolutional parameters are solely learned via back-propagation. In contrast, our QGHC consists of question-dependent kernels that are predicted based on language features and question-independent kernels that are freely updated.\"","":""}
{"id":"2887575628","dialogue":"\"Zero-shot learning transfers knowledge from seen classes to novel unseen classes to reduce human labor of labelling data for building new classifiers. Much effort on zero-shot learning however has focused on the standard multi-class setting, the more challenging multi-label zero-shot problem has received limited attention. In this paper we propose a transfer-aware embedding projection approach to tackle multi-label zero-shot learning. The approach projects the label embedding vectors into a low-dimensional space to induce better inter-label relationships and explicitly facilitate information transfer from seen labels to unseen labels, while simultaneously learning a max-margin multi-label classifier with the projected label embeddings. Auxiliary information can be conveniently incorporated to guide the label embedding projection to further improve label relation structures for zero-shot knowledge transfer. We conduct experiments for zero-shot multi-label image classification. The results demonstrate the efficacy of the proposed approach.\"","summary":"\"Multi-label Classification Multi-label classification is relevant in many application domains, where each data instance can be assigned into multiple classes. Many multi-label learning works developed in the literature have centered on exploiting the correlation interdependency information between the multiple labels, including the max-margin learning methods with pairwise ranking loss @cite_10 , weighted approximate pairwise ranking loss (WARP) @cite_11 , and calibrated separation ranking loss (CSRL) @cite_1 . Moreover, incomplete labels are frequently encountered in many multi-label applications due to noise or crowd-sourcing, where only a subset of true labels are provided on some training instances. Multi-label learning methods with missing labels have largely depended on observed label correlations to overcome the label incompleteness of the training data @cite_30 @cite_28 @cite_23 . These methods however assumed that all the labels are at least observed on a subset of training data and they cannot handle the more challenging zero-shot learning setting where some labels are completely missing from the training instances.\"","":""}
{"id":"2886572283","dialogue":"\"Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.\"","summary":"\"The most used data augmentation method for TSC is the slicing window technique, originally introduced for deep CNNs in @cite_2 . The method was originally inspired by the image cropping technique for data augmentation in computer vision tasks @cite_10 . This data transformation technique can, to a certain degree, guarantee that the cropped image still holds the same information as the original image. On the other hand, for time series data, one cannot make sure that the discriminative information has not been lost when a certain region of the time series is cropped. Nevertheless, this method was used in several TSC problems, such as in @cite_7 where it improved the Support Vector Machines accuracy for classifying electroencephalographic time series. @cite_15 , this slicing window technique was also adopted to improve the CNNs' mortgage delinquency prediction using customers' historical transactional data. In addition to the slicing window technique, jittering, scaling, warping and permutation were proposed in @cite_9 as generic time series data augmentation approaches. The authors in @cite_9 proposed a novel data augmentation method specific to wearable sensor time series data that rotates the trajectory of a person's arm around an axis (e.g. @math axis).\"","":""}
{"id":"2886572283","dialogue":"\"Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.\"","summary":"\"@cite_13 , the authors proposed to extend the slicing window technique with a warping window that generates synthetic time series by warping the data through time. This method was used to improve the classification of their deep CNN for TSC, which was also shown to significantly decrease the accuracy of a NN-DTW classifier when compared to our adopted data augmentation algorithm @cite_3 . We should note that the use of a window slicing technique means that the model should classify each subsequence alone and then finally classify the whole time series using a majority voting approach. Alternatively, our method does not crop time series into shorter subsequences which enables the network to learn discriminative properties from the whole time series in an end-to-end manner.\"","":""}
{"id":"2952978061","dialogue":"\"We consider practical hardware implementation of Polar decoders. To reduce latency due to the serial nature of successive cancellation (SC), existing optimizations improve parallelism with two approaches, i.e., multi-bit decision or reduced path splitting. In this paper, we combine the two procedures into one with an error-pattern-based architecture. It simultaneously generates a set of candidate paths for multiple bits with pre-stored patterns. For rate-1 (R1) or single parity-check (SPC) nodes, we prove that a small number of deterministic patterns are required to guarantee performance preservation. For general nodes, low-weight error patterns are indexed by syndrome in a look-up table and retrieved in O(1) time. The proposed flip-syndrome-list (FSL) decoder fully parallelizes all constituent code blocks without sacrificing performance, thus is suitable for ultra-low-latency applications. Meanwhile, two code construction optimizations are presented to further reduce complexity and improve performance, respectively.\"","summary":"\"Polar codes @cite_8 @cite_16 have been selected for the fifth generation (5G) wireless standard. With state-of-the-art code construction techniques @cite_19 @cite_0 @cite_5 and SC-List (SCL) decoding algorithm @cite_10 @cite_9 @cite_11 @cite_12 @cite_1 @cite_14 @cite_3 @cite_7 , Polar codes demonstrate competitive performance over LDPC and Turbo codes in terms of block error rate (BLER). Beyond 5G, ultra-low decoding latency emerges as a key requirement for applications such as autonomous driving and virtual reality. The latency of practical Polar decoders, e.g., an SC-list decoder with list size @math , is relatively long due to the serial processing nature.\"","":""}
{"id":"2952978061","dialogue":"\"We consider practical hardware implementation of Polar decoders. To reduce latency due to the serial nature of successive cancellation (SC), existing optimizations improve parallelism with two approaches, i.e., multi-bit decision or reduced path splitting. In this paper, we combine the two procedures into one with an error-pattern-based architecture. It simultaneously generates a set of candidate paths for multiple bits with pre-stored patterns. For rate-1 (R1) or single parity-check (SPC) nodes, we prove that a small number of deterministic patterns are required to guarantee performance preservation. For general nodes, low-weight error patterns are indexed by syndrome in a look-up table and retrieved in O(1) time. The proposed flip-syndrome-list (FSL) decoder fully parallelizes all constituent code blocks without sacrificing performance, thus is suitable for ultra-low-latency applications. Meanwhile, two code construction optimizations are presented to further reduce complexity and improve performance, respectively.\"","summary":"\"Continuous efforts @cite_10 @cite_9 @cite_11 @cite_12 @cite_1 @cite_14 @cite_3 have been made to significantly reduce decoding latency. Among them, we are particularly interested in hardware implementations, which are dominant in real-world products, due to better power- and area-efficiency. According to our cross-validation, three approaches are shown to be cost-effective, yet incur no or negligible performance loss compared to the original SCL decoder, as summarized below: Pruning on the SC decoding tree @cite_10 (parallelizing constituent code blocks with mult-bit decision) Rate-0 (R0), repetition (Rep) nodes @cite_9 @cite_11 . General (Gen) nodes comprised of consecutive bits @cite_12 @cite_1 . Reduce the number of path splitting Rate-1 (R1), single parity-check (SPC) nodes @cite_11 . Do not split upon the most reliable (good) bits @cite_14 @cite_3 . Reduce the latency of list pruning Adopt bitonic sort @cite_6 for efficient pruning. Quick list pruning @cite_4 .\"","":""}
{"id":"2949534134","dialogue":"\"The electroencephalography classifier is the most important component of brain-computer interface based systems. There are two major problems hindering the improvement of it. First, traditional methods do not fully exploit multimodal information. Second, large-scale annotated EEG datasets are almost impossible to acquire because biological data acquisition is challenging and quality annotation is costly. Herein, we propose a novel deep transfer learning approach to solve these two problems. First, we model cognitive events based on EEG data by characterizing the data using EEG optical flow, which is designed to preserve multimodal EEG information in a uniform representation. Second, we design a deep transfer learning framework which is suitable for transferring knowledge by joint training, which contains a adversarial network and a special loss function. The experiments demonstrate that our approach, when applied to EEG classification tasks, has many advantages, such as robustness and accuracy.\"","summary":"\"Transfer learning enables the use of different domains, tasks, and distributions for training and testing @cite_0 . @cite_12 reviewed the current state-of-the-art transfer learning approaches in BCI. @cite_10 transferred general features via a convolutional network across subjects and experiments. @cite_19 applied kernel principle analysis and transductive parameter transfer to identify the relationships between classifier parameter vectors across subjects. @cite_9 evaluated the transferability between subjects by calculating distance and transferred knowledge in comparable feature spaces to improve accuracy. @cite_15 proposed an approach which can simultaneously transfer knowledge across domains and tasks. @cite_1 and @cite_13 attempted the learning of transferable features by embedding task-specific layers in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched.\"","":""}
{"id":"2952888378","dialogue":"\"With multiple crowd gatherings of millions of people every year in events ranging from pilgrimages to protests, concerts to marathons, and festivals to funerals; visual crowd analysis is emerging as a new frontier in computer vision. In particular, counting in highly dense crowds is a challenging problem with far-reaching applicability in crowd safety and management, as well as gauging political significance of protests and demonstrations. In this paper, we propose a novel approach that simultaneously solves the problems of counting, density map estimation and localization of people in a given dense crowd image. Our formulation is based on an important observation that the three problems are inherently related to each other making the loss function for optimizing a deep CNN decomposable. Since localization requires high-quality images and annotations, we introduce UCF-QNRF dataset that overcomes the shortcomings of previous datasets, and contains 1.25 million humans manually marked with dot annotations. Finally, we present evaluation measures and comparison with recent deep CNN networks, including those developed specifically for crowd counting. Our approach significantly outperforms state-of-the-art on the new dataset, which is the most challenging dataset with the largest number of crowd annotations in the most diverse set of scenes.\"","summary":"\"Crowd counting is active an area of research with works tackling the three aspects of the problem: counting-by-regression @cite_28 , @cite_11 , @cite_16 , @cite_22 , @cite_2 , density map estimation @cite_11 , @cite_15 , @cite_8 , @cite_29 , @cite_1 and localization @cite_20 , @cite_17 .\"","":""}
{"id":"2952888378","dialogue":"\"With multiple crowd gatherings of millions of people every year in events ranging from pilgrimages to protests, concerts to marathons, and festivals to funerals; visual crowd analysis is emerging as a new frontier in computer vision. In particular, counting in highly dense crowds is a challenging problem with far-reaching applicability in crowd safety and management, as well as gauging political significance of protests and demonstrations. In this paper, we propose a novel approach that simultaneously solves the problems of counting, density map estimation and localization of people in a given dense crowd image. Our formulation is based on an important observation that the three problems are inherently related to each other making the loss function for optimizing a deep CNN decomposable. Since localization requires high-quality images and annotations, we introduce UCF-QNRF dataset that overcomes the shortcomings of previous datasets, and contains 1.25 million humans manually marked with dot annotations. Finally, we present evaluation measures and comparison with recent deep CNN networks, including those developed specifically for crowd counting. Our approach significantly outperforms state-of-the-art on the new dataset, which is the most challenging dataset with the largest number of crowd annotations in the most diverse set of scenes.\"","summary":"\"Earlier regression-based approaches mapped global image features or a combination of local patch features to obtain counts @cite_25 , @cite_4 , @cite_16 , @cite_5 . Since these methods only produce counts, they cannot be used for density map estimation or localization. The features were hand-crafted and in some cases multiple features were used @cite_22 , @cite_16 to handle low resolution, perspective distortion and severe occlusion. On the other hand, CNNs inherently learn multiple feature maps automatically, and therefore are now being extensively used for crowd counting and density map estimation.\"","":""}
{"id":"2952888378","dialogue":"\"With multiple crowd gatherings of millions of people every year in events ranging from pilgrimages to protests, concerts to marathons, and festivals to funerals; visual crowd analysis is emerging as a new frontier in computer vision. In particular, counting in highly dense crowds is a challenging problem with far-reaching applicability in crowd safety and management, as well as gauging political significance of protests and demonstrations. In this paper, we propose a novel approach that simultaneously solves the problems of counting, density map estimation and localization of people in a given dense crowd image. Our formulation is based on an important observation that the three problems are inherently related to each other making the loss function for optimizing a deep CNN decomposable. Since localization requires high-quality images and annotations, we introduce UCF-QNRF dataset that overcomes the shortcomings of previous datasets, and contains 1.25 million humans manually marked with dot annotations. Finally, we present evaluation measures and comparison with recent deep CNN networks, including those developed specifically for crowd counting. Our approach significantly outperforms state-of-the-art on the new dataset, which is the most challenging dataset with the largest number of crowd annotations in the most diverse set of scenes.\"","summary":"\"For localization in crowded scenes, Rodriguez al @cite_17 use density map as a regularizer during the detection. They optimize an objective function that prefers density map generated on detected locations to be similar to predicted density map @cite_11 . This results in both better precision and recall. The density map is generated by placing a Gaussian kernel at the location of each detection. Zheng al @cite_20 first obtain density map using sliding window over the image through @cite_11 , and then use integer programming to localize objects on the density maps. Similarly, in the domain of medical imaging, Sirinukunwattana al @cite_21 introduced spatially-constrained CNNs for detection and classification of cancer nuclei. In this paper, we present results and analysis for simultaneous crowd counting, density map estimation, and localization using Composition Loss on the proposed UCF-QNRF dataset.\"","":""}
{"id":"2950055289","dialogue":"\"Collaborative filtering is widely used in modern recommender systems. Recent research shows that variational autoencoders (VAEs) yield state-of-the-art performance by integrating flexible representations from deep neural networks into latent variable models, mitigating limitations of traditional linear factor models. VAEs are typically trained by maximizing the likelihood (MLE) of users interacting with ground-truth items. While simple and often effective, MLE-based training does not directly maximize the recommendation-quality metrics one typically cares about, such as top-N ranking. In this paper we investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to directly optimize the non-differentiable quality metrics of interest. Specifically, we train a critic network to approximate ranking-based metrics, and then update the actor network (represented here by a VAE) to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require to re-run the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. Empirically, we show that the proposed methods outperform several state-of-the-art baselines, including recently-proposed deep learning approaches, on three large-scale real-world datasets. The code to reproduce the experimental results and figure plots is on Github: https: github.com samlobel RaCT_CF\"","summary":"\"Deep Learning for Collaborative Filtering. To take advantage of the expressiveness of DNNs, there are many recent efforts focused on developing deep learning models for collaborative filtering @cite_34 @cite_55 @cite_8 @cite_51 @cite_15 @cite_3 . Early work on DNNs focused on explicit feedback settings @cite_54 @cite_10 @cite_26 , such as rating predictions. Recent research gradually recognized the importance of implicit feedback @cite_58 @cite_57 @cite_11 , where the user's preference is not explicitly presented @cite_0 . This setting is more practical but challenging, and is the focus of our work. Our method is closely related to three papers, on VAEs @cite_11 , collaborative denoising autoencoder (CDAE) @cite_58 and neural collaborative filtering (NCF) @cite_57 . CDAE and NCF may suffer from scalability issues: the model size grows linearly with both the number of users as well as items. The VAE @cite_11 alleviates this problem via amortized inference. Our work builds on top of the VAE, and improves it by optimizing to the ranking-based metric.\"","":""}
{"id":"2950055289","dialogue":"\"Collaborative filtering is widely used in modern recommender systems. Recent research shows that variational autoencoders (VAEs) yield state-of-the-art performance by integrating flexible representations from deep neural networks into latent variable models, mitigating limitations of traditional linear factor models. VAEs are typically trained by maximizing the likelihood (MLE) of users interacting with ground-truth items. While simple and often effective, MLE-based training does not directly maximize the recommendation-quality metrics one typically cares about, such as top-N ranking. In this paper we investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to directly optimize the non-differentiable quality metrics of interest. Specifically, we train a critic network to approximate ranking-based metrics, and then update the actor network (represented here by a VAE) to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require to re-run the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. Empirically, we show that the proposed methods outperform several state-of-the-art baselines, including recently-proposed deep learning approaches, on three large-scale real-world datasets. The code to reproduce the experimental results and figure plots is on Github: https: github.com samlobel RaCT_CF\"","summary":"\"Learned Metrics in Vision & Languages. Recent research in computer vision and natural language processing has generated excellent results, by using learned metrics instead of hand-crafted metrics. Among the rich literature of generating realistic images via generative adversarial networks (GANs) @cite_19 @cite_47 @cite_45 @cite_21 , our work is most similar to @cite_29 , where the VAE objective @cite_49 @cite_36 is augmented with the learned representations in the GAN discriminator @cite_19 to better measure image similarities. For language generation, the discrepancy between word-level MLE training and sequence-level semantic evaluation has been alleviated with GANs or RL techniques @cite_22 @cite_56 @cite_50 @cite_41 @cite_42 . The RL approach directly optimizes the metric used at test time, and has shown improvement on various applications, including dialogue @cite_4 , image captioning @cite_35 and translations @cite_24 . Despite the significant successes in vision and language analysis, there has been little if any research reported for directly learning the metrics with deep neural networks for collaborative filtering. Our work fills the gap, and we hope it inspires more research in this direction.\"","":""}
{"id":"2950055289","dialogue":"\"Collaborative filtering is widely used in modern recommender systems. Recent research shows that variational autoencoders (VAEs) yield state-of-the-art performance by integrating flexible representations from deep neural networks into latent variable models, mitigating limitations of traditional linear factor models. VAEs are typically trained by maximizing the likelihood (MLE) of users interacting with ground-truth items. While simple and often effective, MLE-based training does not directly maximize the recommendation-quality metrics one typically cares about, such as top-N ranking. In this paper we investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to directly optimize the non-differentiable quality metrics of interest. Specifically, we train a critic network to approximate ranking-based metrics, and then update the actor network (represented here by a VAE) to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require to re-run the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. Empirically, we show that the proposed methods outperform several state-of-the-art baselines, including recently-proposed deep learning approaches, on three large-scale real-world datasets. The code to reproduce the experimental results and figure plots is on Github: https: github.com samlobel RaCT_CF\"","summary":"\"Learning to Rank (L2R). The idea of L2R has existed for two decades in the information-retrieval community. The goal is to directly optimize against ranking-based evaluation metrics @cite_32 @cite_43 . Previous work on L2R employs objective relaxations @cite_33 . Some techniques can be extended to recommendation settings @cite_46 @cite_6 @cite_44 @cite_9 @cite_13 . Many L2R methods in recommendation are essentially trained by optimizing a classification function, such as the popular pairwise L2R method BPR @cite_46 and WARP @cite_12 described Section 2.1. One limitation is that they are computationally expensive when the number of items is large. To accelerate these approaches, cheap approximations are made in each training step, which results in degraded performance. In contrast, the proposed RaCT is efficient and scalable. In fact, the traditional L2R methods can be integrated into our actor-critic framework, yielding improved performance as shown in our experiments.\"","":""}
{"id":"2885930799","dialogue":"\"In this paper, we study parametric analysis of semidefinite optimization problems with respect to the perturbation of objective function. We investigate the behavior of the optimal partition and optimal set mapping in a so called nonlinearity interval. Furthermore, we investigate the sensitivity of the approximation of the optimal partition, which have been recently studied by Mohammad-Nezhad and Terlaky. The approximation of the optimal partition was obtained from a bounded sequence of interior solutions on, or in a neighborhood of the central path. We derive an upper bound on the distance between the invariant subspaces spanned by the approximation of the optimal partition.\"","summary":"\"Adler and Monteiro @cite_34 studied the parametric analysis of LO problems using the concept of optimal partition. Another treatment of sensitivity analysis for LO based on the optimal partition approach was given by @cite_15 and Greenberg @cite_3 . @cite_12 @cite_33 extended the optimal partition approach to linearly constrained quadratic optimization (LCQO) with perturbation in the right hand side vector and showed that the optimal value function is convex and piecewise quadratic. There have been further studies on optimal partition and parametric analysis of conic optimization problems. In contrast to LO, the optimal partition of SDO is defined as a 3-tuple of mutually orthogonal subspaces of @math , see . Goldfarb and Scheinberg @cite_22 considered a parametric SDO problem, where the objective is perturbed along a fixed direction. They derived auxiliary problems to compute the directional derivatives of the optimal value function and the so-called invariancy set of the optimal partition. Yildirim @cite_18 extended the concept of the optimal partition and the auxiliary problems in @cite_22 for linear conic optimization problems.\"","":""}
{"id":"2950710378","dialogue":"\"Most neural-network based speaker-adaptive acoustic models for speech synthesis can be categorized into either layer-based or input-code approaches. Although both approaches have their own pros and cons, most existing works on speaker adaptation focus on improving one or the other. In this paper, after we first systematically overview the common principles of neural-network based speaker-adaptive models, we show that these approaches can be represented in a unified framework and can be generalized further. More specifically, we introduce the use of scaling and bias codes as generalized means for speaker-adaptive transformation. By utilizing these codes, we can create a more efficient factorized speaker-adaptive model and capture advantages of both approaches while reducing their disadvantages. The experiments show that the proposed method can improve the performance of speaker adaptation compared with speaker adaptation based on the conventional input code.\"","summary":"\"Constrained Maximum Likelihood Linear Regression (CMLLR) @cite_30 @cite_31 , also known as feature-space MLLR (fMLLR), is a widely used speaker adaptation technique for hidden Markov model (HMM)-based speech processing systems in which a speaker-dependent affine transformation is applied to source acoustic features to explain target data better. In the case of automatic speech recognition (ASR), the transformation acts as a method of normalization, whereas in the case of speech synthesis, the transformation purpose is to diverge the acoustic output to each target speaker @cite_24 . The fMLLR method can be described using the following equation: where @math is the source acoustic features, @math represents approximated acoustic features of the target speaker @math , @math is a full linear matrix and @math is the bias vector. @math and @math are transformation parameters specific to each speaker.\"","":""}
{"id":"2950710378","dialogue":"\"Most neural-network based speaker-adaptive acoustic models for speech synthesis can be categorized into either layer-based or input-code approaches. Although both approaches have their own pros and cons, most existing works on speaker adaptation focus on improving one or the other. In this paper, after we first systematically overview the common principles of neural-network based speaker-adaptive models, we show that these approaches can be represented in a unified framework and can be generalized further. More specifically, we introduce the use of scaling and bias codes as generalized means for speaker-adaptive transformation. By utilizing these codes, we can create a more efficient factorized speaker-adaptive model and capture advantages of both approaches while reducing their disadvantages. The experiments show that the proposed method can improve the performance of speaker adaptation compared with speaker adaptation based on the conventional input code.\"","summary":"\"Next we explain the existing DNN-based speaker adaptation methods, that is, speaker-dependent layers and speaker-dependent input code using similar notations to the above fMLLR. For the speaker-dependent layers @cite_13 @cite_27 approach, the weight matrices and bias vectors of specific layers are fine-tuned using adaptation data, therefore we can rewrite Equation as: where @math and @math are now specific to a target speaker @math and @math also represents an adapted hidden layer . The method has the advantage of modeling both a full matrix @math and the bias vector @math , which usually yield favorable result when the adaptation data is sufficient @cite_29 @cite_27 . However when the amount of adaptation data is limited, the result is unstable as number of parameters estimated is very large @cite_22 . This is also the reason that this method typically involves reducing the number of parameters estimated @cite_11 @cite_7 @cite_27 in order to retain the adaptation performance.\"","":""}
{"id":"2950710378","dialogue":"\"Most neural-network based speaker-adaptive acoustic models for speech synthesis can be categorized into either layer-based or input-code approaches. Although both approaches have their own pros and cons, most existing works on speaker adaptation focus on improving one or the other. In this paper, after we first systematically overview the common principles of neural-network based speaker-adaptive models, we show that these approaches can be represented in a unified framework and can be generalized further. More specifically, we introduce the use of scaling and bias codes as generalized means for speaker-adaptive transformation. By utilizing these codes, we can create a more efficient factorized speaker-adaptive model and capture advantages of both approaches while reducing their disadvantages. The experiments show that the proposed method can improve the performance of speaker adaptation compared with speaker adaptation based on the conventional input code.\"","summary":"\"Learning Hidden Unit Contribution (LHUC) @cite_12 is an adaptation method that transforms outputs of the activation function using a speaker-dependent diagonal transformation matrix, which significantly reduces the number of parameters: where @math is a diagonal matrix for speaker @math , @math is an operation to extract diagonal elements of a @math matrix as a @math vector, and @math is an element-wise multiplication of vectors. In LHUC, since we apply the transformation after the activation function of the current layer, we may write the LHUC operation at the next hidden layer as follows: From these equations, we see that a speaker-specific weight matrix @math is factorized as @math .\"","":""}
{"id":"2883912069","dialogue":"\"Original and learnt clauses in Conflict-Driven Clause Learning (CDCL) SAT solvers often contain redundant literals. This may have a negative impact on performance because redundant literals may deteriorate both the effectiveness of Boolean constraint propagation and the quality of subsequent learnt clauses. To overcome this drawback, we propose a clause vivification approach that eliminates redundant literals by applying unit propagation. The proposed clause vivification is activated before the SAT solver triggers some selected restarts, and only affects a subset of original and learnt clauses, which are considered to be more relevant according to metrics like the literal block distance (LBD). Moreover, we conducted an empirical investigation with instances coming from the hard combinatorial and application categories of recent SAT competitions. The results show that a remarkable number of additional instances are solved when the proposed approach is incorporated into five of the best performing CDCL SAT solvers (Glucose, TC_Glucose, COMiniSatPS, MapleCOMSPS and MapleCOMSPS_LRB). More importantly, the empirical investigation includes an in-depth analysis of the effectiveness of clause vivification. It is worth mentioning that one of the SAT solvers described here was ranked first in the main track of SAT Competition 2017 thanks to the incorporation of the proposed clause vivification. That solver was further improved in this paper and won the bronze medal in the main track of SAT Competition 2018.\"","summary":"\"Eliminating redundant literals in clauses (see e.g. @cite_11 @cite_19 @cite_30 @cite_33 @cite_13 @cite_7 @cite_8 @cite_16 @cite_17 ) before and during the search is crucial for the performance of CDCL SAT solvers for several reasons: (i) shorter clauses need less memory; (ii) shorter clauses are easier to become unit, and thus increase the power of unit propagation; and (iii) shorter clauses can lead to shorter learnt clauses.\"","":""}
{"id":"2950691724","dialogue":"\"Superpixels provide an efficient low mid-level representation of image data","summary":"which greatly reduces the number of image primitives for subsequent vision tasks. Existing superpixel algorithms are not differentiable","":""}
{"id":"2950691724","dialogue":"\"Superpixels provide an efficient low mid-level representation of image data","summary":"which greatly reduces the number of image primitives for subsequent vision tasks. Existing superpixel algorithms are not differentiable","":""}
{"id":"2884915206","dialogue":"\"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.\"","summary":"\"Traditional approaches to image-based gaze estimation are typically categorized as or . Feature-based approaches reduce an eye image down to a set of features based on hand-crafted rules @cite_39 @cite_1 @cite_26 @cite_0 and then feed these features into simple, often linear machine learning models to regress the final gaze estimate. Model-based methods instead attempt to fit a known 3D model to the eye image @cite_6 @cite_7 @cite_10 @cite_21 by minimizing a suitable energy.\"","":""}
{"id":"2884915206","dialogue":"\"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.\"","summary":"\"Early works in appearance-based methods were restricted to laboratory settings with fixed head pose @cite_25 @cite_2 . These initial constraints have become progressively relaxed, notably by the introduction of new datasets collected in everyday settings @cite_36 @cite_13 or in simulated environments @cite_3 @cite_40 @cite_46 . The increasing scale and complexity of training data has given rise to a wide variety of learning-based methods including variations of linear regression @cite_41 @cite_38 @cite_23 , random forests @cite_3 , @math -nearest neighbours @cite_3 @cite_46 , and CNNs @cite_36 @cite_13 @cite_9 @cite_33 @cite_40 @cite_14 . CNNs have proven to be more robust to visual appearance variations, and are capable of person-independent gaze estimation when provided with sufficient scale and diversity of training data. Person-independent gaze estimation can be performed without a user calibration step, and can directly be applied to areas such as visual attention analysis on unmodified devices @cite_47 , interaction on public displays @cite_16 , and identification of gaze targets @cite_27 , albeit at the cost of increased need for training data and computational cost.\"","":""}
{"id":"2884915206","dialogue":"\"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.\"","summary":"\"Several CNN architectures have been proposed for person-independent gaze estimation in unconstrained settings, mostly differing in terms of possible input data modalities. Zhang al @cite_36 @cite_33 adapt the LeNet-5 and VGG-16 architectures such that head pose angles (pitch and yaw) are concatenated to the first fully-connected layers. Despite its simplicity this approach yields the current best gaze estimation error of @math when evaluating for the within-dataset cross-person case on MPIIGaze with single eye image and head pose input. In @cite_13 separate convolutional streams are used for left right eye images, a face image, and a @math grid indicating the location and scale of the detected face in the image frame. Their experiments demonstrate that this approach yields improvements compared to @cite_36 . In @cite_33 a single face image is used as input and so-called spatial-weights are learned. These emphasize important features based on the input image, yielding considerable improvements in gaze estimation accuracy.\"","":""}
{"id":"2884915206","dialogue":"\"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.\"","summary":"\"It has been shown @cite_4 @cite_28 that by applying a loss function on intermediate outputs of a network, better performance can be yielded in different tasks. This technique was introduced to address the vanishing gradients problem during the training of deeper networks. In addition, such intermediate supervision allows for the network to quickly learn an estimate for the final output then learn to refine the predicted features - simplifying the mappings which need to be learned at every layer. Subsequent works have adopted intermediate supervision @cite_15 @cite_18 to good effect for human pose estimation, by replicating the final output loss.\"","":""}
{"id":"2884915206","dialogue":"\"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.\"","summary":"\"Most similar to our contribution is the work in @cite_24 where facial landmark localization performance is improved by applying an auxiliary emotion classification loss. A key aspect to note is that their network is sequential, that is, the emotion recognition network takes only facial landmarks as input. The detected facial landmarks thus act as a manually defined representation for emotion classification, and creates a bottleneck in the full data flow. It is shown experimentally that applying such an auxiliary loss (for a different task) yields improvement over state-of-the-art results on the AFLW dataset. In our work, we learn to regress an intermediate and minimal representation for gaze direction, forming a bottleneck before the main task of regressing two angle values. Thus, an important distinction to @cite_24 is that while we employ an auxiliary loss term, it directly contributes to the task of gaze direction estimation. Furthermore, the auxiliary loss is applied as an intermediate task. We detail this further in Sec. .\"","":""}
{"id":"2884915206","dialogue":"\"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.\"","summary":"\"Recent work in multi-person human pose estimation @cite_17 learns to estimate joint location heatmaps alongside so-called part affinity fields''. When combined, the two outputs then enable the detection of multiple peoples' joints with reduced ambiguity in terms of which person a joint belongs to. In addition, at the end of every image scale, the architecture concatenates feature maps from each separate stream such that information can flow between the part confidence'' and part affinity'' maps. Thus, they operate on the image representation space, taking advantage of the strengths of convolutional neural networks. Our work is similar in spirit in that it introduces a novel image-based representation.\"","":""}
{"id":"2950503707","dialogue":"\"We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.\"","summary":"\"Deep learning techniques have improved the accuracy of various vision systems @cite_12 @cite_9 @cite_10 @cite_30 . Especially, a lot of generative problems @cite_43 @cite_17 @cite_4 @cite_40 have been solved by GANs @cite_18 . However, traditional frameworks fail to handle complicated tasks, e.g., to generate fine-grained images or videos with large motion changes. Recent approaches @cite_6 @cite_33 @cite_25 prove that coarse-to-fine strategy can handle these cases. Our model also employs this strategy for video generation.\"","":""}
{"id":"2950503707","dialogue":"\"We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.\"","summary":"\"@cite_33 proposed an algorithm to generate video in two stages, but there are important differences between their work and ours. First, @cite_33 is proposed for time-lapse videos while we can generate general videos. Second, we make use of structure conditions to guide the generation in the first stage but @cite_33 models this stage with 3D convolutional networks. Finally, we can make long-term predictions while @cite_33 only generates videos with fixed length.\"","":""}
{"id":"2950503707","dialogue":"\"We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.\"","summary":"\"Video Generation. Recent methods @cite_27 @cite_29 @cite_26 @cite_31 solve image-to-video generation problem by training transformation networks that translate the input image into each future frame separately, together with a generator predicting the structure sequence which conditions the future frames. However, due to the absence of pixel-level temporal knowledge during the training process, motion artifacts can be observed from the results of these methods.\"","":""}
{"id":"2950503707","dialogue":"\"We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.\"","summary":"\"Other approaches explore learning temporal visual features from video with spatiotemporal networks. @cite_39 showed how 3D convolutional networks could be applied to human action recognition. @cite_13 employed spatiotemporal 3D convolutions to model features encoded in videos. @cite_35 built a model to generate scene dynamics with 3D generative adversarial networks. Our method differs from the two-stream model of @cite_35 in two aspects. First, our residual motion map disentangles motion from the input: the generated frame is conditioned on the current and future motion structures. Second, we can control object motions in future frames efficiently by using structure conditions. Therefore, our method can be applied to motion manipulation problems.\"","":""}
{"id":"2964281094","dialogue":"\"In recent years, with the popularization of deep learning frameworks and large datasets, researchers have started parallelizing their models in order to train faster. This is crucially important, because they typically explore many hyperparameters in order to find the best ones for their applications. This process is time consuming and, consequently, speeding up training improves productivity. One approach to parallelize deep learning models followed by many researchers is based on weak scaling. The minibatches increase in size as new GPUs are added to the system. In addition, new learning rates schedules have been proposed to fix optimization issues that occur with large minibatch sizes. In this paper, however, we show that the recommendations provided by recent work do not apply to models that lack large datasets. In fact, we argument in favor of using strong scaling for achieving reliable performance in such cases. We evaluated our approach with up to 32 GPUs and show that weak scaling not only does not have the same accuracy as the sequential model, it also fails to converge most of time. Meanwhile, strong scaling has good scalability while having exactly the same accuracy of a sequential implementation.\"","summary":"Goyal @cite_9 present a strategy to train ResNet-50 @cite_0 in one hour using the ImageNet dataset. They employed a distributed synchronous Stochastic Gradient Descent (SGD) approach with up to 256 GPUs. They used a large minibatch---8192 examples---and reached an accuracy similar to a much smaller minibatch of 256 examples.","":""}
{"id":"2964281094","dialogue":"\"In recent years, with the popularization of deep learning frameworks and large datasets, researchers have started parallelizing their models in order to train faster. This is crucially important, because they typically explore many hyperparameters in order to find the best ones for their applications. This process is time consuming and, consequently, speeding up training improves productivity. One approach to parallelize deep learning models followed by many researchers is based on weak scaling. The minibatches increase in size as new GPUs are added to the system. In addition, new learning rates schedules have been proposed to fix optimization issues that occur with large minibatch sizes. In this paper, however, we show that the recommendations provided by recent work do not apply to models that lack large datasets. In fact, we argument in favor of using strong scaling for achieving reliable performance in such cases. We evaluated our approach with up to 32 GPUs and show that weak scaling not only does not have the same accuracy as the sequential model, it also fails to converge most of time. Meanwhile, strong scaling has good scalability while having exactly the same accuracy of a sequential implementation.\"","summary":"\"Tensorflow has also parallel execution capabilities built-in @cite_1 . In this framework, the programmer specifies a set of operations to be placed in the available devices. These devices can be processors and accelerators (GPUs and TPUs) that are distributed across a cluster of computers. A common strategy is to split the data and processing units into parameter servers and workers nodes. The parameter servers are responsible for holding the network weights and other parameters, while the workers are responsible for computing the forward and backward passes of the model.\"","":""}
{"id":"2964281094","dialogue":"\"In recent years, with the popularization of deep learning frameworks and large datasets, researchers have started parallelizing their models in order to train faster. This is crucially important, because they typically explore many hyperparameters in order to find the best ones for their applications. This process is time consuming and, consequently, speeding up training improves productivity. One approach to parallelize deep learning models followed by many researchers is based on weak scaling. The minibatches increase in size as new GPUs are added to the system. In addition, new learning rates schedules have been proposed to fix optimization issues that occur with large minibatch sizes. In this paper, however, we show that the recommendations provided by recent work do not apply to models that lack large datasets. In fact, we argument in favor of using strong scaling for achieving reliable performance in such cases. We evaluated our approach with up to 32 GPUs and show that weak scaling not only does not have the same accuracy as the sequential model, it also fails to converge most of time. Meanwhile, strong scaling has good scalability while having exactly the same accuracy of a sequential implementation.\"","summary":"\"The Tensorflow framework has grown in great popularity. However, its parallelization capabilities have been criticized for its unnecessary complexity. One alternative was proposed by Sergeev and Balso @cite_10 . They used a simplified strategy based on MPI to parallelize and run distributed SGD models. A key feature of the approach is to combine small messages so that it better uses the network.\"","":""}
{"id":"2964281094","dialogue":"\"In recent years, with the popularization of deep learning frameworks and large datasets, researchers have started parallelizing their models in order to train faster. This is crucially important, because they typically explore many hyperparameters in order to find the best ones for their applications. This process is time consuming and, consequently, speeding up training improves productivity. One approach to parallelize deep learning models followed by many researchers is based on weak scaling. The minibatches increase in size as new GPUs are added to the system. In addition, new learning rates schedules have been proposed to fix optimization issues that occur with large minibatch sizes. In this paper, however, we show that the recommendations provided by recent work do not apply to models that lack large datasets. In fact, we argument in favor of using strong scaling for achieving reliable performance in such cases. We evaluated our approach with up to 32 GPUs and show that weak scaling not only does not have the same accuracy as the sequential model, it also fails to converge most of time. Meanwhile, strong scaling has good scalability while having exactly the same accuracy of a sequential implementation.\"","summary":"\"In medical imaging, the current state-of-art for classification and segmentation of 3D exams are 3D deep learning models @cite_15 . However, the widely known computational burden of such models due to 3D convolutions, in many times hinders the development of real-time systems for aiding in the diagnosis of 3D exams. This creates opportunities for methods that aim at providing time-efficient support to process 3D models in parallel.\"","":""}
{"id":"2884188791","dialogue":"\"Residual networks, which use a residual unit to supplement the identity mappings, enable very deep convolutional architecture to operate well, however, the residual architecture has been proved to be diverse and redundant, which may leads to low-efficient modeling. In this work, we propose a competitive squeeze-excitation (SE) mechanism for the residual network. Re-scaling the value for each channel in this structure will be determined by the residual and identity mappings jointly, and this design enables us to expand the meaning of channel relationship modeling in residual blocks. Modeling of the competition between residual and identity mappings cause the identity flow to control the complement of the residual feature maps for itself. Furthermore, we design a novel inner-imaging competitive SE block to shrink the consumption and re-image the global features of intermediate network structure, by using the inner-imaging mechanism, we can model the channel-wise relations with convolution in spatial. We carry out experiments on the CIFAR, SVHN, and ImageNet datasets, and the proposed method can challenge state-of-the-art results.\"","summary":"\"ResNet @cite_46 has become popular by virtue of its assistance in deep model training. Numerous works based thereon improve performance by expanding its structure @cite_18 @cite_43 @cite_39 @cite_19 or use its explanation of ordinary differential equations to explore its reversible form @cite_23 @cite_36 . Because ResNet is internally diverse without operations such as the \"\"drop-path\"\" @cite_33 and has been proven to be structurally redundant @cite_14","":""}
{"id":"2884188791","dialogue":"\"Residual networks, which use a residual unit to supplement the identity mappings, enable very deep convolutional architecture to operate well, however, the residual architecture has been proved to be diverse and redundant, which may leads to low-efficient modeling. In this work, we propose a competitive squeeze-excitation (SE) mechanism for the residual network. Re-scaling the value for each channel in this structure will be determined by the residual and identity mappings jointly, and this design enables us to expand the meaning of channel relationship modeling in residual blocks. Modeling of the competition between residual and identity mappings cause the identity flow to control the complement of the residual feature maps for itself. Furthermore, we design a novel inner-imaging competitive SE block to shrink the consumption and re-image the global features of intermediate network structure, by using the inner-imaging mechanism, we can model the channel-wise relations with convolution in spatial. We carry out experiments on the CIFAR, SVHN, and ImageNet datasets, and the proposed method can challenge state-of-the-art results.\"","summary":"\"A parallel line of research has deemed that intermediate feature maps should be modeled repeatedly @cite_41 . This compact architecture enables intermediate features to be refined and expanded, thereby enhancing the representation ability with concentrated parameter sizes. @cite_16 proposed a more compact model by circulating the dense block. Furthermore, dual-path networks (DPNs) @cite_2 combine the advantages of ResNet and DenseNet, and cause the residual units to perform extra modeling of the relationship between the identity and densely connected flow. A trend of compact architectures is to expand the mission of the network subassemblies while refining the intermediate features. Based on the SE block @cite_25 , our proposed CMPE-SE design also refines the intermediate features and develops the role of the SE unit. The difference is that our model focuses on self-controlling of components in ResNet, rather than simple feature reuse. Moreover, the re-imaging of channel signals presents a novel modeling view of intermediate features.\"","":""}
{"id":"2884188791","dialogue":"\"Residual networks, which use a residual unit to supplement the identity mappings, enable very deep convolutional architecture to operate well, however, the residual architecture has been proved to be diverse and redundant, which may leads to low-efficient modeling. In this work, we propose a competitive squeeze-excitation (SE) mechanism for the residual network. Re-scaling the value for each channel in this structure will be determined by the residual and identity mappings jointly, and this design enables us to expand the meaning of channel relationship modeling in residual blocks. Modeling of the competition between residual and identity mappings cause the identity flow to control the complement of the residual feature maps for itself. Furthermore, we design a novel inner-imaging competitive SE block to shrink the consumption and re-image the global features of intermediate network structure, by using the inner-imaging mechanism, we can model the channel-wise relations with convolution in spatial. We carry out experiments on the CIFAR, SVHN, and ImageNet datasets, and the proposed method can challenge state-of-the-art results.\"","summary":"\"Attention is widely applied in the modeling process of CNNs @cite_37 and is typically used to re-weight the image spatial signals @cite_24 @cite_5 @cite_11 @cite_22 , including multi-scale @cite_1 @cite_35 and multi-shape @cite_38 features. As a tool for biasing the allocation of resources @cite_25 , attention is also used to regulate the internal CNN features of @cite_3 @cite_42 . Unlike channel switching, combination @cite_32 @cite_40 or using reinforcement learning to reorganize the network paths @cite_30 , channel-wise attention, typically such as @cite_25 , provides an end-to-end training solution for re-weighting the intermediate channel features. Moreover, certain models combine spatial and channel-wise attention @cite_0 @cite_8 @cite_20 , and their modeling scope is still limited in total attentional elements. In contrast, our proposed CMPE-SE block considers the additional related factors (identity mappings) apart from the objects of attention (residual mappings). Furthermore, we test the effects of various convolutional filters in channel-wise attention with channel signal inner-imaging, which can mine the spatial channel-wise relations.\"","":""}
{"id":"2759245808","dialogue":"\"Named Entity Recognition for social media data is challenging because of its inherent noisiness. In addition to improper grammatical structures, it contains spelling inconsistencies and numerous informal abbreviations. We propose a novel multi-task approach by employing a more general secondary task of Named Entity (NE) segmentation together with the primary task of fine-grained NE categorization. The multi-task neural network architecture learns higher order feature representations from word and character sequences along with basic Part-of-Speech tags and gazetteer information. This neural network acts as a feature extractor to feed a Conditional Random Fields classifier. We were able to obtain the first position in the 3rd Workshop on Noisy User-generated Text (WNUT-2017) with a 41.86 entity F1-score and a 40.24 surface F1-score.\"","summary":"\"Traditional NER systems use hand-crafted features, gazetteers and other external resources to perform well @cite_12 . obtain state-of-the-art results by relying on heavily hand-crafted features, which are expensive to develop and maintain. Recently, many studies have outperformed traditional NER systems by applying neural network architectures. For instance, use a bidirectional LSTM-CRF architecture. They obtain a state-of-the-art performance without relying on hand-crafted features. , who achieved the first place on WNUT-2016 shared task, use a BLSTM neural network to leverage orthographic features. We use a similar approach but we employ CNN and BLSTM in parallel instead of forwarding the CNN output to the BLSTM. Nevertheless, our main contribution resides on Multi-Task Learning (MTL) and a combination of POS tags and gazetteers representation to feed the network. Recently, MTL has gained significant attention. Researchers have tried to correlate the success of MTL with label entropy, regularizers, training data size, and other aspects @cite_13 @cite_17 . For instance, use a multi-task network for different NLP tasks and show that the multi-task setting improves generality among shared tasks. In this paper, we take advantage of the multi-task setting by adding a more general secondary task, NE segmentation, along with the primary NE categorization task.\"","":""}
{"id":"2884493853","dialogue":"\"Nowadays, there are plenty of works introducing convolutional neural networks (CNNs) to the steganalysis and exceeding conventional steganalysis algorithms. These works have shown the improving potential of deep learning in information hiding domain. There are also several works based on deep learning to do image steganography, but these works still have problems in capacity, invisibility and security. In this paper, we propose a novel CNN architecture named as to conceal a secret gray image into a color cover image on the sender side and exactly extract the secret image out on the receiver side. There are three contributions in our work: (i) we improve the invisibility by hiding the secret image only in the Y channel of the cover image; (ii) We introduce the generative adversarial networks to strengthen the security by minimizing the divergence between the empirical probability distributions of stego images and natural images. (iii) In order to associate with the human visual system better, we construct a mixed loss function which is more appropriate for steganography to generate more realistic stego images and reveal out more better secret images. Experiment results show that ISGAN can achieve start-of-art performances on LFW, Pascal VOC2012 and ImageNet datasets.\"","summary":"\"There have been plenty of works using deep learning to do image steganalysis and got excellent performance. @cite_20 proposed a CNN-based steganalysis model GNCNN, the model introduced the hand-crafted KV filter to extract residual noise and used the gaussian activation function to get more useful features. The performance of the GNCNN is inferior to the state-of-the-art hand-crafted feature set spatial rich model (SRM) @cite_3 slightly. Based on GNCNN, @cite_0 presented Batch Normalization @cite_29 in to prevent the network falling into the local minima. XuNet was equipped with Tanh, @math convolution, global average pooling, and got comparable performance to SRM @cite_3 . @cite_16 put forward YeNet which surpassed SRM and its several variants. YeNet used 30 hand-crafted filters from SRM to prepropose images, applied well-designed activation function named TLU and selection-channel module to strengthen features from rich texture region where is more suitable for hiding information. @cite_13 proposed a JPEG steganalysis model with less parameters than XuNet and got better performance than XuNet. These works have applied deep learning to steganalysis successfully, but there is still space for improvement.\"","":""}
{"id":"2884493853","dialogue":"\"Nowadays, there are plenty of works introducing convolutional neural networks (CNNs) to the steganalysis and exceeding conventional steganalysis algorithms. These works have shown the improving potential of deep learning in information hiding domain. There are also several works based on deep learning to do image steganography, but these works still have problems in capacity, invisibility and security. In this paper, we propose a novel CNN architecture named as to conceal a secret gray image into a color cover image on the sender side and exactly extract the secret image out on the receiver side. There are three contributions in our work: (i) we improve the invisibility by hiding the secret image only in the Y channel of the cover image; (ii) We introduce the generative adversarial networks to strengthen the security by minimizing the divergence between the empirical probability distributions of stego images and natural images. (iii) In order to associate with the human visual system better, we construct a mixed loss function which is more appropriate for steganography to generate more realistic stego images and reveal out more better secret images. Experiment results show that ISGAN can achieve start-of-art performances on LFW, Pascal VOC2012 and ImageNet datasets.\"","summary":"\"Baluja @cite_22 designed a CNN model to conceal a color secret image into a color cover image yielding state-of-art performance. @cite_4 proposed another encoder-decoder based model to finish the same steganography task (their secret images are gray images). This is a novel steganography method which gets rid of hand-crafted algorithms. It can learn how to merge the cover image and the secret image together automatically. But stego images generated by their models are distorted in color. As shown in Fig. , Atique's stego images are yellowing when compared with the corresponding cover images. And their stego images are easily recognized by well trained CNN-based steganalyzer @cite_22 because of the large capacity. Inspired by works of Baluja and Atique, we improve each shortcoming and get .\"","":""}
{"id":"2884959301","dialogue":"\"We introduce a new system for automatic image content removal and inpainting. Unlike traditional inpainting algorithms, which require advance knowledge of the region to be filled in, our system automatically detects the area to be removed and infilled. Region segmentation and inpainting are performed jointly in a single pass. In this way, potential segmentation errors are more naturally alleviated by the inpainting module. The system is implemented as an encoder-decoder architecture, with two decoder branches, one tasked with segmentation of the foreground region, the other with inpainting. The encoder and the two decoder branches are linked via neglect nodes, which guide the inpainting process in selecting which areas need reconstruction. The whole model is trained using a conditional GAN strategy. Comparative experiments show that our algorithm outperforms state-of-the-art inpainting techniques (which, unlike our system, do not segment the input image and thus must be aided by an external segmentation module.)\"","summary":"\"Pixel-level semantic image segmentation has received considerable attention over the past few years. Most recently published techniques are based on fully convolutional networks (FCN) @cite_22 , possibly combined with fully connected CRF @cite_10 @cite_13 @cite_28 @cite_33 . The general architecture of a FCN includes a sequence of convolution and downsampling layers ( encoder ), which extract multi--scale features, followed by a sequence of deconvolution layers ( decoder ), which produce a high--resolution segmentation (or prediction''). Skip layers are often added, providing shortcut links from an encoder layer to its corresponding decoder layer. The role of skip layers is to provide well-localized information to a decoder layer, in addition to the semantic-rich but poorly resolved information from the prior decoder layer. In this way, skip layers enable good pixel-level localization while facilitating gradient flow during training. Similar architectures have been used in various applications such as text segmentation @cite_9 @cite_34 @cite_33 @cite_29 , edge detection @cite_12 , face segmentation @cite_25 , and scene parsing @cite_18 . Although these algorithms could be used for the foreground segmentation component of a content removal system, unavoidable inaccuracies are liable to dramatically decrease the quality of the recovered background region.\"","":""}
{"id":"2884959301","dialogue":"\"We introduce a new system for automatic image content removal and inpainting. Unlike traditional inpainting algorithms, which require advance knowledge of the region to be filled in, our system automatically detects the area to be removed and infilled. Region segmentation and inpainting are performed jointly in a single pass. In this way, potential segmentation errors are more naturally alleviated by the inpainting module. The system is implemented as an encoder-decoder architecture, with two decoder branches, one tasked with segmentation of the foreground region, the other with inpainting. The encoder and the two decoder branches are linked via neglect nodes, which guide the inpainting process in selecting which areas need reconstruction. The whole model is trained using a conditional GAN strategy. Comparative experiments show that our algorithm outperforms state-of-the-art inpainting techniques (which, unlike our system, do not segment the input image and thus must be aided by an external segmentation module.)\"","summary":"\"Image inpainting @cite_31 has a long history. The goal of inpainting is to fill in a missing region with realistic image content. A variety of inpainting methods have been proposed, including those based on prior image statistics @cite_23 @cite_0 , and those based on CNNs @cite_21 @cite_4 . More recently, outstanding results have been demonstrated with the use of GANs to inpaint even large missing areas @cite_8 @cite_2 @cite_15 @cite_35 . While appropriate for certain domains ( face inpainting), methods in this category often suffer from serious limitations, including the requirement that the missing region have fixed size and shape. All of the inpainting methods mentioned above assume that the corrupted region mask is known (typically as provided by the user). This limits their scope of application, as in most cases, this mask is unavailable. This shortcoming is addressed by blind inpainting algorithms @cite_17 @cite_30 , which do not need access to the foreground mask. However, prior blind inpainting work has been demonstrated only for very simple cases, with constant-valued foreground occupying a small area of the image.\"","":""}
{"id":"2942641522","dialogue":"\"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.\"","summary":"\"Single image SR is one of the most relevant inverse problems in the field of generative image processing tasks @cite_21 @cite_42 . Since the initial work by @cite_12 which applied small convolutional neural networks to the task of single image SR, several better neural network architectures have been proposed that have achieved a significantly higher PSNR across various datasets @cite_19 @cite_4 @cite_35 @cite_13 @cite_37 @cite_7 @cite_1 . Generally, advances in network architectures for image detection tasks have also helped in SR, e.g. adding residual connections @cite_28 enables the use of much deeper networks and speeds up training @cite_38 . We refer the reader to Agustsson and Timofte @cite_10 for a survey of the state of the art in single image SR.\"","":""}
{"id":"2942641522","dialogue":"\"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.\"","summary":"\"Since maximizing for PSNR leads to generally blurry images @cite_31 , another line of research has investigated alternative loss functions. @cite_16 and Alexey and Brox @cite_24 replace the mean squared error (MSE) in the image space with an MSE measurement in feature space of large pre-trained image recognition networks. @cite_27 extend this idea by adding an adversarial loss and @cite_31 combine perceptual, adversarial and texture synthesis loss terms to produce sharper images with hallucinated details. Although these methods produce detailed images, they typically contain small artifacts that are visible upon close inspection. While such artifacts are bearable in images, they lead to flickering in super-resolved videos. For this reason, applying these perceptual loss functions to the problem of video SR is more involved.\"","":""}
{"id":"2942641522","dialogue":"\"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.\"","summary":"\"Amongst classical video SR methods, @cite_20 have achieved notable image quality using Bayesian optimization methods, but the computational complexity of the approach prohibits use in real-time applications. Neural network based approaches include @cite_25 who use a bidirectional recurrent architecture with comparably shallow networks without explicit motion compensation. More recently, neural network based methods operate on a sliding window of input frames. The main idea of @cite_34 is to align and warp neighboring frames to the current frame before all images are fed into a SR network which combines details from all frames into a single image. Inspired by this idea, @cite_29 take a similar approach but employ a flow estimation network for the frame alignment. Similarly, @cite_14 use a sliding window approach but they combine the frame alignment and SR steps. @cite_30 also propose a method which operates on a stack of video frames. They estimate the motion in the frames and subsequently map them into high-resolution space before another SR network combines the information from all frames. @cite_41 operate on varying numbers of frames at the same time to generate different high-resolution images and then condense the results into a single image in a final step.\"","":""}
{"id":"2942641522","dialogue":"\"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.\"","summary":"\"For generative video processing methods, temporal consistency of the output is crucial. Since most recent methods operate on a sliding window @cite_29 @cite_30 @cite_41 @cite_14 , it is hard to optimize the networks to produce temporally consistent results as no information of the previously super-resolved frame is directly included in the next step. To accommodate for this, @cite_32 use a frame-recurrent approach where the estimated high-resolution frame of the previous step is fed into the network for the following step. This encourages more temporally consistent results, however the authors do not explicitly employ a loss term for the temporal consistency of the output.\"","":""}
{"id":"2942641522","dialogue":"\"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.\"","summary":"\"To the best of our knowledge, VSR methods have so far been restricted to MSE optimization methods and recent advancements in perceptual image quality in single image SR have not yet been successfully transferred to VSR. A possible explanation is that perceptual losses lead to sharper images which makes temporal inconsistencies significantly more evident in the results, leading to unpleasing flickering in the high-resolution videos @cite_31 .\"","":""}
{"id":"2942641522","dialogue":"\"With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.\"","summary":"\"The style transfer community has faced similar problems in their transition from single-image to video processing. Single-image style-transfer networks might produce very distant images for adjacent frames @cite_2 , creating very strong transitions from frame to frame. Several recent works have overcome this problem by including a temporal-consistency loss that ensures that the stylized consecutive frames are similar to each other when warped with the optical flow of the scene @cite_8 @cite_5 @cite_23 .\"","":""}
{"id":"2948558869","dialogue":"\"Adversarial training is one of the main defenses against adversarial attacks. In this paper","summary":"we provide the first rigorous study on diagnosing elements of adversarial training","":""}
{"id":"2948290717","dialogue":"\"Group convolution works well with many deep convolutional neural networks (CNNs) that can effectively compress the model by reducing the number of parameters and computational cost. Using this operation, feature maps of different group cannot communicate, which restricts their representation capability. To address this issue, in this work, we propose a novel operation named Hierarchical Group Convolution (HGC) for creating computationally efficient neural networks. Different from standard group convolution which blocks the inter-group information exchange and induces the severe performance degradation, HGC can hierarchically fuse the feature maps from each group and leverage the inter-group information effectively. Taking advantage of the proposed method, we introduce a family of compact networks called HGCNets. Compared to networks using standard group convolution, HGCNets have a huge improvement in accuracy at the same model size and complexity level. Extensive experimental results on the CIFAR dataset demonstrate that HGCNets obtain significant reduction of parameters and computational cost to achieve comparable performance over the prior CNN architectures designed for mobile devices such as MobileNet and ShuffleNet.\"","summary":"\"Most of works applied this approach improve the efficiency of CNNs via weight pruning @cite_7 @cite_6 @cite_0 and quantization @cite_13 . These approaches are effective because deep neural networks often have a substantial number of redundant weights that can be pruned or quantized without sacrificing much accuracy. For convolutional neural networks, different pruning techniques may lead to different levels of granularity. Fine-grained pruning, e.g., independent weight pruning @cite_7 , generally achieves a high degree of sparsity. However, it requires storing a large number of indices, and relies on special hardware software accelerators. In contrast, coarse-grained pruning methods such as filter-level pruning @cite_0 achieve a lower degree of sparsity, but the resulting networks are much more regular, which facilitates efficient implementations. These approaches are simple and intuitive, however, iterative optimization strategy is commonly utilized in these approaches, which slows down the training procedure.\"","":""}
{"id":"2948563793","dialogue":"\"Augmenting transformation knowledge onto a convolutional neural network's weights has often yielded significant improvements in performance. For rotational transformation augmentation, an important element to recent approaches has been the use of a steerable basis i.e. the circular harmonics. Here, we propose a scale-steerable filter basis for the locally scale-invariant CNN, denoted as log-radial harmonics. By replacing the kernels in the locally scale-invariant CNN lsi_cnn with scale-steered kernels, significant improvements in performance can be observed on the MNIST-Scale and FMNIST-Scale datasets. Training with a scale-steerable basis results in filters which show meaningful structure, and feature maps demonstrate which demonstrate visibly higher spatial-structure preservation of input. Furthermore, the proposed scale-steerable CNN shows on-par generalization to global affine transformation estimation methods such as Spatial Transformers, in response to test-time data distortions.\"","summary":"\"Scale-transformed weights were proposed in @cite_9 , where it was observed to improve performance over the normal baseline CNN, on MNIST-Scale. On the same dataset (with a 10k, 2k and 50k split), better performance was observed in @cite_3 , where in addition to forwarding the maximum filter response to a range of scales, the actual scale at which the response was obtained was also forwarded. In both works, weight scaling was only indirectly emulated, by rather scaling the input and the resizing back the convolution response to a fix size for max-pooling across scales.\"","":""}
{"id":"2884370868","dialogue":"\"Semantic segmentation architectures are mainly built upon an encoder-decoder structure. These models perform subsequent downsampling operations in the encoder. Since operations on high-resolution activation maps are computationally expensive, usually the decoder produces output segmentation maps by upsampling with parameters-free operators like bilinear or nearest-neighbor. We propose a Neural Network named Guided Upsampling Network which consists of a multiresolution architecture that jointly exploits high-resolution and large context information. Then we introduce a new module named Guided Upsampling Module (GUM) that enriches upsampling operators by introducing a learnable transformation for semantic maps. It can be plugged into any existing encoder-decoder architecture with little modifications and low additional computation cost. We show with quantitative and qualitative experiments how our network benefits from the use of GUM module. A comprehensive set of experiments on the publicly available Cityscapes dataset demonstrates that Guided Upsampling Network can efficiently process high-resolution images in real-time while attaining state-of-the art performances.\"","summary":"\"@cite_15 @cite_20 @cite_24 represent the pioneer works that employed CNNs for semantic segmentation. FCN @cite_22 laid the foundations for modern architectures where CNNs are employed in a fully-convolutional way. Authors used a pre-trained encoder together with a simple decoder module that takes advantage of skip-connections from lower layers to exploit high-resolution feature maps. They obtained a significant improvement both in terms of accuracy and efficiency. DeepLab @cite_27 made use of Dilated Convolutions @cite_19 to increase the receptive field of inner layers without increasing the overall number of parameters. After the introduction of Residual Networks (Resnets) @cite_17 most methods employed a very deep Resnet as encoder DeepLabv2 @cite_10 Resnet38 @cite_7 FRRN @cite_25 , pushing forward the performance boundary on semantic segmentation task. PSPNet @cite_3 and DeepLabv3 @cite_21 introduced context layers in order to expand the theoretical receptive field of inner layers. All these methods attain high accuracy on different benchmarks but at high computational costs.\"","":""}
{"id":"2884999974","dialogue":"\"Software systems are not static, they have to undergo frequent changes to stay fit for purpose, and in the process of doing so, their complexity increases. It has been observed that this process often leads to the erosion of the systems design and architecture and with it, the decline of many desirable quality attributes, such as maintainability. This process can be captured in terms of antipatterns-atomic violations of widely accepted design principles. We present a visualisation that exposes the design of evolving Java programs, highlighting instances of selected antipatterns including their emergence and cancerous growth. This visualisation assists software engineers and architects in assessing, tracing and therefore combating design erosion. We evaluated the effectiveness of the visualisation in four case studies with ten participants.\"","summary":"\"Empirical studies on larger corpora of real-world programs started in the early 2000s and revealed that surprisingly, antipatterns are prevalent @cite_7 . This was first discovered for circular dependencies @cite_18 , and later confirmed to apply to other antipatterns as well @cite_30 . Antipatterns can be detected by means of static analysis before a system is deployed. The main issue here is the use of dynamic programming language features that create dependencies that may not be visible when the static analysis models are built. This area is generally under-researched, and we must assume that the models used only under-approximate the behaviour of the actual program. In particular, dependency graphs may not contain all edges showing actual program dependencies.\"","":""}
{"id":"2884999974","dialogue":"\"Software systems are not static, they have to undergo frequent changes to stay fit for purpose, and in the process of doing so, their complexity increases. It has been observed that this process often leads to the erosion of the systems design and architecture and with it, the decline of many desirable quality attributes, such as maintainability. This process can be captured in terms of antipatterns-atomic violations of widely accepted design principles. We present a visualisation that exposes the design of evolving Java programs, highlighting instances of selected antipatterns including their emergence and cancerous growth. This visualisation assists software engineers and architects in assessing, tracing and therefore combating design erosion. We evaluated the effectiveness of the visualisation in four case studies with ten participants.\"","summary":"\"There exist many approaches to visualise software evolution @cite_16 . Most visualisations want to provide an improved understanding of the development activities by visualising structural changes, e.g. by using added and removed lines as metrics @cite_43 @cite_3 @cite_46 @cite_5 @cite_25 @cite_45 @cite_28 or by providing highly aggregated information @cite_13 @cite_32 @cite_4 . Our use case requires the visualisation of the structural evolution of the system and the antipattern instances at the same time. We are not aware of any evolution visualisation that supports this. There exist evolution visualisations of call graphs @cite_38 @cite_19 @cite_36 . However, they do not provide any structural information.\"","":""}
{"id":"2949567128","dialogue":"\"Survival analysis in the presence of multiple possible adverse events, i.e., competing risks, is a pervasive problem in many industries (healthcare, finance, etc.). Since only one event is typically observed, the incidence of an event of interest is often obscured by other related competing events. This nonidentifiability, or inability to estimate true cause-specific survival curves from empirical data, further complicates competing risk survival analysis. We introduce Siamese Survival Prognosis Network (SSPN), a novel deep learning architecture for estimating personalized risk scores in the presence of competing risks. SSPN circumvents the nonidentifiability problem by avoiding the estimation of cause-specific survival curves and instead determines pairwise concordant time-dependent risks, where longer event times are assigned lower risks. Furthermore, SSPN is able to directly optimize an approximation to the C-discrimination index, rather than relying on well-known metrics which are unable to capture the unique requirements of survival analysis with competing risks.\"","summary":"\"Previous work on classical survival analysis has demonstrated the advantages of deep learning over statistical methods @cite_26 @cite_1 @cite_8 . Cox proportional hazards model @cite_15 is the baseline statistical model for survival analysis, but is limited since the dependent risk function is the product of a linear covariate function and a time dependent function, which is insufficient for modeling complex non-linear medical data. @cite_1 replaced the linear covariate function with a feed-forward neural network as input for the Cox PH model and demonstrated improved performance. The current literature addresses competing risks based on statistical methods (the Fine Gray model @cite_21 ), classical machine learning (Random Survival Forest @cite_7 @cite_22 ), multi-task learning @cite_4 ) etc., with limited success. These existing competing risk models are challenged by computational scalability issues for datasets with many patients and multiple covariates. To address this challenge, we propose a deep learning architecture for survival analysis with competing risks to optimize the time-dependent discrimination index. This is not trivial and will be elaborated in the next section.\"","":""}
{"id":"2884491022","dialogue":"\"Deep learning has revolutionised many fields, but it is still challenging to transfer its success to small mobile robots with minimal hardware. Specifically, some work has been done to this effect in the RoboCup humanoid football domain, but results that are performant and efficient and still generally applicable outside of this domain are lacking. We propose an approach conceptually different from those taken previously. It is based on semantic segmentation and does achieve these desired properties. In detail, it is being able to process full VGA images in real-time on a low-power mobile processor. It can further handle multiple image dimensions without retraining, it does not require specific domain knowledge for achieving a high frame rate and it is applicable on a minimal mobile hardware.\"","summary":"\"Instead, the optimisations applied to our networks are very much motivated by MobileNets @cite_3 . Most notably, we utilise to significantly reduce the computational complexity of our segmentation networks. Such convolutions split a regular convolution into a filter and a combination step: first a separate 2D filter is applied to each input channel, after which a 1x1 convolution is applied to combine the results of these features. This can be seen as a factorisation of a full convolution that reduces the computational cost by a factor of @math , where @math is the number of output features and @math the kernel size. Not all convolutions can be factorised like this, so separable convolutions have less expressive power, but the results of the original MobileNets and those reported here show they can still perform at high accuracy.\"","":""}
{"id":"2951039794","dialogue":"\"Investigative journalism in recent years is confronted with two major challenges: 1) vast amounts of unstructured data originating from large text collections such as leaks or answers to Freedom of Information requests, and 2) multi-lingual data due to intensified global cooperation and communication in politics, business and civil society. Faced with these challenges, journalists are increasingly cooperating in international networks. To support such collaborations, we present the new version of new s leak 2.0, our open-source software for content-based searching of leaks. It includes three novel main features: 1) automatic language detection and language-dependent information extraction for 40 languages, 2) entity and keyword visualization for efficient exploration, and 3) decentral deployment for analysis of confidential data from various formats. We illustrate the new analysis capabilities with an exemplary case study.\"","summary":"\"@cite_10 is a more advanced open-source application developed by computer scientists in collaboration with journalists to support investigative journalism. The application supports import of PDF, MS Office and HTML documents, document clustering based on topic similarity, a simplistic location entity detection, full-text search, and document tagging. Since this tool is already mature and has successfully been used in a number of published news stories, we adapted some of its most useful features such as document tagging and a keyword-in-context (KWIC) view for search hits. Furthermore, in we concentrate on intuitive and visually pleasing approaches to display extracted contents for a fast exploration of collections.\"","":""}
{"id":"2951039794","dialogue":"\"Investigative journalism in recent years is confronted with two major challenges: 1) vast amounts of unstructured data originating from large text collections such as leaks or answers to Freedom of Information requests, and 2) multi-lingual data due to intensified global cooperation and communication in politics, business and civil society. Faced with these challenges, journalists are increasingly cooperating in international networks. To support such collaborations, we present the new version of new s leak 2.0, our open-source software for content-based searching of leaks. It includes three novel main features: 1) automatic language detection and language-dependent information extraction for 40 languages, 2) entity and keyword visualization for efficient exploration, and 3) decentral deployment for analysis of confidential data from various formats. We illustrate the new analysis capabilities with an exemplary case study.\"","summary":"The @cite_6 @cite_11 @cite_0 system is a third tool that supports investigative analysis of textual documents. Jigsaw focuses on the extraction of entities (using multiple NER tools for English) and their correlation with metadata. It provides visualization of entities as lists and document contents as a (tree-structured) word graph. instead visualizes coherence of such information as network graphs.","":""}
{"id":"2948751033","dialogue":"\"In this paper, we propose a new CNN model DiCENet, that is built using: (1) dimension-wise convolutions and (2) efficient channel fusion. The introduced blocks maximize the use of information in the input tensor by learning representations across all dimensions while simultaneously reducing the complexity of the network and achieving high accuracy. Our model shows significant improvements over state-of-the-art models across various visual recognition tasks, including image classification, object detection, and semantic segmentation. Our model delivers either the same or better performance than existing models with fewer FLOPs, including task-specific models. Notably, DiCENet delivers competitive performance to neural architecture search-based methods at fewer FLOPs (70-100 MFLOPs). On the MS-COCO object detection, DiCENet is 4.5 more accurate and has 5.6 times fewer FLOPs than YOLOv2. On the PASCAL VOC 2012 semantic segmentation dataset, DiCENet is 4.3 more accurate and has 3.2 times fewer FLOPs than a recent efficient semantic segmentation network, ESPNet. Our source code is available at this https URL\"","summary":"\"Recent successes in visual recognition tasks, including object classification, detection, and segmentation, can be attributed to exploration of different CNN designs @cite_5 @cite_30 @cite_13 @cite_74 @cite_10 @cite_25 . To make these network designs more efficient, they have been extended with efficient and sparse forms of convolutions, such as depth-wise and group convolutions @cite_28 @cite_7 @cite_48 @cite_12 @cite_36 @cite_56 . In this paper, we introduce dimension-wise convolutions that generalize depth-wise convolutions to all dimensions of the input tensor.\"","":""}
{"id":"2948751033","dialogue":"\"In this paper, we propose a new CNN model DiCENet, that is built using: (1) dimension-wise convolutions and (2) efficient channel fusion. The introduced blocks maximize the use of information in the input tensor by learning representations across all dimensions while simultaneously reducing the complexity of the network and achieving high accuracy. Our model shows significant improvements over state-of-the-art models across various visual recognition tasks, including image classification, object detection, and semantic segmentation. Our model delivers either the same or better performance than existing models with fewer FLOPs, including task-specific models. Notably, DiCENet delivers competitive performance to neural architecture search-based methods at fewer FLOPs (70-100 MFLOPs). On the MS-COCO object detection, DiCENet is 4.5 more accurate and has 5.6 times fewer FLOPs than YOLOv2. On the PASCAL VOC 2012 semantic segmentation dataset, DiCENet is 4.3 more accurate and has 3.2 times fewer FLOPs than a recent efficient semantic segmentation network, ESPNet. Our source code is available at this https URL\"","summary":"\"Recently, neural search methods, including reinforcement learning and genetic algorithms, have been proposed to automatically construct network architectures @cite_53 @cite_38 @cite_16 @cite_6 @cite_24 @cite_29 . These methods search over a huge network space (e.g. MNASNet @cite_24 searches over 8K different design choices) using a dictionary of pre-defined search space parameters, including different types of convolutional layers and kernel sizes, to identify a network structure, usually non-homogeneous, that satisfies optimization constraints, such as inference time. Recent search-based methods @cite_24 @cite_14 @cite_58 use MobileNetv2 @cite_36 as a basic search block for automatic network design. Since the proposed unit delivers better performance than MobileNetv2 (see Section ), we believe that neural architecture search with our proposed unit would enable finding a better network design.\"","":""}
{"id":"2948751033","dialogue":"\"In this paper, we propose a new CNN model DiCENet, that is built using: (1) dimension-wise convolutions and (2) efficient channel fusion. The introduced blocks maximize the use of information in the input tensor by learning representations across all dimensions while simultaneously reducing the complexity of the network and achieving high accuracy. Our model shows significant improvements over state-of-the-art models across various visual recognition tasks, including image classification, object detection, and semantic segmentation. Our model delivers either the same or better performance than existing models with fewer FLOPs, including task-specific models. Notably, DiCENet delivers competitive performance to neural architecture search-based methods at fewer FLOPs (70-100 MFLOPs). On the MS-COCO object detection, DiCENet is 4.5 more accurate and has 5.6 times fewer FLOPs than YOLOv2. On the PASCAL VOC 2012 semantic segmentation dataset, DiCENet is 4.3 more accurate and has 3.2 times fewer FLOPs than a recent efficient semantic segmentation network, ESPNet. Our source code is available at this https URL\"","summary":"\"These approaches include network quantization @cite_62 @cite_40 @cite_43 @cite_17 @cite_60 @cite_54 @cite_64 , compression @cite_72 @cite_11 @cite_18 @cite_39 @cite_26 @cite_3 , and distillation @cite_8 @cite_9 @cite_70 @cite_31 @cite_61 . Network quantization-based approaches approximate convolution operations with fewer bits instead of using 32-bit full precision floating points. This improves speed at inference and also, reduces memory required for storing network weights. Network compression-based approaches improve the efficiency of a network by removing redundant weights and connections. Unlike network quantization and compression, distillation-based approaches improve the accuracy of (usually shallow) networks by supervising the training with large pre-trained networks.\"","":""}
{"id":"2951370248","dialogue":"\"This work provides a simple approach to discover tight object bounding boxes with only image-level supervision, called Tight box mining with Surrounding Segmentation Context (TS2C). We observe that object candidates mined through current multiple instance learning methods are usually trapped to discriminative object parts, rather than the entire object. TS2C leverages surrounding segmentation context derived from weakly-supervised segmentation to suppress such low-quality distracting candidates and boost the high-quality ones. Specifically, TS2C is developed based on two key properties of desirable bounding boxes: 1) high purity, meaning most pixels in the box are with high object response, and 2) high completeness, meaning the box covers high object response pixels comprehensively. With such novel and computable criteria, more tight candidates can be discovered for learning a better object detector. With TS2C, we obtain 48.0 and 44.4 mAP scores on VOC 2007 and 2012 benchmarks, which are the new state-of-the-arts.\"","summary":"\"Multiple Instance Learning (MIL) provides a suitable way for formulating and solving WSOD. In specific, if an image is annotated with a specific class, at least one proposal instance from the image is positive for this class; and no proposal instance is positive for unlabeled classes. Previous works on applying MIL to WSOD can be roughly categorized into two-step @cite_17 @cite_13 @cite_42 @cite_41 and end-to-end @cite_33 @cite_38 @cite_40 @cite_29 @cite_25 @cite_28 based approaches.\"","":""}
{"id":"2951370248","dialogue":"\"This work provides a simple approach to discover tight object bounding boxes with only image-level supervision, called Tight box mining with Surrounding Segmentation Context (TS2C). We observe that object candidates mined through current multiple instance learning methods are usually trapped to discriminative object parts, rather than the entire object. TS2C leverages surrounding segmentation context derived from weakly-supervised segmentation to suppress such low-quality distracting candidates and boost the high-quality ones. Specifically, TS2C is developed based on two key properties of desirable bounding boxes: 1) high purity, meaning most pixels in the box are with high object response, and 2) high completeness, meaning the box covers high object response pixels comprehensively. With such novel and computable criteria, more tight candidates can be discovered for learning a better object detector. With TS2C, we obtain 48.0 and 44.4 mAP scores on VOC 2007 and 2012 benchmarks, which are the new state-of-the-arts.\"","summary":"\"first extract proposal representation leveraging hand-crafted features or pre-trained CNN models and employ MIL to select the best object candidate for learning the object detector. For instance, Wang al @cite_17 presented a latent semantic clustering approach to select the most discriminative cluster for each category. Cibis al @cite_13 learned a multi-fold MIL detector by re-labeling proposals and re-training the object classifier iteratively. Li al @cite_42 first trained a multi-label classification network on entire images and then selected class-specific proposal candidates using a mask-out strategy, followed by MIL for learning a Fast R-CNN detector. Recently, Jie al @cite_41 took a similar strategy as Li al @cite_42 and proposed a more robust self-taught approach to learn a detector by harvesting more accurate supportive proposals in an online manner. However, splitting the WSOD into two steps results in a non-convex optimization problem, making such approaches trapped in local optima.\"","":""}
{"id":"2951370248","dialogue":"\"This work provides a simple approach to discover tight object bounding boxes with only image-level supervision, called Tight box mining with Surrounding Segmentation Context (TS2C). We observe that object candidates mined through current multiple instance learning methods are usually trapped to discriminative object parts, rather than the entire object. TS2C leverages surrounding segmentation context derived from weakly-supervised segmentation to suppress such low-quality distracting candidates and boost the high-quality ones. Specifically, TS2C is developed based on two key properties of desirable bounding boxes: 1) high purity, meaning most pixels in the box are with high object response, and 2) high completeness, meaning the box covers high object response pixels comprehensively. With such novel and computable criteria, more tight candidates can be discovered for learning a better object detector. With TS2C, we obtain 48.0 and 44.4 mAP scores on VOC 2007 and 2012 benchmarks, which are the new state-of-the-arts.\"","summary":"\"Beyond the above mentioned related works, some fully-supervised object detection approaches @cite_2 @cite_12 @cite_37 @cite_44 also exploit contextual information of bounding boxes for benefiting object detection. Both Chen al @cite_12 and Li al @cite_44 leveraged information of enlarged contextual proposals to enhance the accuracy of the classifier. Zhu al @cite_37 proposed to use a pool of segments obtained in the bottom-up manner to obtain better detection boxes. Our TS 2 C is totally different from these works in terms of both motivation and methodology. In particular, our motivation is to employ surrounding segmentation context to suppress these false positive objects parts. In addition, our approach can be easily embedded into any WSOD framework to make a further performance improvement.\"","":""}
{"id":"2879658391","dialogue":"\"Vector-Quantized Variational Autoencoders (VQ-VAE)[1] provide an unsupervised model for learning discrete representations by combining vector quantization and autoencoders. In this paper, we study the use of VQ-VAE for representation learning for downstream tasks, such as image retrieval. We first describe the VQ-VAE in the context of an information-theoretic framework. We show that the regularization term on the learned representation is determined by the size of the embedded codebook before the training and it affects the generalization ability of the model. As a result, we introduce a hyperparameter to balance the strength of the vector quantizer and the reconstruction error. By tuning the hyperparameter, the embedded bottleneck quantizer is used as a regularizer that forces the output of the encoder to share a constrained coding space such that learned latent features preserve the similarity relations of the data space. In addition, we provide a search range for finding the best hyperparameter. Finally, we incorporate the product quantization into the bottleneck stage of VQ-VAE and propose an end-to-end unsupervised learning model for the image retrieval task. The product quantizer has the advantage of generating large-size codebooks. Fast retrieval can be achieved by using the lookup tables that store the distance between any pair of sub-codewords. State-of-the-art retrieval results are achieved by the learned codebooks.\"","summary":"Several works have studied the end-to-end discrete representation learning model with different incorporated structures in the bottleneck stages. @cite_23 and @cite_17 introduce scalar quantization in the latent space and optimize jointly the entire model for rate-distortion performance over a database of training images. @cite_5 proposes a compression model by performing vector quantization on the network activations. The model uses a continuous relaxation of vector quantization which is annealed over time to obtain a hard clustering. @cite_18 and @cite_7 introduce the Gumbel-Softmax gradient estimator for non-differentiable discrete distributions. The Gumbel-Softmax estimator determines the gradient of discrete distributions by sampling from a differentiable Gumbel-Softmax distribution which can be smoothly annealed into a categorical distribution.","":""}
{"id":"2949166717","dialogue":"\"The automatic analysis of ultrasound sequences can substantially improve the efficiency of clinical diagnosis. In this work we present our attempt to automate the challenging task of measuring the vascular diameter of the fet al abdominal aorta from ultrasound images. We propose a neural network architecture consisting of three blocks: a convolutional layer for the extraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for enforcing the temporal coherence across video frames and exploiting the temporal redundancy of a signal, and a regularized loss function, called , to impose our prior knowledge about the periodicity of the observed signal. We present experimental evidence suggesting that the proposed architecture can reach an accuracy substantially superior to previously proposed methods, providing an average reduction of the mean squared error from @math (state-of-art) to @math , and a relative error reduction from @math to @math . The mean execution speed of the proposed approach of 289 frames per second makes it suitable for real time clinical use.\"","summary":"\"The interest for measuring the diameter and intima-media thickness (IMT) of major vessels has stemmed from its importance as biomarker of hypertension damage and atherosclerosis in adults. Typically, the IMT is assessed on the carotid artery by identifying its lumen and the different layers of its wall on high resolution US images. The improvements provided by the design of semi-automatic and automatic methods based mainly on the image intensity profile, distribution and gradients analysis, and more recently on active contours. For a comprehensive review of these classical methods we refer the reader to @cite_11 and @cite_14 . In the prenatal setting, the lower image quality, due to the need of imaging deeper in the mother's womb and by the movement of the fetus, makes the measurement of a IMT biomarker, although measured on the abdominal aorta, challenging.\"","":""}
{"id":"2949166717","dialogue":"\"The automatic analysis of ultrasound sequences can substantially improve the efficiency of clinical diagnosis. In this work we present our attempt to automate the challenging task of measuring the vascular diameter of the fet al abdominal aorta from ultrasound images. We propose a neural network architecture consisting of three blocks: a convolutional layer for the extraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for enforcing the temporal coherence across video frames and exploiting the temporal redundancy of a signal, and a regularized loss function, called , to impose our prior knowledge about the periodicity of the observed signal. We present experimental evidence suggesting that the proposed architecture can reach an accuracy substantially superior to previously proposed methods, providing an average reduction of the mean squared error from @math (state-of-art) to @math , and a relative error reduction from @math to @math . The mean execution speed of the proposed approach of 289 frames per second makes it suitable for real time clinical use.\"","summary":"\"However, the exploitation of temporal redundancy on US sequences was shown to be a solution for improving overall detection results of the fet al heart @cite_3 , where the use of a CNN coupled with a recurrent neural network (RNN) is strategic. Other works, propose similar approach in order to detect the presence of standard planes from prenatal US data using CNN with Long-Short Term Memory (LSTM) @cite_17 .\"","":""}
{"id":"2810239506","dialogue":"\"Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.\"","summary":"\"Face editing or manipulation has been widely studied in the field of computer vision and graphics, including face morphing @cite_25 , expression edits @cite_35 @cite_43 , age progression @cite_3 , facial reenactment @cite_39 @cite_4 @cite_30 . However, these models are designed for a particular task, thus rely heavily on domain knowledge and certain assumptions. For example, @cite_30 assumes neutral and frontal faces to begin with while @cite_4 assumes the availability of target videos with variation in both poses and expressions. Our model differs from them as it is a data-driven approach that does not require domain knowledge, designed to handle general face manipulations.\"","":""}
{"id":"2810239506","dialogue":"\"Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.\"","summary":"\"Our work can be categorized into image translation with generative adversarial networks @cite_28 @cite_41 @cite_31 @cite_49 @cite_46 @cite_44 @cite_27 , whose goal is to learn a mapping @math that induces an indistinguishable distribution to target domain @math , through adversarial training between a pair of generator @math and discriminator @math . For example, @cite_28 takes image as a condition for general image-to-image translation trained on paired samples. Later, Zhu et.al @cite_42 extends @cite_28 by introducing cycle-consistency loss to obviate the need of matched training pairs. In addition, it alleviates many-to-one mapping during training generative adversarial networks also known as mode collapse. Inspired by this, we integrate this loss into our model for identity preservation between different domains.\"","":""}
{"id":"2810239506","dialogue":"\"Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.\"","summary":"\"Another seminal work that inspired our design is StarGAN @cite_21 , where target facial attributes are encoded into a one-hot vector. In StarGAN, each attribute is treated as a different domain and an auxiliary classifier used to distinguish these attributes is essential for supervising the training process. Different from StarGAN, our goal is to perform continuous edits in the pixel space that cannot be enumerated with discrete labels. This implicitly implies a smooth and continuous latent space where each point in this space encodes meaningful axis of variation in the data. We treat different style modalities as domains in this paper and use two words interchangeably. In this sense, applications like beautification de-beautification, aging younger, with beard without beard can also be included into our general framework. We compare our approach against CycleGAN @cite_42 and StarGAN @cite_21 in and illustrate in more details about our design in .\"","":""}
{"id":"2810239506","dialogue":"\"Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.\"","summary":"\"We are aware of works that use pose as condition in the task of person re-identification for person image generation @cite_19 @cite_11 @cite_12 @cite_18 . For example @cite_5 concatenates one-hot pose feature maps in a channel-wise fashion to control pose generation similar to @cite_26 , where keypoints and segmentation mask of birds are used to manipulate locations and poses of birds. To synthesize more plausible human poses, Siarohin et.al @cite_12 develop deformable skip connections and compute a set of affine transformations to approximate joint deformations. These works share some similarity with ours as both facial landmark and human skeleton can be seen as a form of pose representation. However, all those works deal with manipulation in the original domain and does not preserve identity. Moreover, generated results in those works are low-resolution whereas our model can successfully generate 512x512 resolution with photo-realistic quality.\"","":""}
{"id":"2810239506","dialogue":"\"Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.\"","summary":"\"Neural style transfer was first proposed by @cite_38 . The idea is to preserve content from the original image and mimic style'' from the reference image. We adopt Gram matrix in our model to enforce texture consistency and replace L-BFGS iteration with back propagation for end-to-end training. Also, considering the lack of groundtruth data of many face manipulation tasks, we apply a fast neural style transfer algorithm @cite_22 to generate pseudo targets for multi-modality manipulations. Note that our model is easily extensible to any desired target domains with current design unchanged.\"","":""}
{"id":"2811328330","dialogue":"\"Semantic segmentation is a challenging vision problem that usually necessitates the collection of large amounts of finely annotated data, which is often quite expensive to obtain. Coarsely annotated data provides an interesting alternative as it is usually substantially more cheap. In this work, we present a method to leverage coarsely annotated data along with fine supervision to produce better segmentation results than would be obtained when training using only the fine data. We validate our approach by simulating a scarce data setting with less than 200 low resolution images from the Cityscapes dataset and show that our method substantially outperforms solely training on the fine annotation data by an average of 15.52 mIoU and outperforms the coarse mask by an average of 5.28 mIoU.\"","summary":"\"Providing low level information (such as the coarse masks in our method) in the form of an embedding layer is explored in previous works @cite_1 @cite_12 . In @cite_1 , the noisy labels are jointly embedded with the visual features extracted from an Inception-V3 @cite_13 ConvNet. We explore an analogous approach, which concatenates the coarse masks with the convolution blocks at different network locations in the scarce data semantic segmentation domain.\"","":""}
{"id":"2964169444","dialogue":"\"Recent work linking deep neural networks and dynamical systems opened up new avenues to analyze deep learning. In particular, it is observed that new insights can be obtained by recasting deep learning as an optimal control problem on difference or differential equations. However, the mathematical aspects of such a formulation have not been systematically explored. This paper introduces the mathematical formulation of the population risk minimization problem in deep learning as a mean-field optimal control problem. Mirroring the development of classical optimal control, we state and prove optimality conditions of both the Hamilton–Jacobi–Bellman type and the Pontryagin type. These mean-field results reflect the probabilistic nature of the learning problem. In addition, by appealing to the mean-field Pontryagin’s maximum principle, we establish some quantitative relationships between population and empirical learning problems. This serves to establish a mathematical foundation for investigating the algorithmic and theoretical connections between optimal control and deep learning.\"","summary":"\"The connection between back-propagation and optimal control of dynamical systems is known since the earlier works on control and deep learning @cite_16 @cite_34 @cite_0 . Recently, the dynamical systems approach to deep learning was proposed in @cite_25 and explored in the direction of training algorithms based on the PMP and the method of successive approximations @cite_30 @cite_21 . In another vein, there are also studies on the continuum limit of neural networks @cite_39 @cite_54 and on designing network architectures for deep learning @cite_11 @cite_10 @cite_31 @cite_53 based on dynamical systems and differential equations. Instead of analysis of algorithms or architectures, the present paper focuses on the mathematical aspects of the control formulation itself, and develops a mean-field theory that characterize the optimality conditions and value functions using both PDE (HJB) and ODE (PMP) approaches. The over-arching goal is to develop the mathematical foundations of the optimal control formulation of deep learning.\"","":""}
{"id":"2952145882","dialogue":"\"Crowd counting from unconstrained scene images is a crucial task in many real-world applications like urban surveillance and management, but it is greatly challenged by the camera's perspective that causes huge appearance variations in people's scales and rotations. Conventional methods address such challenges by resorting to fixed multi-scale architectures that are often unable to cover the largely varied scales while ignoring the rotation variations. In this paper, we propose a unified neural network framework, named Deep Recurrent Spatial-Aware Network, which adaptively addresses the two issues in a learnable spatial transform module with a region-wise refinement process. Specifically, our framework incorporates a Recurrent Spatial-Aware Refinement (RSAR) module iteratively conducting two components: i) a Spatial Transformer Network that dynamically locates an attentional region from the crowd density map and transforms it to the suitable scale and rotation for optimal crowd estimation; ii) a Local Refinement Network that refines the density map of the attended region with residual learning. Extensive experiments on four challenging benchmarks show the effectiveness of our approach. Specifically, comparing with the existing best-performing methods, we achieve an improvement of 12 on the largest dataset WorldExpo'10 and 22.8 on the most challenging dataset UCF_CC_50.\"","summary":"\"Spatial transformer Network (STN) @cite_24 is a sub-differentiable sampling-based module, which is designed to spatially transform its input map to an output map that corresponds to a subregion of the input map and can be hence regarded as an effective region selection mechanism. It is convenient to incorporate a spatial transformer layer to the convolutional neural network and train it with the standard back-propagation algorithm. A parameter matrix is used to determine the location of the subregion, as well as its resize scale and the rotation angle. Recently, the spatial transformer has been applied to various computer vision tasks, e.g., multi-label image recognition @cite_15 and saliency detection @cite_17 . To the best of our knowledge, our work is the first in successfully using multiple iterations of STN within a LSTM framework for crowd counting.\"","":""}
{"id":"2913126208","dialogue":"\"Transient gradual typing imposes run-time type tests that typically cause a linear slowdown in programs' performance. This performance impact discourages the use of type annotations because adding types to a program makes the program slower. A virtual machine can employ standard just-in-time optimizations to reduce the overhead of transient checks to near zero. These optimizations can give gradually-typed languages performance comparable to state-of-the-art dynamic languages, so programmers can add types to their code without affecting their programs' performance.\"","summary":"\"Although syntaxes for type annotations in dynamic languages go back at least as far as Lisp, the first attempts at adding a comprehensive static type system to a dynamically typed language involved Smalltalk, with the first practical system being Bracha's Strongtalk. Strongtalk (independently replicated for Ruby) provided a powerful and flexible static type system, where crucially, the system was (also known as pluggable @cite_15 ). Programmers could run the static checker over their Smalltalk code (or not); either way the type annotations had no impact whatsoever of the semantics of the underlying Smalltalk program.\"","":""}
{"id":"2913126208","dialogue":"\"Transient gradual typing imposes run-time type tests that typically cause a linear slowdown in programs' performance. This performance impact discourages the use of type annotations because adding types to a program makes the program slower. A virtual machine can employ standard just-in-time optimizations to reduce the overhead of transient checks to near zero. These optimizations can give gradually-typed languages performance comparable to state-of-the-art dynamic languages, so programmers can add types to their code without affecting their programs' performance.\"","summary":"\"The diversity of semantics and language designs incorporating gradual typing has been captured recently via surveys incorporating formal models of different design options. Chung et al. present an object-oriented model covering optional semantics (erasure), transient semantics, concrete semantics (from Thorn @cite_11 ), and behavioural semantics (from Typed Racket), and give a series of programs to clarify the semantics of a particular language. Greenman et al. take a more functional approach, again modelling erasure, transient ( first order''), and behavioural ( higher order'') semantics @cite_23 , and also present performance information based on Typed Racket. Wilson et al. take a rather different approach, employing questionnaires to investigate the semantics programmers expect of a gradual typing system @cite_25 .\"","":""}
{"id":"2913126208","dialogue":"\"Transient gradual typing imposes run-time type tests that typically cause a linear slowdown in programs' performance. This performance impact discourages the use of type annotations because adding types to a program makes the program slower. A virtual machine can employ standard just-in-time optimizations to reduce the overhead of transient checks to near zero. These optimizations can give gradually-typed languages performance comparable to state-of-the-art dynamic languages, so programmers can add types to their code without affecting their programs' performance.\"","summary":"\"Most recently, Kuhlenschmidt @cite_18 employ an ahead of time ( traditional, static) compiler for a custom language called Grift and demonstrate good performance for code where more than half of the program is annotated with types, and reasonable performance for code without type annotations.\"","":""}
{"id":"2913126208","dialogue":"\"Transient gradual typing imposes run-time type tests that typically cause a linear slowdown in programs' performance. This performance impact discourages the use of type annotations because adding types to a program makes the program slower. A virtual machine can employ standard just-in-time optimizations to reduce the overhead of transient checks to near zero. These optimizations can give gradually-typed languages performance comparable to state-of-the-art dynamic languages, so programmers can add types to their code without affecting their programs' performance.\"","summary":"\"Perhaps the closest to our approach are Vitousek @cite_13 (incl. ) and Richards @cite_1 . Vitousek describe dynamically checking transient types for Reticulated Python (termed tag-type'' soundness by Greenman and Migeed @cite_3 ). As with our work, Vitousek 's transient checks inspect only the top-level'' type of an object. Reticulated Python undertakes these transient type checks at different places to Moth. Moth explicitly checks type anotations, while Reticulated Python implicitly checks whenever values flow from dynamic to static types. We refrain from a direct performance comparison since Reticulated Python is an interpreter without just-in-time compilation and thus performance tradeoffs are different.\"","":""}
{"id":"2913126208","dialogue":"\"Transient gradual typing imposes run-time type tests that typically cause a linear slowdown in programs' performance. This performance impact discourages the use of type annotations because adding types to a program makes the program slower. A virtual machine can employ standard just-in-time optimizations to reduce the overhead of transient checks to near zero. These optimizations can give gradually-typed languages performance comparable to state-of-the-art dynamic languages, so programmers can add types to their code without affecting their programs' performance.\"","summary":"\"Richards @cite_1 take a similar implementation approach to our work, demonstrating that key mechanisms such as object shapes used by a VM to optimize dynamic languages can be used to eliminate most of the overhead of dynamic type checks. Unlike our work, Richards implement monotonic'' gradual typing with blame, rather than the simpler transient checks, and do so on top of an adapted Higgs VM. The Higgs VM implements a baseline just-in-time compiler based on basic-block versioning. In contrast, our implementation of dynamic checks is built on top of the Truffle framework for the Graal VM, and reaches performance approaching that of V8 (cf. sec:baseline-perf ). The performance difference is of relevance here since any small constant factors introduced into a VM with a lower baseline performance can remain hidden, while they stand out more prominently on a faster baseline.\"","":""}
{"id":"2913126208","dialogue":"\"Transient gradual typing imposes run-time type tests that typically cause a linear slowdown in programs' performance. This performance impact discourages the use of type annotations because adding types to a program makes the program slower. A virtual machine can employ standard just-in-time optimizations to reduce the overhead of transient checks to near zero. These optimizations can give gradually-typed languages performance comparable to state-of-the-art dynamic languages, so programmers can add types to their code without affecting their programs' performance.\"","summary":"\"Overall, it is unclear whether our results confirm the ones reported by Richards @cite_1 , because our system is simpler. It does not introduce the polymorphism issues caused by accumulating cast information on object shapes, which could be important for performance. Considering that Richards report ca. 4 the classic Richards benchmark, while we see , further work seems necessary to understand the performance implications of their approach for a highly optimizing just-in-time compiler.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"Consequently, numerous works focus on providing SFC in SDNs. An SFC taxonomy that considers architecture and performance dimensions as the basis for the subsequent state-of-the-art analysis is introduced in @cite_1 .\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"Recently, the authors in @cite_9 presented a scheduling and routing solution in SDN NFV time-triggered flows. In detail, they approximate the optimal solution over a corresponding static scheduling problem and solve it using ILP. As in our approach, hard constraints on the overall execution times are considered by @cite_9 . However, we point out that, unlike our approach: (i) the focus of @cite_9 is on the traffic routing and scheduling between SDN-enabled switches per time-flow, so that the resulting flow scheduler does not support, by design, failure and fault tolerance per link and switch of data time-flow; (ii) the joint flow and computing rate mapping afforded in @cite_9 is, by design, static; (iii) the scheduler in @cite_9 does not perform real-time reconfiguration rerouting, real-time traffic hosted by the serving controller; (iv) the work in @cite_9 does not consider SFC and rerouting; and (v) the scheduler in @cite_9 does not enforce per-flow QoS guarantees on the limited time minimum energy and or the minimum side-effect. Although the aforementioned solutions are interesting, however, none of them considers the problem of service chaining with respect to the energy consumption of the VMs.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"The available literature ranges from the joint problem of fault-aware distributing and routing the traffic flows content in SDNs NFVs infrastructure @cite_22 @cite_10 to the problem of fault detection and recovery solutions in SDNs NFVs @cite_14 @cite_19 . In detail, in @cite_10 the authors analyze the fault tolerance over SDN. They present a discussion about fault tolerance and failure happening in the OpenFlow (OF) protocol that is applied in SDNs. Specifically, they propose a link node failure detection and failure recovery method in the data plane that can be controlled through the controller. However, they do not present any discussion about the application plane side-effect and do not cover the SFC fault-awareness.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"In @cite_36 , they present a controller-based fault recovery solution that covers path-failure detection and preconfigured backup paths. However, we point out that, unlike our approach: (i) the focus of @cite_36 is on presenting the network configuration in order to manage the traffic flows, which is not an effective solution, by design, in real scenarios; and (ii) the presented fault prevention method in @cite_36 does not support the SFC over the SDN-enabled switches. The authors in @cite_0 propose a solution to quickly detect link failures in order to increase the fault tolerance by combining the flow retrieval which is achieved through analyzing the protection switching times and using a fast protection method. Interestingly, this paper supports the fault minimization over the links and addresses the end-to-end fault tolerance method per flow, but not radically. Overall, the contribution does not afford, by design, jointly the QoSs in the node and link of SDN and does not support the SFC fault minimization, both of which are adopted in this paper.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"Besides, authors in @cite_31 present NFV-FD, a fault-tolerant unreliable failure detector that is adapted based on information (it includes communication links states and the flow characteristics) obtained from an SDN. The paper presents flavor of novelties, but it fails to address the SFC traffic flows. Moreover, our solution utilizes a network equipment fault-aware technique that spreads out the fault tolerance process all over the components running in the SDN. In @cite_21 , authors applied novel rule-based programming language presented in @cite_18 to talk between the controller and the data plane to manage the adopted in-network fast-fail over mechanisms of incoming traffic flows in FatTire programs. Although this method is an interesting step toward to the fault-aware SDN traffic flow policy management, it suffers from fault recovery and fault prevention that matter in our solution.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"Numerous works address the switch energy efficiency and energy-aware routing strategies in SDNs NFVs @cite_20 @cite_34 @cite_23 @cite_27 @cite_16 . In detail, the authors in @cite_20 present a network-wide energy-aware routing method using OF maximizing aggregate network utilization and optimized load balancing in SDN. Their practical solution has problem with scalability and does not even support the FRFP SFC aspects that this paper also targets.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"Focusing on Fog computing appliances in SDNs NFVs, there are limited works that target the routing in fog-supported SDNs NFVs, such as @cite_24 @cite_37 @cite_6 @cite_12 . In particular, the authors in @cite_24 address Fog computing over SDNs structures that preserve safety and non-safety services and are validated across two use cases: Data streaming and lane-change assistance services. The authors do not present any discussion about the SFC fault probability minimization, or failure recovery prevention. In another work, in @cite_37 the authors push the Fog Node to remain in edge to manage the on-demand location-based applications flows received from mobile users engaged in SDNs NFVs and analyze the possible routing in such network. Unlike our method, it suffers from a lack of service chain management, fault-awareness, failure prevention and SDN-enabled switch energy minimization.\"","":""}
{"id":"2964012674","dialogue":"\"Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.\"","summary":"\"Moreover, recently in another work @cite_17 , the authors address the resource allocation and total energy minimization over the Fog Nodes by proposing a novel QoS-aware distributed and scalable scheduler. Although based on the authors’ claim that it can be applied in real-time services, it fails to address the chain of services when it faces fault and failure in such a dynamic network. Interestingly, our architecture, FRFP, can cover all the benefits of this method by covering all the limitations addressed. The most recent method similar to our current work is our previous work @cite_12 on SFC management in SDNs NFVs. We present energy-aware resource reallocation SFC algorithms for SDNs. We allocate VNF to a set of flows and find several optimal and near-optimal solutions to optimize such network. Compared to our contribution, the paper @cite_12 has several limitations: (i) the presented routing algorithms do not exploit the capability of routing all flows simultaneously, i.e., it is impossible to reroute a flow considering the possible routes of other flows; and, (ii), we did not adopt the fog nodes to support fault probability minimization and failure recovery prevention.\"","":""}
{"id":"2810856661","dialogue":"\"Parsing Expression Grammars (PEGs) are a formalism used to describe top-down parsers with backtracking. As PEGs do not provide a good error recovery mechanism, PEG-based parsers usually do not recover from syntax errors in the input, or recover from syntax errors using ad-hoc, implementation-specific features. The lack of proper error recovery makes PEG parsers unsuitable for using with Integrated Development Environments (IDEs), which need to build syntactic trees even for incomplete, syntactically invalid programs. We propose a conservative extension, based on PEGs with labeled failures, that adds a syntax error recovery mechanism for PEGs. This extension associates recovery expressions to labels, where a label now not only reports a syntax error but also uses this recovery expression to reach a synchronization point in the input and resume parsing. We give an operational semantics of PEGs with this recovery mechanism, and use an implementation based on such semantics to build a robust parser for the Lua language. We evaluate the effectiveness of this parser, alone and in comparison with a Lua parser with automatic error recovery generated by ANTLR, a popular parser generator.\"","summary":"\"In this section, we discuss other error recovery approaches used by top-down parsers with backtracking, focusing on PEGs. Error handling in top-down parsing based on CFGs is a well-studied subject. Grune and Jacobs @cite_14 presents an overview of several error handling techniques used in this context.\"","":""}
{"id":"2810856661","dialogue":"\"Parsing Expression Grammars (PEGs) are a formalism used to describe top-down parsers with backtracking. As PEGs do not provide a good error recovery mechanism, PEG-based parsers usually do not recover from syntax errors in the input, or recover from syntax errors using ad-hoc, implementation-specific features. The lack of proper error recovery makes PEG parsers unsuitable for using with Integrated Development Environments (IDEs), which need to build syntactic trees even for incomplete, syntactically invalid programs. We propose a conservative extension, based on PEGs with labeled failures, that adds a syntax error recovery mechanism for PEGs. This extension associates recovery expressions to labels, where a label now not only reports a syntax error but also uses this recovery expression to reach a synchronization point in the input and resume parsing. We give an operational semantics of PEGs with this recovery mechanism, and use an implementation based on such semantics to build a robust parser for the Lua language. We evaluate the effectiveness of this parser, alone and in comparison with a Lua parser with automatic error recovery generated by ANTLR, a popular parser generator.\"","summary":"\"Swierstra and Duponcheel @cite_7 shows an implementation of parser combinators for error recovery, but is restricted to LL(1) grammars. The recovery strategy is based on a noskip set, computed by taking the FIRST set of every symbol in the tails of the pending rules in the parser stack. Associated with each token in this set is a sequence of symbols (including non-terminals) that would have to be inserted to reach that point in the parse, taken from the tails of the pending rules. Tokens are then skipped until reaching a token in this set, and the parser then takes actions as if it found the sequence of inserted symbols for this token.\"","":""}
{"id":"2811104224","dialogue":"\"When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The algorithm thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, geologists, architects, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training, meaning that there are no labels for parts of images. We demonstrate the method on the CIFAR-10 dataset and 10 classes from the CUB-200-2011 dataset.\"","summary":"\"Our work relates to (but constrasts with) those that perform interpretability analysis for trained networks. In posthoc analysis, one interprets a trained network by fitting explanations to how the network performs classification. There are two general approaches to understanding networks posthoc: one is class-specific activation maximization @cite_40 @cite_26 @cite_39 @cite_16 @cite_5 @cite_24 @cite_37 , and the other is input-specific posthoc visualization such as deconvolution @cite_12 and gradient-based saliency visualization @cite_24 @cite_11 @cite_34 @cite_19 . All of these posthoc visualization methods do not explain the reasoning process of how a network makes its decisions. In contrast, our network has a built-in case-based reasoning process, and the explanations generated by our network are actually used during classification and are not created posthoc.\"","":""}
{"id":"2811104224","dialogue":"\"When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The algorithm thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, geologists, architects, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training, meaning that there are no labels for parts of images. We demonstrate the method on the CIFAR-10 dataset and 10 classes from the CUB-200-2011 dataset.\"","summary":"\"Our work relates closely to works that build interpretability into deep neural networks. Attention mechanisms that identify the most relevant parts of an input for various tasks have been integrated into neural networks: various methods have been proposed to jointly train networks with integrated class-specific attention maps @cite_36 @cite_31 . There are also works that not only identify the important parts but also make use of them directly for classification: these works usually single'' out the important parts and use only these parts in the downstream reasoning process. They either use heavy supervision to locate the most relevant parts for classification (e.g. @cite_17 @cite_32 @cite_28 ), or rely on an auxiliary (pre-trained) network to extract image patches for unsupervised identification of important parts (e.g. @cite_21 @cite_13 ), or propose a number of candidate parts using selective search-based region proposal network @cite_23 @cite_29 @cite_35 @cite_4 or Monte Carlo sampling @cite_0 . However, none of these works learn prototypical cases for comparison and prediction as we do in our work.\"","":""}
{"id":"2948318272","dialogue":"\"The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincare embeddings in addition to the distributional information to detect compositionality for noun phrases. Using a weighted average of the distributional similarity and a Poincare similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. Further, we publicly release our Poincare embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.\"","summary":"\"Some of the initial efforts on compositionality prediction were undertaken by , who use LSA to calculate the similarity between a phrase and its components, whereas extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax @cite_13 and selectional preferences @cite_26 . Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively . define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings . also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction.\"","":""}
{"id":"2948283202","dialogue":"\"One of the biggest challenges in robotics systems is interacting under uncertainty. Unlike robots, humans learn, adapt and perceive their body as a unity when interacting with the world. We hypothesize that the nervous system counteracts sensor and motor uncertainties by unconscious processes that robustly fuse the available information for approximating their body and the world state. Being able to unite perception and action under a common principle has been sought for decades and active inference is one of the potential unification theories. In this work, we present a humanoid robot interacting with the world by means of a human brain-like inspired perception and control algorithm based on the free-energy principle. Until now, active inference was only tested in simulated examples. Their application on a real robot shows the advantages of such an algorithm for real world applications. The humanoid robot iCub was capable of performing robust reaching behaviors with both arms and active head object tracking in the visual field, despite the visual noise, the artificially introduced noise in the joint encoders (up to 40 degrees deviation), the differences between the model and the real robot and the misdetections of the hand.\"","summary":"\"Multisensory perception has been widely studied in the literature and enables the robot to combine joint information with other sensors such as images and tactile cues. Bayesian estimation has been proved to achieve robust and accurate model based robot arm tracking @cite_12 even under occlusion @cite_20 . Furthermore, integrated visuomotor processes enabled humanoid robots to learn object representations through manipulation without any prior knowledge about them @cite_9 , learn motor representations for robust reaching @cite_5 @cite_4 and even visuotactile motor representations for reaching and avoidance behaviors @cite_19 .\"","":""}
{"id":"2948283202","dialogue":"\"One of the biggest challenges in robotics systems is interacting under uncertainty. Unlike robots, humans learn, adapt and perceive their body as a unity when interacting with the world. We hypothesize that the nervous system counteracts sensor and motor uncertainties by unconscious processes that robustly fuse the available information for approximating their body and the world state. Being able to unite perception and action under a common principle has been sought for decades and active inference is one of the potential unification theories. In this work, we present a humanoid robot interacting with the world by means of a human brain-like inspired perception and control algorithm based on the free-energy principle. Until now, active inference was only tested in simulated examples. Their application on a real robot shows the advantages of such an algorithm for real world applications. The humanoid robot iCub was capable of performing robust reaching behaviors with both arms and active head object tracking in the visual field, despite the visual noise, the artificially introduced noise in the joint encoders (up to 40 degrees deviation), the differences between the model and the real robot and the misdetections of the hand.\"","summary":"\"Active inference (under the free-energy principle) includes action as a classical spinal reflex arc pathway triggered by perception prediction errors and has been mainly studied in theoretical or simulated conditions. Friston presented in @cite_8 a theoretical motor model with two degrees of freedom as an extension of the dynamic expectation maximization algorithm. It was recently studied in robot control of a simulated PR2 robot @cite_11 , one degree of freedom simulated vehicle @cite_10 and two degrees of freedom simulated robot arm @cite_2 .\"","":""}
{"id":"2948283202","dialogue":"\"One of the biggest challenges in robotics systems is interacting under uncertainty. Unlike robots, humans learn, adapt and perceive their body as a unity when interacting with the world. We hypothesize that the nervous system counteracts sensor and motor uncertainties by unconscious processes that robustly fuse the available information for approximating their body and the world state. Being able to unite perception and action under a common principle has been sought for decades and active inference is one of the potential unification theories. In this work, we present a humanoid robot interacting with the world by means of a human brain-like inspired perception and control algorithm based on the free-energy principle. Until now, active inference was only tested in simulated examples. Their application on a real robot shows the advantages of such an algorithm for real world applications. The humanoid robot iCub was capable of performing robust reaching behaviors with both arms and active head object tracking in the visual field, despite the visual noise, the artificially introduced noise in the joint encoders (up to 40 degrees deviation), the differences between the model and the real robot and the misdetections of the hand.\"","summary":"\"A first model of the free-energy optimization in a real robot was performed in @cite_15 working as an approximate Bayesian filter estimation, where the robot was able to perceive its arm location fusing visual, proprioceptive and tactile information. However, authors left out the action. In this work, we took one step further and modelled and applied active inference to the iCub robot for dual arm reaching with active head tracking. For reproducibility, the code is publicly available tobereleased . While the arms goal is to minimize the prediction error between the goal (object) and the end-effector visual location, the head goal is to maintain the object centered in the field of view to provide wider and more accurate reaching capabilities.\"","":""}
{"id":"2809710758","dialogue":"\"Abstract One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated. In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).\"","summary":"\"Saliency detection One of the first methods for computational saliency was proposed by @cite_14 . Their model based on the feature integration theory of @cite_9 and the work of @cite_26 decomposes the input image into low level feature maps including color, intensity and orientation. These maps are subsequently merged together using linear filtering and center surround structures to form a final saliency map. Their seminal work initiated much research in biologically inspired saliency models as well as more mathematical models for computational saliency . The central surround allows to measure contrast with the context, however it is confined to predefined shapes; normally the circle shape of the Gaussian filters or rectangle shapes in the work of @cite_21 . In this paper we will propose a method for arbitrary shaped contexts.\"","":""}
{"id":"2809710758","dialogue":"\"Abstract One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated. In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).\"","summary":"\"Local and global approaches for visual saliency can be classified in the category of bottom-up approaches. Local approaches compute local center-surround contrast and rarity of a region over its neighborhoods. @cite_14 derive a bottom-up visual saliency based on center surround difference through multiscale image features. @cite_21 propose a binary saliency estimation method by training a CRF to combine a set of local, regional, and global features. @cite_5 propose the GBVS method which is a bottom-up saliency approach that consists of two steps: the generation of feature channels as in Itti's approach, and their normalization using a graph based approach. A saliency model that computes local descriptors from a given image in order to measure the similarity of a pixel to its neighborhoods was proposed by @cite_29 . @cite_1 propose a AWS method which is based on the decorrelation and the distinctiveness of local responses.\"","":""}
{"id":"2809710758","dialogue":"\"Abstract One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated. In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).\"","summary":"\"Another class of features for saliency are based on global context or rarity; the saliency of a feature is based on its rarity with respect to the whole image. @cite_16 consider the difference of patches with all other patches in the image to compute global saliency. @cite_20 compute saliency by considering the reconstruction error which is left after reconstructing a patch from other patches (other patches can be from the same image or from the whole dataset). @cite_30 compute the rarity of a feature by comparing the contrast between a 15 pixel border around the image and the object proposal histogram. Other than these methods we propose a method to compute the saliency with respect to the direct context of the object. Finally, to compute saliency @cite_27 combined local and global objectness cues with a set of candidates location.\"","":""}
{"id":"2809710758","dialogue":"\"Abstract One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated. In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).\"","summary":"\"Deep convolutional neural networks have revolutionized computer vision over the last few years. This has recently led to several papers on deep learning for saliency detection . Both @cite_17 and @cite_2 consider parallel networks which evaluate the image at various scales. @cite_3 use two networks to describe local and global saliency. @cite_15 combine a local and global model to compute saliency. The main challenge for saliency detection with deep networks is the amount of training data which is not always available. This is solved in by training on the largest available saliency dataset, namely MSRA-B , and testing on the other datasets (both also use pretrained network weights trained on the 1M Imagenet dataset). Like these method, we will use a pretrained network for the extraction of features for saliency detection.\"","":""}
{"id":"2809710758","dialogue":"\"Abstract One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated. In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).\"","summary":"\"Among the first object proposal methods the work of @cite_11 , named the Constrained Parametric Min-Cuts (CPMC) method, uses graph cuts with different random seeds to obtain multiple binary foreground and background segments. @cite_0 proposes to measure the objectness of an image window, where they rank randomly sampled image windows based on their likelihood of containing the object by using multiple cues among which edges density, multiscale saliency, superpixels straddling and color contrast. @cite_23 proposed an object proposal method similar to the CPMC method by generating multiple foreground and background segmentations. A very fast method for object proposals was proposed by @cite_4 , which generates box proposals at 300 images per second.\"","":""}
{"id":"2809710758","dialogue":"\"Abstract One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated. In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).\"","summary":"\"An extensive comparison of object proposal methods was performed by @cite_18 . Among the best evaluated object proposal methods (which generate object segmentation) are the selective search , the geodesic object proposals and the multiscale combinatorial grouping method . Selective search proposes a set of segments based on hierarchical segmentations of the image where the underlying distance measures and color spaces are varied to yield a large variety of segmentations. @cite_25 , propose the geodesic object proposals method, which applies a geodesic distance transfer to compute object proposals. Finally, Multiscale Combinatorial Grouping is based on a bottom-up hierarchical image segmentation. Object candidates are generated by a grouping procedure which is based on edge strength.\"","":""}
{"id":"2810576705","dialogue":"\"Online contention resolution schemes (OCRSs) were proposed by Feldman, Svensson, and Zenklusen as a generic technique to round a fractional solution in the matroid polytope in an online fashion. It has found applications in several stochastic combinatorial problems where there is a commitment constraint: on seeing the value of a stochastic element, the algorithm has to immediately and irrevocably decide whether to select it while always maintaining an independent set in the matroid. Although OCRSs immediately lead to prophet inequalities, these prophet inequalities are not optimal. Can we instead use prophet inequalities to design optimal OCRSs? We design the first optimal @math -OCRS for matroids by reducing the problem to designing a matroid prophet inequality where we compare to the stronger benchmark of an ex-ante relaxation. We also introduce and design optimal @math -random order CRSs for matroids, which are similar to OCRSs but the arrival is chosen uniformly at random.\"","summary":"\"Krengel and Sucheston gave the first tight @math -single item prophet inequality @cite_24 @cite_11 . The connection between multiple-choice prophet inequalities and mechanism design was recognized in @cite_6 ; they proved a prophet inequality for uniform matroids. This bound was later improved by Alaei @cite_1 using the , which is an OCRS in disguise. @cite_7 further developed the connection between prophet inequalities and mechanism design, and showed how to be @math -prophet inequality for general matroids in a variant where the algorithm may choose the element order. Yan @cite_0 improved this result to @math -competitive using the for submodular functions, first studied in @cite_16 @cite_4 . @cite_21 adapted correlation gaps to a polytope to design CRSs. Improved correlation gaps were presented in @cite_0 @cite_17 . The matroid prophet inequality was first explicitly formulated in @cite_18 . @cite_23 gave an alternate proof, and extended to Bernoulli submodular functions, using OCRSs. Finally, information theoretic @math -prophet inequalities are also known for general downward-closed constraints @cite_3 @cite_12 .\"","":""}
{"id":"2954351205","dialogue":"\"Connectionist sequence models (e.g.","summary":"RNNs) applied to musical sequences suffer from two known problems: First","":""}
{"id":"2808872418","dialogue":"\"This article addresses the navigation problem of small and medium-sized unmanned aerial vehicles (UAVs) built to perform missions in forest environments (e.g., search and rescue) by exploiting the presence of natural or man-made paths, typically present in this type of environments. The proposed system extends a previous monocular-based technique for trail detection and tracking so as to take into account volumetric data acquired from a Visual SLAM algorithm and, as a result, to increase its sturdiness upon challenging trails. The experimental results, obtained via a set of 12 videos recorded with a camera installed in a tele-operated, unmanned small-sized aerial vehicle, show the ability of the proposed system to overcome some of the difficulties of the original detector, attaining a success rate of 97.8 .\"","summary":"\"Current path detection methods rely considerably on work developed for ill-structured unpaved rural roads. The typical solution in this case is to segment the road region from its surroundings by considering a set of pixels whose the probability of belonging to the road surface is above a given threshold calculated through models learned off-line @cite_24 @cite_15 or on-line in a self-supervised way @cite_36 @cite_37 . Frequently, a simplified known model of the road (e.g., trapezoidal) is fit to the segmented images. The Region Growing technique is an alternative to the model fitting process for less structured roads @cite_11 @cite_18 @cite_24 . By enforcing a global shape constraint, the model-based approach enables the substitution of the road non-road pixel classification process by an unsupervised clustering mechanism @cite_16 .\"","":""}
{"id":"2808872418","dialogue":"\"This article addresses the navigation problem of small and medium-sized unmanned aerial vehicles (UAVs) built to perform missions in forest environments (e.g., search and rescue) by exploiting the presence of natural or man-made paths, typically present in this type of environments. The proposed system extends a previous monocular-based technique for trail detection and tracking so as to take into account volumetric data acquired from a Visual SLAM algorithm and, as a result, to increase its sturdiness upon challenging trails. The experimental results, obtained via a set of 12 videos recorded with a camera installed in a tele-operated, unmanned small-sized aerial vehicle, show the ability of the proposed system to overcome some of the difficulties of the original detector, attaining a success rate of 97.8 .\"","summary":"\"The models referred in the previous paragraph are the basis of most work on path detection. An example is the use of knowledge about the color distributions of both the path and their surroundings for segmentation @cite_21 . The main direction of the path can also be learned off-line in a supervised way @cite_24 . The use of off-line learned models has the limitation that the robot is only able to perceive environments that have been covered by the training set. Robustness can be increased if these models are substituted by models learned on-line @cite_13 @cite_17 . In contrast to the road domain, the definition of the reference regions from which it is possible to supervise the learning process is challenging. With varying width and orientation, it is difficult to assure that the robot is on the path, and from that, which regions of the input image can be used as reference. Moreover, paths and its surroundings often exhibit the same height, which hampers a straightforward use of depth information to determine a path reference patch.\"","":""}
{"id":"2808872418","dialogue":"\"This article addresses the navigation problem of small and medium-sized unmanned aerial vehicles (UAVs) built to perform missions in forest environments (e.g., search and rescue) by exploiting the presence of natural or man-made paths, typically present in this type of environments. The proposed system extends a previous monocular-based technique for trail detection and tracking so as to take into account volumetric data acquired from a Visual SLAM algorithm and, as a result, to increase its sturdiness upon challenging trails. The experimental results, obtained via a set of 12 videos recorded with a camera installed in a tele-operated, unmanned small-sized aerial vehicle, show the ability of the proposed system to overcome some of the difficulties of the original detector, attaining a success rate of 97.8 .\"","summary":"\"The use of a global shape constraint (e.g., triangular) to avoid the learning process has also been tested in the path detection domain @cite_19 . This is done by over-segmenting the image, creating sets of segments, and then scoring them against the global shape constraint. Accurate image over-segmentation is a computationally demanding task and usually requires clear edges segmenting the object from the background. Moreover, global shape constraints limit the type of paths that can be detected.\"","":""}
{"id":"2808872418","dialogue":"\"This article addresses the navigation problem of small and medium-sized unmanned aerial vehicles (UAVs) built to perform missions in forest environments (e.g., search and rescue) by exploiting the presence of natural or man-made paths, typically present in this type of environments. The proposed system extends a previous monocular-based technique for trail detection and tracking so as to take into account volumetric data acquired from a Visual SLAM algorithm and, as a result, to increase its sturdiness upon challenging trails. The experimental results, obtained via a set of 12 videos recorded with a camera installed in a tele-operated, unmanned small-sized aerial vehicle, show the ability of the proposed system to overcome some of the difficulties of the original detector, attaining a success rate of 97.8 .\"","summary":"\"To overcome the previous work's limitations, @cite_9 proposed the use of local appearance contrast for path detection. Later, @cite_30 exploited the related concept of visual saliency to detect paths. Visual saliency was exploited with a swarm-based solution, inspired by the social insects metaphor @cite_1 , capable of interpreting the input images without the cost of explicitly over-segmenting the input image and the brittleness of depending on accurate appearance and shape models. As mentioned before, this method has shown to not exhibit sufficient robustness in the presence of strong distractors in the environment. This paper proposes an extension to the this previous method, based on 3-D data acquired from a visual SLAM technique, to reduce the chances of the method to be mistakenly attracted by distractors in the environment.\"","":""}
{"id":"2808872418","dialogue":"\"This article addresses the navigation problem of small and medium-sized unmanned aerial vehicles (UAVs) built to perform missions in forest environments (e.g., search and rescue) by exploiting the presence of natural or man-made paths, typically present in this type of environments. The proposed system extends a previous monocular-based technique for trail detection and tracking so as to take into account volumetric data acquired from a Visual SLAM algorithm and, as a result, to increase its sturdiness upon challenging trails. The experimental results, obtained via a set of 12 videos recorded with a camera installed in a tele-operated, unmanned small-sized aerial vehicle, show the ability of the proposed system to overcome some of the difficulties of the original detector, attaining a success rate of 97.8 .\"","summary":"\"The use of three-dimensional information to improve the targeting methods for detection of trails was initially explored by @cite_12 @cite_7 . More specifically, @cite_7 used a binocular vision system composed of two omni-directional cameras and a laser scanner to map the vehicle's surrounding area. Thus, this previous method also presents itself as a solid solution for trail segmentation but requires the use of a laser scanner, which should be avoided if size, weight, and energy constraints found in small UAV are to be met. @cite_5 also proposed a system based on binocular vision for detection and tracking of paths in a forest environment. In this paper we offer a solution that is based on monocular vision and, thus, is more easily integrated in current commercial UAV solutions. Moreover, the ability to solve the trail detection problem with a single camera is key for robots equipped with binocular systems when one of the camera fails or when stereo calibration data becomes deprecated.\"","":""}
{"id":"2761398658","dialogue":"\"Bit matrix compression is a highly relevant operation in computer arithmetic. Essentially being a multi-operand addition, it is the key operation behind fast multiplication and many higher-level operations such as multiply-accumulate, the computation of the dot product or the implementation of FIR filters. Compressor implementations have been constantly evolving for greater efficiency both in general and in the context of concrete applications or specific implementation technologies. This paper is building on this history and describes a generic implementation of a bit matrix compressor for Xilinx FPGAs, which does not require a generator tool. It contributes FPGA-oriented metrics for the evaluation of elementary parallel bit counters, a systematic analysis and partial decomposition of previously proposed counters and a fully implemented construction heuristic with a flexible compression target matching the device capabilities. The generic implementation is agnostic of the aspect ratio of the input matrix and can be used for multiplication the same way as it can be for single-column population count operations.\"","summary":"\"The scheduling of counters to build a compressor depends naturally on the selection of available modules. It is the backing technology that defines which counters can be implemented most efficiently. A discussion of the choices for ASICs was composed by Verma and Ienne @cite_11 . FPGA-targeted counters have been most prominently proposed by Parandeh- @cite_8 @cite_12 @cite_5 as well as Kumm and Zipf @cite_7 @cite_6 . As this paper focuses on the construction of compressors within a modern Xilinx FPGA fabric, it will heavily build on the work of these latter two groups.\"","":""}
{"id":"2761398658","dialogue":"\"Bit matrix compression is a highly relevant operation in computer arithmetic. Essentially being a multi-operand addition, it is the key operation behind fast multiplication and many higher-level operations such as multiply-accumulate, the computation of the dot product or the implementation of FIR filters. Compressor implementations have been constantly evolving for greater efficiency both in general and in the context of concrete applications or specific implementation technologies. This paper is building on this history and describes a generic implementation of a bit matrix compressor for Xilinx FPGAs, which does not require a generator tool. It contributes FPGA-oriented metrics for the evaluation of elementary parallel bit counters, a systematic analysis and partial decomposition of previously proposed counters and a fully implemented construction heuristic with a flexible compression target matching the device capabilities. The generic implementation is agnostic of the aspect ratio of the input matrix and can be used for multiplication the same way as it can be for single-column population count operations.\"","summary":"\"A heuristic for constructing compressors for Altera devices was proposed by Parandeh- in 2008 @cite_8 . They used a single-pass heuristic selecting the most efficient from a selection of parallel counter that would fit into the work still to do by the compression step starting from the least-significant and proceeding to the most-significant bit position. The compression goal was a matrix of, at most, three rows. This relaxed goal definition exploits the fact that ternary adders map well onto modern FPGA architectures. It also has the tremendous benefit that half adders can be avoided altogether. Half-adders only have a reshaping function and do not reduce the number of bits in the matrix. As shown in figHAmust , they must be used to reshape an almost done two-row matrix in parallel so that it can accommodate just one more carry efficiently. This pressure disappears with a goal of three rows.\"","":""}
{"id":"2761398658","dialogue":"\"Bit matrix compression is a highly relevant operation in computer arithmetic. Essentially being a multi-operand addition, it is the key operation behind fast multiplication and many higher-level operations such as multiply-accumulate, the computation of the dot product or the implementation of FIR filters. Compressor implementations have been constantly evolving for greater efficiency both in general and in the context of concrete applications or specific implementation technologies. This paper is building on this history and describes a generic implementation of a bit matrix compressor for Xilinx FPGAs, which does not require a generator tool. It contributes FPGA-oriented metrics for the evaluation of elementary parallel bit counters, a systematic analysis and partial decomposition of previously proposed counters and a fully implemented construction heuristic with a flexible compression target matching the device capabilities. The generic implementation is agnostic of the aspect ratio of the input matrix and can be used for multiplication the same way as it can be for single-column population count operations.\"","summary":"\"In their follow-up work @cite_12 , Parandeh- start considering mapping counters to the broader structural context of an Altera Adaptive Logic Module (ALM) rather than assuming an indifferent pool of lookup tables (LUTs). This enables them to exploit the carry-chain links between adjacent LUT stages for fast and yet more capable counters. Finally @cite_5 , they tie individual counters together by merging the carry output of one module with the carry input of another into one LUT stage. While this, indeed, reduces LUT usage, it also creates unwieldy structures that severely limit the mobility of individual counters during the logic placement, which complicates the routing optimization to be performed by the tools. Last but not least, this work also looks into a generalization for Xilinx architectures.\"","":""}
{"id":"2809204870","dialogue":"\"In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.\"","summary":"\"Most of the action recognition methodologies models the action using full-body motion based features, which only works well for the specific class of action recognition problem where action set is relatively small such as in @cite_19 , @cite_17 , @cite_0 . These approach do not look useful when it comes to their application on real everyday actions. Research in the area of human action recognition has been mainly focused on full-body motions that can be characterized by movement and change of posture like walking, waving, etc.\"","":""}
{"id":"2809204870","dialogue":"\"In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.\"","summary":"\"In many action recognition approaches @cite_12 , @cite_8 , @cite_4 , human motion information have been used. The problem of action recognition has been dealt using motion trajectories with the use of depth cameras like Kinect. These approaches (e.g. see @cite_16 ) are typically considered to be more robust to generate human pose information which can be used for the purpose of action recognition. However, Kinect body pose recognition is not accurate when there are human-object interactions due to occlusions. Motion dynamics based action recognition still cannot capture the representation for the subtle object manipulations. Another interesting aspect is the variations in goal of the task with similar motion dynamics.\"","":""}
{"id":"2809204870","dialogue":"\"In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.\"","summary":"\"Hand gesture recognition is more closer to the problem of object manipulation action recognition. Hand gesture recognition has also been addressed using depth data generated from Kinect in @cite_18 and @cite_27 . But these techniques mainly target sign language gestures and not the human hand-object interactions. @cite_27 treats an action sequence as a 4D shape and propose random occupancy pattern (ROP) features, extracted from random sampling of 4D subvolumes with different sizes and at different locations. In gesture depth sequences, the semantics of the gestures are mainly understood by the large movement of the hand. These approaches use cropped portion of hand using some hand detection approach, to determine these large hand movements to model different gestures. But, these clear motion information are not easily perceivable in the case object manipulation tasks.\"","":""}
{"id":"2809204870","dialogue":"\"In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.\"","summary":"\"At this point, we note here that the above mentioned works involve processing low-level information (e.g. feature extraction from videos images), whereas our goal in this work is to convey the importance of grasp and motion-constraints information at the higher semantic level (e.g. types of grasps and motion-constraints). Such high level attributes for manipulating actions, are indeed available @cite_25 , @cite_9 , @cite_20 as a part of the Yale human grasping dataset that we are considering in this work.\"","":""}
{"id":"2809204870","dialogue":"\"In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.\"","summary":"\"To the best of our knowledge, apart from @cite_21 , @cite_14 and @cite_23 , there has been no work using grasp information for action recognition. @cite_21 semantically group action intentions using grasp based information into three coarse and somewhat abstract classes: Force-oriented, Skill-oriented, and Casual actions. They use hand grasps recognized through convolutional neural network to understand the class of action, each image belong to. @cite_23 develop a grammatical formalism for parsing and interpreting action sequences. Their basic idea is to divide actions into sub-actions of when the object is grasped and released, or if there is change in the grasp type during the course of an action motion. This grammatical formalism provides a syntax and semantics of action, over which basic tools for understanding of actions can be developed. @cite_28 considers the problem of grasp classification on Yale human grasping dataset, again based on the coarsely defined task attributes such as force (interaction and weight), motion-constraints on objects and functional class (use and hold), whereas we propose a solution to task or manipulation action classification based on the grasp information, motion-constraints, and object class.\"","":""}
{"id":"2809204870","dialogue":"\"In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.\"","summary":"\"The important aspects of our work include: a) A compact representation of the grasp and motion-constraints using some popular and some contemporary schemes. b) Demonstrating the usefulness of information from coarse-grained and fine-grained grasp attributes as well as motion-constraints for fine-grained and coarse-grained action recognition. c) A differential experimental analysis involving subsets of grasp and motion-constraints features, to provide more insights on the usefulness of grasp information alone, motion-constraints information alone, and grasp and motion-constraints based information together for intended classification problem. d) Comparisons between Instance and Sequence level modeling of object manipulation actions using fine-grained grasp information. e) An extensive experimental evaluation using different contemporary multi-class and binary classifiers (with a multi-class voting strategy), which also serves as a useful comparative study of popular classifiers for the manipulation action recognition problem. This analysis also helps to demonstrate that different classification frameworks, largely arrive at a consensus with respect to our hypothesis about using grasp and motion-constraints for fine-grained action classification. We demonstrate our results on a large Yale Human Grasping dataset @cite_29 which involves various tasks on different objects.\"","":""}
{"id":"2950799833","dialogue":"\"Online news media sometimes use misleading headlines to lure users to open the news article. These catchy headlines that attract users but disappointed them at the end, are called Clickbaits. Because of the importance of automatic clickbait detection in online medias, lots of machine learning methods were proposed and employed to find the clickbait headlines. In this research, a model using deep learning methods is proposed to find the clickbaits in Clickbait Challenge 2017's dataset. The proposed model gained the first rank in the Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data analytics and visualization techniques are employed to explore and discover the provided dataset to get more insight from the data.\"","summary":"Two content marketing platforms and millions of headlines were studied to find features that contribute to increasing users’ engagement and change of unsubscribed readers into subscribers. This study suggested that clickbait techniques may increase the users’ engagement temporarily @cite_15 .","":""}
{"id":"2950799833","dialogue":"\"Online news media sometimes use misleading headlines to lure users to open the news article. These catchy headlines that attract users but disappointed them at the end, are called Clickbaits. Because of the importance of automatic clickbait detection in online medias, lots of machine learning methods were proposed and employed to find the clickbait headlines. In this research, a model using deep learning methods is proposed to find the clickbaits in Clickbait Challenge 2017's dataset. The proposed model gained the first rank in the Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data analytics and visualization techniques are employed to explore and discover the provided dataset to get more insight from the data.\"","summary":"An interesting model was proposed by Zhou for Clickbait Challenge 2017 @cite_0 . He employed automatic approach to find clickbait in the tweet stream. Self-attentive neural network was employed for the first time in this article to examine each tweet’s probability of click baiting.","":""}
{"id":"2950799833","dialogue":"\"Online news media sometimes use misleading headlines to lure users to open the news article. These catchy headlines that attract users but disappointed them at the end, are called Clickbaits. Because of the importance of automatic clickbait detection in online medias, lots of machine learning methods were proposed and employed to find the clickbait headlines. In this research, a model using deep learning methods is proposed to find the clickbaits in Clickbait Challenge 2017's dataset. The proposed model gained the first rank in the Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data analytics and visualization techniques are employed to explore and discover the provided dataset to get more insight from the data.\"","summary":"\"Another successful method @cite_2 , which was proposed in Clickbait Challenge 2017, used ensemble of Linear SVM models. They showed that how the clickbait can be detected using a small ensemble of linear models. Since the competitors were allowed to use external data sources, they were used in their research in order to find the pattern of non-clickbait headlines and expand the size of their training set.\"","":""}
{"id":"2950799833","dialogue":"\"Online news media sometimes use misleading headlines to lure users to open the news article. These catchy headlines that attract users but disappointed them at the end, are called Clickbaits. Because of the importance of automatic clickbait detection in online medias, lots of machine learning methods were proposed and employed to find the clickbait headlines. In this research, a model using deep learning methods is proposed to find the clickbaits in Clickbait Challenge 2017's dataset. The proposed model gained the first rank in the Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data analytics and visualization techniques are employed to explore and discover the provided dataset to get more insight from the data.\"","summary":"\"In @cite_8 , authors developed linguistically-infused network model for the Clickbait Challenge 2017 that is able to learn strength of clickbait content from not only the texts of the tweets but also the passage of the articles and the linked images. They believed using the passage of the articles and the linked images can lead to a substantial boost in the model’s performance. They trained two neural network architectures which are Long Short-Term Memory (LSTM) @cite_7 and Convolutional Neural Network (CNN). Their text sequence sub-network was constructed using embedding layer and two 1-dimensional convolution layers followed by a max-pooling layer. They initialize their embedding layer with pre-trained Glove embeddings @cite_21 using 200-dimensional embeddings.\"","":""}
{"id":"2950799833","dialogue":"\"Online news media sometimes use misleading headlines to lure users to open the news article. These catchy headlines that attract users but disappointed them at the end, are called Clickbaits. Because of the importance of automatic clickbait detection in online medias, lots of machine learning methods were proposed and employed to find the clickbait headlines. In this research, a model using deep learning methods is proposed to find the clickbaits in Clickbait Challenge 2017's dataset. The proposed model gained the first rank in the Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data analytics and visualization techniques are employed to explore and discover the provided dataset to get more insight from the data.\"","summary":"\"In @cite_1 , another model was proposed using neural networks for the Clickbait Challenge 2017. In the text processing phase, they used whitespace tokenizer with lower casing and without using any domain specific processing such as Unicode normalization or any lexical text normalization. Then all the tokens were converted to the word embeddings which were then fed into LSTM units. The embedding vectors were initialized randomly. They employed batch normalization to normalize inputs to reduce internal covariate shift. Also, the risk of over-fitting was reduced through using dropout between individual neural network layers. At the end, individual networks are fused by concatenating the dense output layers of the individual networks which then were fed into a fully connected neural network.\"","":""}
{"id":"2950799833","dialogue":"\"Online news media sometimes use misleading headlines to lure users to open the news article. These catchy headlines that attract users but disappointed them at the end, are called Clickbaits. Because of the importance of automatic clickbait detection in online medias, lots of machine learning methods were proposed and employed to find the clickbait headlines. In this research, a model using deep learning methods is proposed to find the clickbaits in Clickbait Challenge 2017's dataset. The proposed model gained the first rank in the Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data analytics and visualization techniques are employed to explore and discover the provided dataset to get more insight from the data.\"","summary":"\"A machine learning based clickbait detection system was designed in @cite_18 . They extracted six novel features for clickbait detection and they showed in their results that these novel features are the most effective ones for detecting clickbait news headlines. Totally, they extracted 331 features but to prevent overfitting, they just kept 180 features among them. They used all the fields in the dataset such as titles, passages, and key words in their model for extracting these features.\"","":""}
{"id":"2952389078","dialogue":"\"With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.\"","summary":"\"Distributed representations of code were first suggested by @cite_15 , followed by several works leveraging embeddings to apply NLP techniques to programming languages @cite_10 @cite_50 .\"","":""}
{"id":"2952389078","dialogue":"\"With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.\"","summary":"\"Learned representations of code are commonly used for two types of tasks: uncovering program semantics or optimizing programs. For the former task, code embeddings have been used to perform function or variable naming @cite_15 @cite_24 , clone detection @cite_2 , code completion @cite_54 @cite_45 , summarization @cite_23 , and algorithm classification @cite_38 . As for program optimization, research has been conducted on automatic feature generation for code @cite_64 @cite_11 ; and @cite_8 notably leverage embeddings of OpenCL code to predict optimal device mapping and thread coarsening factors. Their work differs from ours in that the method is restricted to the OpenCL language, and that they process programs in a sequential order, which does not capture complex code structures. Furthermore, the state-of-the-art in automatic tuning for program optimization @cite_17 uses surrogate performance models and active learning, and does not take code semantics into account.\"","":""}
{"id":"2952389078","dialogue":"\"With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.\"","summary":"\"Previous works that use code embeddings do not evaluate the quality of the trained space on its own merit, but rather through the performance of subsequent (downstream) tasks. One exception is @cite_15 , who present empirical evidence of vector similarities for similar method names. To the best of our knowledge, we are the first to quantify the quality of a code embedding space itself in the form of clustering, syntactic analogies, semantic analogies, and categorical distance tests.\"","":""}
{"id":"2968805489","dialogue":"\"For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.\"","summary":"Deep learning has achieved far more accurate results than traditional methods on image processing. There are several works on lane makings and parking slots detection using deep learning on front sight images. J Kim and M Lee @cite_2 presented a robust lane detection method based on the combined convolutional neural network with random sample consensus algorithm; S @cite_25 proposed a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition guided by a vanishing point; G @cite_7 proposed a decentralized and efficient solution for visual parking slots occupancy detection based on a deep convolutional neural network.","":""}
{"id":"2968805489","dialogue":"\"For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.\"","summary":"\"Traditional image processing methods on panoramic images and deep learning methods on front sight images both achieve excellent results, but few work is aimed to use deep leaning on panoramic images. This is because the accuracy of deep learning model is largely related to datasets, and there are few public dataset of panoramic images which can be used to train a model. The first public panoramic dataset for lane markings and parking slots is panoramic surround view (PSV) dataset, released by Yan @cite_14 . And this dataset is specially used for semantic segmentation, which is labeled pixel by pixel, and each pixel has its corresponding class. In order to combine the advantages of panoramic images and deep learning, we use the semantic segmentation method with convolutional network to segment the area and classify the class of lane makings and parking slots on the PSV dataset.\"","":""}
{"id":"2968805489","dialogue":"\"For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.\"","summary":"\"In @cite_14 , they achieved the segmentation of lane markings and parking slots using semantic segmentation method on PSV dataset, and proposed a VH-stage module special for linear structures. But the size of their model is too large to meet the requirement of using in embedded and mobile platform. In this paper, we put forward a smaller size model, but with higher accuracy. And the two main improved methods we proposed are proved to be significant.\"","":""}
{"id":"2964174544","dialogue":"\"We analyze a millimeter wave network, deployed along the streets of a city, in terms of positioning and downlink data-rate performance, respectively. First, we present a transmission scheme where the base stations provide jointly positioning and data-communication functionalities. Accordingly, we study the trade- off between the localization and the data rate performance based on theoretical bounds. Then, we obtain an upper bound on the probability of beam misalignment based on the derived localization error bound. Finally, we prescribe the network operator a scheme to select the beamwidth and the power splitting factor between the localization and communication functions to address different quality of service requirements, while limiting cellular outage.\"","summary":"\"* -0.2cm In the context of sub-6GHz systems, @cite_4 have studied a distributed antenna system providing both data communication and positioning functionalities. The authors assumed that the UEs know the positions of the BSs and attempt to estimate their own positions based on the received signals. @cite_5 have shown that localization using mm-wave frequencies is efficient in terms of accuracy, even in the presence of a limited number of anchor nodes. In fact, mm-wave beam-forming allows for accurate localization and orientation of UEs with respect to the BSs @cite_11 . @cite_12 have studied a location-aided initial access strategy for mm-wave networks, in which the information of UE locations enables to speed up the channel estimation and beam-forming procedures. @cite_11 have studied the trade-off between communication rate and positioning quality in a single user mm-wave link. Similarly @cite_16 have studied the beamforming optimization and spectral power allocation based on theoretical localization bounds.\"","":""}
{"id":"2964174544","dialogue":"\"We analyze a millimeter wave network, deployed along the streets of a city, in terms of positioning and downlink data-rate performance, respectively. First, we present a transmission scheme where the base stations provide jointly positioning and data-communication functionalities. Accordingly, we study the trade- off between the localization and the data rate performance based on theoretical bounds. Then, we obtain an upper bound on the probability of beam misalignment based on the derived localization error bound. Finally, we prescribe the network operator a scheme to select the beamwidth and the power splitting factor between the localization and communication functions to address different quality of service requirements, while limiting cellular outage.\"","summary":"\"The downlink communication performance in random wireless networks is typically characterized by signal to interference and noise ratio (SINR) coverage probability and rate coverage probability, using stochastic geometry @cite_18 . For this, the positions of the BSs are modeled using homogeneous Poisson point process (PPP) @cite_2 or using repulsive point processes @cite_17 . Recently, Ghatak et.al. @cite_9 investigated a more realistic scenario, where mm-wave BSs are deployed along the roads of a city. We use this model in this paper, and accordingly we study a one dimensional setting where the BSs and the served users are assumed to be on the same street.\"","":""}
{"id":"2808381151","dialogue":"\"We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.\"","summary":"\"A large number of prior works have implemented policy gradient algorithms with entropy regularization @cite_30 @cite_25 @cite_32 @cite_1 , which boost exploration by greedily maximizing policy entropy at each time step. In contrast to such greedy procedure, maximum entropy objective considers entropy over the entire policy trajectories @cite_24 @cite_4 @cite_10 . Though entropy regularization is simpler to implement in practice, @cite_14 @cite_24 argues in favor of maximum entropy objective by showing that trained policies can be robust to noise, which is desirable for real life robotics tasks; and multi-modal, a potentially desired property for exploration and fine-tuning for downstream tasks. However, their training procedure is fairly complex, which consists of training a soft Q function by fixed point iteration and a neural sampler by Stein variational gradient @cite_33 . We argue that properties as robustness and multi-modality are attainable through simple entropy regularized policy gradient algorithms combined with expressive policy representations.\"","":""}
{"id":"2808381151","dialogue":"\"We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.\"","summary":"\"Prior works have studied the property of maximum entropy objective @cite_4 @cite_7 , entropy regularization @cite_1 and their connections with variants of operators @cite_29 . It is commonly believed that entropy regularization greedily maximizes local policy entropy and does not account for how a policy update impacts future states. In Section 4, we show that entropy regularized policy gradient update maximizes a lower bound of maximum entropy objective, given constraints on the differences between consecutive policy iterates. This partially justifies why simple entropy regularization combined with expressive policy classes can achieve competitive empirical performance in practice.\"","":""}
{"id":"2808381151","dialogue":"\"We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.\"","summary":"\"There is a number of prior works that discuss different policy architectures. The most common policy for continuous control is unimodal Gaussian @cite_30 @cite_25 @cite_32 . @cite_6 discusses mixtures of Gaussian, which can represent multi-modal policies but it is necessary to specify the number of modes in advance. @cite_24 also represents a policy using implicit model, but the policy is trained to sample from the soft Q function instead of being trained directly. Recently, we find @cite_9 also uses Normalizing Flows to represent policies, but their focus is learning an hierarchy and involves layers of pre-training. Contrary to early works, we propose to represent flexible policies using implicit models Normalizing Flows and efficient algorithms to train the policy end-to-end.\"","":""}
{"id":"2808381151","dialogue":"\"We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.\"","summary":"\"Implicit models have been extensively studied in probabilistic inference and generative modeling @cite_16 @cite_2 @cite_0 @cite_13 . Implicit models define distributions by transforming source noise via a forward pass of neural networks, which in general sacrifice tractable probability density for more expressive representation. Normalizing Flows are a special case of implicit models @cite_5 @cite_18 @cite_35 , where transformations from source noise to output are invertible and allow for maximum likelihood inference. Borrowing inspirations from prior works, we introduce implicit models into policy representation and empirically show that such rich policy class entails multi-modal behavior during training. In @cite_13 , GAN @cite_16 is used as an optimal density estimator for likelihood free inference. In our work, we apply similar idea to compute entropy regularization when policy density is not available.\"","":""}
{"id":"2808040094","dialogue":"\"In this paper, we propose an efficient and reproducible deep learning model for musical onset detection (MOD). We first review the state-of-the-art deep learning models for MOD, and identify their shortcomings and challenges: (i) the lack of hyper-parameter tuning details, (ii) the non-availability of code for training models on other datasets, and (iii) ignoring the network capability when comparing different architectures. Taking the above issues into account, we experiment with seven deep learning architectures. The most efficient one achieves equivalent performance to our implementation of the state-of-the-art architecture. However, it has only 28.3 of the total number of trainable parameters compared to the state-of-the-art. Our experiments are conducted using two different datasets: one mainly consists of instrumental music excerpts, and another developed by ourselves includes only solo singing voice excerpts. Further, inter-dataset transfer learning experiments are conducted. The results show that the model pre-trained on one dataset fails to detect onsets on another dataset, which denotes the importance of providing the implementation code to enable re-training the model for a different dataset. Datasets, code and a Jupyter notebook running on Google Colab are publicly available to make this research understandable and easy to reproduce.\"","summary":"\"Various audio input representations have been used for the first step of the pipeline, such as filtered logarithmic magnitude and phase spectrum @cite_15 @cite_13 . The former can be subdivided by the filterbank type -- Bark scale bands @cite_7 , Mel scale bands @cite_26 @cite_20 or constant-Q bands @cite_22 @cite_12 .\"","":""}
{"id":"2808040094","dialogue":"\"In this paper, we propose an efficient and reproducible deep learning model for musical onset detection (MOD). We first review the state-of-the-art deep learning models for MOD, and identify their shortcomings and challenges: (i) the lack of hyper-parameter tuning details, (ii) the non-availability of code for training models on other datasets, and (iii) ignoring the network capability when comparing different architectures. Taking the above issues into account, we experiment with seven deep learning architectures. The most efficient one achieves equivalent performance to our implementation of the state-of-the-art architecture. However, it has only 28.3 of the total number of trainable parameters compared to the state-of-the-art. Our experiments are conducted using two different datasets: one mainly consists of instrumental music excerpts, and another developed by ourselves includes only solo singing voice excerpts. Further, inter-dataset transfer learning experiments are conducted. The results show that the model pre-trained on one dataset fails to detect onsets on another dataset, which denotes the importance of providing the implementation code to enable re-training the model for a different dataset. Datasets, code and a Jupyter notebook running on Google Colab are publicly available to make this research understandable and easy to reproduce.\"","summary":"\": The state-of-the-art performance in the MIREX Audio Onset Detection is defined by deep learning-based methods. @cite_26 proposed using recurrent neural networks (RNNs) with LSTM units to predict the input frames binarily as onset or non-onset. Schl \"\" u ter and B \"\" o ck @cite_20 used the similar idea but replaced RNNs by convolutional neural networks (CNNs) and achieved the best performance in the MIREX Audio Onset Detection task. @cite_1 used convolutional-recurrent neural networks (CRNNs) to detect drum onset and produced a better score than CNNs on several percussion datasets.\"","":""}
{"id":"2808040094","dialogue":"\"In this paper, we propose an efficient and reproducible deep learning model for musical onset detection (MOD). We first review the state-of-the-art deep learning models for MOD, and identify their shortcomings and challenges: (i) the lack of hyper-parameter tuning details, (ii) the non-availability of code for training models on other datasets, and (iii) ignoring the network capability when comparing different architectures. Taking the above issues into account, we experiment with seven deep learning architectures. The most efficient one achieves equivalent performance to our implementation of the state-of-the-art architecture. However, it has only 28.3 of the total number of trainable parameters compared to the state-of-the-art. Our experiments are conducted using two different datasets: one mainly consists of instrumental music excerpts, and another developed by ourselves includes only solo singing voice excerpts. Further, inter-dataset transfer learning experiments are conducted. The results show that the model pre-trained on one dataset fails to detect onsets on another dataset, which denotes the importance of providing the implementation code to enable re-training the model for a different dataset. Datasets, code and a Jupyter notebook running on Google Colab are publicly available to make this research understandable and easy to reproduce.\"","summary":"The last step of the pipeline -- onset selection can be done by peak-picking @cite_12 or hidden Markov model (HMM) inference @cite_14 @cite_3 if the musical score is available.","":""}
{"id":"2808467956","dialogue":"\"Traditional image steganography often leans interests towards safely embedding hidden information into cover images with payload capacity almost neglected. This paper combines recent deep convolutional neural network methods with image-into-image steganography. It successfully hides the same size images with a decoding rate of 98.2 or bpp (bits per pixel) of 23.57 by changing only 0.76 of the cover image on average. Our method directly learns end-to-end mappings between the cover image and the embedded image and between the hidden image and the decoded image. We further show that our embedded image, while with mega payload capacity, is still robust to statistical analysis.\"","summary":"\"LSB (Least Significant Bit)-based methods @cite_19 are the most commonly used image domain steganography methods which hide information at the pixel level. Most LSB methods aim at altering parts of the cover image to such an extent that human visual system can barely notice. These methods are motivated by the fact that the visual part of most figures is dominated by the highest bits of each pixel, and the LSB bits (the underlined part of one pixel as shown in Figure ) are statistically similar to randomly generated data, and therefore, hiding information via altering LSB cannot change the visual result apparently.\"","":""}
{"id":"2808467956","dialogue":"\"Traditional image steganography often leans interests towards safely embedding hidden information into cover images with payload capacity almost neglected. This paper combines recent deep convolutional neural network methods with image-into-image steganography. It successfully hides the same size images with a decoding rate of 98.2 or bpp (bits per pixel) of 23.57 by changing only 0.76 of the cover image on average. Our method directly learns end-to-end mappings between the cover image and the embedded image and between the hidden image and the decoded image. We further show that our embedded image, while with mega payload capacity, is still robust to statistical analysis.\"","summary":"\"Since the least significant bits of the image data should look like random data, there are major two schemes in distributing the hiding data. The first kind of methods is to put in the hiding message sequentially after encrypting or compressing to achieve the randomness. The second kind of methods is scattering the hiding data by adopting a mutually acknowledged random seed by which generates the actual hiding sequence @cite_39 .\"","":""}
{"id":"2808467956","dialogue":"\"Traditional image steganography often leans interests towards safely embedding hidden information into cover images with payload capacity almost neglected. This paper combines recent deep convolutional neural network methods with image-into-image steganography. It successfully hides the same size images with a decoding rate of 98.2 or bpp (bits per pixel) of 23.57 by changing only 0.76 of the cover image on average. Our method directly learns end-to-end mappings between the cover image and the embedded image and between the hidden image and the decoded image. We further show that our embedded image, while with mega payload capacity, is still robust to statistical analysis.\"","summary":"\"However, designing and tuning handcrafted patterns are highly technical and might be effective for only some tasks. On the contrary, convolutional neural networks have the advantage of automatically creating patterns for specific tasks through back-propagation @cite_50 on its own, and even further, high-level features can be easily learned through combinations of convolution operations @cite_13 @cite_47 @cite_20 .\"","":""}
{"id":"2808467956","dialogue":"\"Traditional image steganography often leans interests towards safely embedding hidden information into cover images with payload capacity almost neglected. This paper combines recent deep convolutional neural network methods with image-into-image steganography. It successfully hides the same size images with a decoding rate of 98.2 or bpp (bits per pixel) of 23.57 by changing only 0.76 of the cover image on average. Our method directly learns end-to-end mappings between the cover image and the embedded image and between the hidden image and the decoded image. We further show that our embedded image, while with mega payload capacity, is still robust to statistical analysis.\"","summary":"\"Our method is inspired by traditional autoencoder neural networks @cite_5 , which was originally trained to generate an output image the same as input image in appearance. It is usually made up of two neural networks, one encoding network (h = f(x) ) and one decoding network (d = g(h) ), restricted under (d = x ), who finally can learn the conditional probability distribution of (p(h|x) ) and (p(x|h) ) correspondently. The autoencoder architecture has shown the ability to extract salient features in from images seen through shrinking hidden layer ( (h ))'s dimension, which has been applied to various fields, i.e., denoising @cite_18 , dimension reduction @cite_21 , image generation @cite_8 , etc.\"","":""}
{"id":"2808467956","dialogue":"\"Traditional image steganography often leans interests towards safely embedding hidden information into cover images with payload capacity almost neglected. This paper combines recent deep convolutional neural network methods with image-into-image steganography. It successfully hides the same size images with a decoding rate of 98.2 or bpp (bits per pixel) of 23.57 by changing only 0.76 of the cover image on average. Our method directly learns end-to-end mappings between the cover image and the embedded image and between the hidden image and the decoded image. We further show that our embedded image, while with mega payload capacity, is still robust to statistical analysis.\"","summary":"\"Recently there are some works on applying neural networks for steganography. El-Emam @cite_22 and Saleema @cite_1 work on using neural networks to refine the embedded image generated via traditional steganography methods, i.e., LSB method. Volkhonskiy's @cite_48 and Shi's @cite_34 work focus on generating secure cover images for traditional steganography methods to apply image steganography. Baluja @cite_31 is working on the same field as StegNet. However, the hidden image is slightly visible on residual images of the generated embedded images. Moreover, his architecture uses three networks which requires much more GPU memory and takes more time to embed.\"","":""}
{"id":"2807838819","dialogue":"\"IoT deployments have been growing manifold, encompassing sensors, networks, edge, fog and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose VIoLET, a virtual environment for defining and launching large-scale IoT deployments within cloud VMs. It offers a declarative model to specify container-based compute resources that match the performance of the native edge, fog and cloud devices using Docker. These can be inter-connected by complex topologies on which private public networks, and bandwidth and latency rules are enforced. Users can configure synthetic sensors for data generation on these devices as well. We validate VIoLET for deployments with >400 devices and >1500 device-cores, and show that the virtual IoT environment closely matches the expected compute and network performance at modest costs. This fills an important gap between IoT simulators and real deployments.\"","summary":"\"The growing interest in IoT and edge fog computing has given rise to several . @cite_16 extends the prior work on CloudSim @cite_14 to simulate the behavior of applications over fog devices, sensors and actuators that are connected by a network topology. Users define the compute, network and energy profiles of fog devices, and the properties and distributions of tuples from sensors. DAG-based applications with tasks consuming compute capacity and bandwidth can be defined by the user, and its execution over the fog network is simulated using an extensible resource manager. The goal is to evaluate different scheduling strategies synthetically. We similarly let devices, network and sensors to be defined, but actually instantiate the first two -- only the sensor stream is simulated. This allows users to evaluate real applications and schedulers.\"","":""}
{"id":"2807838819","dialogue":"\"IoT deployments have been growing manifold, encompassing sensors, networks, edge, fog and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose VIoLET, a virtual environment for defining and launching large-scale IoT deployments within cloud VMs. It offers a declarative model to specify container-based compute resources that match the performance of the native edge, fog and cloud devices using Docker. These can be inter-connected by complex topologies on which private public networks, and bandwidth and latency rules are enforced. Users can configure synthetic sensors for data generation on these devices as well. We validate VIoLET for deployments with >400 devices and >1500 device-cores, and show that the virtual IoT environment closely matches the expected compute and network performance at modest costs. This fills an important gap between IoT simulators and real deployments.\"","summary":"\"@cite_1 offers similar capabilities, but also introduces mobility models for the edge into the mix. They simulate network characteristics like transmission delay for LAN and WAN, and also task failures due to mobility for a single use-case. , despite its name, simulates the execution of Map Reduce and stream processing tasks on top of a cloud data center, and uses CloudSim as the base simulation engine. While IoT motivates the synthetic application workloads for their big data platform simulation, they do not actually simulate an IoT deployment.\"","":""}
{"id":"2807838819","dialogue":"\"IoT deployments have been growing manifold, encompassing sensors, networks, edge, fog and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose VIoLET, a virtual environment for defining and launching large-scale IoT deployments within cloud VMs. It offers a declarative model to specify container-based compute resources that match the performance of the native edge, fog and cloud devices using Docker. These can be inter-connected by complex topologies on which private public networks, and bandwidth and latency rules are enforced. Users can configure synthetic sensors for data generation on these devices as well. We validate VIoLET for deployments with >400 devices and >1500 device-cores, and show that the virtual IoT environment closely matches the expected compute and network performance at modest costs. This fills an important gap between IoT simulators and real deployments.\"","summary":"\"Other have proposed IoT data stream and application workloads for evaluating big data platforms, particularly stream processing ones. Here, the sensor data is simulated at large-scales while maintaining realistic distributions @cite_18 @cite_2 . These can be used in place of the synthetic sensor streams that we provide. Our prior work has proposed stream and stream processing application workloads for IoT domains @cite_13 . These can potentially use for evaluating execution on edge and fog, besides just cloud resources.\"","":""}
{"id":"2808220856","dialogue":"\"A major challenge in recommender systems is handling new users, whom are also called @math users. In this paper, we propose a novel approach for learning an optimal series of questions with which to interview cold-start users for movie recommender systems. We propose learning interview questions using Deep Q Networks to create user profiles to make better recommendations to cold-start users. While our proposed system is trained using a movie recommender system, our Deep Q Network model should generalize across various types of recommender systems.\"","summary":"\"A lot of recent work in collaborative filtering leverages reinforcement learning techniques. One approach by @cite_12 is to treat recommender systems as a (MDP) and use RL techniques to solve it. @cite_1 also formalize the problem as a MDP and learn the connection between the time sequence of user ratings using Q-learning. @cite_2 formulate the problem as a gridworld game using a a bi-clustering technique to reduce the action and state space substantially. To address the cold-start problem using RL techniques, @cite_13 cast it as a contextual bandit problem. They propose a new method based on the popular LinUCB algorithm @cite_6 for contextual-bandit problems.\"","":""}
{"id":"2808220856","dialogue":"\"A major challenge in recommender systems is handling new users, whom are also called @math users. In this paper, we propose a novel approach for learning an optimal series of questions with which to interview cold-start users for movie recommender systems. We propose learning interview questions using Deep Q Networks to create user profiles to make better recommendations to cold-start users. While our proposed system is trained using a movie recommender system, our Deep Q Network model should generalize across various types of recommender systems.\"","summary":"\"In the past, @cite_7 combined content and collaborative data under a single probabilistic framework. @cite_4 proposed a pairwise predictive system to build feature-based regression models, where user demographic information, item content features, and other information about users and items are utilized to address cold-start problems. The model uses a user's ratings as targets and uses a pair of vectors @math and @math , to predict the rating @math on the item @math given by the user @math , where @math is an index over users and @math is an index over items.\"","":""}
{"id":"2808220856","dialogue":"\"A major challenge in recommender systems is handling new users, whom are also called @math users. In this paper, we propose a novel approach for learning an optimal series of questions with which to interview cold-start users for movie recommender systems. We propose learning interview questions using Deep Q Networks to create user profiles to make better recommendations to cold-start users. While our proposed system is trained using a movie recommender system, our Deep Q Network model should generalize across various types of recommender systems.\"","summary":"\"One successful method for addressing cold-start is a preliminary interview process to learn user's interests for good recommendation. @cite_8 propose building a decision tree for the initial interview process by bootstrapping. They construct a decision tree for the initial interview with each node being an interview question, enabling the recommender to query a user adaptively according to his her prior responses.\"","":""}
{"id":"2808220856","dialogue":"\"A major challenge in recommender systems is handling new users, whom are also called @math users. In this paper, we propose a novel approach for learning an optimal series of questions with which to interview cold-start users for movie recommender systems. We propose learning interview questions using Deep Q Networks to create user profiles to make better recommendations to cold-start users. While our proposed system is trained using a movie recommender system, our Deep Q Network model should generalize across various types of recommender systems.\"","summary":"\"In @cite_3 , the authors build on the previous work and propose . In this, latent profiles are associated for each node of the decision tree and hence the user profile is the function of all possible answers to the interview. The novelty is an iterative optimization algorithm that alternates between decision tree construction and latent profiles' generation. This helps to learn the best decision tree to ask questions and generate good embeddings simultaneously. The profile @math is tied to user @math ’s responses in the form of a function; thus the name functional matrix factorization (fMF). Given an answer set @math , the user profile is generated by @math . The goal is to learn T and @math (item profiles) from the observed ratings set.\"","":""}
{"id":"2963100392","dialogue":"\"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societ al contexts. Several recent works have focused on studying classification with respect to specific fairness metrics","summary":"modeled the corresponding fair classification problem as constrained optimization problems","":""}
{"id":"2963100392","dialogue":"\"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societ al contexts. Several recent works have focused on studying classification with respect to specific fairness metrics","summary":"modeled the corresponding fair classification problem as constrained optimization problems","":""}
{"id":"2963100392","dialogue":"\"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societ al contexts. Several recent works have focused on studying classification with respect to specific fairness metrics","summary":"modeled the corresponding fair classification problem as constrained optimization problems","":""}
{"id":"2963100392","dialogue":"\"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societ al contexts. Several recent works have focused on studying classification with respect to specific fairness metrics","summary":"modeled the corresponding fair classification problem as constrained optimization problems","":""}
{"id":"2963100392","dialogue":"\"Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societ al contexts. Several recent works have focused on studying classification with respect to specific fairness metrics","summary":"modeled the corresponding fair classification problem as constrained optimization problems","":""}
{"id":"2807817052","dialogue":"\"Millimeter wave (mmWave) signals are much more sensitive to blockage, which results in a significant increase of the outage probability, especially for the users at the edge of the cells. In this paper, we exploit the technique of base station (BS) cooperation to improve the performance of the cell-edge users in the downlink transmission of mmWave cellular networks. We design two cooperative schemes, which are referred to as fixed-number BS cooperation (FNC) scheme and fixed-region BS cooperation (FRC) scheme, respectively. In FNC scheme, the cooperative BSs consist of the M nearest BSs around the served cell-edge users, and in FRC scheme, the cooperative BSs include all the BSs located within a given region. We derive the expressions for the average rate and outage probability of a typical cell-edge user located at the origin based on the stochastic geometry framework. To reduce the computational complexity of our analytical results for the outage probability, we further propose a Gamma approximation based method to provide approximations with satisfying accuracy. Our analytical results incorporate the critical characteristics of mmWave channels, i.e., the blockage effects, the different path loss of LOS and NLOS links and the highly directional antenna arrays. Simulation results show that the performance of the cell-edge users is greatly improved when mmWave networks are combined with the technique of BS cooperation.\"","summary":"\"To analyze the performance of the mmWave networks, theoretical channel models have been proposed to describe the new propagation characteristics in mmWave bands @cite_24 @cite_28 @cite_14 @cite_7 @cite_39 @cite_6 . In @cite_24 @cite_28 , the authors proposed a LOS ball model to approximate the irregular LOS region, which was shown to be flexible yet accurate enough to capture the features of the blockage effects in mmWave bands by the field measurements @cite_7 . The LOS ball model was further extended to the two-ball-based blockage model in @cite_14 and the multiple-ball-based blockage model in @cite_6 to account for the three different states of each link, i.e., LOS, NLOS and outage. Based on the established theoretical channel models, the network-wide system performance of the mmWave networks was investigated, e.g., in @cite_7 @cite_39 @cite_0 .\"","":""}
{"id":"2807817052","dialogue":"\"Millimeter wave (mmWave) signals are much more sensitive to blockage, which results in a significant increase of the outage probability, especially for the users at the edge of the cells. In this paper, we exploit the technique of base station (BS) cooperation to improve the performance of the cell-edge users in the downlink transmission of mmWave cellular networks. We design two cooperative schemes, which are referred to as fixed-number BS cooperation (FNC) scheme and fixed-region BS cooperation (FRC) scheme, respectively. In FNC scheme, the cooperative BSs consist of the M nearest BSs around the served cell-edge users, and in FRC scheme, the cooperative BSs include all the BSs located within a given region. We derive the expressions for the average rate and outage probability of a typical cell-edge user located at the origin based on the stochastic geometry framework. To reduce the computational complexity of our analytical results for the outage probability, we further propose a Gamma approximation based method to provide approximations with satisfying accuracy. Our analytical results incorporate the critical characteristics of mmWave channels, i.e., the blockage effects, the different path loss of LOS and NLOS links and the highly directional antenna arrays. Simulation results show that the performance of the cell-edge users is greatly improved when mmWave networks are combined with the technique of BS cooperation.\"","summary":"\"The stochastic geometry framework has been widely adopted to analyze the network-wide performance of cellular networks, including the works in @cite_40 @cite_30 @cite_16 @cite_37 @cite_18 . In @cite_40 @cite_16 , the authors modeled the random locations of the BSs as a homogeneous poisson point process (HPPP). And based on this model, the outage probability of a typical user was derived in single tier cellular network @cite_16 and in multiple tiers heterogeneous networks (HetNets) @cite_37 @cite_30 @cite_18 . Compared with the traditional grid model, using HPPP to model the locations of BSs and mobile users provides analytical tractability while guarantees satisfying accuracy of the analytical results @cite_40 .\"","":""}
{"id":"2807817052","dialogue":"\"Millimeter wave (mmWave) signals are much more sensitive to blockage, which results in a significant increase of the outage probability, especially for the users at the edge of the cells. In this paper, we exploit the technique of base station (BS) cooperation to improve the performance of the cell-edge users in the downlink transmission of mmWave cellular networks. We design two cooperative schemes, which are referred to as fixed-number BS cooperation (FNC) scheme and fixed-region BS cooperation (FRC) scheme, respectively. In FNC scheme, the cooperative BSs consist of the M nearest BSs around the served cell-edge users, and in FRC scheme, the cooperative BSs include all the BSs located within a given region. We derive the expressions for the average rate and outage probability of a typical cell-edge user located at the origin based on the stochastic geometry framework. To reduce the computational complexity of our analytical results for the outage probability, we further propose a Gamma approximation based method to provide approximations with satisfying accuracy. Our analytical results incorporate the critical characteristics of mmWave channels, i.e., the blockage effects, the different path loss of LOS and NLOS links and the highly directional antenna arrays. Simulation results show that the performance of the cell-edge users is greatly improved when mmWave networks are combined with the technique of BS cooperation.\"","summary":"\"Based on the stochastic geometry model, BS cooperation in conventional microwave networks has already been extensively studied in the past few years @cite_2 @cite_20 @cite_9 @cite_17 @cite_10 @cite_1 @cite_26 @cite_34 .\"","":""}
{"id":"2808434342","dialogue":"\"Textual analytics based on representations of documents as bags of words have been reasonably successful. However, analysis that requires deeper insight into language, into author properties, or into the contexts in which documents were created requires a richer representation. Systemic nets are one such representation. They have not been extensively used because they required human effort to construct. We show that systemic nets can be algorithmically inferred from corpora, that the resulting nets are plausible, and that they can provide practical benefits for knowledge discovery problems. This opens up a new class of practical analysis techniques for textual analytics.\"","summary":"\"There have been several applications of predefined systemic nets to textual prediction problems. For example, Whitelaw @cite_12 show improvement in sentiment analysis using the Appraisal Net mentioned above. Argamon show how to predict personality type from authored text, again using systemic functional ideas @cite_1 . Herke-Couchman and Patrick derive interpersonal distance from systemic network attributes @cite_10 .\"","":""}
{"id":"2808434342","dialogue":"\"Textual analytics based on representations of documents as bags of words have been reasonably successful. However, analysis that requires deeper insight into language, into author properties, or into the contexts in which documents were created requires a richer representation. Systemic nets are one such representation. They have not been extensively used because they required human effort to construct. We show that systemic nets can be algorithmically inferred from corpora, that the resulting nets are plausible, and that they can provide practical benefits for knowledge discovery problems. This opens up a new class of practical analysis techniques for textual analytics.\"","summary":"\"Kappagoda @cite_0 shows that word-function tags can be added to words using conditional random fields, in the same kind of general way that parsers add part-of-speech tags to words. These word-function tags provide hints of the systemic-functional role that words carry. This is limited because there is no hierarchy. Nevertheless, he is able to show that the process of labelling can be partially automated and that the resulting tags aid in understanding documents.\"","":""}
{"id":"2964070281","dialogue":"\"Person re-identification (Re-ID) aims to match the image frames which contain the same person in the surveillance videos. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. The significant difference between the source training dataset and the target testing dataset makes it challenging to incrementally optimize the model. To address this challenge, we propose a novel solution by transforming the unlabeled images in the target domain to fit the original classifier by using our proposed similarity preserved generative adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the generative adversarial networks with the cycle consistency constraint to transform the unlabeled images in the target domain to the style of the source domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm is better than the state-of-the-art cross-dataset unsupervised person Re-ID algorithms.\"","summary":"\": Most existing person Re-ID models are supervised, and based on either invariant feature learning @cite_3 , metric learning @cite_10 or deep learning @cite_4 . However, in the practical deployment of Re-ID algorithms in large-scale camera networks, it is usually costly and unpractical to label the massive online surveillance videos to support supervised learning as mentioned in @cite_14 .\"","":""}
{"id":"2964070281","dialogue":"\"Person re-identification (Re-ID) aims to match the image frames which contain the same person in the surveillance videos. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. The significant difference between the source training dataset and the target testing dataset makes it challenging to incrementally optimize the model. To address this challenge, we propose a novel solution by transforming the unlabeled images in the target domain to fit the original classifier by using our proposed similarity preserved generative adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the generative adversarial networks with the cycle consistency constraint to transform the unlabeled images in the target domain to the style of the source domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm is better than the state-of-the-art cross-dataset unsupervised person Re-ID algorithms.\"","summary":"\": In order to improve the effectiveness of the Re-ID algorithms towards large-scale unlabeled datasets, some unsupervised Re-ID methods @cite_0 are proposed to learn cross-view identity-specific information from unlabeled datasets. However, due to the lack of the knowledge about identity labels, these unsupervised approaches usually yield much weaker performance compared to supervised learning approaches.\"","":""}
{"id":"2963977175","dialogue":"\"This paper describes the development of finite abstractions of Max-Plus-Linear (MPL) systems using tropical operations. The idea of tropical abstraction is inspired by the fact that an MPL system is a discrete-event model updating its state with operations in the tropical algebra. The abstract model is a finite-state transition system: we show that the abstract states can be generated by operations on the tropical algebra, and that the generation of transitions can be established by tropical multiplications of matrices. The complexity of the algorithms based on tropical algebra is discussed and their performance is tested on a numerical benchmark against an existing alternative abstraction approach.\"","summary":"\"The notion of abstraction of an MPL system has been first discussed in @cite_4 . The procedure starts by transforming the MPL system characterised by @math into a PWA (piece-wise affine) model [Algorithm 2] Dieky1 , and then considering the partitions associated to the obtained PWA [Algorithm 6] Dieky1 . The abstract states associated to the partitions are represented by DBMs. The transitions are then generated using one-step forward-reachability analysis @cite_4 : first, the image of each abstract state w.r.t. the MPL system is computed; then, each image is intersected with partitions associated to other abstract states; finally, transition relations are defined for each non-empty intersection. This procedure is summarised in [Algorithm 7] Dieky1 .\"","":""}
{"id":"2963977175","dialogue":"\"This paper describes the development of finite abstractions of Max-Plus-Linear (MPL) systems using tropical operations. The idea of tropical abstraction is inspired by the fact that an MPL system is a discrete-event model updating its state with operations in the tropical algebra. The abstract model is a finite-state transition system: we show that the abstract states can be generated by operations on the tropical algebra, and that the generation of transitions can be established by tropical multiplications of matrices. The complexity of the algorithms based on tropical algebra is discussed and their performance is tested on a numerical benchmark against an existing alternative abstraction approach.\"","summary":"\"The computation of image and of inverse image of a DBM is described in @cite_7 . These computations are used to perform forward and backward reachability analysis, respectively. The worst-case complexity of both procedures is @math , where @math is the number of variables in @math excluding @math . A more detailed explanation about image and inverse image computation of a DBM is in Section 3.3.\"","":""}
{"id":"2808393080","dialogue":"\"We investigate the design challenges of constructing effective and efficient neural sequence labeling systems, by reproducing twelve neural sequence labeling models, which include most of the state-of-the-art structures, and conduct a systematic model comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners.\"","summary":"\"proposed a seminal neural architecture for sequence labeling. It captures word sequence information with a one-layer CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. extended this model by integrating character-level CNN features. built a deeper dilated CNN architecture to capture larger local features. was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM @cite_6 @cite_2 , GRU @cite_15 , and CNN @cite_24 @cite_14 features. proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature.\"","":""}
{"id":"2808393080","dialogue":"\"We investigate the design challenges of constructing effective and efficient neural sequence labeling systems, by reproducing twelve neural sequence labeling models, which include most of the state-of-the-art structures, and conduct a systematic model comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners.\"","summary":"\"compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks Based on their detailed experiment report @cite_7 , the F1-scores on CoNLL 2003 NER task are generally less than 90\"","":""}
{"id":"2964328737","dialogue":"\"Abstract Image patch matching, which is the process of identifying corresponding patches across images, has been used as a subroutine for many computer vision and image processing tasks. State -of-the-art patch matching techniques take image patches as input to a convolutional neural network to extract the patch features and evaluate their similarity. Our aim in this paper is to improve on the state of the art patch matching techniques by observing the fact that a sparse-overcomplete representation of an image posses statistical properties of natural visual scenes which can be exploited for patch matching. We propose a new paradigm which encodes image patch details by encoding the patch and subsequently using this sparse representation as input to a neural network to compare the patches. As sparse coding is based on a generative model of natural image patches, it can represent the patch in terms of the fundamental visual components from which it has been composed of, leading to similar sparse codes for patches which are built from similar components. Once the sparse coded features are extracted, we employ a fully-connected neural network, which captures the non-linear relationships between features, for comparison. We have evaluated our approach using the Liberty and Notredame subsets of the popular UBC patch dataset and set a new benchmark outperforming all state-of-the-art patch matching techniques for these datasets.\"","summary":"\"The previous work related to image patch matching can be categorized under three categories: approaches using image intensities, approaches using hand engineered features and approaches using deep learned features. start with using pixel based distance to identify patch correspondence, where @math distance was used to compare pixel values of two patch images. The next stage of this category image patch matching has used normalized correlation between patches to identify their correspondence @cite_1 @cite_14 . Calculating image descriptors such as Scale-invariant feature transform (SIFT) @cite_13 and DAISY @cite_17 and estimating the descriptor distance is the main concept behind the hand engineered feature based methods .\"","":""}
{"id":"2964328737","dialogue":"\"Abstract Image patch matching, which is the process of identifying corresponding patches across images, has been used as a subroutine for many computer vision and image processing tasks. State -of-the-art patch matching techniques take image patches as input to a convolutional neural network to extract the patch features and evaluate their similarity. Our aim in this paper is to improve on the state of the art patch matching techniques by observing the fact that a sparse-overcomplete representation of an image posses statistical properties of natural visual scenes which can be exploited for patch matching. We propose a new paradigm which encodes image patch details by encoding the patch and subsequently using this sparse representation as input to a neural network to compare the patches. As sparse coding is based on a generative model of natural image patches, it can represent the patch in terms of the fundamental visual components from which it has been composed of, leading to similar sparse codes for patches which are built from similar components. Once the sparse coded features are extracted, we employ a fully-connected neural network, which captures the non-linear relationships between features, for comparison. We have evaluated our approach using the Liberty and Notredame subsets of the popular UBC patch dataset and set a new benchmark outperforming all state-of-the-art patch matching techniques for these datasets.\"","summary":"\"In the most recent literature, there are two main approaches which have been motivated by the recent advances in neural networks and deep learning @cite_29 @cite_7 . The aim of these are to generate more robust descriptors which can overcome the drawbacks of hand crafted features such that the descriptors and the matching algorithms are not vulnerable to challenging factors in the patches such as illumination changes , occlusions and shadings.\"","":""}
{"id":"2964328737","dialogue":"\"Abstract Image patch matching, which is the process of identifying corresponding patches across images, has been used as a subroutine for many computer vision and image processing tasks. State -of-the-art patch matching techniques take image patches as input to a convolutional neural network to extract the patch features and evaluate their similarity. Our aim in this paper is to improve on the state of the art patch matching techniques by observing the fact that a sparse-overcomplete representation of an image posses statistical properties of natural visual scenes which can be exploited for patch matching. We propose a new paradigm which encodes image patch details by encoding the patch and subsequently using this sparse representation as input to a neural network to compare the patches. As sparse coding is based on a generative model of natural image patches, it can represent the patch in terms of the fundamental visual components from which it has been composed of, leading to similar sparse codes for patches which are built from similar components. Once the sparse coded features are extracted, we employ a fully-connected neural network, which captures the non-linear relationships between features, for comparison. We have evaluated our approach using the Liberty and Notredame subsets of the popular UBC patch dataset and set a new benchmark outperforming all state-of-the-art patch matching techniques for these datasets.\"","summary":"\"In the approach suggested by Zagoruyko and Komodakis @cite_7 they have evaluated three main neural network architectures for the patch matching. The architectures they have suggested are 1) Siamese, 2) Pseudo-siamese and 3) 2-channel. The siamese and pseudo-siamese architectures contain two branches and as the input each branch takes one of the two patches to be compared. The output of these two branches are analogous to the feature descriptors in the traditional approaches and the branches are merged at the top to make the comparison. In contrast to the siamese architecture where the weights of the two branches are shared in the pseudo siamese architecture the weights are uncoupled. In 2-channel architecture two patches are considered as a 2 channel image where there is no explicit separation on feature descriptor generation and matching. The evaluations on these architectures have been carried out on UBC dataset @cite_9 . Their evaluations conclude that when the convolution layer was divided into small kernels of size @math , 2 channel architecture performs the best.\"","":""}
{"id":"2964328737","dialogue":"\"Abstract Image patch matching, which is the process of identifying corresponding patches across images, has been used as a subroutine for many computer vision and image processing tasks. State -of-the-art patch matching techniques take image patches as input to a convolutional neural network to extract the patch features and evaluate their similarity. Our aim in this paper is to improve on the state of the art patch matching techniques by observing the fact that a sparse-overcomplete representation of an image posses statistical properties of natural visual scenes which can be exploited for patch matching. We propose a new paradigm which encodes image patch details by encoding the patch and subsequently using this sparse representation as input to a neural network to compare the patches. As sparse coding is based on a generative model of natural image patches, it can represent the patch in terms of the fundamental visual components from which it has been composed of, leading to similar sparse codes for patches which are built from similar components. Once the sparse coded features are extracted, we employ a fully-connected neural network, which captures the non-linear relationships between features, for comparison. We have evaluated our approach using the Liberty and Notredame subsets of the popular UBC patch dataset and set a new benchmark outperforming all state-of-the-art patch matching techniques for these datasets.\"","summary":"Matchnet @cite_29 is a convolution neural network (CNN) based approach where the architecture consists of two sub components as feature network - to extract the features of the patches and metric network - to model the similarity between the patches. To train the networks they have used cross entropy error. Their evaluations are also based on the standard UBC dataset @cite_9 .","":""}
{"id":"2964328737","dialogue":"\"Abstract Image patch matching, which is the process of identifying corresponding patches across images, has been used as a subroutine for many computer vision and image processing tasks. State -of-the-art patch matching techniques take image patches as input to a convolutional neural network to extract the patch features and evaluate their similarity. Our aim in this paper is to improve on the state of the art patch matching techniques by observing the fact that a sparse-overcomplete representation of an image posses statistical properties of natural visual scenes which can be exploited for patch matching. We propose a new paradigm which encodes image patch details by encoding the patch and subsequently using this sparse representation as input to a neural network to compare the patches. As sparse coding is based on a generative model of natural image patches, it can represent the patch in terms of the fundamental visual components from which it has been composed of, leading to similar sparse codes for patches which are built from similar components. Once the sparse coded features are extracted, we employ a fully-connected neural network, which captures the non-linear relationships between features, for comparison. We have evaluated our approach using the Liberty and Notredame subsets of the popular UBC patch dataset and set a new benchmark outperforming all state-of-the-art patch matching techniques for these datasets.\"","summary":"\"Coefficients of an encoding mechanism for a set of images such that the accuracy of reconstruction and the sparseness of coefficient are maximized, can possess statistical properties of natural visual scenes @cite_21 @cite_32 . have successfully been used in computer vision applications as well as in signal processing applications in general. While some applications of this concept are image classification @cite_0 , image reconstruction @cite_18 and audio analysis @cite_33 @cite_23 , this concept has not been adopted for patchmatching processes. In the previous approaches of image patch matching visual features have majorly been used. In our approach we use coefficient resulted from over-complete sparse coding of patches (Figure , Figure ) as the feature representation and to estimate the patch similarity we use a neural network while tolerating non linear dependencies in input images.\"","":""}
{"id":"2948641749","dialogue":"\"Recently, a parametrized class of loss functions called @math -loss, @math , has been introduced for classification. This family, which includes the log-loss and the 0-1 loss as special cases, comes with compelling properties including an equivalent margin-based form which is classification-calibrated for all @math . We introduce a generalization of this family to the entire range of @math and establish how the parameter @math enables the practitioner to choose among a host of operating conditions that are important in modern machine learning tasks. We prove that smaller @math values are more conducive to faster optimization; in fact, @math -loss is convex for @math and quasi-convex for @math . Moreover, we establish bounds to quantify the degradation of the local-quasi-convexity of the optimization landscape as @math increases; we show that this directly translates to a computational slow down. On the other hand, our theoretical results also suggest that larger @math values lead to better generalization performance. This is a consequence of the ability of the @math -loss to limit the effect of less likely data as @math increases from 1, thereby facilitating robustness to outliers and noise in the training data. We provide strong evidence supporting this assertion with several experiments on benchmark datasets that establish the efficacy of @math -loss for @math in robustness to errors in the training data. Of equal interest is the fact that, for @math , our experiments show that the decreased robustness seems to counteract class imbalances in training data.\"","summary":"\"Surrogate loss functions for the @math - @math loss have been widely of interest to the machine learning and statistics communities @cite_18 @cite_2 @cite_0 @cite_10 @cite_13 @cite_17 @cite_3 @cite_14 @cite_20 @cite_23 . Recently, there has been renewed interest in alternative losses for classification @cite_6 @cite_3 @cite_23 @cite_17 @cite_5 other than the oft-used log-loss. During the inception of the field, convex losses were widely considered optimal @cite_18 @cite_13 @cite_10 @cite_0 . However, more recent works propose the use of non-convex losses as a means to moderate the behavior of an algorithm @cite_17 @cite_9 @cite_3 @cite_2 . In particular, motivated by superior robustness and classification accuracy of non-convex losses, Mei @cite_17 studied the empirical landscape of such functions. Along these lines, Hazan @cite_7 introduce SLQC as a means to study the unimodality of quasi-convex functions and their optimization characteristics. For SLQC functions, they introduce Normalized Gradient Descent algorithm and prove its convergence guarantees. Using their methodology, we consider @math -loss under logistic regression for binary classification and derive intuition about the operating characteristics of @math -loss.\"","":""}
{"id":"2948641749","dialogue":"\"Recently, a parametrized class of loss functions called @math -loss, @math , has been introduced for classification. This family, which includes the log-loss and the 0-1 loss as special cases, comes with compelling properties including an equivalent margin-based form which is classification-calibrated for all @math . We introduce a generalization of this family to the entire range of @math and establish how the parameter @math enables the practitioner to choose among a host of operating conditions that are important in modern machine learning tasks. We prove that smaller @math values are more conducive to faster optimization; in fact, @math -loss is convex for @math and quasi-convex for @math . Moreover, we establish bounds to quantify the degradation of the local-quasi-convexity of the optimization landscape as @math increases; we show that this directly translates to a computational slow down. On the other hand, our theoretical results also suggest that larger @math values lead to better generalization performance. This is a consequence of the ability of the @math -loss to limit the effect of less likely data as @math increases from 1, thereby facilitating robustness to outliers and noise in the training data. We provide strong evidence supporting this assertion with several experiments on benchmark datasets that establish the efficacy of @math -loss for @math in robustness to errors in the training data. Of equal interest is the fact that, for @math , our experiments show that the decreased robustness seems to counteract class imbalances in training data.\"","summary":"\"Our work is similar to @cite_3 , wherein Nguyen present a tunable sigmoid loss which can be made arbitrarily close to the @math - @math loss. In essence, their loss moves from a smooth to non-smooth loss. Our loss is always smooth and moves from convex to quasi-convex. We find that in the setting of deep neural networks, some quasi-convexity of the @math -loss smooths the empirical landscape. In particular, we provide strong experimental evidence for a narrow range of @math to be used in practice (a limit on the amount of convexity and quasi-convexity of the loss), which significantly reduces the range of hyperparameter tuning induced by @math -loss. Increasing the degree of convexity of the optimization landscape is conducive to faster optimization. Hence, our approach could serve as an alternative to other approaches whose objective is to accelerate the optimization process, e.g., the activation function tuning in @cite_21 @cite_4 @cite_16 and references therein.\"","":""}
{"id":"2948641749","dialogue":"\"Recently, a parametrized class of loss functions called @math -loss, @math , has been introduced for classification. This family, which includes the log-loss and the 0-1 loss as special cases, comes with compelling properties including an equivalent margin-based form which is classification-calibrated for all @math . We introduce a generalization of this family to the entire range of @math and establish how the parameter @math enables the practitioner to choose among a host of operating conditions that are important in modern machine learning tasks. We prove that smaller @math values are more conducive to faster optimization; in fact, @math -loss is convex for @math and quasi-convex for @math . Moreover, we establish bounds to quantify the degradation of the local-quasi-convexity of the optimization landscape as @math increases; we show that this directly translates to a computational slow down. On the other hand, our theoretical results also suggest that larger @math values lead to better generalization performance. This is a consequence of the ability of the @math -loss to limit the effect of less likely data as @math increases from 1, thereby facilitating robustness to outliers and noise in the training data. We provide strong evidence supporting this assertion with several experiments on benchmark datasets that establish the efficacy of @math -loss for @math in robustness to errors in the training data. Of equal interest is the fact that, for @math , our experiments show that the decreased robustness seems to counteract class imbalances in training data.\"","summary":"\"For binary classification, where @math , it is common to use classification functions of the form @math such that the classifier, for any given @math , outputs the hypothesis (hard decision) @math @cite_18 @cite_0 @cite_10 @cite_2 @cite_9 . A classification function corresponds to the certainty of an algorithm's prediction (e.g., SVM). Examples of loss functions that act on classification functions include logistic loss, hinge loss, and square loss.\"","":""}
{"id":"2948641749","dialogue":"\"Recently, a parametrized class of loss functions called @math -loss, @math , has been introduced for classification. This family, which includes the log-loss and the 0-1 loss as special cases, comes with compelling properties including an equivalent margin-based form which is classification-calibrated for all @math . We introduce a generalization of this family to the entire range of @math and establish how the parameter @math enables the practitioner to choose among a host of operating conditions that are important in modern machine learning tasks. We prove that smaller @math values are more conducive to faster optimization; in fact, @math -loss is convex for @math and quasi-convex for @math . Moreover, we establish bounds to quantify the degradation of the local-quasi-convexity of the optimization landscape as @math increases; we show that this directly translates to a computational slow down. On the other hand, our theoretical results also suggest that larger @math values lead to better generalization performance. This is a consequence of the ability of the @math -loss to limit the effect of less likely data as @math increases from 1, thereby facilitating robustness to outliers and noise in the training data. We provide strong evidence supporting this assertion with several experiments on benchmark datasets that establish the efficacy of @math -loss for @math in robustness to errors in the training data. Of equal interest is the fact that, for @math , our experiments show that the decreased robustness seems to counteract class imbalances in training data.\"","summary":"\"While the previous result establishes an equivalent margin-based form for @math -loss, our next result establishes some of its basic optimization characteristics. Finally, we conclude this section with another basic property of @math -loss that highlights its suitability for classification. For binary classification and margin-based losses, Bartlett in @cite_18 introduce as a means to compare the performance of a loss function relative to the 0-1 loss. A margin-based loss function @math is classification-calibrated if its minimum conditional risk given @math is attained by a @math such that @math , where @math is the true posterior. Building upon such a result in @cite_9 for @math -loss, the following proposition shows that @math is classification-calibrated for all @math . The proof of Proposition is given in Appendix .\"","":""}
{"id":"2948135979","dialogue":"\"Exploration strategy design is one of the challenging problems in reinforcement learning (RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward (quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called c lustered r einforcement l earning (CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area (cluster) of the current state is given to the agent. Experiments on a continuous control task and several games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.\"","summary":"\"In the tabular setting, there is a finite number of state-action pairs that can directly define a decreasing function of the true visitation count as the exploration bonus. MBIE-EB @cite_6 adds the square root of counts of state-action pairs as the bonus reward to the augmented Bellman equation for exploring less visited ones with theoretical guarantee.\"","":""}
{"id":"2948135979","dialogue":"\"Exploration strategy design is one of the challenging problems in reinforcement learning (RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward (quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called c lustered r einforcement l earning (CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area (cluster) of the current state is given to the agent. Experiments on a continuous control task and several games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.\"","summary":"\"In finite MDPs, @math @cite_12 , R-Max @cite_28 and UCRL @cite_11 all make use of state-action counts and are activated by the idea of optimism under uncertainty. @math @cite_12 determines online to choose an efficient learning policy. R-Max @cite_28 assumes the received reward is not in quality area and trains a fictitious model to learn the optimal policy. UCRL @cite_11 chooses an optimistic policy by using upper confidence bounds. Bayesian RL methods maintain a distribution of belief state as the uncertainty over possible MDPs @cite_25 @cite_19 @cite_13 @cite_3 and use counts to explore .\"","":""}
{"id":"2948135979","dialogue":"\"Exploration strategy design is one of the challenging problems in reinforcement learning (RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward (quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called c lustered r einforcement l earning (CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area (cluster) of the current state is given to the agent. Experiments on a continuous control task and several games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.\"","summary":"\"In the continuous and high-dimensional space, the number of states is too large to be counted. @cite_23 @cite_1 , the exploration bonus reward is designed based on a state pseudo-count quantity, which is estimated from a density model. In the hash-based method @cite_10 , the hash function encodes states to hash codes and then it explores with the reciprocal of visitation as a reward bonus, which performs well on some hard exploration games. Hash-based method is limited by the hash function. Static hashing, using locality-sensitive hashing, is stable but random. Learned hashing, using an autoencoder (AE) to capture the semantic features, updates during the training time. A related work is @cite_7 , which record the number of cluster center and action pairs which used to select an action from the Gibbs distribution given to a state.\"","":""}
{"id":"2806956262","dialogue":"\"We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work morcos2018importance . However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.\"","summary":"\"Whole networks have been studied to characterize their learning ability. It has been demonstrated @cite_12 that the same classification networks that successfully generalize meaningful labels are also able to memorize data with meaningless random labels. Conversely, it has been observed @cite_5 that networks are susceptible to being fooled on tiny perturbations of test inputs. These observations have challenged traditional assumptions about what a network learns, and why. Our current work contributes to the effort to characterize the mechanisms deep networks use to achieve generalization.\"","":""}
{"id":"2806956262","dialogue":"\"We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work morcos2018importance . However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.\"","summary":"\"Networks have also have been studied at the granularity of individual layers. It has been found that the representations produced by a layer summarize the lessons learned in training in a meaningful way @cite_3 : low-level layers can be used to summarize simpler patterns, and higher-level layers encode more abstract representations. Layers trained on one problem can be reused to transfer knowledge to other problems @cite_6 , with a small amount of additional training. This fact has profound practical importance, reducing the need to build separate large training sets for every problem.\"","":""}
{"id":"2806956262","dialogue":"\"We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work morcos2018importance . However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.\"","summary":"\"Finally, the role of individual network units has been studied. It has been found that individual units are sensitive to specific concepts that can be visualized by generating or sampling inputs that maximize unit activations @cite_1 @cite_14 , and that unit directions match meaningful concepts more closely than other random directions in representation space @cite_2 . However, whether meaningful units are helpful or harmful to a network's ability to generalize has been questioned: @cite_18 found that units that are selective to one class do not appear to damage overall classification performance more than other units when removed from a network. Our current work is a further examination of the impact of individual units on generalization accuracy: we ask, what is the role of a single unit in contributing to classification performance, and what characteristics of units are predictive of their contribution?\"","":""}
{"id":"2805042136","dialogue":"\"Temporal action proposal generation is an important yet challenging problem","summary":"since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries","":""}
{"id":"2805042136","dialogue":"\"Temporal action proposal generation is an important yet challenging problem","summary":"since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries","":""}
{"id":"2805042136","dialogue":"\"Temporal action proposal generation is an important yet challenging problem","summary":"since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries","":""}
{"id":"2805042136","dialogue":"\"Temporal action proposal generation is an important yet challenging problem","summary":"since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries","":""}
{"id":"2807538047","dialogue":"\"In many face recognition applications, there is large amount of face data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (relatively small class number) and sufficient depth (many samples for each class). They would meet great challenges when applied on this ID vs. Spot (IvS) data, including the under-represented intra-class variations and the excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem that there are only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning applicable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance than previous ones, validating the effectiveness of LBL on IvS face recognition.\"","summary":"\"Recently there are two schemes to train deep models for face recognition: classification and verification. The classification scheme considers each identity as a unique category and classifies each sample into one of the classes. During testing, the classification layer is removed and the top-level feature is regarded as the face representation @cite_22 . The most popular loss is softmax @cite_22 @cite_6 @cite_42 . Based on that, the center loss @cite_3 proposes to learn the class-specific feature centers to make features more compact in the embedding space. The L2-softmax @cite_9 adds a L2-constraint on features to promote the under-represented classes. The normface @cite_43 normalizes both features and prototypes to make the training and testing phases closer. Recently, enhancing margins between different classes is found to be effective in improving feature discrimination, including large-margin softmax @cite_45 , A-softmax @cite_58 , GA-softmax @cite_61 and AM-softmax @cite_11 . Benefiting from the prototypes in the classification layer, the scheme can distinguish a sample from all the other classes, leading to fast convergence and good generalization ability @cite_43 .\"","":""}
{"id":"2807538047","dialogue":"\"In many face recognition applications, there is large amount of face data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (relatively small class number) and sufficient depth (many samples for each class). They would meet great challenges when applied on this ID vs. Spot (IvS) data, including the under-represented intra-class variations and the excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem that there are only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning applicable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance than previous ones, validating the effectiveness of LBL on IvS face recognition.\"","summary":"\"Most contemporary face recognition methods are based on wild datasets, e.g., CASIA-Webface @cite_40 , Ms-Celeb-1M @cite_30 , MF2 @cite_53 and VGG2 @cite_19 . These well-posed datasets have a limited number of identities and sufficient samples per identity. However, this is not the case in IvS datasets. Table gives a brief comparison between wild and IvS datasets. Our CASIA-IvS has more than @math million identities but only two samples per identity, on which existing well-studied methods cannot work well any more. Exploring IvS-specific training strategies is necessary.\"","":""}
{"id":"2807538047","dialogue":"\"In many face recognition applications, there is large amount of face data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (relatively small class number) and sufficient depth (many samples for each class). They would meet great challenges when applied on this ID vs. Spot (IvS) data, including the under-represented intra-class variations and the excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem that there are only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning applicable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance than previous ones, validating the effectiveness of LBL on IvS face recognition.\"","summary":"\"intends to recognize new classes by few samples @cite_17 . Generally, low-shot learning transfers the knowledge from a well-posed source domain to the low-shot target domain. Siamese net @cite_49 trains a siamese CNN by same-or-different classification on the source domain and extracts the deep features for nearest neighbour matching in the target domain. MANN @cite_33 @cite_66 @cite_52 memorizes the features of examples in the source domain to help predict the under-labeled classes. Model regression @cite_51 @cite_59 directly transfers the neural network weights across domains. The L2-regularization on features @cite_24 @cite_44 @cite_62 can prevent the network from ignoring low-shot classes. Besides, virtual sample generation @cite_44 @cite_23 and semi-supervised samples @cite_7 are found effective in promoting low-shot classes. Although both low-shot learning and bisample learning intend to learn a concept with insufficient samples, they differ in that low-shot learning is close-set classification but bisample learning is open-set classification where the testing samples definitely belong to unseen classes.\"","":""}
{"id":"2807538047","dialogue":"\"In many face recognition applications, there is large amount of face data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (relatively small class number) and sufficient depth (many samples for each class). They would meet great challenges when applied on this ID vs. Spot (IvS) data, including the under-represented intra-class variations and the excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem that there are only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning applicable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance than previous ones, validating the effectiveness of LBL on IvS face recognition.\"","summary":"\"refers to the situation that only a limited number of classes appear frequently, while most of the others remain far less existing. Deep models trained on long-tailed data tend to ignore the classes in the tail. To resolve the problem, @cite_64 retrieves more samples from the tail classes. @cite_25 makes samples uniformly distributed by random sampling. @cite_31 proposes a range loss to balance the rich and poor classes, where the largest intra-class distance is reduced and the shortest class-center distance is enlarged.\"","":""}
{"id":"2807538047","dialogue":"\"In many face recognition applications, there is large amount of face data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (relatively small class number) and sufficient depth (many samples for each class). They would meet great challenges when applied on this ID vs. Spot (IvS) data, including the under-represented intra-class variations and the excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem that there are only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning applicable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance than previous ones, validating the effectiveness of LBL on IvS face recognition.\"","summary":"\"Large-scale classification aims to perform classification on a vast number of classes, where the class number reaches millions or tens of millions. This task presents a great problem for deep learning: the common softmax loss can not be adopted due to the prohibitive parameter size and computation cost. The Megaface challenge @cite_53 proposes four methods for training models on @math k identities. Model-A trains the network on random @math identities via softmax. Model-B finetunes Model-A on all the @math k identities with the triplet loss. Model-C adopts rotating softmax that randomly selects @math identities every @math epoches. After each rotation the parameters in the softmax layer are randomly initialized. Model-D further triplet-finetunes Model-C on all the identities.\"","":""}
{"id":"2948039154","dialogue":"\"A Cyber Physical System (CPS) consists of cyber components for computation and communication, and physical components such as sensors and actuators for process control. These components are networked and interact in a feedback loop. CPS are found in critical infrastructure such as water distribution, power grid, and mass transportation. Often these systems are vulnerable to attacks as the cyber components such as Supervisory Control and Data Acquisition workstations, Human Machine Interface and Programmable Logic Controllers are potential targets for attackers. In this work, we report a study to investigate the impact of cyber attacks on a water distribution (WADI) system. Attacks were designed to meet attacker objectives and launched on WADI using a specially designed tool. This tool enables the launch of single and multi-point attacks where the latter are designed to specifically hide one or more attacks. The outcome of the experiments led to a better understanding of attack propagation and behavior of WADI in response to the attacks as well as to the design of an attack detection mechanism for water distribution system.\"","summary":"\"Researchers have presented challenges in safety and security against cyber attacks that need to be addressed while designing a CPS , @cite_36 @cite_19 @cite_2 . , @cite_1 explained the integration of IoT and SCADA systems with a focus on security and how to integrate and create intelligent ICS using the Internet. @cite_20 surveyed literature on cyber physical systems security, and presented a orthogonal framework consists of security, components, system perspectives. They focused mainly on four CPS systems such as ICS, smart grids, medical devices, and smart cars.\"","":""}
{"id":"2948039154","dialogue":"\"A Cyber Physical System (CPS) consists of cyber components for computation and communication, and physical components such as sensors and actuators for process control. These components are networked and interact in a feedback loop. CPS are found in critical infrastructure such as water distribution, power grid, and mass transportation. Often these systems are vulnerable to attacks as the cyber components such as Supervisory Control and Data Acquisition workstations, Human Machine Interface and Programmable Logic Controllers are potential targets for attackers. In this work, we report a study to investigate the impact of cyber attacks on a water distribution (WADI) system. Attacks were designed to meet attacker objectives and launched on WADI using a specially designed tool. This tool enables the launch of single and multi-point attacks where the latter are designed to specifically hide one or more attacks. The outcome of the experiments led to a better understanding of attack propagation and behavior of WADI in response to the attacks as well as to the design of an attack detection mechanism for water distribution system.\"","summary":"\"Attacks have been modeled as noise in sensor data , @cite_49 . Attack models designed specifically for CPS include a variety of deception attacks including surge, bias, and geometric , @cite_3 . Such models have been used in experiments to understand the effectiveness of statistical techniques in detecting cyber attacks. The attacks designed in this work are based on a cyber-physical attacker model , @cite_21 . @cite_43 proposed a detailed procedure for modeling cyber systems using attack graphs. Such graphs model practical vulnerabilities in distributed networked systems. , @cite_24 have proposed argument graphs as a means to capture the workflow in a CPS. The graphs are intended to assess a system in the presence of an attacker. The graphs are formed based on information in the workflow such as use case or state, physical system topology such as network type, and an attacker model such as an order to interrupt, power supply, physical tampering, network connection, denial of service, etc. Typed graphs , @cite_35 and Bayesian defense graphs , @cite_30 are a few other important contributions to the modeling of cyber attacks.\"","":""}
{"id":"2948039154","dialogue":"\"A Cyber Physical System (CPS) consists of cyber components for computation and communication, and physical components such as sensors and actuators for process control. These components are networked and interact in a feedback loop. CPS are found in critical infrastructure such as water distribution, power grid, and mass transportation. Often these systems are vulnerable to attacks as the cyber components such as Supervisory Control and Data Acquisition workstations, Human Machine Interface and Programmable Logic Controllers are potential targets for attackers. In this work, we report a study to investigate the impact of cyber attacks on a water distribution (WADI) system. Attacks were designed to meet attacker objectives and launched on WADI using a specially designed tool. This tool enables the launch of single and multi-point attacks where the latter are designed to specifically hide one or more attacks. The outcome of the experiments led to a better understanding of attack propagation and behavior of WADI in response to the attacks as well as to the design of an attack detection mechanism for water distribution system.\"","summary":"\"Attack detection in water systems: Mitchel and Chen surveyed , @cite_27 intrusion detection techniques for CPS. They presented existing works based on a classification tree. They also presented the advantages and limitations of the techniques. The use of invariants for detecting attacks on CPS has been proposed and evaluated by several researchers such as in , @cite_18 @cite_51 @cite_55 . In this work it is claimed that the use of controlled invariant sets in detecting cyber attacks uses little information about the controller and hence is useful for a large range of control laws. Yuqi et. al. , @cite_12 proposed an approach for learning physical invariants that combine machine learning with ideas from mutation testing. Data driven , @cite_4 @cite_28 approaches for attack detection is studied on a water treatment system.\"","":""}
{"id":"2948619677","dialogue":"\"Miniaturization and cost, two of the main attractive factors of swarm robotics, have motivated its use as a solution in object collecting tasks, search & rescue missions, and other applications. However, in the current literature only a few papers consider energy allocation efficiency within a swarm. Generally, robots recharge to their maximum level every time unconditionally, and do not incorporate estimates of the energy needed for their next task. In this paper we present an energy efficiency maximization method that minimizes the overall energy cost within a swarm while simultaneously maximizing swarm performance on an object gathering task. The method utilizes dynamic thresholds for upper and lower battery limits. This method has also shown to improve the efficiency of existing energy management methods.\"","summary":"\"Liu focused on energy efficiency by changing the foraging time for each robot @cite_10 . The time spent foraging for resources depended on various cues such as personal successful food retrievals, collisions with teammates for food, and success among other robots in food retrieval. So the method was able to find an adaptable and optimal time for foraging, but did not optimize the amount of energy allocated for it.\"","":""}
{"id":"2948619677","dialogue":"\"Miniaturization and cost, two of the main attractive factors of swarm robotics, have motivated its use as a solution in object collecting tasks, search & rescue missions, and other applications. However, in the current literature only a few papers consider energy allocation efficiency within a swarm. Generally, robots recharge to their maximum level every time unconditionally, and do not incorporate estimates of the energy needed for their next task. In this paper we present an energy efficiency maximization method that minimizes the overall energy cost within a swarm while simultaneously maximizing swarm performance on an object gathering task. The method utilizes dynamic thresholds for upper and lower battery limits. This method has also shown to improve the efficiency of existing energy management methods.\"","summary":"\"Stirling took a novel approach at finding energy efficient searching algorithms for flying swarms in indoor environments @cite_4 . The strategy involves having robots create a network of beacons that communicate with each other to direct where other robots in the swarm should go. When an exploring robot arrives at an unexplored location, it becomes a beacon to help sense for the other exploring robots. They found that launching the robots incrementally rather than all at once decreased total energy consumption as well as collision rate but increased search time. Their method provides a trade-off between energy consumption and search time. Once again the metric is energy consumption and not energy allocation so it ignores the unused charge in each robot after the task was done.\"","":""}
{"id":"2948619677","dialogue":"\"Miniaturization and cost, two of the main attractive factors of swarm robotics, have motivated its use as a solution in object collecting tasks, search & rescue missions, and other applications. However, in the current literature only a few papers consider energy allocation efficiency within a swarm. Generally, robots recharge to their maximum level every time unconditionally, and do not incorporate estimates of the energy needed for their next task. In this paper we present an energy efficiency maximization method that minimizes the overall energy cost within a swarm while simultaneously maximizing swarm performance on an object gathering task. The method utilizes dynamic thresholds for upper and lower battery limits. This method has also shown to improve the efficiency of existing energy management methods.\"","summary":"\"Labella took inspiration from ants. He created an adaptation method that controls the number of robots foraging in the environment @cite_0 . Each robot has a probability variable that increases and decreases based on the number of successes and failures the robot had when foraging. The probability variable dictates the probability that the specific robot would leave the nest and start foraging. This allows for robots who consistently find and retrieve prey to keep foraging, while also eventually reaching an equilibrium or state in which the necessary number of robots are active. Thus, it efficiently uses energy when needed, similarly to @cite_10 @cite_4 in that they measure the time it takes to finish foraging but not how much energy was already allocated.\"","":""}
{"id":"2805206884","dialogue":"\"Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset levesque2011winograd . In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.\"","summary":"\"Neural LMs have also been applied successfully to improve downstream applications @cite_35 @cite_25 @cite_27 @cite_14 . @cite_35 @cite_25 @cite_27 @cite_14 , researchers have shown that pre-trained LMs can be used as feature representations for a sentence, or a paragraph to improve NLP applications such as document classification, machine translation, question answering, etc. The combined evidence suggests that LMs trained on a massive amount of unlabeled data can capture many aspects of natural language and the world's knowledge, especially commonsense information.\"","":""}
{"id":"2805206884","dialogue":"\"Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset levesque2011winograd . In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.\"","summary":"\"Previous attempts on solving the Winograd Schema Challenge usually involve heavy utilization of annotated knowledge bases","":""}
{"id":"2805206884","dialogue":"\"Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset levesque2011winograd . In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.\"","summary":"\"Using language models in reading comprehension tests also produced many great successes. Namely @cite_33 used bi-directional RNNs to predict the last word of a passage in the LAMBADA challenge. Similarly, LMs are also used to produce features for a classifier in the Store Close Test 2017, giving best accuracy against other methods @cite_36 . In a broader context, LMs are used to produce good word embeddings, significantly improved a wide variety of downstream tasks, including the general problem of question answering @cite_27 @cite_7 .\"","":""}
{"id":"2805228397","dialogue":"\"Autonomous exploration of unknown environments has been widely applied in inspection, surveillance, and search and rescue. In exploration task, the basic requirement for robots is to detect the unknown space as fast as possible. In this paper, we propose an autonomous collaborative system consists of an aerial robot and a ground vehicle to explore in unknown environments. We combine the frontier based method and the harmonic field to generate a path. Then, For the ground robot, a minimum jerk piecewise Bezier curve which can guarantee safety and dynamical feasibility is generated amid obstacles. For the aerial robot, a motion primitive method is adopted for local path planning. We implement the proposed framework on an autonomous collaborative aerial-ground system. Extensive field experiments as well as simulations are presented to validate the method and demonstrate its higher efficiency against each single vehicle.\"","summary":"\"Exploration of unknown environments using autonomous robots has been considered as a fundamental problem in robotics applications such as search and rescue @cite_0 , industrial inspection and 3D modelling. For exploration, the basic requirement for robots is to scan unknown space or detect free space as fast as possible. UGV (unmanned ground vehicle) @cite_2 and UAV (unmanned aerial vehicle) @cite_8 both have been employed for such a task with differences primarily in: (1) UGVs are more payload-capable. A ground vehicle can carry heavy, long-range laser scanners which are inapplicable for weight-constrained UAVs; (2) UAVs have superiors mobility and agility. UAV can fly above obstacles and cover areas that are inaccessible to UGVs, like obstacle's top surfaces. Consequently, a UGV often enjoys a larger sensor-coverage, yet cluttered and view-blocking environments could hamper its performance; on the other hand, a UAV may deliver inferior exploration efficiency due to its short-range sensor, but enjoys unblocked downward-looking view. Therefore, UGV favors open areas while UAV prefers cluttered places.\"","":""}
{"id":"2805228397","dialogue":"\"Autonomous exploration of unknown environments has been widely applied in inspection, surveillance, and search and rescue. In exploration task, the basic requirement for robots is to detect the unknown space as fast as possible. In this paper, we propose an autonomous collaborative system consists of an aerial robot and a ground vehicle to explore in unknown environments. We combine the frontier based method and the harmonic field to generate a path. Then, For the ground robot, a minimum jerk piecewise Bezier curve which can guarantee safety and dynamical feasibility is generated amid obstacles. For the aerial robot, a motion primitive method is adopted for local path planning. We implement the proposed framework on an autonomous collaborative aerial-ground system. Extensive field experiments as well as simulations are presented to validate the method and demonstrate its higher efficiency against each single vehicle.\"","summary":"\"For robotics exploration, @cite_2 first proposed the concept of frontier, which is defined as unknown grid-map cells adjacent to free ones and thus represents accessible new information. Harmonic function, the solution to @math , is used to plan path to frontiers @cite_5 . This method generates a scalar field in free-space based on its surrounding boundary conditions (occupied cells and frontier cells) and obtains the path using gradient-descent. For air-ground exploration, @cite_4 uses the UAV as an back-up instead of an independent explorer. It is only deployed when UGV encounters high, invisible areas. @cite_12 is also proposed based on the same spirit that one vehicle helps another, failing to exploit both vehicles' full potential. Compared to these works, the collaborative system proposed in this paper fully utilizes advantages of different vehicles and thus results in a more efficient exploration. We summarize our contribution as: 1. An efficient exploration framework that combines UAV and UGV's advantages. 2. A more efficient computation method of harmonic function for robotic exploration tasks. 3. Integration of the proposed collaborative exploration framework with the state estimation, sensor fusion and trajectory optimization. Extensive field experiments are presented to validate the efficiency and robustness of the proposed method.\"","":""}
{"id":"2950870928","dialogue":"\"We propose accelerated randomized coordinate descent algorithms for stochastic optimization and online learning. Our algorithms have significantly less per-iteration complexity than the known accelerated gradient algorithms. The proposed algorithms for online learning have better regret performance than the known randomized online coordinate descent algorithms. Furthermore, the proposed algorithms for stochastic optimization exhibit as good convergence rates as the best known randomized coordinate descent algorithms. We also show simulation results to demonstrate performance of the proposed algorithms.\"","summary":"\"Stochastic gradient descent (SGD) @cite_0 was introduced by where as Online convex optimization and the associated projected gradient (OGD) @cite_4 were introduced by Zinkevich . derived accelerated versions of SGD and OGD; they refer to these algorithms as Stochastic Accelerated GradiEnt (SAGE) @cite_6 . Recently, proposed stochastic average gradient (SAG) @cite_14 and Johnson and Zhang proposed Stochastic variation reduced gradient (SVRG) @cite_7 , both aimed at improving the convergence rate of SGD. @cite_5 and McMahan and Streeter studied delay tolerant OGD algorithms @cite_13 where parameter updates are based on stale gradient information. Cyclic block coordinate descent algorithms were introduced by Luo and Tseng @cite_8 @cite_17 . Nesterov proposed randomized block coordinate descent (RBCD) @cite_12 algorithms for large scale optimization problems. Fercoq and Richtarik @cite_16 and @cite_1 proposed accelerated randomized block coordinate algorithms. More recently, Allen- @cite_10 proposed faster accelerated coordinate descent methods in which sampling frequencies depend on coordinate wise smoothness parameters (i.e., Lipschitz parameters of the corresponding partial derivatives).\"","":""}
{"id":"2950870928","dialogue":"\"We propose accelerated randomized coordinate descent algorithms for stochastic optimization and online learning. Our algorithms have significantly less per-iteration complexity than the known accelerated gradient algorithms. The proposed algorithms for online learning have better regret performance than the known randomized online coordinate descent algorithms. Furthermore, the proposed algorithms for stochastic optimization exhibit as good convergence rates as the best known randomized coordinate descent algorithms. We also show simulation results to demonstrate performance of the proposed algorithms.\"","summary":"\"More recently, there has been interest in machine learning settings in which training data and features are distributed across nodes of a computing cluster, or more generally, of a network. Nathan and Klabjan @cite_15 proposed an algorithm where nodes parallelly update (possibly overlapping) blocks of feature parameters based on locally available data samples; this was seen as a combination of SVRG and block coordinate descent. @cite_18 studied algorithms for nodes connected through a network; this scenario was referred to as federated learning.\"","":""}
{"id":"2805930887","dialogue":"\"We investigate the properties of multidimensional probability distributions in the context of latent space prior distributions of implicit generative models. Our work revolves around the phenomena arising while decoding linear interpolations between two random latent vectors -- regions of latent space in close proximity to the origin of the space are sampled causing distribution mismatch. We show that due to the Central Limit Theorem, this region is almost never sampled during the training process. As a result, linear interpolations may generate unrealistic data and their usage as a tool to check quality of the trained model is questionable. We propose to use multidimensional Cauchy distribution as the latent prior. Cauchy distribution does not satisfy the assumptions of the CLT and has a number of properties that allow it to work well in conjunction with linear interpolations. We also provide two general methods of creating non-linear interpolations that are easily applicable to a large family of common latent distributions. Finally we empirically analyze the quality of data generated from low-probability-mass regions for the DCGAN model on the CelebA dataset.\"","summary":"\"Some approaches to counteract this phenomena were proposed: recommended using spherical interpolations to avoid traversing unlikely regions; @cite_9 suggest normalizing the norms of the points along the interpolation to match the prior distribution; propose using a modified prior distribution, which saturates the origin of the latent space. gives an interesting discussion on latent space traversal using the theory of Riemannian spaces.\"","":""}
{"id":"2953177585","dialogue":"\"Stochastic Gradient Descent (SGD) is a central tool in machine learning. We prove that SGD converges to zero loss, even with a fixed (non-vanishing) learning rate - in the special case of homogeneous linear classifiers with smooth monotone loss functions, optimized on linearly separable data. Previous works assumed either a vanishing learning rate, iterate averaging, or loss assumptions that do not hold for monotone loss functions used for classification, such as the logistic loss. We prove our result on a fixed dataset, both for sampling with or without replacement. Furthermore, for logistic loss (and similar exponentially-tailed losses), we prove that with SGD the weight vector converges in direction to the @math max margin vector as @math for almost all separable datasets, and the loss converges as @math - similarly to gradient descent. Lastly, we examine the case of a fixed learning rate proportional to the minibatch size. We prove that in this case, the asymptotic convergence rate of SGD (with replacement) does not depend on the minibatch size in terms of epochs, if the support vectors span the data. These results may suggest an explanation to similar behaviors observed in deep networks, when trained with SGD.\"","summary":"\"Second, among other results, also examined optimizing logistic regression with SGD on a fixed dataset using random sampling with replacement, iterate averaging and a vanishing learning rate. There, in Theorems 3.2 and 3.3, it is shown that the expectation of the loss converges as @math and the expectation of the averaged iterates converges in the norm as @math , which is slower than our result. Thus, in contrast to both works @cite_2 @cite_6 , we did not assume iterate averaging or decreasing learning rate. Additionally, our new results on sampling with replacement give a linear relationship between the learning rate and the minibatch size, and Corollary shows the affect of the minibatch size on the asymptotic convergence rate.\"","":""}
{"id":"2806235972","dialogue":"\"With the advent of the big data, graph are processed in an iterative manner, which incrementally described in the form of graph in big data applications. Most currently, graph processing methods treat the underlying map data as black boxes. However, as shown in experimental evaluation, graph structures often have diversity, different graph processing methods are very sensitive to the graph structure and show different performance for different data sets. Based on this, a graph processing method for graph structure analysis is proposed in this paper: (1) This paper calculates the vertex activity of a graph according to the in-degree and out-degree, and divide the corresponding vertices into the hot or cold partitions; (2) According to the change of graph structure caused by partial vertex convergence after iteration, this paper reclassifies the partitions, divides the lower active vertices into cold partition and reduces the frequency of calculation, which thereby reducing the cache miss rate and the I O overhead caused by active vertices as well; (3) The partition with highest vertex status degree are given a priority calculation in this paper. In detail, more pronounced and more frequent vertices have higher processing priority. In this way, the convergence speed of the graph vertices is accelerated, and the running time of the graph algorithm in the big data environment is reduced. Our experiments show that compared with the latest system, the proposed method can double the performance of different graph algorithms and data sets.\"","summary":"\"Pregel @cite_11 divide the graph by hashing the vertex id which ensure loading balancing. Yet Pregel uses message communication paradigm, messages that need to be processed will be huge when vertices has many adjacent points. Coincidentally, Pregel performs inefficiency in power-law graph and only allows global synchronization. X-Pregel @cite_25 optimizes Pregel's messaging mechanism by reducing the number of messages that needed to be delivered in every iteration. Giraph @cite_26 adds more features compare to Pregel, including master computation, out-of-core computation, etc. But the poor locality of data access limits its effective.\"","":""}
{"id":"2806235972","dialogue":"\"With the advent of the big data, graph are processed in an iterative manner, which incrementally described in the form of graph in big data applications. Most currently, graph processing methods treat the underlying map data as black boxes. However, as shown in experimental evaluation, graph structures often have diversity, different graph processing methods are very sensitive to the graph structure and show different performance for different data sets. Based on this, a graph processing method for graph structure analysis is proposed in this paper: (1) This paper calculates the vertex activity of a graph according to the in-degree and out-degree, and divide the corresponding vertices into the hot or cold partitions; (2) According to the change of graph structure caused by partial vertex convergence after iteration, this paper reclassifies the partitions, divides the lower active vertices into cold partition and reduces the frequency of calculation, which thereby reducing the cache miss rate and the I O overhead caused by active vertices as well; (3) The partition with highest vertex status degree are given a priority calculation in this paper. In detail, more pronounced and more frequent vertices have higher processing priority. In this way, the convergence speed of the graph vertices is accelerated, and the running time of the graph algorithm in the big data environment is reduced. Our experiments show that compared with the latest system, the proposed method can double the performance of different graph algorithms and data sets.\"","summary":"\"GraphChi @cite_29 is a vertex-centric graph processing system and improve IO access efficiency by parallel Sliding Window processing strategy. But the outgoing edges of all vertices have to be loaded into memory before computation, resulting in unnecessary transfer of disk data. Also, all memory blocks have to be scanned when accessing neighboring vertices, which lead to inefficient graph traversal. TurboGraph @cite_19 proposed a Pin-And-Slide model to solve this problem. PAS has no delay in dealing with local graph data, but only applies to some specific parallel algorithms. Compare to the two above, VENUS @cite_36 expands to nearly every algorithm and enables streamlined processing which performs computation while the data is streaming in. Moreover, it uses a fixed buffer to cache the v-shard, which can reduce random IO.\"","":""}
{"id":"2806235972","dialogue":"\"With the advent of the big data, graph are processed in an iterative manner, which incrementally described in the form of graph in big data applications. Most currently, graph processing methods treat the underlying map data as black boxes. However, as shown in experimental evaluation, graph structures often have diversity, different graph processing methods are very sensitive to the graph structure and show different performance for different data sets. Based on this, a graph processing method for graph structure analysis is proposed in this paper: (1) This paper calculates the vertex activity of a graph according to the in-degree and out-degree, and divide the corresponding vertices into the hot or cold partitions; (2) According to the change of graph structure caused by partial vertex convergence after iteration, this paper reclassifies the partitions, divides the lower active vertices into cold partition and reduces the frequency of calculation, which thereby reducing the cache miss rate and the I O overhead caused by active vertices as well; (3) The partition with highest vertex status degree are given a priority calculation in this paper. In detail, more pronounced and more frequent vertices have higher processing priority. In this way, the convergence speed of the graph vertices is accelerated, and the running time of the graph algorithm in the big data environment is reduced. Our experiments show that compared with the latest system, the proposed method can double the performance of different graph algorithms and data sets.\"","summary":"\"GridGraph @cite_9 uses a 2-level Hierarchical Partitioning scheme to reduce the amount of data transfer, enable streamlined disk access, and maintain locality. But it requires more disk data transfer using TurboGraph-like updating strategy. Besides, it cannot fully utilize the parallelism of multi-thread CPU without sorted edges. NXgraph @cite_32 propose the Destination-Sorted Sub-Shard (DSSS) structure to store graph with three updating strategies: SPU, DPU and MPU. it adaptively choose suitable one to fully utilize the memory space and reduce the amount of data transfer. It achieves higher locality than v-shards in VENUS @cite_36 and reduces the amount of data transfer and enables streamlined disk access pattern. Mosaic @cite_35 combines fast host processors for concentrated memory-intensive operations, with coprocessors for compute and I O intensive components.\"","":""}
{"id":"2952158189","dialogue":"\"Fair division has long been an important problem in the economics literature. In this note, we consider the existence of proportionally fair allocations of indivisible goods, i.e., allocations of indivisible goods in which every agent gets at least her proportionally fair share according to her own utility function. We show that when utilities are additive and utilities for individual goods are drawn independently at random from a distribution, proportionally fair allocations exist with high probability if the number of goods is a multiple of the number of agents or if the number of goods grows asymptotically faster than the number of agents.\"","summary":"\"@cite_11 considered asymptotic existence and nonexistence of envy-free allocations. They showed that under additive utilities, envy-free allocations are unlikely to exist even when the number of goods is larger than the number of agents by a linear fraction. On the other hand, they proved that when the number of goods is larger than the number of agents by a logarithmic factor, such allocations are likely to exist under certain technical conditions on the probability distribution. @cite_14 and @cite_8 considered allocations that gives each agent a maximin share guarantee and showed that such allocations exist with high probability when utilities are additive. Asymptotic statements have been considered in other areas of economics as well. For instance, Manea @cite_3 established that the allocation obtained by the random serial dictatorship mechanism is ordinally inefficient with high probability when agents' preference profiles are drawn at random. Incentives and stability in large matching markets have also been considered in the literature @cite_7 @cite_5 .\"","":""}
{"id":"2952158189","dialogue":"\"Fair division has long been an important problem in the economics literature. In this note, we consider the existence of proportionally fair allocations of indivisible goods, i.e., allocations of indivisible goods in which every agent gets at least her proportionally fair share according to her own utility function. We show that when utilities are additive and utilities for individual goods are drawn independently at random from a distribution, proportionally fair allocations exist with high probability if the number of goods is a multiple of the number of agents or if the number of goods grows asymptotically faster than the number of agents.\"","summary":"\"We now make some comments on the relation between our results and those of @cite_11 concerning envy-free allocations. As we will later elaborate, proportional fairness is a weaker notion of fairness than envy-freeness, i.e., envy-free allocations are also proportionally fair when utilities are additive. showed that under additive utilities, envy-free allocations are unlikely to exist when the number of goods is larger than the number of agents by a linear fraction. Theorem contrasts that result by showing that proportionally fair allocations are likely to exist even when the number of goods is the same as the number of agents.\"","":""}
{"id":"2805697608","dialogue":"\"Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch","summary":"interpolating between measures in a musically meaningful way","":""}
{"id":"2805697608","dialogue":"\"Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch","summary":"interpolating between measures in a musically meaningful way","":""}
{"id":"2805697608","dialogue":"\"Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch","summary":"interpolating between measures in a musically meaningful way","":""}
{"id":"2805697608","dialogue":"\"Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch","summary":"interpolating between measures in a musically meaningful way","":""}
{"id":"2806902227","dialogue":"\"We propose a novel method for the blind separation of audio signals produced by musical instruments. While the approach of applying non-negative matrix factorization (NMF) has been studied in many papers, it does not make use of the pitch-invariance that the sounds of instruments exhibit. This limitation can be overcome by using tensor factorization, in which context the use of log-frequency spectrograms was initiated, but this still requires the specific tuning of the instruments to be hard-coded into the algorithm. We develop a time-frequency representation that is both shift-invariant and frequency-aligned, with a variant that can also be used for wideband signals. Our separation algorithm exploits this shift-invariance in order to find patterns of peaks related to specific instruments, while non-linear optimization enables it to represent arbitrary frequencies and incorporate inharmonicity, and the reasonability of the representation is ensured by a sparsity condition. The relative amplitudes of the harmonics are saved in a dictionary, which is trained via a modified version of ADAM. For a realistic monaural piece with acoustic recorder and violin, we achieve qualitatively good separation with a signal-to-distortion ratio (SDR) of 12.5 dB, a signal-to-interference ratio (SIR) of 25.7 dB, and a signal-to-artifacts ratio (SAR) of 12.7 dB, averaged.\"","summary":"\"In order to apply dictionary learning on audio data, it is helpful to regard a time-frequency representation which subdivides the problem into smaller time frames and highlights the frequency characteristics of the signal. Classically, such a representation is computed via application of the short-time Fourier transform (STFT) (cf. @cite_8 ) on the audio signal, and the resulting spectrogram (i.e., the magnitude of the STFT) is then decomposed via non-negative matrix factorization (NMF) @cite_22 in order to obtain a dictionary. This approach was initially studied by Smaragdis and Brown @cite_19 for the purpose of polyphonic music transcription and then applied to audio source separation by Wang and Plumbley @cite_27 .\"","":""}
{"id":"2806902227","dialogue":"\"We propose a novel method for the blind separation of audio signals produced by musical instruments. While the approach of applying non-negative matrix factorization (NMF) has been studied in many papers, it does not make use of the pitch-invariance that the sounds of instruments exhibit. This limitation can be overcome by using tensor factorization, in which context the use of log-frequency spectrograms was initiated, but this still requires the specific tuning of the instruments to be hard-coded into the algorithm. We develop a time-frequency representation that is both shift-invariant and frequency-aligned, with a variant that can also be used for wideband signals. Our separation algorithm exploits this shift-invariance in order to find patterns of peaks related to specific instruments, while non-linear optimization enables it to represent arbitrary frequencies and incorporate inharmonicity, and the reasonability of the representation is ensured by a sparsity condition. The relative amplitudes of the harmonics are saved in a dictionary, which is trained via a modified version of ADAM. For a realistic monaural piece with acoustic recorder and violin, we achieve qualitatively good separation with a signal-to-distortion ratio (SDR) of 12.5 dB, a signal-to-interference ratio (SIR) of 25.7 dB, and a signal-to-artifacts ratio (SAR) of 12.7 dB, averaged.\"","summary":"\"In many cases, a single musical instrument can generate different sounds which are perceptually similar but only vary in the pitch of the tones. In the STFT spectrogram, different pitch manifests in linear scaling of the distances between the peaks in the frequency axis, which is computationally hard to handle. Therefore, @cite_2 applied the constant-Q transform (CQT) @cite_28 , which turns scaling into shifts, and developed the shifted non-negative matrix factorization (SNMF), that is actually a tensor factorization, in order to train a shift-invariant dictionary. This approach was later refined by @cite_24 @cite_13 @cite_15 .\"","":""}
{"id":"2806902227","dialogue":"\"We propose a novel method for the blind separation of audio signals produced by musical instruments. While the approach of applying non-negative matrix factorization (NMF) has been studied in many papers, it does not make use of the pitch-invariance that the sounds of instruments exhibit. This limitation can be overcome by using tensor factorization, in which context the use of log-frequency spectrograms was initiated, but this still requires the specific tuning of the instruments to be hard-coded into the algorithm. We develop a time-frequency representation that is both shift-invariant and frequency-aligned, with a variant that can also be used for wideband signals. Our separation algorithm exploits this shift-invariance in order to find patterns of peaks related to specific instruments, while non-linear optimization enables it to represent arbitrary frequencies and incorporate inharmonicity, and the reasonability of the representation is ensured by a sparsity condition. The relative amplitudes of the harmonics are saved in a dictionary, which is trained via a modified version of ADAM. For a realistic monaural piece with acoustic recorder and violin, we achieve qualitatively good separation with a signal-to-distortion ratio (SDR) of 12.5 dB, a signal-to-interference ratio (SIR) of 25.7 dB, and a signal-to-artifacts ratio (SAR) of 12.7 dB, averaged.\"","summary":"\"While the constant-Q transform ensures the shift-invariance of patterns of sinusoids when varying the pitch, its transient response varies with frequency. To overcome this, the scattering transform by Andén and Mallat @cite_25 subsequently employs smoothing in the time domain.\"","":""}
{"id":"2806902227","dialogue":"\"We propose a novel method for the blind separation of audio signals produced by musical instruments. While the approach of applying non-negative matrix factorization (NMF) has been studied in many papers, it does not make use of the pitch-invariance that the sounds of instruments exhibit. This limitation can be overcome by using tensor factorization, in which context the use of log-frequency spectrograms was initiated, but this still requires the specific tuning of the instruments to be hard-coded into the algorithm. We develop a time-frequency representation that is both shift-invariant and frequency-aligned, with a variant that can also be used for wideband signals. Our separation algorithm exploits this shift-invariance in order to find patterns of peaks related to specific instruments, while non-linear optimization enables it to represent arbitrary frequencies and incorporate inharmonicity, and the reasonability of the representation is ensured by a sparsity condition. The relative amplitudes of the harmonics are saved in a dictionary, which is trained via a modified version of ADAM. For a realistic monaural piece with acoustic recorder and violin, we achieve qualitatively good separation with a signal-to-distortion ratio (SDR) of 12.5 dB, a signal-to-interference ratio (SIR) of 25.7 dB, and a signal-to-artifacts ratio (SAR) of 12.7 dB, averaged.\"","summary":"\"Another approach is the computation of the Mel spectrogram @cite_4 , which applies logarithmically spaced windows in the frequency direction. This, however, unfavorably amplifies transients in the higher frequencies.\"","":""}
{"id":"2806902227","dialogue":"\"We propose a novel method for the blind separation of audio signals produced by musical instruments. While the approach of applying non-negative matrix factorization (NMF) has been studied in many papers, it does not make use of the pitch-invariance that the sounds of instruments exhibit. This limitation can be overcome by using tensor factorization, in which context the use of log-frequency spectrograms was initiated, but this still requires the specific tuning of the instruments to be hard-coded into the algorithm. We develop a time-frequency representation that is both shift-invariant and frequency-aligned, with a variant that can also be used for wideband signals. Our separation algorithm exploits this shift-invariance in order to find patterns of peaks related to specific instruments, while non-linear optimization enables it to represent arbitrary frequencies and incorporate inharmonicity, and the reasonability of the representation is ensured by a sparsity condition. The relative amplitudes of the harmonics are saved in a dictionary, which is trained via a modified version of ADAM. For a realistic monaural piece with acoustic recorder and violin, we achieve qualitatively good separation with a signal-to-distortion ratio (SDR) of 12.5 dB, a signal-to-interference ratio (SIR) of 25.7 dB, and a signal-to-artifacts ratio (SAR) of 12.7 dB, averaged.\"","summary":"\"The Heisenberg uncertainty principle (cf. @cite_8 @cite_6 ) sets a fundamental bound to the time-frequency resolution achievable. If we use Gaussian windows where @math is the standard deviation of the window in the time domain and @math is the standard deviation of its Fourier transform, then we have @math . If, for instance, @math , then @math . For speech processing, this may be sufficient, but considering music that contains frequencies as low as @math , this deviation equals almost half an octave (or a tritone).\"","":""}
{"id":"2807350215","dialogue":"\"We show that any language in nondeterministic time @math , where the number of iterated exponentials is an arbitrary function @math , can be decided by a multiprover interactive proof system with a classical polynomial-time verifier and a constant number of quantum entangled provers, with completeness @math and soundness @math , where the number of iterated exponentials is @math and @math is a universal constant. The result was previously known for @math and @math ; we obtain it for any time-constructible function @math . The result is based on a compression technique for interactive proof systems with entangled provers that significantly simplifies and strengthens a protocol compression result of Ji (STOC'17). As a separate consequence of this technique we obtain a different proof of Slofstra's recent result (unpublished) on the uncomputability of the entangled value of multiprover games. Finally, we show that even minor improvements to our compression result would yield remarkable consequences in computational complexity theory and the foundations of quantum mechanics: first, it would imply that the class MIP* contains all computable languages; second, it would provide a negative resolution to a multipartite version of Tsirelson's problem on the relation between the commuting operator and tensor product models for quantum correlations.\"","summary":"\"We were informed of a forthcoming paper @cite_39 by Coudron and Slofstra that establishes a result similar (though strictly incomparable) to Theorem , using completely different techniques. In particular, the authors show that distinguishing between entangled value @math or @math for games with provers in the commuting operator model is hard for nondeterministic @math time (whereas our result shows hardness for nondeterministic @math time for games with @math provers in the tensor product model). This result relies on the group-theoretic framework that was pioneered in @cite_19 @cite_21 .\"","":""}
{"id":"2949112265","dialogue":"\"In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit.\"","summary":"\"As we noted above, most studies that examine misinformation spread focus on individual events such as natural disasters @cite_15 , political elections @cite_11 , or crises @cite_16 and examine the response to the event on social media. A recent study by Vosoughi al found that news stories that were fact-checked and found to be false spread faster and to more people than news items found to be true. In contrast, our methodology considers immediate reactions to news of varying credibility, so we can determine whether certain reactions or reactions to trusted or deceptive news sources evoke more or faster responses from social media users.\"","":""}
{"id":"2805862648","dialogue":"\"Deep neural networks have demonstrated impressive performance in various machine learning tasks. However, they are notoriously sensitive to changes in data distribution. Often, even a slight change in the distribution can lead to drastic performance reduction. Artificially augmenting the data may help to some extent, but in most cases, fails to achieve model invariance to the data distribution. Some examples where this sub-class of domain adaptation can be valuable are various imaging modalities such as thermal imaging, X-ray, ultrasound, and MRI, where changes in acquisition parameters or acquisition device manufacturer will result in different representation of the same input. Our work shows that standard finetuning fails to adapt the model in certain important cases. We propose a novel method of adapting to a new data source, and demonstrate near perfect adaptation on a customized ImageNet benchmark.\"","summary":"\"Recently, domain adaptation methods inspired by generative adversarial networks (GAN) @cite_11 have gained popularity. For example, @cite_0 added a domain discriminator to enforce domain invariance of the extracted features. It can be seen as two agents competing in a minmax game. One agent is trying to minimize classification error and maximize discriminator domain confusion. Another agent is trying to minimize discriminator domain confusion. Work by @cite_7 use this technique to overcome different outdoor lighting conditions between source and destination domains. @cite_4 add feature augmentation on top of the domain discriminator. GAN inspired methods differ from our approach, as our approach which does not involve a discriminator and or generator. GANs are also notoriously hard to train. Despite the development of techniques to improve the training process, gradient explosion and mode collapse are just a few of the many common practical issues faced when trying to train a GAN-based model.\"","":""}
{"id":"2805862648","dialogue":"\"Deep neural networks have demonstrated impressive performance in various machine learning tasks. However, they are notoriously sensitive to changes in data distribution. Often, even a slight change in the distribution can lead to drastic performance reduction. Artificially augmenting the data may help to some extent, but in most cases, fails to achieve model invariance to the data distribution. Some examples where this sub-class of domain adaptation can be valuable are various imaging modalities such as thermal imaging, X-ray, ultrasound, and MRI, where changes in acquisition parameters or acquisition device manufacturer will result in different representation of the same input. Our work shows that standard finetuning fails to adapt the model in certain important cases. We propose a novel method of adapting to a new data source, and demonstrate near perfect adaptation on a customized ImageNet benchmark.\"","summary":"\"Recent surveys @cite_9 , @cite_10 , @cite_12 may provide additional background on domain adaptation and transfer learning.\"","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"\"Learning under Privileged Information (LUPI) is initially proposed by Vapnik and Vashist @cite_31 @cite_19 . It extends the Support Vector Machine (SVM) by empirically estimating the slack values via privileged information. This method is further applied to various computer vision problems @cite_33 @cite_44 @cite_29 as well as ranking @cite_3 , clustering @cite_30 and metric learning @cite_7 problems. These method are based on max-margin learning and are not applicable to CNNs or RNNs.\"","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"\"One closely related work is @cite_18 , extending Gaussian processes to the LUPI paradigm. Hern ' a ndez-Lobato @cite_18 use privileged information to estimate the variance of the noise in their model. Similarly, we use the privileged information to control the variance of the dropout in CNN and RNN models. However, their method only applies to Gaussian processes, whereas we target neural networks.\"","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"\"The LUPI paradigm has also been studied recently in the context of CNNs. In contrast to max-margin methods, the literature on learning CNNs under privileged information heavily uses the distillation framework, following the close relationship between distillation and LUPI studied in @cite_41 .\"","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"Hoffman demonstrated a multi-modal distillation approach to incorporating an additional modality as side information @cite_13 . They start with a pre-trained network and distill the information from the privileged network to a main neural network in an end-to-end fashion.","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"We use multiplicative Gaussian dropout instead of Bernoulli dropout. Gaussian dropout is first introduced in @cite_36 . Its variational extension @cite_26 uses local re-parameterization to perform Bayesian learning.","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"\"The Information Bottleneck (IB) @cite_10 is a powerful framework which can enforce various structural assumptions. The IB framework has been applied to CNNs and RNNs using stochastic gradient variational Bayes and the re-parametrization trick @cite_4 . Perhaps closest to our method, Achille and Soatto @cite_38 use the information bottleneck principle to learn disentangled representations when a CNN with Gaussian Dropout is used. The authors introduce many ideas upon which we build; specifically, our hypothesis class (Eqn. 4) is very similar to the architecture they propose. The main architectural difference is their choice to define the variance as a function of @math , whereas we make it a function of @math . We also use similar distributional priors and a similar training procedure. On the other hand, we apply these ideas to a completely different problem with a different theoretical analysis. The information bottleneck has been applied to LUPI for SVMs @cite_21 . However, this method does not apply to neural networks.\"","":""}
{"id":"2963368804","dialogue":"\"Unlike machines, humans learn through rapid, Abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (i.e. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in sample efficiency by providing a generalization error bound decreasing with O(1 n), where n is the number of training examples, in an oracle case.\"","summary":"\"Although we use IB @cite_6 , Gaussian dropout @cite_36 and the re-parametrization trick @cite_4 , we are the first to our knowledge to apply any of these methods to the LUPI problem.\"","":""}
{"id":"2798720755","dialogue":"\"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graph entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.\"","summary":"\"There has been extensive work on predicting the ontological types of entities in large knowledge graphs @cite_11 @cite_14 @cite_6 @cite_7 , in semi-structured resources such as Wikipedia @cite_22 @cite_27 , as well as in text @cite_0 @cite_19 @cite_10 . However, the major shortcoming of these sorts of methods, including those aiming at more fine-grained typing, is that they assume that the set of candidate types is given as input, and the main remaining challenge is to pick the correct one(s). In contrast, our work yields descriptions that often indicate the type of entity, but typically are more natural-sounding and descriptive (e.g. ) than the oftentimes abstract ontological types (such as or ) chosen by type prediction methods.\"","":""}
{"id":"2798720755","dialogue":"\"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graph entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.\"","summary":"\"A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text @cite_1 @cite_13 , possibly also inducing taxonomies from them @cite_21 @cite_15 @cite_3 . However, these methods typically just need to select existing spans of text from the input as the output description.\"","":""}
{"id":"2798720755","dialogue":"\"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graph entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.\"","summary":"\"Generating entity descriptions is related to the task of text summarization. Most traditional work in this area was extractive in nature, i.e. it selects the most salient sentences from a given input text and concatenates them to form a shorter summary or presents them differently to the user @cite_9 . Abstractive summarization goes beyond this in generating new text not necessarily encountered in the input, as is typically necessary in our setting. The surge of sequence-to-sequence modeling of text via LSTMs naturally extends to the task of abstractive summarization by training a model to accept a longer sequence as input and learning to generate a shorter compressed sequence as a summary.\"","":""}
{"id":"2803788852","dialogue":"\"Conditional domain generation is a good way to interactively control sample generation process of deep generative models. However, once a conditional generative model has been created, it is often expensive to allow it to adapt to new conditional controls, especially the network structure is relatively deep. We propose a conditioned latent domain transfer framework across latent spaces of unconditional variational autoencoders(VAE). With this framework, we can allow unconditionally trained VAEs to generate images in its domain with conditionals provided by a latent representation of another domain. This framework does not assume commonalities between two domains. We demonstrate effectiveness and robustness of our model under widely used image datasets.\"","summary":"\"Generative adverserial networks or GAN for short, is well known for generating realistic samples through mapping latent manifolds with noise and conditionals inputed into generator(s).Many prior works has illustrates successful attempts to disentangle how GAN encode features into latent space and conditional GANs(cGAN) is a particular class of GANs that are proven to control output contents through concatenating tractable conditionals with selected noise. In the conditional setting, a lot of previous work has been done to explore possibilities to control samples generated from GANs through encoded conditionals @cite_2 @cite_9 @cite_7 .Here in particular, Zhu et. al. @cite_10 proposed a way to transform style of image from one domain to another through a cycle encoding and verification structure and has achieved decent results. We view the BiGAN proposed by Donahue ei.al. @cite_5 as a concurrent work of ours in that it also explores the concept of conditional noise mapping. It would be interesting to see if the model can be adaptable to the setting not being trained end-to-end. The notion of approximating an implicit distribution is not restricted to GAN alone and many works have focused on incorporating adversarial loss to achieve more flexible latent representation approximation @cite_14 @cite_13 @cite_12 .\"","":""}
{"id":"2803788852","dialogue":"\"Conditional domain generation is a good way to interactively control sample generation process of deep generative models. However, once a conditional generative model has been created, it is often expensive to allow it to adapt to new conditional controls, especially the network structure is relatively deep. We propose a conditioned latent domain transfer framework across latent spaces of unconditional variational autoencoders(VAE). With this framework, we can allow unconditionally trained VAEs to generate images in its domain with conditionals provided by a latent representation of another domain. This framework does not assume commonalities between two domains. We demonstrate effectiveness and robustness of our model under widely used image datasets.\"","summary":"Autoencoder(AE) is another class of deep generative model that compiles inputs and reconstruct samples through encoding features in latent space. Recent work done by Liu et.al. @cite_11 has shown that VAE can perform well on matching the latent space between images from two domains. The added adversarial loss facilitates the training of the VAE by preventing it from mode collapsing.","":""}
{"id":"2804401007","dialogue":"\"We study an extention of total variation denoising over images to over Cartesian power graphs and its applications to estimating non-parametric network models. The power graph fused lasso (PGFL) segments a matrix by exploiting a known graphical structure, @math , over the rows and columns. Our main results shows that for any connected graph, under subGaussian noise, the PGFL achieves the same mean-square error rate as 2D total variation denoising for signals of bounded variation. We study the use of the PGFL for denoising an observed network @math , where we learn the graph @math as the @math -nearest neighborhood graph of an estimated metric over the vertices. We provide theoretical and empirical results for estimating graphons, a non-parametric exchangeable network model, and compare to the state of the art graphon estimation methods.\"","summary":"\"Graph signal processing refers to methods that denoise, localize, detect, and predict signals over graphs. For example, each vertex corresponds to a low-powered sensor, and we would like to denoise sensor measurements, and we use the graph structure is based on communication between the sensors or spatial proximity. The driving assumption is that there is some underlying signal that in some way respects' the graph topology, and specifies the distribution of the observations. Many of the tools in signal processing and supervised learning can be extended to the graph case, such as Fourier analysis @cite_17 @cite_3 , wavelets @cite_10 @cite_34 @cite_8 , graph kernels @cite_32 , and convolutional networks @cite_13 @cite_19 . Graph structure has previously been used in matrix completion and network denoising problems (see for example, @cite_30 @cite_4 @cite_12 @cite_5 ), but these methods require some predetermined graph structure, such as knowledge graphs, so are not well suited to estimating graphons, and they do not perform segmentation, which is the focus of this work.\"","":""}
{"id":"2804401007","dialogue":"\"We study an extention of total variation denoising over images to over Cartesian power graphs and its applications to estimating non-parametric network models. The power graph fused lasso (PGFL) segments a matrix by exploiting a known graphical structure, @math , over the rows and columns. Our main results shows that for any connected graph, under subGaussian noise, the PGFL achieves the same mean-square error rate as 2D total variation denoising for signals of bounded variation. We study the use of the PGFL for denoising an observed network @math , where we learn the graph @math as the @math -nearest neighborhood graph of an estimated metric over the vertices. We provide theoretical and empirical results for estimating graphons, a non-parametric exchangeable network model, and compare to the state of the art graphon estimation methods.\"","summary":"\"There is an extensive body of literature on solving the fused lasso, . Algorithms for solving the fused lasso can be divided into two categories: solvers for a fixed @math , and path algorithms that find the solution for every @math within a range. The fused lasso for a fixed @math has a quadratic program dual form, and some popular algorithms for this are the projected Newton algorithm of @cite_7 @cite_22 , first-order primal-dual algorithm @cite_11 , and split-Bregman iteration @cite_9 . Some path algorithms include the generalized lasso path algorithm of @cite_31 , and a max-flow version for the fused lasso in @cite_2 . If applied directly to C2-power graphs, these methods would have computational and memory complexity that scale with the number of dyads, @math .\"","":""}
{"id":"2804401007","dialogue":"\"We study an extention of total variation denoising over images to over Cartesian power graphs and its applications to estimating non-parametric network models. The power graph fused lasso (PGFL) segments a matrix by exploiting a known graphical structure, @math , over the rows and columns. Our main results shows that for any connected graph, under subGaussian noise, the PGFL achieves the same mean-square error rate as 2D total variation denoising for signals of bounded variation. We study the use of the PGFL for denoising an observed network @math , where we learn the graph @math as the @math -nearest neighborhood graph of an estimated metric over the vertices. We provide theoretical and empirical results for estimating graphons, a non-parametric exchangeable network model, and compare to the state of the art graphon estimation methods.\"","summary":"\"We next turn our attention to graphon estimation using the PGFL on a learned graph. The statistical limits of graphon estimation have been well characterized for smooth graphons","":""}
{"id":"2804401007","dialogue":"\"We study an extention of total variation denoising over images to over Cartesian power graphs and its applications to estimating non-parametric network models. The power graph fused lasso (PGFL) segments a matrix by exploiting a known graphical structure, @math , over the rows and columns. Our main results shows that for any connected graph, under subGaussian noise, the PGFL achieves the same mean-square error rate as 2D total variation denoising for signals of bounded variation. We study the use of the PGFL for denoising an observed network @math , where we learn the graph @math as the @math -nearest neighborhood graph of an estimated metric over the vertices. We provide theoretical and empirical results for estimating graphons, a non-parametric exchangeable network model, and compare to the state of the art graphon estimation methods.\"","summary":"\"Another related approach to segment the dyads is to group the vertices via a community detection method. The stochastic block model is a special instance of the graphon model which assumes that there are latent communities for the vertices and the probability of attachment between two vertices is a function only of the communities to which the vertices belong. This can be thought of as segmenting the dyads by taking the Cartesian product of the vertex communities, but this type of segmentation is restrictive because of this specialized structure. Heuristic or greedy methods for fitting the SBM for graphon estimation have been proposed in @cite_16 @cite_21 , but little is known about the statistical performance and whether these can achieve minimax performance. In another approach, @cite_25 proposed a spectral method that thresholds singular values and provided some MSE consistency guarantees. Currently, the best rate guarantee for a computationally tractable estimator of Lipschitz graphons is achieved by the aforementioned neighborhood smoothing method of @cite_1 , and the MSE scales like @math , which is significantly worse than the minimax rate of @math .\"","":""}
{"id":"2804680395","dialogue":"\"Motivated by the difficulty of effecting fundamental change in the architecture of the Internet, in this paper, we study from a theoretical perspective the question of how individuals can join forces toward collective ventures. To that end, we draw on an elementary concept in Internet systems engineering, namely, that of incremental deployability, which we study mathematically and computationally. For example, we show that incremental deployability is at least as general a concept as the Nash equilibrium (in that the latter can be derived from the former). We then draw on this foundation to design and analyze institutional mechanisms that are not only promising to bootstrap emerging Internet architectures but they also have broader applications in social organization beyond its predominant market (and finance)-based character.\"","summary":"\"For example, @cite_39 show that the deployment of Secure-BGP can be facilitated by a pair of mechanisms, namely, (i) routing policies that prefer Internet paths safeguarded by Secure-BGP in partial deployment and (ii) offloading cryptography from autonomous systems without a customer (known as stub autonomous systems) to their providers. These techniques improve the incremental deployability of Secure-BGP, however, the incentive structure these techniques induce remains akin to the stag hunt, and, in fact, Gill propose concentrating peer pressure and regulatory efforts to a small fraction of core autonomous systems for their techniques to be effective in driving the growth of Secure-BGP. Our point, therefore, that, although such line of effort is certainly helpful to the adoption of emerging technologies it is not in general conclusive, remains.\"","":""}
{"id":"2804680395","dialogue":"\"Motivated by the difficulty of effecting fundamental change in the architecture of the Internet, in this paper, we study from a theoretical perspective the question of how individuals can join forces toward collective ventures. To that end, we draw on an elementary concept in Internet systems engineering, namely, that of incremental deployability, which we study mathematically and computationally. For example, we show that incremental deployability is at least as general a concept as the Nash equilibrium (in that the latter can be derived from the former). We then draw on this foundation to design and analyze institutional mechanisms that are not only promising to bootstrap emerging Internet architectures but they also have broader applications in social organization beyond its predominant market (and finance)-based character.\"","summary":"\"The problem of diffusing innovation in social systems has been the subject of extensive scrutiny primarily by social scientists. The study of diffusion of technology falls squarely in the ballpark of social science: Quoting @cite_7 , The people who have thought hardest about the general questions of technology have mostly been social scientists and philosophers, and understandably they have tended to view technology from the outside as stand-alone objects. 0.2 mm Seeing technology this way, from the outside, works well enough if we want to know how technologies enter the economy and spread within it.'' But computer scientists have also been involved with the diffusion of innovation from an algorithmic perspective. The term innovation assumes various interpretations in the literature from agricultural practices, social norms (such as which hand to extent in a handshake), medical drugs, commercial products (such as fax machines and cellphones), to networking technologies such as secure versions of BGP and quality-of-service capabilities in IP networks.\"","":""}
{"id":"2804680395","dialogue":"\"Motivated by the difficulty of effecting fundamental change in the architecture of the Internet, in this paper, we study from a theoretical perspective the question of how individuals can join forces toward collective ventures. To that end, we draw on an elementary concept in Internet systems engineering, namely, that of incremental deployability, which we study mathematically and computationally. For example, we show that incremental deployability is at least as general a concept as the Nash equilibrium (in that the latter can be derived from the former). We then draw on this foundation to design and analyze institutional mechanisms that are not only promising to bootstrap emerging Internet architectures but they also have broader applications in social organization beyond its predominant market (and finance)-based character.\"","summary":"\"Related to the problem of diffusing innovation is that of inciting collective action (in the sense that the decision to act in a particular fashion diffuses in a social system) a problem whose study was initiated in the seminal work of @cite_14 . The study of collective action was later taken up by several authors such as, for example, in the form of critical mass theories (bearing relevance to nuclear fission explosions in physics) . @cite_35 motivates the study of collective action in settings bearing a political nature related to citizen oppression in rogue states, however, models of collective action range from crowdsourcing in the Internet to enabling countermeasures against climate change (such as controlling carbon emissions). Although various authors discuss the problem of inciting collective action through incentives mechanisms, the idea of creating institutions to that effect (as we do in this paper) and proving their efficacy has not, to the extent of our knowledge, been considered before by these earlier works. (An exception is where the analysis is at an elementary stage.)\"","":""}
{"id":"2804680395","dialogue":"\"Motivated by the difficulty of effecting fundamental change in the architecture of the Internet, in this paper, we study from a theoretical perspective the question of how individuals can join forces toward collective ventures. To that end, we draw on an elementary concept in Internet systems engineering, namely, that of incremental deployability, which we study mathematically and computationally. For example, we show that incremental deployability is at least as general a concept as the Nash equilibrium (in that the latter can be derived from the former). We then draw on this foundation to design and analyze institutional mechanisms that are not only promising to bootstrap emerging Internet architectures but they also have broader applications in social organization beyond its predominant market (and finance)-based character.\"","summary":"\"The archetypical model to study the evolution of cooperation in society is the prisoner's dilemma. The stag hunt is the archetypical model for the study of coordination. But as @cite_36 notes the models are related: Iterated prisoner's dilemma with an infinite number of stages can assume the form of a stag hunt. The literature on the game-theoretic study of cooperation and coordination (starting with ) has been concerned with the emergence of cooperative phenomena without a centralized entity to facilitate their manifestation. For example, @cite_24 studies how signaling systems can facilitate coordination. In this paper, we study the emergence of cooperation with the assistance of an exterior to the players entity that is able to enforce commitments but our analysis suggests that players may coordination while altogether eschewing commitments.\"","":""}
{"id":"2803761331","dialogue":"\"Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.\"","summary":"\"In related work","":""}
{"id":"2803761331","dialogue":"\"Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.\"","summary":"\"Anomaly detection @cite_7 is related but not directly applicable, as blind spots are not rare instances. Instead, they are regions of the state space where the training environment does not match the testing environment, and we learn to efficiently identify these regions through oracle feedback.\"","":""}
{"id":"2803761331","dialogue":"\"Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.\"","summary":"\"Many approaches improve transfer of information across tasks @cite_17 @cite_16 @cite_0 @cite_22 , as tasks cannot be learned from scratch each time. Much of this literature has focused on learning mappings between state and action spaces to enable Q-value function or policy transfer. Several works have also considered hierarchical approaches to RL that involve transferring subtasks across domains @cite_10 . In distinction to transfer learning, where the labels (actions) of examples (states) may change, domain adaptation deals with situations where the distribution of examples changes from one domain to another @cite_4 @cite_18 . Our work differs from these in that we relax the assumption that the agent's state representation is complete and sufficient to learn autonomously.\"","":""}
{"id":"2804022949","dialogue":"\"The success of deep learning models is heavily tied to the use of massive amount of labeled data and excessively long training time. With the emergence of intelligent edge applications that use these models, the critical challenge is to obtain the same inference capability on a resource-constrained device while providing adaptability to cope with the dynamic changes in the data. We propose AgileNet, a novel lightweight dictionary-based few-shot learning methodology which provides reduced complexity deep neural network for efficient execution at the edge while enabling low-cost updates to capture the dynamics of the new data. Evaluations of state-of-the-art few-shot learning benchmarks demonstrate the superior accuracy of AgileNet compared to prior arts. Additionally, AgileNet is the first few-shot learning approach that prevents model updates by eliminating the knowledge obtained from the primary training. This property is ensured through the dictionaries learned by our novel end-to-end structured decomposition, which also reduces the memory footprint and computation complexity to match the edge device constraints.\"","summary":"\"The key challenge of few-shot learning is to use primary knowledge obtained through original training data to make predictions about unseen classes of data with a limited number of available samples. Following the long history of research on few-shot learning approaches, the first work to leverage modern machine learning for one-shot learning was proposed by @cite_10 . In recent years, the work in @cite_3 and @cite_14 have established two standard benchmarks, Omniglot and Mini-ImageNet respectively, to compare few-shot learning approaches in terms of accuracy. @cite_3 leverages a Bayesian model while the authors of @cite_14 utilized a Siamese network which learns pairwise similarity metrics to generalize the predictive power of the model to new classes. These works were followed by other pairwise similarity-based few-shot learning approaches in @cite_16 @cite_21 @cite_9 .\"","":""}
{"id":"2804022949","dialogue":"\"The success of deep learning models is heavily tied to the use of massive amount of labeled data and excessively long training time. With the emergence of intelligent edge applications that use these models, the critical challenge is to obtain the same inference capability on a resource-constrained device while providing adaptability to cope with the dynamic changes in the data. We propose AgileNet, a novel lightweight dictionary-based few-shot learning methodology which provides reduced complexity deep neural network for efficient execution at the edge while enabling low-cost updates to capture the dynamics of the new data. Evaluations of state-of-the-art few-shot learning benchmarks demonstrate the superior accuracy of AgileNet compared to prior arts. Additionally, AgileNet is the first few-shot learning approach that prevents model updates by eliminating the knowledge obtained from the primary training. This property is ensured through the dictionaries learned by our novel end-to-end structured decomposition, which also reduces the memory footprint and computation complexity to match the edge device constraints.\"","summary":"\"From a different perspective, few-shot learning through combining graph-based analytics with deep learning has been proposed in @cite_20 . In a separate trend of work, meta-learners @cite_17 @cite_19 @cite_23 are developed to generalize the DNN model to new related tasks. The aforementioned works have incrementally increased the accuracy on few-shot learning benchmarks. However, all these works are negligent to the model accuracy on old classes. Therefore, their proposal can degrade the predictive power of the model on old data. Additionally, many of the aforementioned approaches incur a high computation cost to adapt the model and thus are not amenable to resource-constrained settings. preserves the prior knowledge of the model on old data while outperforming all state-of-the-art approaches in terms of few-shot learning accuracy. Additionally, lightweight model updates of complies with stringent limitations of edge devices.\"","":""}
{"id":"2803838227","dialogue":"\"Taller and sleeker smartphone devices are becoming the new norm. More screen space and very responsive touchscreens have made for enjoyable experiences available to us at all times. However, after years of interacting with smaller, portable devices, we still try to use these large smartphones on the go, and do not want to change how, where, and when we interact with them. The older devices were easier to use with one hand, when mobile. Now, with bigger devices, users have trouble accessing all parts of the screen with one hand. We need to recognize the limitations in usability due to these large screens. We must start designing user interfaces that are more conducive to one hand usage, which is the preferred way of interacting with the phone. This paper introduces Adaptive App Design, a design methodology that promotes dynamic and adaptive interfaces for one handed usage. We present a novel method of recognizing which hand the user is interacting with and suggest how to design friendlier interfaces for them by presenting a set of design guidelines for this methodology.\"","summary":"\"@cite_33 @cite_16 @cite_9 @cite_5 are only few of many studies over several years stating that current interface designs on mobile devices are not suitable for one handed usage. Studies such as @cite_15 @cite_13 determine the functional area of the thumb, which quantifies the struggles that users face when operating these ill-designed interfaces.\"","":""}
{"id":"2803838227","dialogue":"\"Taller and sleeker smartphone devices are becoming the new norm. More screen space and very responsive touchscreens have made for enjoyable experiences available to us at all times. However, after years of interacting with smaller, portable devices, we still try to use these large smartphones on the go, and do not want to change how, where, and when we interact with them. The older devices were easier to use with one hand, when mobile. Now, with bigger devices, users have trouble accessing all parts of the screen with one hand. We need to recognize the limitations in usability due to these large screens. We must start designing user interfaces that are more conducive to one hand usage, which is the preferred way of interacting with the phone. This paper introduces Adaptive App Design, a design methodology that promotes dynamic and adaptive interfaces for one handed usage. We present a novel method of recognizing which hand the user is interacting with and suggest how to design friendlier interfaces for them by presenting a set of design guidelines for this methodology.\"","summary":"Researchers have tried to identify which hand is holding the phone with the help of additional sensors or equipment. @cite_2 @cite_7 @cite_36 @cite_37 @cite_19 used capacitive sensors along the sides of the phone to detect grip while @cite_34 used additional accelerometers. @cite_26 on the other hand used the position of the fingers on the back of the phone to determine the finger interacting with the phone. This method also used additional capacitive sensors.","":""}
{"id":"2804816149","dialogue":"\"Deep neural network learning can be formulated as a non-convex optimization problem. Existing optimization algorithms, e.g., Adam, can learn the models fast, but may get stuck in local optima easily. In this paper, we introduce a novel optimization algorithm, namely GADAM (Genetic-Evolutionary Adam). GADAM learns deep neural network models based on a number of unit models generations by generations: it trains the unit models with Adam, and evolves them to the new generations with genetic algorithm. We will show that GADAM can effectively jump out of the local optima in the learning process to obtain better solutions, and prove that GADAM can also achieve a very fast convergence. Extensive experiments have been done on various benchmark datasets, and the learning results will demonstrate the effectiveness and efficiency of the GADAM algorithm.\"","summary":"\": In addressing the optimization problems, SGD (Stochastic Gradient Descent) @cite_5 with sound theoretic foundations has been one of the most frequently used algorithms. Given an objective function @math , SGD adopts an iterative updating schema to learn the model variables as follows: Here, the parameter @math is usually a small constant, which is referred to as the learning rate. Meanwhile, to further accelerate the learning process, a variant of SGD, namely SGDM (SGD with Momentum) @cite_28 , has been proposed, which updates the variables according to the following equations In the equation, @math is a momentum parameter and vector @math is initialized with value @math .\"","":""}
{"id":"2804816149","dialogue":"\"Deep neural network learning can be formulated as a non-convex optimization problem. Existing optimization algorithms, e.g., Adam, can learn the models fast, but may get stuck in local optima easily. In this paper, we introduce a novel optimization algorithm, namely GADAM (Genetic-Evolutionary Adam). GADAM learns deep neural network models based on a number of unit models generations by generations: it trains the unit models with Adam, and evolves them to the new generations with genetic algorithm. We will show that GADAM can effectively jump out of the local optima in the learning process to obtain better solutions, and prove that GADAM can also achieve a very fast convergence. Extensive experiments have been done on various benchmark datasets, and the learning results will demonstrate the effectiveness and efficiency of the GADAM algorithm.\"","summary":"\"SGD and SGDM scale the gradient uniformly in all directions (i.e., all the variables), which makes the tuning of learning rate @math very tedious and laborious. To resolve such a problem, several adaptive optimization methods have been proposed, including Adam @cite_14 , RMSprop @cite_24 and Adagrad @cite_6 , which uses distinct learning rates for different variables. Formally, the variable updating equations adopted in Adagrad @cite_6 and RMSprop @cite_24 can be represented as follows respectively: In Adagrad, vector @math increases monotonically as the update iteration continues, and its scaling factor will keep decreasing when updating vector @math . RMSprop addresses the problem by employing an average scale instead of a cumulative scale, and Adam @cite_14 resolves such a problem with a bias correction, which adopts an exponential moving average for the step in lieu of the gradient.\"","":""}
{"id":"2803759886","dialogue":"\"Robotic agents that share autonomy with a human should leverage human domain knowledge and account for their preferences when completing a task. This extra knowledge can dramatically improve plan efficiency and user-satisfaction, but these gains are lost if communicating with a robot is taxing and unnatural. In this paper, we show how viewing humanrobot language through the lens of shared autonomy explains the efficiency versus cognitive load trade-offs humans make when deciding how cooperative and explicit to make their instructions.\"","summary":"\"Previous work has grounded natural language navigational commands to executable representations. Graphical model-based approaches using syntactic parses have been applied to controlling robotic forklift actions @cite_9 and mobile navigation in novel environments @cite_8 @cite_3 . Others have utilized CCG semantic parsing of robotic commands in synthetic environments @cite_14 , and with weak supervision @cite_20 . Howard (2014) @cite_21 parsed natural language to constraints in trajectory space in order to reduce the search space of their graphical model that grounds language to instructions. Park (2017) @cite_6 used a similar approach, grounding instructional language to a probabilistic graphical model, though they address the manipulation domain and learn soft cost functions to avoid dynamically-specified regions (e.g., don't put it there'').\"","":""}
{"id":"2803759886","dialogue":"\"Robotic agents that share autonomy with a human should leverage human domain knowledge and account for their preferences when completing a task. This extra knowledge can dramatically improve plan efficiency and user-satisfaction, but these gains are lost if communicating with a robot is taxing and unnatural. In this paper, we show how viewing humanrobot language through the lens of shared autonomy explains the efficiency versus cognitive load trade-offs humans make when deciding how cooperative and explicit to make their instructions.\"","summary":"\"Language interaction provides useful information in solving many of the challenges in shared autonomy","":""}
{"id":"2804470728","dialogue":"\"In this paper, we introduce an alternative approach, namely GEN (Genetic Evolution Network) Model, to the deep learning models. Instead of building one single deep model, GEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. Significantly different from the wellknown representation learning models with extremely deep structures, the unit models covered in GEN are of a much shallower architecture. In the training process, from each generation, a subset of unit models will be selected based on their performance to evolve and generate the child models in the next generation. GEN has significant advantages compared with existing deep representation learning models in terms of both learning effectiveness, efficiency and interpretability of the learning process and learned results. Extensive experiments have been done on diverse benchmark datasets, and the experimental results have demonstrated the outstanding performance of GEN compared with the state-of-the-art baseline methods in both effectiveness of efficiency.\"","summary":"\"The essence of deep learning is to compute hierarchical features or representations of the observational data @cite_1 @cite_10 . With the surge of deep learning research and applications in recent years, lots of research works have appeared to apply the deep learning methods, like deep belief network @cite_5 , deep Boltzmann machine @cite_27 , deep neural network @cite_0 @cite_37 and deep autoencoder model @cite_17 , in various applications, like speech and audio processing @cite_35 @cite_29 , language modeling and processing @cite_22 @cite_20 , information retrieval @cite_18 @cite_11 , objective recognition and computer vision @cite_25 , as well as multimodal and multi-task learning @cite_15 @cite_21 . Traditional deep learning models have too many disadvantages, introduce the deep forest as an alternative approach in @cite_4 .\"","":""}
{"id":"2804470728","dialogue":"\"In this paper, we introduce an alternative approach, namely GEN (Genetic Evolution Network) Model, to the deep learning models. Instead of building one single deep model, GEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. Significantly different from the wellknown representation learning models with extremely deep structures, the unit models covered in GEN are of a much shallower architecture. In the training process, from each generation, a subset of unit models will be selected based on their performance to evolve and generate the child models in the next generation. GEN has significant advantages compared with existing deep representation learning models in terms of both learning effectiveness, efficiency and interpretability of the learning process and learned results. Extensive experiments have been done on diverse benchmark datasets, and the experimental results have demonstrated the outstanding performance of GEN compared with the state-of-the-art baseline methods in both effectiveness of efficiency.\"","summary":"\"In recent years, many research works propose to embed network data into a low-dimensional feature space, i.e., the network embedding problem, in which nodes are represented as feature vectors. In graphs, the relation can be treated as a translation of the entities, and many translation based embedding models have been proposed, like TransE @cite_3 , TransH @cite_19 and TransR @cite_39 . At the same time, many network embedding works based on random walk model and deep learning models have been introduced, like Deepwalk @cite_31 , LINE @cite_9 , node2vec @cite_8 , HNE @cite_12 and DNE @cite_33 . extends the word2vec model @cite_13 to the network scenario and introduce the Deepwalk algorithm @cite_31 . @cite_9 propose to embed the networks with LINE algorithm, which can preserve both the local and global network structures. @cite_8 introduce a flexible notion of a node's network neighborhood and design a biased random walk procedure to sample the neighbors.\"","":""}
{"id":"2804621100","dialogue":"\"Checking various log files from different processes can be a tedious task as these logs contain lots of events, each with a (possibly large) number of attributes. We developed a way to automatically model log files and detect outlier traces in the data. For that we extend Dynamic Bayesian Networks to model the normal behavior found in log files. We introduce a new algorithm that is able to learn a model of a log file starting from the data itself. The model is capable of scoring traces even when new values or new combinations of values appear in the log file.\"","summary":"\"The problem we are interested in is that of finding anomalous sequences (traces) within a large database of discrete multivariate sequences. Different techniques have been proposed to solve this problem both in the anomaly detection field @cite_14 @cite_9 @cite_6 @cite_22 , as in the process mining field @cite_18 @cite_19 . Some of these techniques use signatures of known anomalies that can occur in the system. It is clear that these systems cannot recognize a new type of anomaly and are too limited for our purpose. We are interested in techniques that build a model, such as Markov Chains that represent normal behavior of a system.\"","":""}
{"id":"2804494348","dialogue":"\"Network embedding has become a hot research topic recently which can provide low-dimensional feature representations for many machine learning applications. Current work focuses on either (1) whether the embedding is designed as an unsupervised learning task by explicitly preserving the structural connectivity in the network, or (2) whether the embedding is a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we focus on bridging the gap of the two lines of the research. We propose to adapt the Generative Adversarial model to perform network embedding, in which the generator is trying to generate vertex pairs, while the discriminator tries to distinguish the generated vertex pairs from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. We develop three variations of models, including GANE which applies cosine similarity, GANE-O1 which preserves the first-order proximity, and GANE-O2 which tries to preserves the second-order proximity of the network in the low-dimensional embedded vector space. We later prove that GANE-O2 has the same objective function as GANE-O1 when negative sampling is applied to simplify the training process in GANE-O2. Experiments with real-world network datasets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements on precision in link prediction, as well as on visualizations and accuracy in clustering tasks.\"","summary":"Recent advances in Generative Adversarial Networks (GANs) @cite_15 have proven GANs as a powerful framework for learning complex data distributions. The core idea is to define the generator and the discriminator to be the minimax game players competing with each other to push the generator to produce high quality data to fool the discriminator.","":""}
{"id":"2804494348","dialogue":"\"Network embedding has become a hot research topic recently which can provide low-dimensional feature representations for many machine learning applications. Current work focuses on either (1) whether the embedding is designed as an unsupervised learning task by explicitly preserving the structural connectivity in the network, or (2) whether the embedding is a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we focus on bridging the gap of the two lines of the research. We propose to adapt the Generative Adversarial model to perform network embedding, in which the generator is trying to generate vertex pairs, while the discriminator tries to distinguish the generated vertex pairs from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. We develop three variations of models, including GANE which applies cosine similarity, GANE-O1 which preserves the first-order proximity, and GANE-O2 which tries to preserves the second-order proximity of the network in the low-dimensional embedded vector space. We later prove that GANE-O2 has the same objective function as GANE-O1 when negative sampling is applied to simplify the training process in GANE-O2. Experiments with real-world network datasets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements on precision in link prediction, as well as on visualizations and accuracy in clustering tasks.\"","summary":"\"Mirza & Osindero introduced conditional GANs @cite_11 to control the data generation by setting conditional constraints on the model. InfoGAN @cite_2 , another information-theoretic extension to the GAN model, maximizes the mutual information between a small subset of the latent variables and the observations to learn interpretable and meaningful hidden representations on image datasets. SeqGAN @cite_0 models the data generator as a stochastic policy in reinforcement learning and uses the policy gradient to guide the learning process bypassing the generator differentiation problem for discrete data output.\"","":""}
{"id":"2804494348","dialogue":"\"Network embedding has become a hot research topic recently which can provide low-dimensional feature representations for many machine learning applications. Current work focuses on either (1) whether the embedding is designed as an unsupervised learning task by explicitly preserving the structural connectivity in the network, or (2) whether the embedding is a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we focus on bridging the gap of the two lines of the research. We propose to adapt the Generative Adversarial model to perform network embedding, in which the generator is trying to generate vertex pairs, while the discriminator tries to distinguish the generated vertex pairs from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. We develop three variations of models, including GANE which applies cosine similarity, GANE-O1 which preserves the first-order proximity, and GANE-O2 which tries to preserves the second-order proximity of the network in the low-dimensional embedded vector space. We later prove that GANE-O2 has the same objective function as GANE-O1 when negative sampling is applied to simplify the training process in GANE-O2. Experiments with real-world network datasets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements on precision in link prediction, as well as on visualizations and accuracy in clustering tasks.\"","summary":"\"Despite their successes, GANs are notably difficulty to train and prone to mode collapse @cite_8 , especially for discrete data. Energy-based GAN (EBGAN) @cite_7 tries to achieve a stable training process by viewing the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. However, EBGANs, which regularize the distribution distance as Jensen-Shannon (JS) divergence, share the same problem as classical GANs that the discriminator cannot be trained well enough, as the distance EBGANS adopted cannot offer perfect gradients. Replacing JS with the Earth Mover (EM) distance, Wasserstein-GAN @cite_14 theoretically and experimentally solves the problem of model fragility.\"","":""}
{"id":"2804494348","dialogue":"\"Network embedding has become a hot research topic recently which can provide low-dimensional feature representations for many machine learning applications. Current work focuses on either (1) whether the embedding is designed as an unsupervised learning task by explicitly preserving the structural connectivity in the network, or (2) whether the embedding is a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we focus on bridging the gap of the two lines of the research. We propose to adapt the Generative Adversarial model to perform network embedding, in which the generator is trying to generate vertex pairs, while the discriminator tries to distinguish the generated vertex pairs from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. We develop three variations of models, including GANE which applies cosine similarity, GANE-O1 which preserves the first-order proximity, and GANE-O2 which tries to preserves the second-order proximity of the network in the low-dimensional embedded vector space. We later prove that GANE-O2 has the same objective function as GANE-O1 when negative sampling is applied to simplify the training process in GANE-O2. Experiments with real-world network datasets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements on precision in link prediction, as well as on visualizations and accuracy in clustering tasks.\"","summary":"\"GANs are successfully applied in the field of computer vision for tasks including generating sample images. However, there are few attempts to apply GANs to other machine learning tasks. Recently, IRGAN @cite_21 has been proposed as an information retrieval model in which the generator focuses on predicting relevant documents given a query and the discriminator focuses on distinguish whether the generated documents are relevant. It showed superior performance over the state-of-the-art information retrieval approaches.\"","":""}
{"id":"2803115127","dialogue":"\"In the present paper we describe the technology for translating algorithmic descriptions of discrete functions to SAT. The proposed technology is aimed at applications in algebraic cryptanalysis. We describe how cryptanalysis instances are reduced to SAT in such a way that it should be perceived as natural by the cryptographic community. Therefore, in the theoretical part of the paper we justify the main principles of general reduction to SAT for discrete functions from a class containing the majority of functions employed in cryptography. Based on these principles we describe the Transalg software system, developed with SAT-based cryptanalysis specifics in mind. We show the results of applications of Transalg to construction of a number of attacks on various cryptographic functions. Some of the corresponding attacks are state of the art. We also compare the functional capabilities of the proposed system with that of other software systems that can be used to reduce cryptanalysis instances to SAT, and also with the CBMC system widely employed in symbolic verification. In the paper we also present vast experimental data, obtained using the SAT-solvers that took first places at the SAT-competitions in the recent several years.\"","summary":"\"The software system @cite_51 @cite_91 @cite_28 was developed by Galois inc. In 2010 there appeared the system @cite_1 . Approximately at this time we started the development of the software system, which we describe in the present paper ( was first mentioned in papers in Russian in 2011). In 2012 there appeared the system @cite_113 , aimed at reducing to SAT various constraint programming problems. It can be applied to construct encodings for cryptographic functions as well. Note that , and can encode to SAT algorithmic descriptions of a very wide class of functions working with binary data. Meanwhile, is designed to work only with keystream generators based on shift registers. We considered the pros and cons of all mentioned systems in detail in Section 4.\"","":""}
{"id":"2803115127","dialogue":"\"In the present paper we describe the technology for translating algorithmic descriptions of discrete functions to SAT. The proposed technology is aimed at applications in algebraic cryptanalysis. We describe how cryptanalysis instances are reduced to SAT in such a way that it should be perceived as natural by the cryptographic community. Therefore, in the theoretical part of the paper we justify the main principles of general reduction to SAT for discrete functions from a class containing the majority of functions employed in cryptography. Based on these principles we describe the Transalg software system, developed with SAT-based cryptanalysis specifics in mind. We show the results of applications of Transalg to construction of a number of attacks on various cryptographic functions. Some of the corresponding attacks are state of the art. We also compare the functional capabilities of the proposed system with that of other software systems that can be used to reduce cryptanalysis instances to SAT, and also with the CBMC system widely employed in symbolic verification. In the paper we also present vast experimental data, obtained using the SAT-solvers that took first places at the SAT-competitions in the recent several years.\"","summary":"\"In the basis of","":""}
{"id":"2803115127","dialogue":"\"In the present paper we describe the technology for translating algorithmic descriptions of discrete functions to SAT. The proposed technology is aimed at applications in algebraic cryptanalysis. We describe how cryptanalysis instances are reduced to SAT in such a way that it should be perceived as natural by the cryptographic community. Therefore, in the theoretical part of the paper we justify the main principles of general reduction to SAT for discrete functions from a class containing the majority of functions employed in cryptography. Based on these principles we describe the Transalg software system, developed with SAT-based cryptanalysis specifics in mind. We show the results of applications of Transalg to construction of a number of attacks on various cryptographic functions. Some of the corresponding attacks are state of the art. We also compare the functional capabilities of the proposed system with that of other software systems that can be used to reduce cryptanalysis instances to SAT, and also with the CBMC system widely employed in symbolic verification. In the paper we also present vast experimental data, obtained using the SAT-solvers that took first places at the SAT-competitions in the recent several years.\"","summary":"\"As we mentioned above, SAT-based cryptanalysis is still actively developing. The cryptographic attacks that employ SAT-solvers show very good results for a number of keystream generators: Geffe, Wolfram (present paper), Crypto-1, Hitag2 (see @cite_24 ). In @cite_61 there was described a successful SAT-based attack on the widely known A5 1 cryptographic keystream generator, which has been used in GSM networks to cipher traffic. Later several dozen cryptanalysis instances for this cipher were solved in the SAT@home volunteer computing project @cite_10 . This result together with other attacks on A5 1 (see @cite_66 @cite_124 @cite_12 ) provides an exhaustive argument towards not using A5 1 any more. The Bivium keystream cipher @cite_23 is a popular object of algebraic and SAT-based cryptanalysis @cite_29 @cite_89 @cite_24 @cite_96 . In @cite_92 there was constructed a SAT-based guess-and-determine attack on Bivium the estimated runtime of which is realistic for modern distributed computing systems. In @cite_14 there were described SAT-based guess-and-determine attacks on several variants of the alternating step generator.\"","":""}
{"id":"2803115127","dialogue":"\"In the present paper we describe the technology for translating algorithmic descriptions of discrete functions to SAT. The proposed technology is aimed at applications in algebraic cryptanalysis. We describe how cryptanalysis instances are reduced to SAT in such a way that it should be perceived as natural by the cryptographic community. Therefore, in the theoretical part of the paper we justify the main principles of general reduction to SAT for discrete functions from a class containing the majority of functions employed in cryptography. Based on these principles we describe the Transalg software system, developed with SAT-based cryptanalysis specifics in mind. We show the results of applications of Transalg to construction of a number of attacks on various cryptographic functions. Some of the corresponding attacks are state of the art. We also compare the functional capabilities of the proposed system with that of other software systems that can be used to reduce cryptanalysis instances to SAT, and also with the CBMC system widely employed in symbolic verification. In the paper we also present vast experimental data, obtained using the SAT-solvers that took first places at the SAT-competitions in the recent several years.\"","summary":"\"As we already noted, @cite_25 was the first paper to demonstrate the applicability of SAT-based cryptanalysis to relevant cryptographic algorithms. In that paper using the solver @cite_95 it was possible to quite effectively find single-block collisions for MD4. Using new propositional encoding methods (in particular, the system) and state-of-the-art SAT solvers it is possible to find preimages for MD4 and MD5 several hundred times faster than it was done in @cite_25 . Nevertheless, on the current stage SAT-based cryptanalysis is less effective than specialized methods (see, for example @cite_58 @cite_99 @cite_82 ) on problems of finding collisions of cryptographic hash functions. However, as far as we know, it is the SAT-based approach that yields best known preimage attacks on truncated variants of hash functions @cite_32 @cite_78 @cite_54 . In application to MD4-39 for a long time the SAT-based preimage attack from @cite_32 was considered to be the best. In @cite_15 we significantly improved the results from @cite_32 . It was possible for the large part thanks to functional capabilities of the system.\"","":""}
{"id":"2798590591","dialogue":"\"Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.\"","summary":"\"There is abundant research work on aspect based sentiment analysis. Actually, the name ABSA is used to describe two different subtasks in the literature. We classify the existing work into two main categories based on the descriptions of sentiment analysis tasks in SemEval 2014 Task 4 @cite_31 : Aspect-Term Sentiment Analysis and Aspect-Category Sentiment Analysis.\"","":""}
{"id":"2953037471","dialogue":"\"Many dynamic networks coming from real-world contexts are link streams, i.e. a finite collection of triplets @math where @math and @math are two nodes having a link between them at time @math . A very large number of studies on these objects start by aggregating the data in disjoint time windows of length @math in order to obtain a series of graphs on which are made all subsequent analyses. Here we are concerned with the impact of the chosen @math on the obtained graph series. We address the fundamental question of knowing whether a series of graphs formed using a given @math faithfully describes the original link stream. We answer the question by showing that such dynamic networks exhibit a threshold for @math , which we call the , beyond which the properties of propagation of the link stream are altered, while they are mostly preserved before. We design an automatic method to determine the saturation scale of any link stream, which we apply and validate on several real-world datasets.\"","summary":"\"It is paradoxical to note that, while the question of the influence of aggregation on the properties of the formed graph series is largely ignored in most of the studies on dynamic networks, this question actually already received a lot of specific attention @cite_34 @cite_21 @cite_44 @cite_2 @cite_22 @cite_37 @cite_38 @cite_17 .\"","":""}
{"id":"2953037471","dialogue":"\"Many dynamic networks coming from real-world contexts are link streams, i.e. a finite collection of triplets @math where @math and @math are two nodes having a link between them at time @math . A very large number of studies on these objects start by aggregating the data in disjoint time windows of length @math in order to obtain a series of graphs on which are made all subsequent analyses. Here we are concerned with the impact of the chosen @math on the obtained graph series. We address the fundamental question of knowing whether a series of graphs formed using a given @math faithfully describes the original link stream. We answer the question by showing that such dynamic networks exhibit a threshold for @math , which we call the , beyond which the properties of propagation of the link stream are altered, while they are mostly preserved before. We design an automatic method to determine the saturation scale of any link stream, which we apply and validate on several real-world datasets.\"","summary":"\"Contrastingly, the goal of @cite_2 is precisely to determine an ideal aggregation period. In their method, this period is obtained as a trade-off between two metrics that vary monotonically and oppositely with regard to aggregation: one describing the loss of information (increasing with aggregation) and one describing the noise contained in the series of snapshots (decreasing with aggregation). Compared to them, here, we are concerned only with the loss of information. This allows us to avoid some drawbacks and limitations inherent to the approaches based on a trade-off: i) the value selected for the aggregation period strongly depends on the importance given to each metrics and ii) the selected value does not reveal any particular behavior of the properties of the network used in the trade-off, as each of them varies smoothly and monotonically from one extremal value to another one. On the contrary, our method does not depend on any arbitrary choice of ponderation and reveals a natural change in the way the network responds to aggregation at a certain aggregation scale that we determine.\"","":""}
{"id":"2953037471","dialogue":"\"Many dynamic networks coming from real-world contexts are link streams, i.e. a finite collection of triplets @math where @math and @math are two nodes having a link between them at time @math . A very large number of studies on these objects start by aggregating the data in disjoint time windows of length @math in order to obtain a series of graphs on which are made all subsequent analyses. Here we are concerned with the impact of the chosen @math on the obtained graph series. We address the fundamental question of knowing whether a series of graphs formed using a given @math faithfully describes the original link stream. We answer the question by showing that such dynamic networks exhibit a threshold for @math , which we call the , beyond which the properties of propagation of the link stream are altered, while they are mostly preserved before. We design an automatic method to determine the saturation scale of any link stream, which we apply and validate on several real-world datasets.\"","summary":"\"@cite_22 also aims at determining an appropriate time scale for aggregating a link stream into a graph series. Their method does not take into account the loss of information but is instead based on the modes of periodicity and on the self-similarity of the time series of some properties of the snapshots. They observe that the offset time for which the self similarity of these time series is zero is close to half of the period of the highest frequency visible in their spectra, which is the aggregation period suggested as a result of their method. Though this provides a very relevant time scale for analyzing dynamic networks, its meaning is different from the meaning of the saturation scale we are looking for in this paper. Indeed, an important part of the activity of dynamic networks takes place at time scales much smaller than their modes of periodicity. Therefore, using such periods for aggregation usually induces an important loss of information, which we aim at avoiding here. Let us mention that a similar approach based on modes of periodicity of some time series associated to the network was previously used in @cite_6 @cite_31 .\"","":""}
{"id":"2951227051","dialogue":"The introduction of the schema.org vocabulary was a big step towards making websites machine read- and understandable. Due to schema.org's RDF-like nature storing annotations in a graph database is easy and efficient. In this paper the authors show how they gather touristic data in the Austrian region of Tirol and provide this data publicly in a knowledge graph. The definition of subsets of the vocabulary is followed by providing means to map data sources efficiently to schema.org and then store the annotated content into the graph. To showcase the consumption of the touristic data four scenarios are described which use the knowledge graph for real life applications and data analysis.","summary":"\"The Tyrolean Tourism Knowledge Graph contains static (e.g. phone number, address) and dynamic (e.g. accommodation offers) data based on schema.org annotations collected from different sources such as Destination Management Organizations (DMO) and Geographical Information Systems (GIS). In our previous work @cite_5 , we explained how we annotated the relevant data in the region with schema.org from different sources. Our knowledge graph consolidates these annotations and enables intelligent applications like chatbots to contribute the digitalization of tourism in Tyrol. Additionally, since we store the historical data, the knowledge graph allows data analytics to provide insights from the region.\"","":""}
{"id":"2963289201","dialogue":"\"Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.\"","summary":"\"Statistical Priors. Since blind image deblurring is an ill-posed problem, it requires certain assumptions or prior knowledge to constrain the solution space. Early approaches, e.g., @cite_26 , assume simple parametric blur kernels to deblur images, which cannot deal with complex motion blur. As image gradients of natural images can be modeled well by a heavy-tailed distribution, Fergus et al @cite_21 use a mixture of Gaussians to learn the statistical prior for deblurring. Similarly, Shan et al @cite_44 use a parametric model to approximate the heavy-tailed prior for natural images. @cite_17 , Cai et al assume that the latent images and kernels can be sparsely represented by an over-complete dictionary based on wavelets. On the other hand, it has been shown that the most favorable solution for a maximum a posteriori (MAP) deblurring method with sparsity prior is usually a blurred image rather than a sharp one @cite_45 . As @cite_45 is usually computationally expensive, an efficient algorithm for approximation of marginal likelihood is developed @cite_30 for image deblurring.\"","":""}
{"id":"2963289201","dialogue":"\"Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.\"","summary":"\"Image Priors in Favor of Clear Images. Different image priors that favor clear images instead of blurred images have been introduced for image deblurring. Krishnan et al @cite_7 present a normalized sparsity prior, and Xu et al @cite_32 use the @math constraint on image gradients for kernel estimation. Non-parametric patch priors that model edges and corners have also been proposed @cite_8 for blur kernel estimation. We note that although the use of sparse priors facilitates kernel estimation, it is likely to fail when the blurred images do not contain rich texture. @cite_3 , Michaeli and Irani exploit internal patch recurrence for image deblurring. This method performs well when images contain repetitive patch patterns, but may fail otherwise. Class-specific image prior @cite_31 has been shown to be effective for certain object categories and less effective for scenes with complex background. Recently, @cite_20 develop an image prior based on the dark channel prior @cite_42 for blur kernel estimation. However, this method does not perform well when clear images do not contain zero-intensity pixels or the blurred images contain noise.\"","":""}
{"id":"2963289201","dialogue":"\"Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.\"","summary":"\"Edge Selection. In addition to statistical priors, numerous blind image deblurring methods explicitly exploit edges for kernel estimation @cite_4 @cite_16 @cite_36 @cite_46 . Joshi et al @cite_36 and Cho et al @cite_46 use the restored sharp edges from a blurred image for kernel estimation. @cite_4 , Cho and Lee utilize bilateral and shock filters to predict sharp edges. The blur kernel is determined by alternating between restoring sharp edges and estimating blur kernels in a coarse-to-fine manner. As strong edges restored from a blurred image are not necessarily useful for kernel estimation, Xu and Jia @cite_16 develop a method to select informative ones for deblurring. Despite demonstrated success, these methods rely largely on image filtering methods (e.g., shock and bilateral filters) and heuristics for restoring sharp edges, which are less effective for objects with specific geometric structures.\"","":""}
{"id":"2963289201","dialogue":"\"Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.\"","summary":"\"Face Deblurring. A few algorithms have been developed to deblur face images for the recognition task. Nishiyama et al @cite_41 learn subspaces from blurred face images with known blur kernels for recognition. As the set of blur kernels is pre-defined, the application domain of this approach is limited. Zhang et al @cite_24 propose a joint image restoration and recognition method based on sparse representations. However, this method is most effective for well cropped and aligned face images with simple motion blurs.\"","":""}
{"id":"2963289201","dialogue":"\"Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.\"","summary":"\"Example-based Deblurring. Recently, @cite_5 propose a deblurring method which uses sharp reference examples for guidance. The method requires a reference image with the same contents as the input to obtain dense correspondence for reconstruction. Although it has been shown to deblur specific images well, the assumption of using reference images with same contents limit its application domain. In contrast, the proposed methods do not require the exemplar to have the same or closely similar contents of the input. The blurred face image can be of different identity and background when compared to exemplar images. The proposed methods only require the matched example to have similar structures (in terms of image gradients) for kernel estimation instead of using dense corresponding pixels. As such, the proposed algorithms can be applied to class specific image deblurring with fewer constraints.\"","":""}
{"id":"2963289201","dialogue":"\"Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.\"","summary":"\"Convolutional Neural Networks. Convolutional neural networks have been widely used in low-level vision tasks including image denoising @cite_6 , super-resolution @cite_37 @cite_9 , non-blind deconvolution @cite_38 @cite_23 , blind image deblurring @cite_22 and image filtering @cite_43 @cite_33 . Schuler et al @cite_22 incorporate a sharpening convolutional neural network into an iterative blind deconvolution method to estimate the blur kernel. However, this method needs to re-train different networks for kernels of different sizes, which limits the application domains. @cite_43 , Xu et al propose a method to learn edge-aware filters using a deep convolutional neural network. However, we note that this method can only be applied to approximate edge-aware filters for clear images. This method cannot be directly applied to restore salient edges from blurry images for kernel estimation.\"","":""}
{"id":"2801080321","dialogue":"\"Radio networks are a long-studied model for distributed system of devices which communicate wirelessly. When these devices are mobile or have limited capabilities, the system is often best modeled by the ad-hoc variant, in which the devices do not know the structure of the network. A large body of work has been devoted to designing algorithms for the ad-hoc model, particularly for fundamental communications tasks such as broadcasting. Most of these algorithms, however, assume that devices have some network knowledge (usually bounds on the number of nodes in the network @math , and the diameter @math ), which may not always be realistic in systems with weak devices or gradual deployment. Very little is known about what can be done when this information is not available. This is the issue we address in this work, by presenting the first broadcasting algorithms for networks in which nodes have no prior knowledge whatsoever. We demonstrate that lack of parameter knowledge can be overcome at only a small increase in running time. Specifically, we show that in networks without collision detection, broadcast can be achieved in @math time, almost reaching the @math lower bound. We also give an algorithm for directed networks with collision detection, which requires only @math time.\"","summary":"\"More recently, Ghaffari, Haupler and Khabbazian @cite_2 showed that collision detection can be used to surpass this lower bound, attaining an @math time algorithm. Work by Haeupler and Wajc @cite_14 demonstrated that even without collision detection, the lower bound could be beaten assuming were permitted; that is, nodes have access to a global clock and are allowed to transmit before receiving the source message. Czumaj and Davies @cite_9 extended this approach and obtained a running time of @math for the setting with spontaneous transmissions. However, these algorithms only work in undirected networks.\"","":""}
{"id":"2801080321","dialogue":"\"Radio networks are a long-studied model for distributed system of devices which communicate wirelessly. When these devices are mobile or have limited capabilities, the system is often best modeled by the ad-hoc variant, in which the devices do not know the structure of the network. A large body of work has been devoted to designing algorithms for the ad-hoc model, particularly for fundamental communications tasks such as broadcasting. Most of these algorithms, however, assume that devices have some network knowledge (usually bounds on the number of nodes in the network @math , and the diameter @math ), which may not always be realistic in systems with weak devices or gradual deployment. Very little is known about what can be done when this information is not available. This is the issue we address in this work, by presenting the first broadcasting algorithms for networks in which nodes have no prior knowledge whatsoever. We demonstrate that lack of parameter knowledge can be overcome at only a small increase in running time. Specifically, we show that in networks without collision detection, broadcast can be achieved in @math time, almost reaching the @math lower bound. We also give an algorithm for directed networks with collision detection, which requires only @math time.\"","summary":"\"Deterministic algorithms for broadcasting have also been studied; for undirected networks the fastest known algorithm is the @math -time algorithm of @cite_7 , while for directed networks it is the @math -time algorithm of @cite_3 .\"","":""}
{"id":"2801080321","dialogue":"\"Radio networks are a long-studied model for distributed system of devices which communicate wirelessly. When these devices are mobile or have limited capabilities, the system is often best modeled by the ad-hoc variant, in which the devices do not know the structure of the network. A large body of work has been devoted to designing algorithms for the ad-hoc model, particularly for fundamental communications tasks such as broadcasting. Most of these algorithms, however, assume that devices have some network knowledge (usually bounds on the number of nodes in the network @math , and the diameter @math ), which may not always be realistic in systems with weak devices or gradual deployment. Very little is known about what can be done when this information is not available. This is the issue we address in this work, by presenting the first broadcasting algorithms for networks in which nodes have no prior knowledge whatsoever. We demonstrate that lack of parameter knowledge can be overcome at only a small increase in running time. Specifically, we show that in networks without collision detection, broadcast can be achieved in @math time, almost reaching the @math lower bound. We also give an algorithm for directed networks with collision detection, which requires only @math time.\"","summary":"\"All of these results also , and algorithms that do not require such knowledge have been little studied. The closest analogue in the literature is the work of Jurdzinski and Stachowiak @cite_11 , who give algorithms for wake-up in single-hop radio networks (those in which the underlying graph is a clique, i.e. @math ) under a wide range of node knowledge assumptions. Their Use-Factorial-Representation algorithm is the most relevant; the running time is given as @math for high-probability wake-up with a global clock (a slightly stronger task than broadcasting) in single-hop networks, but a similar analysis as we present here would demonstrate that the algorithm also performs broadcasting in multi-hop networks in @math time.\"","":""}
{"id":"2801080321","dialogue":"\"Radio networks are a long-studied model for distributed system of devices which communicate wirelessly. When these devices are mobile or have limited capabilities, the system is often best modeled by the ad-hoc variant, in which the devices do not know the structure of the network. A large body of work has been devoted to designing algorithms for the ad-hoc model, particularly for fundamental communications tasks such as broadcasting. Most of these algorithms, however, assume that devices have some network knowledge (usually bounds on the number of nodes in the network @math , and the diameter @math ), which may not always be realistic in systems with weak devices or gradual deployment. Very little is known about what can be done when this information is not available. This is the issue we address in this work, by presenting the first broadcasting algorithms for networks in which nodes have no prior knowledge whatsoever. We demonstrate that lack of parameter knowledge can be overcome at only a small increase in running time. Specifically, we show that in networks without collision detection, broadcast can be achieved in @math time, almost reaching the @math lower bound. We also give an algorithm for directed networks with collision detection, which requires only @math time.\"","summary":"\"A deterministic algorithm for broadcasting in radio networks without parameter knowledge is given in @cite_1 , with a running time of @math , where @math is the range of unique IDs with which nodes are equipped.\"","":""}
{"id":"2951286139","dialogue":"\"Matrix completion is one of the key problems in signal processing and machine learning, with applications ranging from image pro- cessing and data gathering to classification and recommender sys- tems. Recently, deep neural networks have been proposed as la- tent factor models for matrix completion and have achieved state- of-the-art performance. Nevertheless, a major problem with existing neural-network-based models is their limited capabilities to extend to samples unavailable at the training stage. In this paper, we propose a deep two-branch neural network model for matrix completion. The proposed model not only inherits the predictive power of neural net- works, but is also capable of extending to partially observed samples outside the training set, without the need of retraining or fine-tuning. Experimental studies on popular movie rating datasets prove the ef- fectiveness of our model compared to the state of the art, in terms of both accuracy and extendability.\"","summary":"\"Neural networks have been proven effective in several domains such as image classification @cite_29 , sequence modeling @cite_11 , and inverse problems in signal processing @cite_17 . In matrix completion, existing work involves autoencoders, graph convolutional networks and deep learning neural networks. Autoencoder-based models @cite_3 @cite_4 learn transformations from original row or column vectors to a latent space and decoders to predict the missing values. Geometric matrix completion models employ graph convolutional networks to learn the feature vectors from column (or row) graphs @cite_33 or bipartite user-item graphs @cite_27 . The CF-NADE (Collaborative Filtering - Neural Autoregressive Distribution Estimator) method @cite_28 , on the other hand, learns directly the latent vectors from columns and rows. The Neural Collaborative Filtering (NCF) @cite_14 and Collaborative Metric Learning (CML) @cite_21 utilize implicit feedback, i.e. interactions between users and items such as like, follows, shares, rather than explicit feedback, e.g. ratings.\"","":""}
{"id":"2949275688","dialogue":"\"Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code are available at this https URL\"","summary":"\"This paper is closely related to the following areas: semantic part segmentation, joint pose and body part estimation, and weakly supervised learning. In this subtask of semantic segmentation, fully convolutional network (FCN) @cite_25 and its variants @cite_4 @cite_0 @cite_24 have demonstrated promising results. In @cite_4 , Chen proposed atrous convolution to capture object features at different scales and they further combined the convolutional neural network (CNN) with a Conditional Random Field (CRF) to improve the accuracy. @cite_0 , the authors proposed an attention mechanism that softly combines the segmentation predictions at different scales according to the context. To tackle the problem of scale and location variance, Xia @cite_24 developed a model that adaptively zoom the input image into the proper scale to refine the parsing results.\"","":""}
{"id":"2799755616","dialogue":"\"We present a novel, reusable and task-agnostic primitive for assessing the outcome of a force-interaction robotic skill, useful e.g. for applications such as quality control in industrial manufacturing. The proposed method is easily programmed by kinesthetic teaching, and the desired adaptability and reusability are achieved by machine learning models. The primitive records sensory data during both demonstrations and reproductions of a movement. Recordings include the end-effector's Cartesian pose and exerted wrench at each time step. The collected data are then used to train Gaussian Processes which create models of the wrench as a function of the robot's pose. The similarity between the wrench models of the demonstration and the movement's reproduction is derived by measuring their Hellinger distance. This comparison creates features that are fed as inputs to a Naive Bayes classifier which estimates the movement's probability of success. The evaluation is performed on two diverse robotic assembly tasks -- snap-fitting and screwing -- with a total of 5 use cases, 11 demonstrations, and more than 200 movement executions. The performance metrics prove the proposed method's capability of generalization to different demonstrations and movements.\"","summary":"\"An alternative solution to assess the task outcome is statistically setting a threshold regarding the wrench signature and the pose trajectories. Costa al @cite_7 verify the success of a process by setting a threshold for eccentricity and typicality, distance metrics for time series. Haidu al @cite_10 define lower and upper bounds of the trajectory profile based on successful trails. Thus, trajectory profiles that exceed the threshold indicates a failure. Nevertheless, the movement reproduction varies for different task parameters (e.g. start and goal state) and demonstrations. In those cases, the bounds have to be changed accordingly which requires reprogramming.\"","":""}
{"id":"2799755616","dialogue":"\"We present a novel, reusable and task-agnostic primitive for assessing the outcome of a force-interaction robotic skill, useful e.g. for applications such as quality control in industrial manufacturing. The proposed method is easily programmed by kinesthetic teaching, and the desired adaptability and reusability are achieved by machine learning models. The primitive records sensory data during both demonstrations and reproductions of a movement. Recordings include the end-effector's Cartesian pose and exerted wrench at each time step. The collected data are then used to train Gaussian Processes which create models of the wrench as a function of the robot's pose. The similarity between the wrench models of the demonstration and the movement's reproduction is derived by measuring their Hellinger distance. This comparison creates features that are fed as inputs to a Naive Bayes classifier which estimates the movement's probability of success. The evaluation is performed on two diverse robotic assembly tasks -- snap-fitting and screwing -- with a total of 5 use cases, 11 demonstrations, and more than 200 movement executions. The performance metrics prove the proposed method's capability of generalization to different demonstrations and movements.\"","summary":"\"Rojas al @cite_3 identify key segments of the wrench signals and create a task-specific hierarchical taxonomy based on its time derivative. The task outcome is indicated by the states at the highest level of this taxonomy. However, the segmentation threshold is predefined, and the taxonomy associated with segments is manually created which significantly reduces its applicability to different tasks.\"","":""}
{"id":"2799755616","dialogue":"\"We present a novel, reusable and task-agnostic primitive for assessing the outcome of a force-interaction robotic skill, useful e.g. for applications such as quality control in industrial manufacturing. The proposed method is easily programmed by kinesthetic teaching, and the desired adaptability and reusability are achieved by machine learning models. The primitive records sensory data during both demonstrations and reproductions of a movement. Recordings include the end-effector's Cartesian pose and exerted wrench at each time step. The collected data are then used to train Gaussian Processes which create models of the wrench as a function of the robot's pose. The similarity between the wrench models of the demonstration and the movement's reproduction is derived by measuring their Hellinger distance. This comparison creates features that are fed as inputs to a Naive Bayes classifier which estimates the movement's probability of success. The evaluation is performed on two diverse robotic assembly tasks -- snap-fitting and screwing -- with a total of 5 use cases, 11 demonstrations, and more than 200 movement executions. The performance metrics prove the proposed method's capability of generalization to different demonstrations and movements.\"","summary":"\"Haidu al @cite_13 segment the task in smaller pose trajectories and train a Hidden Markov Model (HMM) using each segment as a state. They identify promising states by aligning the segmented samples to the HMM, which contains key information for failure detection. A predictor is trained to assess the task from the aligned data at those promising states. Similarly, Di Lello al @cite_2 segment the wrench signatures by the control strategy of a finite state machine. Each wrench signal is treated as a Bayesian time-series model. A HMM is trained via a Bayesian non-parametric method to detect the deviation from the successful execution. Both of those two methods require the predefined segmentation of the trajectory. However, there is often no such clear segmentation such as Zero Velociy Crossing point @cite_15 or contact events that are plausible in human demonstrations.\"","":""}
{"id":"2799755616","dialogue":"\"We present a novel, reusable and task-agnostic primitive for assessing the outcome of a force-interaction robotic skill, useful e.g. for applications such as quality control in industrial manufacturing. The proposed method is easily programmed by kinesthetic teaching, and the desired adaptability and reusability are achieved by machine learning models. The primitive records sensory data during both demonstrations and reproductions of a movement. Recordings include the end-effector's Cartesian pose and exerted wrench at each time step. The collected data are then used to train Gaussian Processes which create models of the wrench as a function of the robot's pose. The similarity between the wrench models of the demonstration and the movement's reproduction is derived by measuring their Hellinger distance. This comparison creates features that are fed as inputs to a Naive Bayes classifier which estimates the movement's probability of success. The evaluation is performed on two diverse robotic assembly tasks -- snap-fitting and screwing -- with a total of 5 use cases, 11 demonstrations, and more than 200 movement executions. The performance metrics prove the proposed method's capability of generalization to different demonstrations and movements.\"","summary":"\"In the field of imitation learning, Calinon al @cite_5 evaluate a movement in order to optimize a controller. Firstly they align the recorded demonstrations with Hidden Markov Model (HMM) and project them into a latent space via PCA. Then they measure the weights of each vector in the latent space by the variations in multiple demonstrations subject to the same task constraint. Eventually, a similarity measure is defined by the weighted sum of Euclidean distances between a new trajectory and the successful one. The application of this method does not involve wrench in the task and also requires a preprocessing procedure which increases the complexity of implementation.\"","":""}
{"id":"2799755616","dialogue":"\"We present a novel, reusable and task-agnostic primitive for assessing the outcome of a force-interaction robotic skill, useful e.g. for applications such as quality control in industrial manufacturing. The proposed method is easily programmed by kinesthetic teaching, and the desired adaptability and reusability are achieved by machine learning models. The primitive records sensory data during both demonstrations and reproductions of a movement. Recordings include the end-effector's Cartesian pose and exerted wrench at each time step. The collected data are then used to train Gaussian Processes which create models of the wrench as a function of the robot's pose. The similarity between the wrench models of the demonstration and the movement's reproduction is derived by measuring their Hellinger distance. This comparison creates features that are fed as inputs to a Naive Bayes classifier which estimates the movement's probability of success. The evaluation is performed on two diverse robotic assembly tasks -- snap-fitting and screwing -- with a total of 5 use cases, 11 demonstrations, and more than 200 movement executions. The performance metrics prove the proposed method's capability of generalization to different demonstrations and movements.\"","summary":"\"In the field of reinforcement learning, Pastor al @cite_0 propose an algorithm which learns the outcome of an assessment system given a predefined threshold based on a reward function. The system is merged with reinforcement learning framework. However, the definition of the reward function may be challenging for certain tasks and also limits the method's re-usability.\"","":""}
{"id":"2800411455","dialogue":"\"Clustering is a fundamental task in data analysis, and spectral clustering has been recognized as a promising approach to it. Given a graph describing the relationship between data, spectral clustering explores the underlying cluster structure in two stages. The first stage embeds the nodes of the graph into real space, and the second stage groups the embedded nodes into several clusters. The use of the @math -means method in the grouping stage is currently standard practice. We present a spectral clustering algorithm that uses convex programming in the grouping stage, and study how well it works. The concept behind the algorithm design lies in the following observation. The nodes with the largest degree in each cluster may be found by computing an enclosing ellipsoid for embedded nodes in real space, and the clusters may be identified by using those nodes. We show that the observations are valid, and the algorithm returns clusters to provide the conductance of graph, if the gap assumption, introduced by Peng el al. at COLT 2015, is satisfied. We also give an experimental assessment of the algorithm's performance.\"","summary":"\"We describe the results of in @cite_20 . Let @math be the clusters of an optimal @math -way partition for the conductance problem. We choose an algorithm for solving the minimization problem of @math shown in ) and suppose that the algorithm has an approximation ratio of @math . Let @math be the output of a spectral clustering algorithm that uses a @math -means method based on the @math -approximation algorithm. They showed the following statement in Theorem 1.2 of @cite_20 . Set @math as and suppose that @math is so large that @math . After suitable renumbering of the output of the algorithm, we have Accordingly, if @math , those can be written as Here, the notation @math denotes the symmetric difference of the sets @math and @math . The results tell us that, if @math is large, the difference between @math and @math is small and the conductance of @math is close to that of @math . also developed a nearly linear time algorithm and examined the performance.\"","":""}
{"id":"2800411455","dialogue":"\"Clustering is a fundamental task in data analysis, and spectral clustering has been recognized as a promising approach to it. Given a graph describing the relationship between data, spectral clustering explores the underlying cluster structure in two stages. The first stage embeds the nodes of the graph into real space, and the second stage groups the embedded nodes into several clusters. The use of the @math -means method in the grouping stage is currently standard practice. We present a spectral clustering algorithm that uses convex programming in the grouping stage, and study how well it works. The concept behind the algorithm design lies in the following observation. The nodes with the largest degree in each cluster may be found by computing an enclosing ellipsoid for embedded nodes in real space, and the clusters may be identified by using those nodes. We show that the observations are valid, and the algorithm returns clusters to provide the conductance of graph, if the gap assumption, introduced by Peng el al. at COLT 2015, is satisfied. We also give an experimental assessment of the algorithm's performance.\"","summary":"\"We next describe the results of Kolev and Mehlhorn in @cite_29 . They showed the following statement in Theorem 1.2 of the paper. Set @math as After suitable renumbering of the output of the algorithm, we have Accordingly, if @math , We see that the results of Kolev and Mehlhorn improve the approximation accuracy of by a factor of @math and weakens the gap assumption. Kolev and Mehlhorn also studied a spectral clustering algorithm that uses a @math -means method based on the Lloyd's algorithm and examined the performance.\"","":""}
{"id":"2800411455","dialogue":"\"Clustering is a fundamental task in data analysis, and spectral clustering has been recognized as a promising approach to it. Given a graph describing the relationship between data, spectral clustering explores the underlying cluster structure in two stages. The first stage embeds the nodes of the graph into real space, and the second stage groups the embedded nodes into several clusters. The use of the @math -means method in the grouping stage is currently standard practice. We present a spectral clustering algorithm that uses convex programming in the grouping stage, and study how well it works. The concept behind the algorithm design lies in the following observation. The nodes with the largest degree in each cluster may be found by computing an enclosing ellipsoid for embedded nodes in real space, and the clusters may be identified by using those nodes. We show that the observations are valid, and the algorithm returns clusters to provide the conductance of graph, if the gap assumption, introduced by Peng el al. at COLT 2015, is satisfied. We also give an experimental assessment of the algorithm's performance.\"","summary":"\"There is also a considerable amount of research on spectral clustering on a random graph. In a planted partition model, we assume that the node set is partitioned into several clusters and edges connecting the nodes are stochastically generated: any two nodes in the same cluster have an edge with probability @math , and any two nodes in different clusters have an edge with probability @math . McSherry @cite_9 showed that spectral clustering can extract the clusters with high probability if @math and @math lie within some range. Rohe @cite_41 and Lei @cite_0 studied KSC on a stochastic block model.\"","":""}
{"id":"2893706439","dialogue":"\"With huge amounts of training data, deep learning has made great breakthroughs in many artificial intelligence (AI) applications. However, such large-scale data sets present computational challenges, requiring training to be distributed on a cluster equipped with accelerators like GPUs. With the fast increase of GPU computing power, the data communications among GPUs have become a potential bottleneck on the overall training performance. In this paper, we first propose a general directed acyclic graph (DAG) model to describe the distributed synchronous stochastic gradient descent (S-SGD) algorithm, which has been widely used in distributed deep learning frameworks. To understand the practical impact of data communications on training performance, we conduct extensive empirical studies on four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and multi-node environments with different data communication techniques, including PCIe, NVLink, 10GbE, and InfiniBand. Through both analytical and experimental studies, we identify the potential bottlenecks and overheads that could be further optimized. At last, we make the data set of our experimental traces publicly available, which could be used to support simulation-based studies.\"","summary":"\"S-SGD requires the set of computing units (e.g., GPUs) to exchange data iteratively, which can be implemented by either parameter server (PS) based methods @cite_27 @cite_2 or decentralized methods. In PS-based methods, there is one or more PSes that store the global model. The PS aggregates parameters at each iteration, updates the model, and then pushes the updated model to each computing unit. Performance models have been built by S. @cite_30 to generalize the performance of the PS-based methods, which provides guidelines for better system scalability.\"","":""}
{"id":"2893706439","dialogue":"\"With huge amounts of training data, deep learning has made great breakthroughs in many artificial intelligence (AI) applications. However, such large-scale data sets present computational challenges, requiring training to be distributed on a cluster equipped with accelerators like GPUs. With the fast increase of GPU computing power, the data communications among GPUs have become a potential bottleneck on the overall training performance. In this paper, we first propose a general directed acyclic graph (DAG) model to describe the distributed synchronous stochastic gradient descent (S-SGD) algorithm, which has been widely used in distributed deep learning frameworks. To understand the practical impact of data communications on training performance, we conduct extensive empirical studies on four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and multi-node environments with different data communication techniques, including PCIe, NVLink, 10GbE, and InfiniBand. Through both analytical and experimental studies, we identify the potential bottlenecks and overheads that could be further optimized. At last, we make the data set of our experimental traces publicly available, which could be used to support simulation-based studies.\"","summary":"\"Decentralized methods implement the gradients aggregation by using the reduction tree (RT) or ring based all-reduce @cite_28 @cite_20 @cite_15 . The gradients are exchanged via MPI-like collectives (e.g., all-reduce). Very recently, some new collective communications libraries like Gloo https: github.com facebookincubator gloo and NCCL2 https: developer.nvidia.com nccl have been developed to support efficient communications among a set of GPUs. A. @cite_15 @cite_13 propose a high performance CUDA-Aware MPI to reduce the overhead of data communications across a GPU cluster. have shown that the optimized all-reduce implementation and the pipeline of all-reduce operations with gradient computation can lead to very good scalability @cite_29 .\"","":""}
{"id":"2963642591","dialogue":"\"Optical Flow algorithms are of high importance for many applications. Recently, the Flow Field algorithm and its modifications have shown remarkable results, as they have been evaluated with top accuracy on different data sets. In our analysis of the algorithm we have found that it produces accurate sparse matches, but there is room for improvement in the interpolation. Thus, we propose in this paper FlowFields++, where we combine the accurate matches of Flow Fields with a robust interpolation. In addition, we propose improved variational optimization as post-processing. Our new algorithm is evaluated on the challenging KITTI and MPI Sintel data sets with public top results on both benchmarks.\"","summary":"\"Flow Fields @cite_13 can be considered as the basis of our method. It was among the first approaches to achieve top performance across multiple data sets and has been refined several times since its publication. Flow Fields+ @cite_0 improved the algorithm by more sophisticated matching, FlowFieldsCNN @cite_11 used deep learning to evaluate the matching cost, and ProbFlowFields @cite_28 improved the results by jointly estimating optical flow and a certainty measure. Our approach shares the basic concepts with the mentioned methods. That is, we also perform dense matching by multi-scale propagation and random search, followed by outlier rejection, post processed by an interpolation mechanism. However, the novel FlowFields++ differs from those approaches by combining the matching accuracy of Flow Fields with robust interpolation.\"","":""}
{"id":"2963642591","dialogue":"\"Optical Flow algorithms are of high importance for many applications. Recently, the Flow Field algorithm and its modifications have shown remarkable results, as they have been evaluated with top accuracy on different data sets. In our analysis of the algorithm we have found that it produces accurate sparse matches, but there is room for improvement in the interpolation. Thus, we propose in this paper FlowFields++, where we combine the accurate matches of Flow Fields with a robust interpolation. In addition, we propose improved variational optimization as post-processing. Our new algorithm is evaluated on the challenging KITTI and MPI Sintel data sets with public top results on both benchmarks.\"","summary":"Dense interpolation has become a very popular post processing step for many applications ever since the publication of the first successful interpolation method EPICFlow @cite_15 . It was used by Flow Fields and many other matching methods to produce dense results. InterpoNet @cite_18 tried to solve the task of optical flow interpolation with a neural network that showed improvements over EPICFlow depending on input matches and data set. Independent of these factors are the advantages of RICFlow @cite_3 over EPICFlow. The basic idea of edge preserving interpolation is complemented by increased robustness in the computation of the piece-wise interpolation models. We will exploit this robustness for our approach and extend it further by improved edge detectors and adjusted variational refinement.","":""}
{"id":"2963642591","dialogue":"\"Optical Flow algorithms are of high importance for many applications. Recently, the Flow Field algorithm and its modifications have shown remarkable results, as they have been evaluated with top accuracy on different data sets. In our analysis of the algorithm we have found that it produces accurate sparse matches, but there is room for improvement in the interpolation. Thus, we propose in this paper FlowFields++, where we combine the accurate matches of Flow Fields with a robust interpolation. In addition, we propose improved variational optimization as post-processing. Our new algorithm is evaluated on the challenging KITTI and MPI Sintel data sets with public top results on both benchmarks.\"","summary":"\"In recent eras of deep learning, there are also approaches that use convolutional neural networks (CNNs) to aid or solve the task of optical flow estimation. Some try to compute optical flow in and end-to-end manner @cite_8 @cite_4 @cite_12 , others use neural networks to compute a semantic segmentation as additional input @cite_19 @cite_5 @cite_16 , and some use deep learning for matching cost computation @cite_11 @cite_22 @cite_17 . Of course, all deep learning approaches require a lot of proper training data and none yet has showed to generalize well across different data sets without retraining or tuning. Our approach maintains its versatility by avoiding deep learning.\"","":""}
{"id":"2802959425","dialogue":"\"Modern deep learning systems successfully solve many perception tasks such as object pose estimation when the input image is of high quality. However, in challenging imaging conditions such as on low-resolution images or when the image is corrupted by imaging artifacts, current systems degrade considerably in accuracy. While a loss in performance is unavoidable, we would like our models to quantify their uncertainty in order to achieve robustness against images of varying quality. Probabilistic deep learning models combine the expressive power of deep learning with uncertainty quantification. In this paper, we propose a novel probabilistic deep learning model for the task of angular regression. Our model uses von Mises distributions to predict a distribution over object pose angle. Whereas a single von Mises distribution is making strong assumptions about the shape of the distribution, we extend the basic model to predict a mixture of von Mises distributions. We show how to learn a mixture model using a finite and infinite number of mixture components. Our model allows for likelihood-based training and efficient inference at test time. We demonstrate on a number of challenging pose estimation datasets that our model produces calibrated probability predictions and competitive or superior point estimates compared to the current state-of-the-art.\"","summary":"\"has been a subject of extensive research in computer vision for a long time @cite_3 @cite_23 and the existing systems vary greatly in terms of feature representation and proposed classifiers. The input to pose estimation systems typically consists of 2D head images @cite_32 @cite_22 @cite_40 , and often one has to cope with low resolution images @cite_53 @cite_11 @cite_10 @cite_3 . Additional modalities such as depth @cite_45 and motion @cite_10 @cite_46 information has been exploited and provides useful cues. However, these are not always available. Also, information about the full body image could be used for joint head and body pose prediction @cite_42 @cite_48 @cite_31 . Notably the work of @cite_48 also promotes a probabilistic view and fuse body and head orientation within a tracking framework. Finally, the output of facial landmarks can be used as an intermediate step @cite_39 @cite_51 .\"","":""}
{"id":"2802959425","dialogue":"\"Modern deep learning systems successfully solve many perception tasks such as object pose estimation when the input image is of high quality. However, in challenging imaging conditions such as on low-resolution images or when the image is corrupted by imaging artifacts, current systems degrade considerably in accuracy. While a loss in performance is unavoidable, we would like our models to quantify their uncertainty in order to achieve robustness against images of varying quality. Probabilistic deep learning models combine the expressive power of deep learning with uncertainty quantification. In this paper, we propose a novel probabilistic deep learning model for the task of angular regression. Our model uses von Mises distributions to predict a distribution over object pose angle. Whereas a single von Mises distribution is making strong assumptions about the shape of the distribution, we extend the basic model to predict a mixture of von Mises distributions. We show how to learn a mixture model using a finite and infinite number of mixture components. Our model allows for likelihood-based training and efficient inference at test time. We demonstrate on a number of challenging pose estimation datasets that our model produces calibrated probability predictions and competitive or superior point estimates compared to the current state-of-the-art.\"","summary":"\"Existing head pose estimation models are diverse and include manifold learning approaches @cite_49 @cite_7 @cite_47 @cite_12 , energy-based models @cite_31 , linear regression based on HOG features @cite_27 , regression trees @cite_45 @cite_18 and convolutional neural networks @cite_9 . A number of probabilistic methods for head pose analysis exist in the literature @cite_52 @cite_20 @cite_48 , but none of them combine probabilistic framework with learnable hierarchical feature representations from deep CNN architectures. At the same time, deep probabilistic models have shown an advantage over purely discriminative models in other computer vision tasks, e.g., depth estimation @cite_36 . To the best of our knowledge, our work is the first to utilize deep probabilistic approach to angular orientation regression task.\"","":""}
{"id":"2803061603","dialogue":"\"Path planning is important for the autonomy of Unmanned Aerial Vehicle (UAV), especially for scheduling UAV delivery. However, the operating environment of UAVs is usually uncertain and dynamic. Without proper planning, collisions may happen where multiple UAVs are congested. Besides, there may also be temporary no-fly zone setup by authorities that makes airspace unusable. Thus, proper pre-departure planning that avoids such places is needed. In this paper, we formulate this problem into a Constraint Satisfaction Problem to find a collision-free shortest path on a dynamic graph. We propose a collision-free path planning algorithm that is based on A* algorithm. The main novelty is that we invent a heuristic function that also considers waiting time. We later show that, with added waiting penalty, the proposed algorithm is optimal because the heuristic is admissible. Implementation of this algorithm simulates UAV delivery using Singapore's airspace structure. Our simulation exhibits desirable runtime performance. Using the proposed algorithm, the percentage of collision-free routes decreases as number of requests per unit area increases, and this percentage drops significantly at boundary value. Our empirical analysis could aid the decision-making of no-fly zone policy and infrastructure of UAV delivery.\"","summary":"\"Online path finding algorithms is designed to deal with uncertainties and emergencies during the flight. @cite_6 presents some common online algorithms, including potential field approach and particle swarm optimization (PSO). They have advantages and disadvantages: potential field approach are simple and straightforward to understand, but may easily fall into local optimum should no adjustment to algorithm is done. PSO is also intuitive, but it can also be too computationally expensive to reroute in real-time. @cite_3 proposed a predictive control scheme that use shared knowledge between nearby UAVs to find collision-free route, meanwhile considering the dynamics of UAV itself.\"","":""}
{"id":"2803061603","dialogue":"\"Path planning is important for the autonomy of Unmanned Aerial Vehicle (UAV), especially for scheduling UAV delivery. However, the operating environment of UAVs is usually uncertain and dynamic. Without proper planning, collisions may happen where multiple UAVs are congested. Besides, there may also be temporary no-fly zone setup by authorities that makes airspace unusable. Thus, proper pre-departure planning that avoids such places is needed. In this paper, we formulate this problem into a Constraint Satisfaction Problem to find a collision-free shortest path on a dynamic graph. We propose a collision-free path planning algorithm that is based on A* algorithm. The main novelty is that we invent a heuristic function that also considers waiting time. We later show that, with added waiting penalty, the proposed algorithm is optimal because the heuristic is admissible. Implementation of this algorithm simulates UAV delivery using Singapore's airspace structure. Our simulation exhibits desirable runtime performance. Using the proposed algorithm, the percentage of collision-free routes decreases as number of requests per unit area increases, and this percentage drops significantly at boundary value. Our empirical analysis could aid the decision-making of no-fly zone policy and infrastructure of UAV delivery.\"","summary":"\"Off-line algorithms plan the path prior to departure. The aircraft are usually modelled as point mass moving in two dimensions with constant speed and upper-bounded turning rate. @cite_7 uses Genetic Algorithm (GA) to find optimal flyable path through numerous iterations, with each iteration improving previous path by rerouting at waypoint. The problem with Genetic Algorithm is that runtime grows exponentially as number of iterations and flights grows. @cite_5 @cite_2 also uses GA, but with a parallel approach to speedup the calculation. @cite_10 formulates vehicle dynamic model, collision avoidance constraint, and multiple waypoint constraint as mixed-integer linear program (MILP). One of its novelty is that it uses trigonometric functions to better approximate the radius constraint. However, it does not consider battery constraint, which is a limiting factor to UAV's delivery capability. In this paper, we will address both collision avoidance constraint and battery constraint.\"","":""}
{"id":"2803061603","dialogue":"\"Path planning is important for the autonomy of Unmanned Aerial Vehicle (UAV), especially for scheduling UAV delivery. However, the operating environment of UAVs is usually uncertain and dynamic. Without proper planning, collisions may happen where multiple UAVs are congested. Besides, there may also be temporary no-fly zone setup by authorities that makes airspace unusable. Thus, proper pre-departure planning that avoids such places is needed. In this paper, we formulate this problem into a Constraint Satisfaction Problem to find a collision-free shortest path on a dynamic graph. We propose a collision-free path planning algorithm that is based on A* algorithm. The main novelty is that we invent a heuristic function that also considers waiting time. We later show that, with added waiting penalty, the proposed algorithm is optimal because the heuristic is admissible. Implementation of this algorithm simulates UAV delivery using Singapore's airspace structure. Our simulation exhibits desirable runtime performance. Using the proposed algorithm, the percentage of collision-free routes decreases as number of requests per unit area increases, and this percentage drops significantly at boundary value. Our empirical analysis could aid the decision-making of no-fly zone policy and infrastructure of UAV delivery.\"","summary":"\"Given we have global information regarding the the existing schedules of UAVs and their current locations, we can optimize the route through off-line planning. One natural way is using A* algorithm. A* algorithm @cite_4 is a best-first, efficient and optimal search algorithm. It uses heuristics to guide the search, and therefore, reduce the number of nodes needed to be explored. Another advantage for using A* is that it can apply on both 2D and 3D space as long as the graph is connected. @cite_1 proposed an offline improved A* algorithm to deal with realistic constraints in UAV movement, including maximum moving angle, minimum route leg length, minimum flight height and maximum route length. Their algorithm can avoid collision with terrain while minimize the flight height. However, it only handles static graph, and can be computationally expensive. proposed fixes by trimming some less-permissble nodes, or imposing some hard nodes that the route must follows, but doing so will lose some potentially optimal solutions, and thus lose its completeness and optimality.\"","":""}
{"id":"2952645808","dialogue":"\"Verifying correctness of deep neural networks (DNNs) is challenging. We study a generic reachability problem for feed-forward DNNs which, for a given set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency, scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.\"","summary":"\"Adversarial Example Generation Most existing works, e.g., @cite_25 @cite_15 @cite_7 @cite_6 @cite_16 , apply various heuristic algorithms, generally using search algorithms based on gradient descent or evolutionary techniques. @cite_14 construct a saliency map of the importance of the pixels based on gradient descent and then modify the pixels. In contrast with our approach based on global optimisation and works on safety verification, these methods may be able to find adversarial examples efficiently, but are of adversarial examples when the algorithm fails to find one.\"","":""}
{"id":"2952645808","dialogue":"\"Verifying correctness of deep neural networks (DNNs) is challenging. We study a generic reachability problem for feed-forward DNNs which, for a given set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency, scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.\"","summary":"\"Output Range Analysis The safety verification approach can be adapted to work on this problem. Moreover, @cite_12 consider determining whether an output value of a DNN is reachable from a given input subspace, and propose an MILP solution. @cite_10 study the range of output values from a given input subspace. Their method interleaves local search (based on gradient descent) with global search (based on reduction to MILP). Both approaches can only work with small networks.\"","":""}
{"id":"2802667455","dialogue":"\"Neural machine translation (NMT) has been accelerated by deep learning neural networks over statistical-based approaches, due to the plethora and programmability of commodity heterogeneous computing architectures such as FPGAs and GPUs and the massive amount of training corpuses generated from news outlets, government agencies and social media. Training a learning classifier for neural networks entails tuning hyper-parameters that would yield the best performance. Unfortunately, the number of parameters for machine translation include discrete categories as well as continuous options, which makes for a combinatorial explosive problem. This research explores optimizing hyper-parameters when training deep learning neural networks for machine translation. Specifically, our work investigates training a language model with Marian NMT. Results compare NMT under various hyper-parameter settings across a variety of modern GPU architecture generations in single node and multi-node settings, revealing insights on which hyper-parameters matter most in terms of performance, such as words processed per second, convergence rates, and translation accuracy, and provides insights on how to best achieve high-performing NMT systems.\"","summary":"\"Britz, et. al. study a massive analysis of NMT hyper-parameters aiming for better optimization being robust to the hyper-parameter variations @cite_9 . Likewise, Bahar et. al. compare various optimization strategies for NMT @cite_10 . In addition, Wu, et. al. @cite_1 utilized the combination of Adam and a simple stochastic gradient descent (SGD) learning algorithm. They run Adam for a fixed number of iterations and switch to SGD to slow down the training phase. To the best of our knowledge, there has not been any work comparing different optimization strategies for NMT. Most of the work in this area focuses on the modeling problem on a vanilla NMT task without exploring the tradeoffs of parameter selection, in terms of performance and stability.\"","":""}
{"id":"2798296308","dialogue":"\"This study explores the creation of a machine learning model to automatically identify whether a Neonatal Intensive Care Unit (NICU) patient was diagnosed with neonatal jaundice during a particular hospitalization based on their associated clinical notes. We develop a number of techniques for text preprocessing and feature selection and compare the effectiveness of different classification models. We show that using ensemble decision tree classification, both with AdaBoost and with bagging, outperforms support vector machines (SVM), the current state-of-the-art technique for neonatal jaundice coding.\"","summary":"\"Given the importance and time-intensive nature of medical coding, many scientists have started developing techniques for automated coding @cite_3 . For most of these attempts, the task was narrow (i.e. attempting to predict a single ICD code or a small set of related codes) and utilized a relatively small dataset.\"","":""}
{"id":"2798296308","dialogue":"\"This study explores the creation of a machine learning model to automatically identify whether a Neonatal Intensive Care Unit (NICU) patient was diagnosed with neonatal jaundice during a particular hospitalization based on their associated clinical notes. We develop a number of techniques for text preprocessing and feature selection and compare the effectiveness of different classification models. We show that using ensemble decision tree classification, both with AdaBoost and with bagging, outperforms support vector machines (SVM), the current state-of-the-art technique for neonatal jaundice coding.\"","summary":"\"At least one attempt has been made to use deep learning (namely character-level recurrent neural networks) to build more general coding models, but with limited success @cite_7 . The main limitation with this method is that deep learning models require a large amount of data to train @cite_24 , but even a large clinical dataset may only contain a few examples of case files with any given ICD code. Additionally, the ICD-9 standard contains over 14,000 different codes; treating coding as a simple multi-class classification problem with 14,000 different classes is infeasible.\"","":""}
{"id":"2798296308","dialogue":"\"This study explores the creation of a machine learning model to automatically identify whether a Neonatal Intensive Care Unit (NICU) patient was diagnosed with neonatal jaundice during a particular hospitalization based on their associated clinical notes. We develop a number of techniques for text preprocessing and feature selection and compare the effectiveness of different classification models. We show that using ensemble decision tree classification, both with AdaBoost and with bagging, outperforms support vector machines (SVM), the current state-of-the-art technique for neonatal jaundice coding.\"","summary":"More successful automated coding models have utilized non-deep machine learning techniques such as support vector machines (SVMs) @cite_15 and focus on training a model to detect the presence of a single ICD code or class of related codes. Some of these methods also leverage structured data stored in health records in addition to free-text narratives @cite_22 .","":""}
{"id":"2798296308","dialogue":"\"This study explores the creation of a machine learning model to automatically identify whether a Neonatal Intensive Care Unit (NICU) patient was diagnosed with neonatal jaundice during a particular hospitalization based on their associated clinical notes. We develop a number of techniques for text preprocessing and feature selection and compare the effectiveness of different classification models. We show that using ensemble decision tree classification, both with AdaBoost and with bagging, outperforms support vector machines (SVM), the current state-of-the-art technique for neonatal jaundice coding.\"","summary":"At least one previous attempt has been made to build an automatic coding model for neonatal jaundice; @cite_10 uses an SVM on the text of clinical notes. Our research improves on this model by employing an additional preprocessing step and using ensemble classification methods. We explore and compare these alternative classification techniques and analyze their efficacy.","":""}
{"id":"2799045403","dialogue":"\"Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper","summary":"we propose a novel superpixel-guided two-view geometric model fitting method (called SDF)","":""}
{"id":"2799045403","dialogue":"\"Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper","summary":"we propose a novel superpixel-guided two-view geometric model fitting method (called SDF)","":""}
{"id":"2799045403","dialogue":"\"Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper","summary":"we propose a novel superpixel-guided two-view geometric model fitting method (called SDF)","":""}
{"id":"2799049915","dialogue":"\"Recently, spiking neural network (SNN) has received significant attentions for its biological plausibility. SNN theoretically has at least the same computational power as traditional artificial neural networks (ANNs), and it has the potential to achieve revolutionary energy-efficiency. However, at current stage, it is still a big challenge to train a very deep SNN. In this paper, we propose an efficient approach to build a spiking version of deep residual network (ResNet), which represents the state-of-the-art convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons named Spiking ResNet. To address the conversion problem, we propose a shortcut normalisation mechanism to appropriately scale continuous-valued activations to match firing rates in SNN, and a layer-wise error compensation approach to reduce the error caused by discretisation. Experimental results on MNIST, CIFAR-10, and CIFAR-100 demonstrate that the proposed Spiking ResNet yields the state-of-the-art performance of SNNs.\"","summary":"\"Due to the indifferentiability in the spiking mechanism of SNN, it is hard to directly apply classic learning methods of ANNs, such as backpropagation, to SNNs. In order to address this problem, researchers have devised a wide variety of learning algorithms to build SNNs. One approach is to directly learn from spikes with backpropagation-like algorithm by approximating the threshold function with linearity. For example, early work such as the @math @cite_29 assumes the potential function to be differentiable for a small region around firing time. More recently, @cite_2 demonstrated learning SNN with stochastic gradient descent (SGD) by regarding membrane potentials of spiking neurons as differentiable signals and discontinuities at spike times as noise.\"","":""}
{"id":"2799049915","dialogue":"\"Recently, spiking neural network (SNN) has received significant attentions for its biological plausibility. SNN theoretically has at least the same computational power as traditional artificial neural networks (ANNs), and it has the potential to achieve revolutionary energy-efficiency. However, at current stage, it is still a big challenge to train a very deep SNN. In this paper, we propose an efficient approach to build a spiking version of deep residual network (ResNet), which represents the state-of-the-art convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons named Spiking ResNet. To address the conversion problem, we propose a shortcut normalisation mechanism to appropriately scale continuous-valued activations to match firing rates in SNN, and a layer-wise error compensation approach to reduce the error caused by discretisation. Experimental results on MNIST, CIFAR-10, and CIFAR-100 demonstrate that the proposed Spiking ResNet yields the state-of-the-art performance of SNNs.\"","summary":"\"Another approach is to learn with spike-timing-dependent plasticity (STDP) @cite_26 , a biologically plausible approach. In @cite_12 @cite_23 , STDP has been proven able to select features in an unsupervised manner. Improved STDP algorithms such as rectangular STDP @cite_7 and exponential STDP @cite_30 can achieve competitive performance on MNIST dataset with a simple two-layer network.\"","":""}
{"id":"2798902623","dialogue":"\"Understanding driving situations regardless the conditions of the traffic scene is a cornerstone on the path towards autonomous vehicles; however, despite common sensor setups already include complementary devices such as LiDAR or radar, most of the research on perception systems has traditionally focused on computer vision. We present a LiDAR-based 3D object detection pipeline entailing three stages. First, laser information is projected into a novel cell encoding for bird's eye view projection. Later, both object location on the plane and its heading are estimated through a convolutional neural network originally designed for image processing. Finally, 3D oriented detections are computed in a post-processing phase. Experiments on KITTI dataset show that the proposed framework achieves state-of-the-art results among comparable methods. Further tests with different LiDAR sensors in real scenarios assess the multi-device capabilities of the approach.\"","summary":"\"Research on autonomous perception has commonly focused on modeling advanced hand-crafted features, such as HoG @cite_16 or Haar @cite_12 . Nonetheless, since the emergence of modern Convolutional Neural Networks (CNNs) and large-scale image datasets such as ImageNet @cite_19 , object detection studies have progressively moved towards feature learning approaches, which produce more robust representations of objects, increasing the performance on classification tasks.\"","":""}
{"id":"2798902623","dialogue":"\"Understanding driving situations regardless the conditions of the traffic scene is a cornerstone on the path towards autonomous vehicles; however, despite common sensor setups already include complementary devices such as LiDAR or radar, most of the research on perception systems has traditionally focused on computer vision. We present a LiDAR-based 3D object detection pipeline entailing three stages. First, laser information is projected into a novel cell encoding for bird's eye view projection. Later, both object location on the plane and its heading are estimated through a convolutional neural network originally designed for image processing. Finally, 3D oriented detections are computed in a post-processing phase. Experiments on KITTI dataset show that the proposed framework achieves state-of-the-art results among comparable methods. Further tests with different LiDAR sensors in real scenarios assess the multi-device capabilities of the approach.\"","summary":"\"Besides 2D detections, some approaches have been made attempting to provide spatial location of objects based on visual information. In @cite_17 , 3D object candidates are placed over a ground plane prior and classified in image space. Similarly, 3D voxel patterns have been used to estimate position and orientation of objects in the scene @cite_7 .\"","":""}
{"id":"2798902623","dialogue":"\"Understanding driving situations regardless the conditions of the traffic scene is a cornerstone on the path towards autonomous vehicles; however, despite common sensor setups already include complementary devices such as LiDAR or radar, most of the research on perception systems has traditionally focused on computer vision. We present a LiDAR-based 3D object detection pipeline entailing three stages. First, laser information is projected into a novel cell encoding for bird's eye view projection. Later, both object location on the plane and its heading are estimated through a convolutional neural network originally designed for image processing. Finally, 3D oriented detections are computed in a post-processing phase. Experiments on KITTI dataset show that the proposed framework achieves state-of-the-art results among comparable methods. Further tests with different LiDAR sensors in real scenarios assess the multi-device capabilities of the approach.\"","summary":"\"Some other works have used point cloud data to compute object detections in 3D, either using information from stereo cameras or laser sensors. Although there are some methods which use hand-crafted features @cite_10 @cite_22 , latest approaches take advantage of feature learning capabilities.\"","":""}
{"id":"2798902623","dialogue":"\"Understanding driving situations regardless the conditions of the traffic scene is a cornerstone on the path towards autonomous vehicles; however, despite common sensor setups already include complementary devices such as LiDAR or radar, most of the research on perception systems has traditionally focused on computer vision. We present a LiDAR-based 3D object detection pipeline entailing three stages. First, laser information is projected into a novel cell encoding for bird's eye view projection. Later, both object location on the plane and its heading are estimated through a convolutional neural network originally designed for image processing. Finally, 3D oriented detections are computed in a post-processing phase. Experiments on KITTI dataset show that the proposed framework achieves state-of-the-art results among comparable methods. Further tests with different LiDAR sensors in real scenarios assess the multi-device capabilities of the approach.\"","summary":"\"Among these latter group, two different strategies are being explored. On the one hand, some approaches work with spatial information by turning the 3D space into a voxel grid and applying 3D convolutions @cite_11 @cite_14 @cite_1 . On the other hand, 2D CNNs are used by projecting LiDAR point cloud into a front view @cite_15 or a bird's eye view (BEV) @cite_2 .\"","":""}
{"id":"2798902623","dialogue":"\"Understanding driving situations regardless the conditions of the traffic scene is a cornerstone on the path towards autonomous vehicles; however, despite common sensor setups already include complementary devices such as LiDAR or radar, most of the research on perception systems has traditionally focused on computer vision. We present a LiDAR-based 3D object detection pipeline entailing three stages. First, laser information is projected into a novel cell encoding for bird's eye view projection. Later, both object location on the plane and its heading are estimated through a convolutional neural network originally designed for image processing. Finally, 3D oriented detections are computed in a post-processing phase. Experiments on KITTI dataset show that the proposed framework achieves state-of-the-art results among comparable methods. Further tests with different LiDAR sensors in real scenarios assess the multi-device capabilities of the approach.\"","summary":"\"Since camera and LiDAR are complementary data sources, many works have tried to build robust object detection frameworks by fusing their information. Traditional methods used to obtain 3D ROIs from the point cloud and perform classification on their image projection @cite_3 . However, when RPNs step in, the candidate proposal stage was outperformed both in ROIs quality and execution time. In this direction, a novel approach has been presented in @cite_0 where regions of interest and classification are computed on the image space, and final location is performed over the LiDAR data.\"","":""}
{"id":"2798779678","dialogue":"\"Several recently proposed methods aim to learn conceptual space representations from large text collections. These learned representations asso- ciate each object from a given domain of interest with a point in a high-dimensional Euclidean space, but they do not model the concepts from this do- main, and can thus not directly be used for catego- rization and related cognitive tasks. A natural solu- tion is to represent concepts as Gaussians, learned from the representations of their instances, but this can only be reliably done if sufficiently many in- stances are given, which is often not the case. In this paper, we introduce a Bayesian model which addresses this problem by constructing informative priors from background knowledge about how the concepts of interest are interrelated with each other. We show that this leads to substantially better pre- dictions in a knowledge base completion task.\"","summary":"\"Considerable attention has also been paid to the problem of learning categories for which no, or only few training examples are available, especially within the area of image recognition. For example, in one common setting, each category is defined w.r.t. a set of features, and the assumption is that we have training examples for some of the categories, but not for all of them. Broadly speaking, the aim is then to learn a model of the individual features, rather than the categories, which then makes it possible to make predictions about previously unseen categories @cite_23 @cite_9 . Other approaches instead exploit the representation of the category names in a word embedding @cite_8 . We will similarly exploit vector space representations of concept names.\"","":""}
{"id":"2964201809","dialogue":"\"Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.\"","summary":"\"Togelius @cite_13 defined Procedural Content Generation (PCG) as the @cite_13 @cite_1 @cite_8 . Examples of game content include game rules, levels, maps mazes, characters, weapons, vehicles, background stories, textures and sound. Automatic game level generation, with little or no human intervention, is a challenging problem. For some games, the levels are represented as maps or mazes @cite_10 . Examples include , , and , one of the classic platform video games created by .\"","":""}
{"id":"2964201809","dialogue":"\"Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.\"","summary":"\"The first academic Procedural Content Generation competition was the 2010 Mario AI Championship @cite_0 , in which the participants were required to submit a level generator which implements a provided Java interface and returns a new level within @math seconds. The competition framework was implemented based on https: tinyurl.com yan4ep7g , a public clone of .\"","":""}
{"id":"2964201809","dialogue":"\"Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.\"","summary":"\"The availability and popularity of the Mario AI framework has led to several approaches for generating levels for . Shaker @cite_15 evolved Mario levels using Grammatical Evolution (GE). In 2016, Summerville and Mateas @cite_14 applied Long Short-Term Memory Recurrent Neural Networks (LSTMs) to generate game levels trained on existing Mario levels, and then improved the generated levels by incorporating player path information. This approach inspired a novel approach to level generation, in which new levels are generated automatically from a sketch of some desired path drawn by a human designer. Another approach that was trained using existing Mario levels is that of Jain @cite_7 , which trained auto-encoders to generate new levels using a binary encoding where empty (accessible) spaces are represented by 0 and the others (e.g., terrain, enemy, tunnel, etc.) by 1. Though this approach could generate interesting levels, the use of random noise inputs into the trained auto-encoder sometimes led to problematic levels. Additionally, because of the binary encoding, no distinction was made between various possible types of tiles.\"","":""}
{"id":"2964201809","dialogue":"\"Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.\"","summary":"\"Generative Adversarial Networks (GANs) were first introduced by Goodfellow @cite_6 in 2014. Their training process can be seen as a two-player adversarial game in which a generator @math (faking samples decoded from a random noise vector) and a discriminator @math (distinguishing real fake samples and outputting 0 or 1) are trained at the same time by playing against each other. The discriminator @math aims at minimizing the probability of mis-judgment, while the generator @math aims at maximizing that probability. Thus, the generator is trained to deceive the discriminator by generating samples that are good enough to be classified as genuine. Training ideally reaches a steady state where @math reliably generates realistic examples and @math is no more accurate than a coin flip.\"","":""}
{"id":"2964201809","dialogue":"\"Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.\"","summary":"\"In another paper @cite_4 present an interactive evolutionary system, in which users can evolve the latent vectors for a GAN trained on different classes of objects (e.g. faces or shoes). Because the GAN is trained on a specific target domain, it becomes a compact and robust genotype-to-phenotype mapping (i.e. most produced phenotypes do resemble valid domain artifacts) and users were able to guide evolution towards images that closely resembled given target images. Such target based evolution has been shown to be challenging with other indirect encodings @cite_12 .\"","":""}
{"id":"2964201809","dialogue":"\"Generative Adversarial Networks (GANs) are a machine learning approach capable of generating novel example outputs across a space of provided training examples. Procedural Content Generation (PCG) of levels for video games could benefit from such models, especially for games where there is a pre-existing corpus of levels to emulate. This paper trains a GAN to generate levels for Super Mario Bros using a level from the Video Game Level Corpus. The approach successfully generates a variety of levels similar to one in the original corpus, but is further improved by application of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions are used to discover levels within the latent space of the GAN that maximize desired properties. Simple static properties are optimized, such as a given distribution of tile types. Additionally, the champion A* agent from the 2009 Mario AI competition is used to assess whether a level is playable, and how many jumping actions are required to beat it. These fitness functions allow for the discovery of levels that exist within the space of examples designed by experts, and also guide the search towards levels that fulfill one or more specified objectives.\"","summary":"\"Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) @cite_2 is a powerful and widely used evolutionary algorithm that is well suited for evolving vectors of real numbers. The CMA-ES is a second-order method using the covariance matrix estimated iteratively by finite differences. It has been demonstrated to be efficient for optimizing non-linear non-convex problems in the continuous domain without a-priori domain knowledge, and it does not rely on the assumption of a smooth fitness landscape.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"As part of the general interest in search-based software engineering (SBSE) approaches @cite_11 , much research attention has been given to the application of meta-heuristic search techniques to address the combinatorial test generation problem. Meta-heuristic techniques have had a big impact on the construction of @math and variable-strength test suites, especially in terms of the optimality of the test suite @cite_30 @cite_9 @cite_27 @cite_13 @cite_28 .\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"@cite_25 @cite_1 developed a simulated annealing-based strategy for supporting the construction of a uniform and variable-strength @math test suite. A large random search space is generated in the implementation. When the algorithm iterates, the strategy chooses better test cases to construct the final test suite using the binary search process and a transformation equation. The search space is transformed from one state to another according to a probability equation. The results of the study are mainly concerned with the interaction strengths of two and three @cite_1 .\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"@cite_26 implemented a @math strategy based on ant colony optimization (ACO). The strategy simulates the behaviour of natural ant colonies in finding paths from the colony to the location of food. Each ant generates one candidate solution and walks through all paths in this solution by probabilistically choosing individual values. When the ant reaches the end of the last path, it returns and updates the initial candidate solution accordingly. This process continues until the iteration is complete. The final test case is chosen according to the maximum coverage of the t-interaction. Unlike the SA, the final test suite is further optimized by a merging algorithm that tries to merge the test cases.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"@cite_33 adopted a genetic algorithm (GA) based on natural selection. Initially, the GA begins with randomly created test cases called chromosomes. These chromosomes undergo crossover and mutation until the termination criterion is met. In each cycle, the best chromosomes are probabilistically selected and added to the final test suite.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"@cite_30 developed a @math strategy based on the harmony search algorithm (HSS). The HSS mimics the behaviour of musicians trying to compose good music either by improvising on the best tune they remember or by random sampling. In doing so, the HSS iteratively exploits the harmonic memory to store the best solution found through a number of defined probabilistic improvisations within its local and global search processes. In each improvisation, one test case is selected for the final test suite until all the required interactions are covered. The notable feature of the HSS is that it supports constraints using the forbidden tuple approach.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"@cite_0 adopted the cuckoo search algorithm (CS), which mimics the unique lifestyle and aggressive reproduction strategy of the cuckoo. First, the CS generates random initial eggs in host nests. Each egg in a nest represents a vector solution that represents a test case. In each generation, two operations are performed. Initially, a new nest is generated (typically through a Lévy flight) and compared with the existing nests. The new nest replaces the current nest if it has a better objective function. Then, the CS adopts probabilistic elitism to maintain the elite solutions for the next generation.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"Particle swarm optimization (PSO) @cite_4 is perhaps the most popular implementation of @math test suite generation. The PSO-based @math strategy searches by mimicking the swarm behaviour of flocking birds. In PSO, global and local searches are guided by the inertia weight and social cognitive parameters. Initially, a random swarm is created. Then, the PSO algorithm iteratively selects a candidate solution within the swarm to be added to the final test suite until all interaction tuples are covered (based on velocity and displacement transformation). developed early PSO-based strategies called the PSTG @cite_16 @cite_3 and APSO @cite_10 . APSO is an improvement on the PSTG integrated with adaptive tuning based on the Mamdani fuzzy inference system @cite_31 @cite_35 . implemented discrete PSO @cite_16 by substantially modifying the displacement and velocity transformation used in PSO. The benchmark results of DPSO @cite_5 demonstrate its superior performance when compared with both the PSTG and APSO.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"Despite the significant number of proposed algorithms in this field, the adoption of new meta-heuristic algorithms is most welcome. The no free lunch (NFL) theorem @cite_14 suggests that no single meta-heuristic algorithm can outperform others even when there is a slight change in the problem of ( @math ) configurations. Therefore, the NFL theorem allows researchers to propose new algorithms or modify current ones to enhance the current solution. In fact, the results could also be applied in other fields.\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"Hybrid integration with machine learning appears to be a viable approach to improving the state-of-the-art meta-heuristic algorithms. Machine learning relates to the study of the fundamental laws that govern the computer learning process concerning the construction of systems that can automatically learned from experience. Machine learning techniques can be classified into three types: supervised, unsupervised, and reinforcement @cite_15 . Supervised learning involves learning a direct functional input-output mapping based on some set of training data and being able to predict new data. Unlike supervised learning, unsupervised learning does not require explicit training data. Specifically, unsupervised learning involves learning by drawing inferences (e.g., clustering) from an input dataset. Reinforcement learning allows mappings between states and actions to maximize reward signals using experimental discovery. This type of learning differs from supervised learning in that it relies on a punishment and reward mechanism and never corrects input-output pairs (even when dealing with suboptimal responses).\"","":""}
{"id":"2798358022","dialogue":"\"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (Le´vy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95 confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95 confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90 confidence level.\"","summary":"\"By building on and complementing the work mentioned above, our work explores the hybridization of the Q-learning algorithm with a recently developed meta-heuristic algorithm called the SCA @cite_42 . Unlike most meta-heuristic algorithms that mimic certain physical or natural phenomena, the equation transformation used in the SCA is solely based on the sine and cosine operations. Therefore, the learning curve of the SCA is low. Although its exploitation is commendable, the exploration of the SCA is strictly bounded due to the (adaptive) shrinking magnitude of the sine and cosine functions’ multipliers during the search process. To address the issues mentioned above, we propose a new algorithm, the QLSCA. Moreover, we augment the QLSCA with two further operations (Lévy flight motion and crossover) to counterbalance its exploration and exploitation. Then, we use the Q-learning algorithm (which is based on the penalty and reward mechanism) to dynamically identify on the best operation (sine, cosine, Lévy flight motion, or crossover) during runtime.\"","":""}
{"id":"2971325325","dialogue":"\"Implicit feedback (e.g., click, dwell time) is an attractive source of training data for Learning-to-Rank, but its naive use leads to learning results that are distorted by presentation bias. For the special case of optimizing average rank for linear ranking functions, however, the recently developed SVM-PropRank method has shown that counterfactual inference techniques can be used to provably overcome the distorting effect of presentation bias. Going beyond this special case, this paper provides a general and theoretically rigorous framework for counterfactual learning-to-rank that enables unbiased training for a broad class of additive ranking metrics (e.g., Discounted Cumulative Gain (DCG)) as well as a broad class of models (e.g., deep networks). Specifically, we derive a relaxation for propensity-weighted rank-based metrics which is subdifferentiable and thus suitable for gradient-based optimization. We demonstrate the effectiveness of this general approach by instantiating two new learning methods. One is a new type of unbiased SVM that optimizes DCG -- called SVM PropDCG --, and we show how the resulting optimization problem can be solved via the Convex Concave Procedure (CCP). The other is Deep PropDCG, where the ranking function can be an arbitrary deep network. In addition to the theoretical support, we empirically find that SVM PropDCG significantly outperforms existing linear rankers in terms of DCG. Moreover, the ability to train non-linear ranking functions via Deep PropDCG further improves performance.\"","summary":"\"While our focus is on directly optimizing ranking performance in the implicit feedback partial-information setting, several approaches have been proposed for the same task in the full-information supervised setting, i.e. when the relevances of all the documents in the training set are known. A common strategy is to use some smoothed version of the ranking metric for optimization, as seen in SoftRank @cite_21 and others @cite_15 @cite_17 @cite_27 @cite_25 . In particular, SoftRank optimizes the expected performance metric over the distribution of rankings induced by smoothed scores, which come from a normal distribution centered at the query-document mean scores predicted by a neural net. This procedure is computationally expensive with an @math dependence on the number of documents for a query. In contrast, our approach employs an upper bound on the performance metric, whose structure makes it amenable to the Convex Concave Procedure for efficient optimization, as well as adaptable to non-linear ranking functions via deep networks.\"","":""}
{"id":"2964134356","dialogue":"\"Wireless access points on Unmanned Aerial Vehicles (UAVs) are being considered for mobile service provisioning in commercial networks. These UAV access points will carry radio infrastructure and will be temporarily deployed in areas of dense user traffic to deal with excess user data demand. In this paper, we analyze the coverage when UAVs act as access points for users on the ground in an urban area. We consider the impact of several UAV placement strategies, either independent of, or responsive to, the locations of the users on the ground. Our analysis allows us to demonstrate how the density of the UAVs in the network will determine whether the UAVs should position themselves closer to user hotspots to improve the received signal strength, or further away from one another to mitigate interference. In addition, we demonstrate how network design parameters such as the UAV height above ground or the antenna beamwidth impact the coverage probability. In addition to simulations, we provide mathematical expressions for the coverage probability, for the scenario where UAVs are positioned directly above the centers of user hotspots.\"","summary":"\"We report an emerging trend which can be observed in the works cited above. With the notable exception of @cite_0 , existing state of the art on UAV network optimisation tends to ignore the effects of interference, instead focusing on scenarios where individual UAVs are operating in isolation. Without interference the wireless links are limited by the geometry of the environment, and therefore the network performance is generally optimised through minimising the distance between the UAV and the receiver, as this minimises the pathloss, increases the LOS probability and enables the network to reduce transmit power. These optimisation strategies may not apply to a scenario where multiple UAVs are operating concurrently and creating interference for each other. In the presence of interference, decreasing the distances between transmitters and receivers may also have the result of decreasing the distances between interferers and receivers, potentially causing a net decrease in channel performance.\"","":""}
{"id":"2798785261","dialogue":"\"Convolutional networks (ConvNets) have achieved great successes in various challenging vision tasks. However, the performance of ConvNets would degrade when encountering the domain shift. The domain adaptation is more significant while challenging in the field of biomedical image analysis, where cross-modality data have largely different distributions. Given that annotating the medical data is especially expensive, the supervised transfer learning approaches are not quite optimal. In this paper, we propose an unsupervised domain adaptation framework with adversarial learning for cross-modality biomedical image segmentations. Specifically, our model is based on a dilated fully convolutional network for pixel-wise prediction. Moreover, we build a plug-and-play domain adaptation module (DAM) to map the target input to features which are aligned with source domain feature space. A domain critic module (DCM) is set up for discriminating the feature space of both domains. We optimize the DAM and DCM via an adversarial loss without using any target domain label. Our proposed method is validated by adapting a ConvNet trained with MRI images to unpaired CT data for cardiac structures segmentations, and achieved very promising results.\"","summary":"\"Most prior studies on unsupervised domain adaptation focused on aligning the distributions between domains in feature space, by minimizing measures of distance between features extracted from the source and target domains. For example, the Maximum Mean Discrepancy (MMD) was minimized together with a task-specific loss to learn the domain-invariant and semantic-meaningful features in @cite_9 . The correlations of layer activations between the domains were aligned in the study of @cite_21 . Based on this, @cite_0 further extended the work and minimized domain difference based on both the first and second order information between source and target domains. Alternatively, with the emergence of generative adversarial network (GAN) @cite_11 and its powerful extensions @cite_17 @cite_18 , the mapping between domains were implicitly learned via the adversarial loss. The @cite_13 proposed to extract domain-invariant features by sharing weights between two ConvNet classifiers. Later, the @cite_3 introduced a more flexible adversarial learning method with untied weight sharing, which helps effective learning in the presence of larger domain shifts. Another GAN based direction of solution is to learn a transformation in the pixel space @cite_12 , adapting the source-domain images to appear as if drawn from the target domain.\"","":""}
{"id":"2963860801","dialogue":"\"Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of back-grounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of back-grounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5 and 2 mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.\"","summary":"\"Boosted decision tree in @cite_13 is trained with hard example mining strategy but hard examples are mined one time only. To begin with, all the positive examples and a random set of negative examples are blended together as the original training set. After reaching convergence on the original training set, the trained model is applied to the rest of negative examples. Then, only false positive examples are selected as hard ones and added to the original training set to form the final training set. Finally, the model is trained on the refreshed training set until convergence.\"","":""}
{"id":"2963860801","dialogue":"\"Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of back-grounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of back-grounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5 and 2 mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.\"","summary":"\"Another hard exampling mining technique named boostrapping is used to train Support Vector Machines(SVMs) @cite_21 and hard examples are mined several times in this case. A working set containing a tiny number of samples is used in boostrapping. Samples are added to and removed from this set according to some specific rules. Processes of training model to convergence on the existing working set and utilizing the trained model to modify the working set are finished alternatively. When modifying the working set, samples in the working set classified correctly by the existing model are removed from it while samples out of the working set misclassified by the model are added to it.\"","":""}
{"id":"2963860801","dialogue":"\"Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of back-grounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of back-grounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5 and 2 mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.\"","summary":"\"The history of utilizing techniques of hard example mining in object detection can date back to the time when it was used to train SVMs for pedestrian detection @cite_15 . After the prevalence of CNN-based model in object detection, hard example mining still played an important role as an SVM classifier is usually attached to the top of detectors for classification, e.g. @cite_11 @cite_6 .\"","":""}
{"id":"2963860801","dialogue":"\"Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of back-grounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of back-grounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5 and 2 mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.\"","summary":"\"However, after SVMs being replaced by layers consisting of neural units in subsequent object detection methods @cite_16 @cite_1 , hard example mining strategies were not utilized in the training of CNN-based detectors until Online Hard Example Mining(OHEM) proposed in @cite_17 . OHEM depends on the RoIs proposed by the region-proposing stage heavily but that stage is removed in state-of-the-art real-time detectors for higher speed, which makes OHEM serve no purpose on those detectors.\"","":""}
{"id":"2963860801","dialogue":"\"Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of back-grounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of back-grounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5 and 2 mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.\"","summary":"\"To tackle severe imbalance issues between backgrounds and foregrounds in real-time detectors, proposed Focal Loss @cite_4 and tried to modify the loss function to mine the hard examples from easy ones. However, Focal Loss depends on the definition of loss function heavily and cannot be applied to plenty of state-of-the-art real-time detectors straightly.\"","":""}
{"id":"2963860801","dialogue":"\"Modern object detectors usually suffer from low accuracy issues, as foregrounds always drown in tons of back-grounds and become hard examples during training. Compared with those proposal-based ones, real-time detectors are in far more serious trouble since they renounce the use of region-proposing stage which is used to filter a majority of back-grounds for achieving real-time rates. Though foregrounds as hard examples are in urgent need of being mined from tons of backgrounds, a considerable number of state-of-the-art real-time detectors, like YOLO series, have yet to profit from existing hard example mining methods, as using these methods need detectors fit series of prerequisites. In this paper, we propose a general hard example mining method named Loss Rank Mining (LRM) to fill the gap. LRM is a general method for real-time detectors, as it utilizes the final feature map which exists in all real-time detectors to mine hard examples. By using LRM, some elements representing easy examples in final feature map are filtered and detectors are forced to concentrate on hard examples during training. Extensive experiments validate the effectiveness of our method. With our method, the improvements of YOLOv2 detector on auto-driving related dataset KITTI and more general dataset PASCAL VOC are over 5 and 2 mAP, respectively. In addition, LRM is the first hard example mining strategy which could fit YOLOv2 perfectly and make it better applied in series of real scenarios where both real-time rates and accurate detection are strongly demanded.\"","summary":"\"Two multi-task loss functions broadly adopted in object detection are illustrated in Fig.. Loss1 consists of two tasks, namely, classification loss and box regression loss. Classification loss in Loss1 is calculated for both foregrounds and backgrounds, but box regression loss is computed for foregrounds only. Though classification loss is the majority of Loss1 and Focal Loss can be applied to it, the impacts produced by box regression loss are totally neglected. Additionally, for the methods in @cite_2 @cite_24 which adopt Loss2 as their loss functions, Focal Loss serves no purpose. This is mainly because Loss2 possesses four subtasks, namely, object loss, non-object loss, classification loss and box regression loss, but all of them cannot be shared by both foregrounds and backgrounds. Though replacing Loss2 with Loss1 makes Focal Loss available for those detectors, it does harm to detection accuracy significantly, as Loss2 fit those detectors better. Our experiments in Section demonstrate that for those detectors, applying Focal Loss compulsively is detrimental but using Loss Rank Mining is helpful.\"","":""}
{"id":"2798779428","dialogue":"\"Statistical analysis on object data presents many challenges. Basic summaries such as means and variances are difficult to compute. We apply ideas from topology to study object data. We present a framework for using death vectors and persistence landscapes to vectorize object data and perform statistical analysis. We apply this method to some common leaf images that were previously shown to be challenging to compare using a 3D shape techniques. Surprisingly, the most persistent features are shown to be “topological noise” and the statistical analysis depends on the less persistent features which we refer to as the “geometric signal”. We also describe the first steps to a new approach to using topology for object data analysis, which applies topology to distributions on object spaces. We introduce a new Frechet-Morse function technique for probability distribution on a compact object space, extending the Frechet means lo a larger number of location parameters, including Frechet antimeans. An example of 3D data analysis to distinguish two flowers using the new location parameters associated with a Veronese-Whitney (VW) embedding of random projective shapes of 3D configurations extracted from a set of pairs of their digital camera images is also given here.\"","summary":"\"Our approach and results are closely related to work by (2016) @cite_22 , who also applied TDA to object data. In their case they considered brain artery structures extracted from magnetic resonance images and applied persistence homology, which was encoded in vectors by the order statistic on the most persistent points in the persistence diagram. Also, closely related is work by Kovacev- (2016) @cite_28 who applied persistent homology and persistence landscapes to protein structure data.\"","":""}
{"id":"2798866735","dialogue":"\"This paper explores the problem of task learning and planning, contributing the Action-Category Representation (ACR) to improve computational performance of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic, abstract data representation that maps objects to action categories (groups of actions), inspired by the psychological concept of action codes. We validate our approach in StarCraft and Lightworld domains; our results demonstrate several benefits of ACR relating to improved computational performance of planning and RL, by reducing the action space for the agent.\"","summary":"\"Traditional approaches to affordance learning often involves behavioral babbling'' @cite_31 @cite_4 @cite_13 wherein the agent physically interacts with objects in a goal-free manner to discover their affordances. Hence, the resulting affordance representation is dissociated from a task, focusing instead on object properties. Such approaches involve several agent-object interactions affecting the scalability of the learning process, making it unfeasible in situations where there is an implicit cost or time constraint on the robot. ACR helps mitigate this cost by the grouping of actions into categories.\"","":""}
{"id":"2798866735","dialogue":"\"This paper explores the problem of task learning and planning, contributing the Action-Category Representation (ACR) to improve computational performance of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic, abstract data representation that maps objects to action categories (groups of actions), inspired by the psychological concept of action codes. We validate our approach in StarCraft and Lightworld domains; our results demonstrate several benefits of ACR relating to improved computational performance of planning and RL, by reducing the action space for the agent.\"","summary":"\"Two works closest to our approach are @cite_14 and @cite_23 . In @cite_14 , describe an approach to visual object-action recognition that use demonstrations to categorize semantically labeled objects based on their functionality. This approach bridges the gap between affordance learning and task context since the learning is coupled with a task demonstration. However, it is unclear how the system would incorporate previously unseen objects unless they are observed from additional demonstrations. For instance, given a demonstration of pouring water into a cup'', the agent would require additional demonstrations to identify the similar functionality of a bowl''.\"","":""}
{"id":"2798866735","dialogue":"\"This paper explores the problem of task learning and planning, contributing the Action-Category Representation (ACR) to improve computational performance of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic, abstract data representation that maps objects to action categories (groups of actions), inspired by the psychological concept of action codes. We validate our approach in StarCraft and Lightworld domains; our results demonstrate several benefits of ACR relating to improved computational performance of planning and RL, by reducing the action space for the agent.\"","summary":"\"in @cite_23 learn visual object categories for affordance prediction (Category-Affordance model), reducing the physical interactions with the objects. They use visual features of objects to categorize them on the basis of their functionality. However, it is unclear how the agent would deal with changing features and categories @cite_0 , since the model is learned offline as compared to ACR which allows online learning of new objects and categories (Details in Sec 3). Regardless, their approach highlights some of the benefits of categorization on the scalability of learning, which motivates our work.\"","":""}
{"id":"2798866735","dialogue":"\"This paper explores the problem of task learning and planning, contributing the Action-Category Representation (ACR) to improve computational performance of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic, abstract data representation that maps objects to action categories (groups of actions), inspired by the psychological concept of action codes. We validate our approach in StarCraft and Lightworld domains; our results demonstrate several benefits of ACR relating to improved computational performance of planning and RL, by reducing the action space for the agent.\"","summary":"\"Human demonstrations have been used for both high-level task learning and low-level skill learning @cite_21 ; a traditional assumption of LfD is that the human demonstrator is an expert, and the demonstrations are examples of desirable behavior that the agent should emulate. Our work focuses on high level task learning, but considers demonstrations more broadly as examples of what the agent do, rather than what it . This interpretation of the data enables our technique to benefit even from non-expert human users. Demonstration errors can be classified to one of 3 categories @cite_21 : Correct but suboptimal (contains extra steps), conflicting or inconsistent (user demonstrates 2 different actions from the same state) and entirely wrong (user took a wrong action) and we demonstrate the robustness of ACR to suboptimal demonstrations in Sec 6.\"","":""}
{"id":"2798905273","dialogue":"\"We propose a process for investigating the extent to which sentence representations arising from neural machine translation (NMT) systems encode distinct semantic phenomena. We use these representations as features to train a natural language inference (NLI) classifier based on datasets recast from existing semantic annotations. In applying this process to a representative NMT system, we find its encoder appears most suited to supporting inferences at the syntax-semantics interface, as compared to anaphora resolution requiring world-knowledge. We conclude with a discussion on the merits and potential deficiencies of the existing process, and how it may be improved and extended as a broader framework for evaluating semantic coverage.\"","summary":"\"Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology @cite_12 @cite_25 , syntax @cite_2 , and lexical semantics @cite_42 , including word senses @cite_27 @cite_19\"","":""}
{"id":"2609940216","dialogue":"This paper describes an avenue for artificial and computational intelligence techniques applied within games research to be deployed for purposes of physical therapy. We provide an overview of prototypical research focussed on the application of motion sensor input devices and virtual reality equipment for rehabilitation of motor impairment: an issue typical of patients of traumatic brain injuries. We highlight how advances in procedural content generation and player modelling can stimulate development in this area by improving quality of rehabilitation programmes and measuring patient performance.","summary":"\"In many respects the ideas of rehabilitation-driven games share similarities with that of dynamic difficulty adjustment. However, as noted in the likes of @cite_7 , the challenges faced are more nuanced given the physical challenges faced by patients. A notable example can be found in the Intelligence Game Engine for Rehabilitation (IGER) detailed in @cite_11 @cite_15 which is focussed on the recording and management of physiotherapy for stroke patients. This system adopts an adaptive fuzzy-driven approach to catering for the patients rehabilitation process. This is achieved while working with a variety of small mini-games that utilise peripherals such as the Microsoft Kinect and Nintendo Wii Fit board. This system places emphasis on ensuring that the parameters of the current game are such that it should not result in activities that player might find painful.\"","":""}
{"id":"2799270168","dialogue":"\"Existing applications include a huge amount of knowledge that is out of reach for deep neural networks. This paper presents a novel approach for integrating calls to existing applications into deep learning architectures. Using this approach, we estimate each application's functionality with an estimator, which is implemented as a deep neural network (DNN). The estimator is then embedded into a base network that we direct into complying with the application's interface during an end-to-end optimization process. At inference time, we replace each estimator with its existing application counterpart and let the base network solve the task by interacting with the existing application. Using this 'Estimate and Replace' method, we were able to train a DNN end-to-end with less data and outperformed a matching DNN that did not interact with the external application.\"","summary":"\"Task-specific architectures for end-to-end deep learning require large datasets and work very well when such data is available, as in the case of neural machine translation @cite_4 . General purpose end-to-end architectures, suitable for multiple tasks, include the Neural Turing Machine @cite_21 and its successor, the Differential Neural Computer @cite_0 (DNC). There is no external application integration in these architectures. Other architectures, such as the Neural Programmer architecture @cite_19 allow end-to-end training while constraining parts of the network to execute predefined operations by re-implementing specific operations as static differentiable components. This approach has two drawbacks. It requires re-implementation of the API in a differentiable way, which may be difficult, and it lacks the accuracy and possible scalability advantages of an external API.\"","":""}
{"id":"2799270168","dialogue":"\"Existing applications include a huge amount of knowledge that is out of reach for deep neural networks. This paper presents a novel approach for integrating calls to existing applications into deep learning architectures. Using this approach, we estimate each application's functionality with an estimator, which is implemented as a deep neural network (DNN). The estimator is then embedded into a base network that we direct into complying with the application's interface during an end-to-end optimization process. At inference time, we replace each estimator with its existing application counterpart and let the base network solve the task by interacting with the existing application. Using this 'Estimate and Replace' method, we were able to train a DNN end-to-end with less data and outperformed a matching DNN that did not interact with the external application.\"","summary":"\"Program induction is a different approach to interaction with external APIs. The goal is to construct a program comprising a series of operations based on the input, and then execute the program to get the results. When the input is a natural language query, as in our focus, it is possible to use semantic parsing to transform the query into a logical form that describes the program @cite_11 . Early works required natural language query-program pairs to learn the mapping. Recent works, (e.g., @cite_6 ) require only query-answer pairs for training.\"","":""}
{"id":"2799270168","dialogue":"\"Existing applications include a huge amount of knowledge that is out of reach for deep neural networks. This paper presents a novel approach for integrating calls to existing applications into deep learning architectures. Using this approach, we estimate each application's functionality with an estimator, which is implemented as a deep neural network (DNN). The estimator is then embedded into a base network that we direct into complying with the application's interface during an end-to-end optimization process. At inference time, we replace each estimator with its existing application counterpart and let the base network solve the task by interacting with the existing application. Using this 'Estimate and Replace' method, we were able to train a DNN end-to-end with less data and outperformed a matching DNN that did not interact with the external application.\"","summary":"\"Learning to execute the right operation can be viewed as a reinforcement learning problem. For a given input, the agent has to select the optimal action from a set of available actions. The action selection repeats following feedback based on the previous action selection. Earlier works that took this approach include @cite_13 , and later @cite_17 . Recently, @cite_23 proposed a reinforcement extension to Neural Turing Machines @cite_21 . In @cite_24 , the authors pose a value iteration based solution for reinforcement learning tasks as an end-to-end learning task with a Value Iteration Network (VIN). VIN are shown to learn how to plan a sequence of actions for a given task.\"","":""}
{"id":"2798539888","dialogue":"\"The task of face attribute manipulation has found increasing applications, but still remains challeng- ing with the requirement of editing the attributes of a face image while preserving its unique details. In this paper, we choose to combine the Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) for photorealistic image genera- tion. We propose an effective method to modify a modest amount of pixels in the feature maps of an encoder, changing the attribute strength contin- uously without hindering global information. Our training objectives of VAE and GAN are reinforced by the supervision of face recognition loss and cy- cle consistency loss for faithful preservation of face details. Moreover, we generate facial masks to en- force background consistency, which allows our training to focus on manipulating the foreground face rather than background. Experimental results demonstrate our method, called Mask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with changing attributes and outperforms prior methods in detail preservation.\"","summary":"\"Most methods of face attribute manipulation are based on generative models. There are two main groups of these methods: the group with extra input vector","":""}
{"id":"2964104244","dialogue":"\"The manipulator workspace mapping is an important problem in robotics and has attracted significant attention in the community. However, most of the pre-existing algorithms have expensive time complexity due to the reliance on sophisticated kinematic equations. To solve this problem, this paper introduces subspace learning (SL), a variant of subspace embedding, where a set of robot and scope parameters is mapped to the corresponding workspace by a deep neural network (DNN). Trained on a large dataset of around 6 × 104 samples obtained from a MATLAB®implementation of a classical method and sampling of designed uniform distributions, the experiments demonstrate that the embedding significantly reduces run-time from 5.23 × 103 s of traditional discretization method to 0.224 s, with high accuracies (average F-measure is 0.9665 with batch gradient descent and resilient backpropagation).1MATLAB® Scripts are available at https: github.com liaopeiyuan Subspace-Learning\"","summary":"\"In the literature, the development of a neural network for workspace generation has been already tested on both serial-link and parallel mechanisms, utilizing several model architectures and optimization methods @cite_20 @cite_24 . The feasibility of using an artificial neural network for workspace analysis is first proposed and investigated by @cite_19 , where a two-layer perceptron learns to generate the orientation workspace of a 6-3 SPM parallel mechanism based on a 3-tuple input. In a similar approach, @cite_21 constructs a three-layer deep network that takes the input joint angles of a 2-DOF parallel manipulator and gives the workspace of the mechanism in a particular Cartesian plane. Levenberg-Marquardt method is used for optimization, and the lowest mean squared error obtained by the algorithm is 0.026.\"","":""}
{"id":"2799024314","dialogue":"\"This paper describes and evaluates the use of Generative Adversarial Networks (GANs) for path planning in support of smart mobility applications such as indoor and outdoor navigation applications, individualized wayfinding for people with disabilities (e.g., vision impairments, physical disabilities, etc.), path planning for evacuations, robotic navigations, and path planning for autonomous vehicles. We propose an architecture based on GANs to recommend accurate and reliable paths for navigation applications. The proposed system can use crowd-sourced data to learn the trajectories and infer new ones. The system provides users with generated paths that help them navigate from their local environment to reach a desired location. As a use case, we experimented with the proposed method in support of a wayfinding application in an indoor environment. Our experiments assert that the generated paths are correct and reliable. The accuracy of the classification task for the generated paths is up to 99 and the quality of the generated paths has a mean opinion score of 89 .\"","summary":"\"Li @cite_25 proposed an indoor navigation system based on off-the-shelf smartphone sensors and magnetic features. They used several approaches to enhance the accuracy of the system including multi-dimensional dynamic time warping, weighted k-nearest neighbors, and exploiting magnetic gradient fingerprints. They also mitigated the impact of magnetic matching mismatches to reduce the position errors. They reported root mean square error between 4.3 m and 5.6 m.\"","":""}
{"id":"2799024314","dialogue":"\"This paper describes and evaluates the use of Generative Adversarial Networks (GANs) for path planning in support of smart mobility applications such as indoor and outdoor navigation applications, individualized wayfinding for people with disabilities (e.g., vision impairments, physical disabilities, etc.), path planning for evacuations, robotic navigations, and path planning for autonomous vehicles. We propose an architecture based on GANs to recommend accurate and reliable paths for navigation applications. The proposed system can use crowd-sourced data to learn the trajectories and infer new ones. The system provides users with generated paths that help them navigate from their local environment to reach a desired location. As a use case, we experimented with the proposed method in support of a wayfinding application in an indoor environment. Our experiments assert that the generated paths are correct and reliable. The accuracy of the classification task for the generated paths is up to 99 and the quality of the generated paths has a mean opinion score of 89 .\"","summary":"\"Chen @cite_20 propose a path planning system for emergency guidance in indoor environments based on IoT technologies. They use the statistical properties of mobility of groups of people to provide an individualized path for each group. In this work, a graph of the corridors, doors, and exits is optimized to minimize the total evacuation time for all groups. They implemented their work by utilizing the iBeacon technology and smartphones.\"","":""}
{"id":"2799024314","dialogue":"\"This paper describes and evaluates the use of Generative Adversarial Networks (GANs) for path planning in support of smart mobility applications such as indoor and outdoor navigation applications, individualized wayfinding for people with disabilities (e.g., vision impairments, physical disabilities, etc.), path planning for evacuations, robotic navigations, and path planning for autonomous vehicles. We propose an architecture based on GANs to recommend accurate and reliable paths for navigation applications. The proposed system can use crowd-sourced data to learn the trajectories and infer new ones. The system provides users with generated paths that help them navigate from their local environment to reach a desired location. As a use case, we experimented with the proposed method in support of a wayfinding application in an indoor environment. Our experiments assert that the generated paths are correct and reliable. The accuracy of the classification task for the generated paths is up to 99 and the quality of the generated paths has a mean opinion score of 89 .\"","summary":"\"GANs have been used widely for visual data. The most related work to path planning is reported by Hirose @cite_2 in which GANs are used to classify images as safe to go to or not for robot navigation purposes. In that work","":""}
{"id":"2799161410","dialogue":"\"Malicious email attachments are a growing delivery vector for malware. While machine learning has been successfully applied to portable executable (PE) malware detection, we ask, can we extend similar approaches to detect malware across heterogeneous file types commonly found in email attachments? In this paper, we explore the feasibility of applying machine learning as a static countermeasure to detect several types of malicious email attachments including Microsoft Office documents and Zip archives. To this end, we collected a dataset of over 5 million malicious benign Microsoft Office documents from VirusTotal for evaluation as well as a dataset of benign Microsoft Office documents from the Common Crawl corpus, which we use to provide more realistic estimates of thresholds for false positive rates on in-the-wild data. We also collected a dataset of approximately 500k malicious benign Zip archives, which we scraped using the VirusTotal service, on which we performed a separate evaluation. We analyze predictive performance of several classifiers on each of the VirusTotal datasets using a 70 30 train test split on first seen time, evaluating feature and classifier types that have been applied successfully in commercial antimalware products and R&D contexts. Using deep neural networks and gradient boosted decision trees, we are able to obtain ROC curves with > 0.99 AUC on both Microsoft Office document and Zip archive datasets. Discussion of deployment viability in various antimalware contexts is provided.\"","summary":"\"While there are many academic papers that discuss the machine learning antimalware approaches, most use outdated and unrealistic datasets and do not map directly to real-world problems @cite_24 . The classifier and feature types that we use in this paper were chosen based on techniques that have worked well for real-world antimalware problems involving detection of malicious PE files. Saxe and Berlin employ a deep neural network similar to ours @cite_3 , while Anderson and Roth employ gradient boosted decision trees @cite_5 . However, document and archive file formats have their own unique challenges because they are specifically designed to store user provided content which may or may not be executed, while PE files contain specified streams of execution.\"","":""}
{"id":"2799161410","dialogue":"\"Malicious email attachments are a growing delivery vector for malware. While machine learning has been successfully applied to portable executable (PE) malware detection, we ask, can we extend similar approaches to detect malware across heterogeneous file types commonly found in email attachments? In this paper, we explore the feasibility of applying machine learning as a static countermeasure to detect several types of malicious email attachments including Microsoft Office documents and Zip archives. To this end, we collected a dataset of over 5 million malicious benign Microsoft Office documents from VirusTotal for evaluation as well as a dataset of benign Microsoft Office documents from the Common Crawl corpus, which we use to provide more realistic estimates of thresholds for false positive rates on in-the-wild data. We also collected a dataset of approximately 500k malicious benign Zip archives, which we scraped using the VirusTotal service, on which we performed a separate evaluation. We analyze predictive performance of several classifiers on each of the VirusTotal datasets using a 70 30 train test split on first seen time, evaluating feature and classifier types that have been applied successfully in commercial antimalware products and R&D contexts. Using deep neural networks and gradient boosted decision trees, we are able to obtain ROC curves with > 0.99 AUC on both Microsoft Office document and Zip archive datasets. Discussion of deployment viability in various antimalware contexts is provided.\"","summary":"\"As classifiers, we use feed-forward deep neural networks and gradient boosted decision ensembles. While one could try more sophisticated types of neural networks -- e.g., convolutional and recurrent, these are difficult to implement in practice due to large file sizes, computational overhead, and a dearth of generic byte-level embeddings. Though character-level embeddings have yielded success for certain antimalware problems, e.g., @cite_16 , these do not yet work well for generic byte-level embeddings of arbitrary length to our knowledge. Thus, we instead transform each document archive to a fixed-length feature vector before using it to train a classifier. Finally, we note that our focus in this paper is on on static detection, because machine learning models require a lot of data in order to work well. While antimalware stacks consist of both static and dynamic components, dynamic detection is very expensive computationally and is often employed to post-process detections from static engines, which operates much faster at scale. Dynamic detection is an important, complementary, and orthogonal area of research to that presented in this paper.\"","":""}
{"id":"2889708918","dialogue":"\"Multi-modal data is becoming more common in big data background. Finding the semantically similar objects from different modality is one of the heart problems of multi-modal learning. Most of the current methods try to learn the inter-modal correlation with extrinsic supervised information, while intrinsic structural information of each modality is neglected. The performance of these methods heavily depends on the richness of training samples. However, obtaining the multi-modal training samples is still a labor and cost intensive work. In this paper, we bring a extrinsic correlation between the space structures of each modalities in coreference resolution. With this correlation, a semi-supervised learning model for multi-modal coreference resolution is proposed. We firstly extract high-level features of images and text, then compute the distances of each object from some reference points to build the space structure of each modality. With a shared reference point set, the space structures of each modality are correlated. We employ the correlation to build a commonly shared space that the semantic distance between multi-modal objects can be computed directly. The experiments on two multi-modal datasets show that our model performs better than the existing methods with insufficient training data.\"","summary":"\"To the best of our knowledge, the first well-known cross-modal correlating model may be the CCA based model proposed by Hardoon et. al @cite_4 . It learnt a linear projection to maximize the correlation between the representation of different modality in the projected space. Inspired by this work, many CCA based models are designed for cross-modal analyzing @cite_20 @cite_25 @cite_5 @cite_28 . @cite_20 utilized CCA to learn two maximally correlated subspaces, and multiclass logistic regression was performed within them to produce the semantic spaces respectively. @cite_5 proposed a Struncated-SVD based algorithms to compute the full regularization path of CCA for multi-modal retrieval efficiently. @cite_28 developed a new hypergraph-based Canonical Correlation Analysis(HCCA) to project low-level features into a shard space where intra-pair and inter-pair correlation be maintained simultaneously. Heterogeneous high-order relationship was used to discover the structure of cross-modal data.\"","":""}
{"id":"2889708918","dialogue":"\"Multi-modal data is becoming more common in big data background. Finding the semantically similar objects from different modality is one of the heart problems of multi-modal learning. Most of the current methods try to learn the inter-modal correlation with extrinsic supervised information, while intrinsic structural information of each modality is neglected. The performance of these methods heavily depends on the richness of training samples. However, obtaining the multi-modal training samples is still a labor and cost intensive work. In this paper, we bring a extrinsic correlation between the space structures of each modalities in coreference resolution. With this correlation, a semi-supervised learning model for multi-modal coreference resolution is proposed. We firstly extract high-level features of images and text, then compute the distances of each object from some reference points to build the space structure of each modality. With a shared reference point set, the space structures of each modality are correlated. We employ the correlation to build a commonly shared space that the semantic distance between multi-modal objects can be computed directly. The experiments on two multi-modal datasets show that our model performs better than the existing methods with insufficient training data.\"","summary":"\"For the rapid growth of data volume, the cost of finding nearest neighbor cannot be dismissed. Hashing is a scalable method for finding nearest neighbors approximately @cite_15 . It projects data into Hamming space, where the neighbor search can be performed efficiently. To improve the efficient of finding similar multi-modal objects, many cross-modal hashing methods have been proposed @cite_15 @cite_0 @cite_3 @cite_34 @cite_29 . Kumar and Udupa @cite_0 proposed a cross view hashing method to generate such hash codes that minimized the distance in Hamming space between similar objects and maximized that between dissimilar ones. @cite_3 used a co-regularization framework to generate such binary code that the hash codes from different modality were consistent. @cite_34 constructed a Hamming space for each modality and build the mapping between them with logistic regression. @cite_29 proposed a sparse multi-modal hashing method for cross-modal retrieval.\"","":""}
{"id":"2889708918","dialogue":"\"Multi-modal data is becoming more common in big data background. Finding the semantically similar objects from different modality is one of the heart problems of multi-modal learning. Most of the current methods try to learn the inter-modal correlation with extrinsic supervised information, while intrinsic structural information of each modality is neglected. The performance of these methods heavily depends on the richness of training samples. However, obtaining the multi-modal training samples is still a labor and cost intensive work. In this paper, we bring a extrinsic correlation between the space structures of each modalities in coreference resolution. With this correlation, a semi-supervised learning model for multi-modal coreference resolution is proposed. We firstly extract high-level features of images and text, then compute the distances of each object from some reference points to build the space structure of each modality. With a shared reference point set, the space structures of each modality are correlated. We employ the correlation to build a commonly shared space that the semantic distance between multi-modal objects can be computed directly. The experiments on two multi-modal datasets show that our model performs better than the existing methods with insufficient training data.\"","summary":"\"Besides these methods above, there still are other models proposed for multi-modal problems. @cite_24 employed voxel-based multi-modal partial least square(PLS) to analyze the correlations between FDG PET glucose uptake-MRI gray matter volume scores and apolipoprotein E epsilon 4 gene dose in cognitively normal adults.\"","":""}
{"id":"2889708918","dialogue":"\"Multi-modal data is becoming more common in big data background. Finding the semantically similar objects from different modality is one of the heart problems of multi-modal learning. Most of the current methods try to learn the inter-modal correlation with extrinsic supervised information, while intrinsic structural information of each modality is neglected. The performance of these methods heavily depends on the richness of training samples. However, obtaining the multi-modal training samples is still a labor and cost intensive work. In this paper, we bring a extrinsic correlation between the space structures of each modalities in coreference resolution. With this correlation, a semi-supervised learning model for multi-modal coreference resolution is proposed. We firstly extract high-level features of images and text, then compute the distances of each object from some reference points to build the space structure of each modality. With a shared reference point set, the space structures of each modality are correlated. We employ the correlation to build a commonly shared space that the semantic distance between multi-modal objects can be computed directly. The experiments on two multi-modal datasets show that our model performs better than the existing methods with insufficient training data.\"","summary":"\"Although these methods have achieved great success in multi-modal learning, most of them need a mass of training data to learn the complex correlation between objects from different modality. To reduce the demand of training data, @cite_27 proposed an active similarity learning model for cross-modal data. Nevertheless, without extra information, the improvement is limited.\"","":""}
{"id":"2799250053","dialogue":"\"Deep neural networks trained over large datasets learn features that are both generic to the whole dataset, and specific to individual classes in the dataset. Learned features tend towards generic in the lower layers and specific in the higher layers of a network. Methods like fine-tuning are made possible because of the ability for one filter to apply to multiple target classes. Much like the human brain this behavior, can also be used to cluster and separate classes. However, to the best of our knowledge there is no metric for how applicable learned features are to specific classes. In this paper we propose a definition and metric for measuring the applicability of learned features to individual classes, and use this applicability metric to estimate input applicability and produce a new method of unsupervised learning we call the CactusNet.\"","summary":"\"For the past several years, advances in deep neural networks have shown to be a powerful tool for a variety of machine learning problems in multiple domains, including computer vision @cite_7 @cite_2 @cite_3 @cite_21 , speech @cite_22 @cite_24 , and text @cite_5 @cite_17 . For many of these domains, and especially for vision, each layer of the deep neural network learns features relevant to the target objective @cite_16 @cite_25 . For many objectives, a deep neural network requires a large-scale dataset to converge and obtain good accuracy @cite_14 . For most tasks, however, large-scale datasets do not exist or are unobtainable. To circumvent this issue, existing deep neural networks can be fine-tuned for specific objectives. Fine-tuning repurposes the learned features of a pretrained deep neural networks which then can learn the unknown features needed for the new objective. Deep convolutional neural networks (CNN) trained on ImageNet @cite_0 are commonly fine-tuned for different computer vision tasks. Fine-tuning significantly reduces the amount of training examples required to converge to a target objective @cite_20 .\"","":""}
{"id":"2799250053","dialogue":"\"Deep neural networks trained over large datasets learn features that are both generic to the whole dataset, and specific to individual classes in the dataset. Learned features tend towards generic in the lower layers and specific in the higher layers of a network. Methods like fine-tuning are made possible because of the ability for one filter to apply to multiple target classes. Much like the human brain this behavior, can also be used to cluster and separate classes. However, to the best of our knowledge there is no metric for how applicable learned features are to specific classes. In this paper we propose a definition and metric for measuring the applicability of learned features to individual classes, and use this applicability metric to estimate input applicability and produce a new method of unsupervised learning we call the CactusNet.\"","summary":"\"Transfer learning has also been explored for unsupervised learning as well. In survey of how transferability can be applied to unsupervised learning @cite_19 , the author stated that while the results look promising, transfer learning applications would improve significantly if the underlying variation in high-level features could be disentangled and made more invariant. In this work, we use applicability to demonstrate where in a network the features of an input go from invariant to variant. This point of inflection is where the CactusNet creates a branch and circumvents invariance at the more varying and more specific layers.\"","":""}
{"id":"2799250053","dialogue":"\"Deep neural networks trained over large datasets learn features that are both generic to the whole dataset, and specific to individual classes in the dataset. Learned features tend towards generic in the lower layers and specific in the higher layers of a network. Methods like fine-tuning are made possible because of the ability for one filter to apply to multiple target classes. Much like the human brain this behavior, can also be used to cluster and separate classes. However, to the best of our knowledge there is no metric for how applicable learned features are to specific classes. In this paper we propose a definition and metric for measuring the applicability of learned features to individual classes, and use this applicability metric to estimate input applicability and produce a new method of unsupervised learning we call the CactusNet.\"","summary":"\"The human mind identifies and clusters objects based on their features regardless of whether an object is known or not @cite_1 . Adaptive resonance theory (ART) @cite_18 @cite_23 is a machine learning theory that attempts to determine whether an object belongs to a known object class by comparing the detected features of the object with the expected features of all known classes individually. If the smallest difference between the detected features of the object and some known class's expected features is within a set threshold then the object is classified and is considered to belong to that class. This threshold is known as the vigilance parameter. If the difference exceeds the vigilance parameter, however, the object is considered to belong to a new class. This allows ART to perform unsupervised learning as it classifies not based on a target class, but differences in features. Over the years, several new variations of ART have been proposed including Fuzzy ART @cite_10 which, uses fuzzy logic to improve ART's stability.\"","":""}
{"id":"2798714888","dialogue":"\"Spanners for low dimensional spaces (e.g. Euclidean space of constant dimension, or doubling metrics) are well understood. This lies in contrast to the situation in high dimensional spaces, where except for the work of Har-Peled, Indyk and Sidiropoulos (SODA 2013), who showed that any @math -point Euclidean metric has an @math -spanner with @math edges, little is known. In this paper we study several aspects of spanners in high dimensional normed spaces. First, we build spanners for finite subsets of @math with @math . Second, our construction yields a spanner which is both sparse and also light , i.e., its total weight is not much larger than that of the minimum spanning tree. In particular, we show that any @math -point subset of @math for @math has an @math -spanner with @math edges and lightness @math . In fact, our results are more general, and they apply to any metric space admitting a certain low diameter stochastic decomposition. It is known that arbitrary metric spaces have an @math -spanner with lightness @math . We exhibit the following tradeoff: metrics with decomposability parameter @math admit an @math -spanner with lightness @math . For example, @math -point Euclidean metrics have @math , metrics with doubling constant @math have @math , and graphs of genus @math have @math . While these families do admit a ( @math )-spanner, its lightness depend exponentially on the dimension (resp. @math ). Our construction alleviates this exponential dependency, at the cost of incurring larger stretch.\"","summary":"\"In addition to the results mentioned above, sparse light spanners with small stretch were studied also for: planar graphs @cite_30 @cite_84 , apex graphs @cite_3 , bounded pathwidth graphs @cite_14 @cite_70 , bounded treewidth graphs @cite_28 @cite_70 , and graphs excluding fixed minors @cite_0 @cite_28 @cite_29 . From the algorithmic perspective, there is a rich study of efficient spanners construction @cite_66 @cite_34 @cite_65 @cite_79 @cite_20 @cite_7 @cite_60 @cite_4 @cite_2 @cite_41 @cite_27 @cite_19 .\"","":""}
{"id":"2963936326","dialogue":"\"A key problem in deep multi-attribute learning is to effectively discover the inter-attribute correlation structures. Typically, the conventional deep multi-attribute learning approaches follow the pipeline of manually designing the network architectures based on task-specific expertise prior knowledge and careful network tunings, leading to the inflexibility for various complicated scenarios in practice. Motivated by addressing this problem, we propose an efficient greedy neural architecture search approach (GNAS) to automatically discover the optimal tree-like deep architecture for multi-attribute learning. In a greedy manner, GNAS divides the optimization of global architecture into the optimizations of individual connections step by step. By iteratively updating the local architectures, the global tree-like architecture gets converged where the bottom layers are shared across relevant attributes and the branches in top layers more encode attribute-specific features. Experiments on three benchmark multi-attribute datasets show the effectiveness and compactness of neural architectures derived by GNAS, and also demonstrate the efficiency of GNAS in searching neural architectures.\"","summary":"\"However, the hand-designed network architecture raises a high demand of knowledges in specific tasks and experience in building neural networks. Motivated by this, researchers investigate the automatic design of deep architectures more recently. Cross-stitching network @cite_27 is proposed to learn an optimal linear combination of shared representations, and he2017adaptively @cite_15 adaptively learn the weights of individual tasks. The work most close to our approach is @cite_10 which first initializes a thin network from a pre-trained model by SOMP @cite_9 and then widening the network through a branching procedure. However, these approaches generally explore a relatively limited search space.\"","":""}
{"id":"2963936326","dialogue":"\"A key problem in deep multi-attribute learning is to effectively discover the inter-attribute correlation structures. Typically, the conventional deep multi-attribute learning approaches follow the pipeline of manually designing the network architectures based on task-specific expertise prior knowledge and careful network tunings, leading to the inflexibility for various complicated scenarios in practice. Motivated by addressing this problem, we propose an efficient greedy neural architecture search approach (GNAS) to automatically discover the optimal tree-like deep architecture for multi-attribute learning. In a greedy manner, GNAS divides the optimization of global architecture into the optimizations of individual connections step by step. By iteratively updating the local architectures, the global tree-like architecture gets converged where the bottom layers are shared across relevant attributes and the branches in top layers more encode attribute-specific features. Experiments on three benchmark multi-attribute datasets show the effectiveness and compactness of neural architectures derived by GNAS, and also demonstrate the efficiency of GNAS in searching neural architectures.\"","summary":"Several approaches explore to accelerate the searching procedure by reducing the expense of neural network training. baker2018accelerating @cite_2 early stop the architecture evaluation process by predicting the performance of unobserved architectures based on a set of architecture features. brock2018smash @cite_29 propose a hypernetwork to generate the neural network weights conditioned on its architecture instead of conducting back propagation training. pham2018efficient @cite_24 search for an optimal sub-graph within a large computational graph where the neural network weights are shared across sub-graphs.","":""}
{"id":"2963936326","dialogue":"\"A key problem in deep multi-attribute learning is to effectively discover the inter-attribute correlation structures. Typically, the conventional deep multi-attribute learning approaches follow the pipeline of manually designing the network architectures based on task-specific expertise prior knowledge and careful network tunings, leading to the inflexibility for various complicated scenarios in practice. Motivated by addressing this problem, we propose an efficient greedy neural architecture search approach (GNAS) to automatically discover the optimal tree-like deep architecture for multi-attribute learning. In a greedy manner, GNAS divides the optimization of global architecture into the optimizations of individual connections step by step. By iteratively updating the local architectures, the global tree-like architecture gets converged where the bottom layers are shared across relevant attributes and the branches in top layers more encode attribute-specific features. Experiments on three benchmark multi-attribute datasets show the effectiveness and compactness of neural architectures derived by GNAS, and also demonstrate the efficiency of GNAS in searching neural architectures.\"","summary":"\"In this work, we propose GNAS to novelly develop neural architecture optimization to multi-task learning. Different from existing neural architecture optimizing approaches, we propose two greedy strategies which largely reduce the computation cost of architecture optimization procedure. The intra-layer greedy strategy of GNAS is proposed based on the property of tree structure. And the inter-layer greedy strategy of GNAS is inspired by the layer-wise pretraining strategy of restricted Boltzmann machine (RBM) @cite_20 @cite_26 @cite_25 . The greedy strategies lead to the efficiency of GNAS, also leading to effectiveness by ensuring a highly efficient searching in a very large search space.\"","":""}
{"id":"2797901525","dialogue":"\"We introduce a method for disentangling independently controllable and uncontrollable factors of variation by interacting with the world. Disentanglement leads to good representations and it is important when applying deep neural networks (DNNs) in fields where explanations are necessary. This article focuses on reinforcement learning (RL) approach for disentangling factors of variation, however, previous methods lacks a mechanism for representing uncontrollable obstacles. To tackle this problem, we train two DNNs simultaneously: one that represents the controllable object and another that represents the uncontrollable obstacles. During training, we used the parameters from a previous RL-based model as our initial parameters to improve stability. We also conduct simple toy simulations to show that our model can indeed disentangle controllable and uncontrollable factors of variation and that it is effective for a task involving the acquisition of extrinsic rewards.\"","summary":"\"All supervised learning-based approaches use the labeled information to disentangle features. For example, @cite_33 proposed a restricted Boltzmann machine-based method that considered a complicated manifolds as collections of sub-manifolds. @cite_30 combined a semi-supervised VAE @cite_25 and a probabilistic graphical model. @cite_9 proposed a hierarchical model that first disentangled identity and non-identity features from face images and then disentangled poses and landmarks from the non-identity features. @cite_21 proposed a disentangled representation learning method for pose-invariant face recognition based on a GAN @cite_7 . To disentangle the GAN generator's input variables, @cite_8 used a VAE, and @cite_10 proposed a semi-supervised InfoGAN.\"","":""}
{"id":"2797901525","dialogue":"\"We introduce a method for disentangling independently controllable and uncontrollable factors of variation by interacting with the world. Disentanglement leads to good representations and it is important when applying deep neural networks (DNNs) in fields where explanations are necessary. This article focuses on reinforcement learning (RL) approach for disentangling factors of variation, however, previous methods lacks a mechanism for representing uncontrollable obstacles. To tackle this problem, we train two DNNs simultaneously: one that represents the controllable object and another that represents the uncontrollable obstacles. During training, we used the parameters from a previous RL-based model as our initial parameters to improve stability. We also conduct simple toy simulations to show that our model can indeed disentangle controllable and uncontrollable factors of variation and that it is effective for a task involving the acquisition of extrinsic rewards.\"","summary":"\"The most studied unsupervised learning-based methods employ generative models, especially VAEs and InfoGANs @cite_36 . VAEs combine Bayes and autoencoder approaches to embed encoded features based on a given probability distribution. Dupont @cite_5 proposed a joint-VAE that disentangled continuous and discrete representations. @cite_35 introduced an adjustable hyperparameter, @math , to a standard VAE to balance latent channel capacity and independence constraints with reconstruction accuracy. Kim and Mnih @cite_17 added a discriminator to @math -VAE to estimate Total Correlation, and @cite_16 decomposed a @math -VAE equation and refined it to improve the disentanglement ability without using additional hyperparameters. InfoGANs attempted to learn a generator such that they cheated the discriminator and maximized the mutual information between synthetic samples and newly introduced latent codes. Inspired by InfoGAN, @cite_3 proposed combining generative adversarial imitation learning @cite_32 with a variational lower bound of the mutual information. On the application side, for example, unsupervised approaches for sequential data @cite_4 @cite_27 @cite_6 , and control and planing @cite_13 @cite_3 were also proposed.\"","":""}
{"id":"2796718880","dialogue":"\"The course description provided by instructors is an important piece of information as it defines what is expected from the instructor and what he she is going to deliver during a particular course. One of the key components of a course description is the Learning Outcomes section. The contents of this section are used by program managers who are tasked to compare and match two different courses during the development of Transfer Agreements between different institutions. This research introduces the development of visual tools for understanding the two different courses and making comparisons. We designed methods to extract the text from a course description document, developed an algorithm to perform semantic analysis, and displayed the results in a web interface. We are able to achieve the intermediate results of the research which includes extracting, analyzing and visualizing the data.\"","summary":"\"The extraction process focuses on segmentation process which involves dividing the document into blocks that are the smallest logical entity and then proceed with extraction in the later stages @cite_8 . The segmentation process is divided into sub parts, which include generating neighbourhood graph, creating page divisions and generating whitespace density graph. For our research, we need to detect headings to make sure the algorithm knows from which part of the document to extract the text. To make sure that the extracted text does not have document header footer or text from another columns(if document has multiple columns), detection of header footer text and multiple columns is also required. So, the most efficient way for our application is to detect the relevant headings and then analyze the area of the document which contains that heading before extracting the text. The authors have not seen such work before.\"","":""}
{"id":"2964239644","dialogue":"\"The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.\"","summary":"\"Multi-surface representations of 3D shapes are popular for categorization tasks. The seminal work by Chen al @cite_28 proposes a 3D shape descriptor based on the silhouettes rendered from the 20 vertices of a dodecahedron surrounding the object. More recently, Su al @cite_3 and Qi al @cite_15 train CNNs on 2D renderings of 3D mesh models for classification. Qi al @cite_15 compares CNNs trained on volumetric representations to those trained on multiview representations. Although both representations encode similar amounts of information, they showed that multiview representations significantly outperform volumetric representations for 3D object classification. Unlike our approach, these approaches use multiple projections as input rather than output.\"","":""}
{"id":"2964239644","dialogue":"\"The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.\"","summary":"\"To synthesize multi-surface output representations, we train multiple decoders. Dosovitskiy al @cite_14 show that CNNs can be used to generate images from high-level descriptions such as object instance, viewpoint, and transformation parameters. Their network jointly predicts an RGB image and its segmentation mask using two up-convolutional output branches sharing a high-dimensional hidden representation. The decoder in our network learns the segmentation for each output view in a similar manner.\"","":""}
{"id":"2964239644","dialogue":"\"The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.\"","summary":"\"In experiments on 2D symbols, Tarr and Pinker @cite_29 found that human perception is largely tied to viewer-centered coordinates; this was confirmed by McMullen and Farah @cite_11 for line drawings, who also found that object-centered coordinates seem to play more of a role for familiar exemplars. Note that in the human vision literature, viewer-centered'' usually means that the object shape is represented as a set of images in the viewer's coordinate frame, and object-centered'' usually means a volumetric shape is represented in the object's coordinate frame. In our work, we consider both the shape representation (volumetric or surface) and coordinate frame (viewer or object) as separate design choices. We do not claim our computational approach has any similarity to human visual processing, but it is interesting to see that in our experiments with 3D objects, we also find a preference for object-centered coordinates for familiar exemplars (i.e., novel view of known object) and for viewer-centered coordinates in other cases.\"","":""}
{"id":"2797962430","dialogue":"\"Many Web platforms rely on user collaboration to generate high-quality content: Wiki, Q&A communities, etc. Understanding and modeling the different collaborative behaviors is therefore critical. However, collaboration patterns are difficult to capture when the relationships between users are not directly observable, since they need to be inferred from the user actions. In this work, we propose a solution to this problem by adopting a systemic view of collaboration. Rather than modeling the users as independent actors in the system, we capture their coordinated actions with embedding methods which can, in turn, identify shared objectives and predict future user actions. To validate our approach, we perform a study on a dataset comprising more than 16M user actions, recorded on the online collaborative sandbox Reddit r place. Participants had access to a drawing canvas where they could change the color of one pixel at every fixed time interval. Users were not grouped in teams nor were given any specific goals, yet they organized themselves into a cohesive social fabric and collaborated to the creation of a multitude of artworks. Our contribution in this paper is multi-fold: i) we perform an in-depth analysis of the Reddit r place collaborative sandbox, extracting insights about its evolution over time; ii) we propose a predictive method that captures the latent structure of the emergent collaborative efforts; and iii) we show that our method provides an interpretable representation of the social structure.\"","summary":"Networks of collaboration between scientists have been studied by Newman @cite_11 . The authors argue that simple unweighted networks are unable to capture the strength of collaboration ties and propose a method to model the strength of collaboration by relying on the number of co-authored papers. The same author has later studied various properties of such networks @cite_11 . @cite_7 have studied collaboration networks from an evolving and self-organizing perspective. Behavioral experiments on the ability to solve problems collaboratively have been conducted by Kearns @cite_26 . Online collaboration with different network topologies has been studied by Suri and Watts @cite_9 . The exploration-exploitation trade-off in a collaborative problem solving task has been discussed by Mason and Watts @cite_19 . Kittur and Kraut @cite_17 studied various types of collaboration taking place between Wikipedia editors and measured the impact on quality of the resulting articles.","":""}
{"id":"2797962430","dialogue":"\"Many Web platforms rely on user collaboration to generate high-quality content: Wiki, Q&A communities, etc. Understanding and modeling the different collaborative behaviors is therefore critical. However, collaboration patterns are difficult to capture when the relationships between users are not directly observable, since they need to be inferred from the user actions. In this work, we propose a solution to this problem by adopting a systemic view of collaboration. Rather than modeling the users as independent actors in the system, we capture their coordinated actions with embedding methods which can, in turn, identify shared objectives and predict future user actions. To validate our approach, we perform a study on a dataset comprising more than 16M user actions, recorded on the online collaborative sandbox Reddit r place. Participants had access to a drawing canvas where they could change the color of one pixel at every fixed time interval. Users were not grouped in teams nor were given any specific goals, yet they organized themselves into a cohesive social fabric and collaborated to the creation of a multitude of artworks. Our contribution in this paper is multi-fold: i) we perform an in-depth analysis of the Reddit r place collaborative sandbox, extracting insights about its evolution over time; ii) we propose a predictive method that captures the latent structure of the emergent collaborative efforts; and iii) we show that our method provides an interpretable representation of the social structure.\"","summary":"\"The study, as well as the interpretation of proximity data from a social perspective has been a prolific research area. Recent studies @cite_16 , have extracted social network properties from proximity sensor data. In particular, the authors propose a method to distinguish between strong and weak social ties, using the Bluetooth signal strength of users' cellphones. The authors observe that weak links, i.e. the interactions that have been observed less than once per day, have a lower probability of being observed at later times. Collaboration patterns between university students, collaborating in teams for their course assignments, have also been studied @cite_1 . The authors consider the time spent in physical proximity, using university wifi logs, as a proxy to measure ties between students. Their analysis suggests that only strong ties matter in order to predict team performances. We also notice that the study of social properties from positional tracking is not limited to the human species, as a colony of ants as been recently tracked, at individual level, revealing complex hierarchical social structures @cite_14 .\"","":""}
{"id":"2797962430","dialogue":"\"Many Web platforms rely on user collaboration to generate high-quality content: Wiki, Q&A communities, etc. Understanding and modeling the different collaborative behaviors is therefore critical. However, collaboration patterns are difficult to capture when the relationships between users are not directly observable, since they need to be inferred from the user actions. In this work, we propose a solution to this problem by adopting a systemic view of collaboration. Rather than modeling the users as independent actors in the system, we capture their coordinated actions with embedding methods which can, in turn, identify shared objectives and predict future user actions. To validate our approach, we perform a study on a dataset comprising more than 16M user actions, recorded on the online collaborative sandbox Reddit r place. Participants had access to a drawing canvas where they could change the color of one pixel at every fixed time interval. Users were not grouped in teams nor were given any specific goals, yet they organized themselves into a cohesive social fabric and collaborated to the creation of a multitude of artworks. Our contribution in this paper is multi-fold: i) we perform an in-depth analysis of the Reddit r place collaborative sandbox, extracting insights about its evolution over time; ii) we propose a predictive method that captures the latent structure of the emergent collaborative efforts; and iii) we show that our method provides an interpretable representation of the social structure.\"","summary":"\"The phenomenon of emergence has been studied in different domains, notably in the field of Complexity Science and in the context of agent based modeling. The term emergence has various definition across fields @cite_4 , alike complexity @cite_20 from which emergence has been suggested to arise from. Emergence generally refers to system-wide behaviors that cannot be explained by the sum of individual behaviors. Moreover, means of modeling emergence are still subject to debate. Counting interaction between agents is, however, a widely used method to infer complex behaviors in a system @cite_12 .\"","":""}
{"id":"2797962430","dialogue":"\"Many Web platforms rely on user collaboration to generate high-quality content: Wiki, Q&A communities, etc. Understanding and modeling the different collaborative behaviors is therefore critical. However, collaboration patterns are difficult to capture when the relationships between users are not directly observable, since they need to be inferred from the user actions. In this work, we propose a solution to this problem by adopting a systemic view of collaboration. Rather than modeling the users as independent actors in the system, we capture their coordinated actions with embedding methods which can, in turn, identify shared objectives and predict future user actions. To validate our approach, we perform a study on a dataset comprising more than 16M user actions, recorded on the online collaborative sandbox Reddit r place. Participants had access to a drawing canvas where they could change the color of one pixel at every fixed time interval. Users were not grouped in teams nor were given any specific goals, yet they organized themselves into a cohesive social fabric and collaborated to the creation of a multitude of artworks. Our contribution in this paper is multi-fold: i) we perform an in-depth analysis of the Reddit r place collaborative sandbox, extracting insights about its evolution over time; ii) we propose a predictive method that captures the latent structure of the emergent collaborative efforts; and iii) we show that our method provides an interpretable representation of the social structure.\"","summary":"\"The task of community detection has been a well-studied problem, whose goal is to assign users to communities @cite_18 . The most relevant line of research is probably the task of detecting communities, whose members can be part of multiple groups. Those lines of research have made use of Matrix Factorization methods in order to relax the assumption of communities being disjoint @cite_25 @cite_3 . Methods providing a direct way to embed the nodes of a network, thus generalizing the notion of network neighborhood, have recently been proposed @cite_8 .\"","":""}
{"id":"2797798064","dialogue":"\"Effective regularisation of neural networks is essential to combat overfitting due to the large number of parameters involved. We present an empirical analogue to the Lipschitz constant of a feed-forward neural network, which we refer to as the maximum gain. We hypothesise that constraining the gain of a network will have a regularising effect, similar to how constraining the Lipschitz constant of a network has been shown to improve generalisation. A simple algorithm is provided that involves rescaling the weight matrix of each layer after each parameter update. We conduct a series of studies on common benchmark datasets, and also a novel dataset that we introduce to enable easier significance testing for experiments using convolutional networks. Performance on these datasets compares favourably with other common regularisation techniques.\"","summary":"\"The idea of constraining the Lipschitz constant of a network is conceptually related to quantifying the flatness of minima. While there is no single formalisation for what constitutes a flat minimum, the unifying intuition is that a minimum is flat when a small perturbation of the model parameters does not have a large impact on the performance of the model. @cite_18 have shown that Lipschitz continuity is not a reliable tool for quantifying the flatness of minima. However, there is a subtle but very important difference between how they employ Lipschitz continuity, and how it is used by @cite_13 and in this work. Neural networks are functions parameterised by two distinct sets of variables: the model parameters, and the features. @cite_18 consider Lipschitz continuity with respect to the model parameters, whereas we consider Lipschitz continuity with respect the features being supplied to the network. The crux of the argument given by is that the Lipschitz constant of a network with respect to its weights is not invariant to reparameterisation.\"","":""}
{"id":"2797798064","dialogue":"\"Effective regularisation of neural networks is essential to combat overfitting due to the large number of parameters involved. We present an empirical analogue to the Lipschitz constant of a feed-forward neural network, which we refer to as the maximum gain. We hypothesise that constraining the gain of a network will have a regularising effect, similar to how constraining the Lipschitz constant of a network has been shown to improve generalisation. A simple algorithm is provided that involves rescaling the weight matrix of each layer after each parameter update. We conduct a series of studies on common benchmark datasets, and also a novel dataset that we introduce to enable easier significance testing for experiments using convolutional networks. Performance on these datasets compares favourably with other common regularisation techniques.\"","summary":"\"Dropout @cite_8 is one of the most widely used methods for regularising neural networks. It is popular because it is efficient and easy to implement, requiring only that each activation is set to zero with some probability, @math , during training. An extension proposed by @cite_8 , known as maxnorm, is to constrain the maginitude of the weight vector associated with each unit in some layer. One can also use multiplicative Gaussian noise, rather than Bernoulli noise. @cite_0 provide a technique that enables automatic tuning of the amount of noise that should be applied in the case of Gaussian dropout. A similar technique exists for automatically tuning @math for Bernoulli dropout---this extension is known as concrete dropout @cite_16 .\"","":""}
{"id":"2797571322","dialogue":"\"There has been remarkable recent work in unpaired image-to-image translation. However, they're restricted to translation on single pairs of distributions, with some exceptions. In this study, we extend one of these works to a scalable multidistribution translation mechanism. Our translation models not only converts from one distribution to another but can be stacked to create composite translation functions. We show that this composite property makes it possible to generate images with characteristics not seen in the training set. We also propose a decoupled training mechanism to train multiple distributions separately, which we show, generates better samples than isolated joint training. Further, we do a qualitative and quantitative analysis to assess the plausibility of the samples. The code is made available at this https URL\"","summary":"\": Supervised image-to-image translation @cite_14 has achieved outstanding results where the data used for training is available in one-to-one pairs. Apart from adversarial loss, it uses L1 (reconstruction) loss as well, which has now become a common practice in these types of tasks.\"","":""}
{"id":"2797571322","dialogue":"\"There has been remarkable recent work in unpaired image-to-image translation. However, they're restricted to translation on single pairs of distributions, with some exceptions. In this study, we extend one of these works to a scalable multidistribution translation mechanism. Our translation models not only converts from one distribution to another but can be stacked to create composite translation functions. We show that this composite property makes it possible to generate images with characteristics not seen in the training set. We also propose a decoupled training mechanism to train multiple distributions separately, which we show, generates better samples than isolated joint training. Further, we do a qualitative and quantitative analysis to assess the plausibility of the samples. The code is made available at this https URL\"","summary":"\"Unsupervised methods take samples of images from two distributions and learn to cross-translate between them. This introduces the well known issue of there being infinitely many mappings between the two unpaired image domains @cite_15 @cite_13 @cite_5 @cite_16 @cite_3 and so further constraints are required to do well on the problem. @cite_15 introduces the requirement that translations be cycle-consistent; mapping image @math to domain @math and back again to @math must yield an image that is close to the original. @cite_13 takes a different approach, enforcing weight sharing between the early layers of the generators and later layers of the discriminators. @cite_5 combines these two ideas and models each image domain using a VAE-GAN. @cite_16 utilizes reconstruction loss and teacher loss instead of VAE using a pretrained teacher network to ensure the encoder output lies in a meaningful subregion.\"","":""}
{"id":"2797571322","dialogue":"\"There has been remarkable recent work in unpaired image-to-image translation. However, they're restricted to translation on single pairs of distributions, with some exceptions. In this study, we extend one of these works to a scalable multidistribution translation mechanism. Our translation models not only converts from one distribution to another but can be stacked to create composite translation functions. We show that this composite property makes it possible to generate images with characteristics not seen in the training set. We also propose a decoupled training mechanism to train multiple distributions separately, which we show, generates better samples than isolated joint training. Further, we do a qualitative and quantitative analysis to assess the plausibility of the samples. The code is made available at this https URL\"","summary":"\"To our knowledge, only @cite_3 has presented results in generating translations between multiple distribution samples. However, their generator is conditioned on supervised labels.\"","":""}
{"id":"2797672404","dialogue":"\"The modularization of Service Function Chains (SFCs) in Network Function Virtualization (NFV) could introduce significant performance overhead and resource efficiency degradation due to introducing frequent packet transfer and consuming much more hardware resources. In response, we exploit the lightweight and individually scalable features of elements in Modularized SFCs (MSFCs) and propose CoCo, a compact and optimized consolidation framework for MSFC in NFV. CoCo addresses the above problems in two ways. First, CoCo Optimized Placer pays attention to the problem of which elements to consolidate and provides a performance-aware placement algorithm to place MSFCs compactly and optimize the global packet transfer cost. Second, CoCo Individual Scaler innovatively introduces a push-aside scaling up strategy to avoid degrading performance and taking up new CPU cores. To support MSFC consolidation, CoCo also provides an automatic runtime scheduler to ensure fairness when elements are consolidated on CPU core. Our evaluation results show that CoCo achieves significant performance improvement and efficient resource utilization.\"","summary":"\"Click @cite_17 proposed the idea of modularization and applies it to routers. Recently, Slick @cite_7 and OpenBox @cite_1 were proposed to detailedly discuss modularized NFs and decouple control plane and data plane of modularized NFs for easy management. Besides, OpenBox focused on merging elements to shorten the processing path length. However, above works mainly focus on orchestration-level module management and are orthogonal to our optimizations on performance-aware placement and dynamically scaling.\"","":""}
{"id":"2797672404","dialogue":"\"The modularization of Service Function Chains (SFCs) in Network Function Virtualization (NFV) could introduce significant performance overhead and resource efficiency degradation due to introducing frequent packet transfer and consuming much more hardware resources. In response, we exploit the lightweight and individually scalable features of elements in Modularized SFCs (MSFCs) and propose CoCo, a compact and optimized consolidation framework for MSFC in NFV. CoCo addresses the above problems in two ways. First, CoCo Optimized Placer pays attention to the problem of which elements to consolidate and provides a performance-aware placement algorithm to place MSFCs compactly and optimize the global packet transfer cost. Second, CoCo Individual Scaler innovatively introduces a push-aside scaling up strategy to avoid degrading performance and taking up new CPU cores. To support MSFC consolidation, CoCo also provides an automatic runtime scheduler to ensure fairness when elements are consolidated on CPU core. Our evaluation results show that CoCo achieves significant performance improvement and efficient resource utilization.\"","summary":"\"CoMb @cite_6 designed a detailed mechanism to consolidate middleboxes together to reduce provisioning cost. Furthermore, Flurries @cite_8 and NFVnice @cite_19 were proposed to share CPU cores among different NFs with the technique of Docker Container @cite_21 . By modifying Linux scheduling methods, they achieved almost no loss in NF sharing. However, they operated on monolithic NF level and did not consider the problem of which elements (NFs) to consolidate. However, their development details and infrastructure designs could complement our work as the low-level implementation.\"","":""}
{"id":"2797251890","dialogue":"\"This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.\"","summary":"\"Several learning-based and CNN-based approaches are also developed for color-guided depth image enhancement @cite_6 @cite_31 @cite_34 , where the structural interdependency between intensity and depth image is modeled and exploited to reconstruct high quality depth image. For guided depth image enhancement, @cite_31 present a CNN model to learn multi-scale guidance, while @cite_34 incorporate weighted analysis representation and truncated inference for dynamic guidance learning. For general guided filtering, @cite_6 construct CNN-based joint filters to transfer structural details from guided image to reconstructed image. However, these approaches assume that the guided image is spatially well aligned with the degraded observation. Due to that the guided image and degraded observation usually are different in pose and expression, such assumption generally does not hold true for guided face restoration. To address this issue, a WarpNet is introduced in our GFRNet to learn a flow field for warping the guided image to the desired pose and expression.\"","":""}
{"id":"2797251890","dialogue":"\"This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.\"","summary":"\"Recently, spatial transformer networks (STNs) are suggested to learn a spatial mapping for warping an image @cite_43 , and appearance flow networks (AFNs) are presented to predict a dense flow field to move pixels @cite_12 @cite_18 . Deep dense flow networks have been applied to view synthesis @cite_12 @cite_23 , gaze manipulation @cite_18 , expression editing @cite_55 , and video frame synthesis @cite_56 . In these approaches, the target image is required to have the similar lighting condition with the input image to be warped, and the dense flow networks can thus be trained via reconstruction learning. However, in our guided face restoration task, the guided image and the target image usually are of different lighting conditions, making it less effective to train the flow network via reconstruction learning. Moreover, the ground-truth dense flow field is not available, further increasing the difficulty to train WarpNet. To tackle this issue, we use the face alignment method @cite_40 to extract the face landmarks of guided and target images. Then, the landmark loss and TV regularization are incorporated to facilitate the WarpNet training.\"","":""}
{"id":"2798208386","dialogue":"\"The trade-off between language expressiveness and system scalability (E&S) is a well-known problem in RDF stream reasoning. Higher expressiveness supports more complex reasoning logic, however, it may also hinder system scalability. Current research mainly focuses on logical frameworks suitable for stream reasoning as well as the implementation and the evaluation of prototype systems. These systems are normally developed in a centralized setting which suffer from inherent limited scalability, while an in-depth study of applying distributed solutions to cover E&S is still missing. In this paper, we aim to explore the feasibility of applying modern distributed computing frameworks to meet E&S all together. To do so, we first propose BigSR, a technical demonstrator that supports a positive fragment of the LARS framework. For the sake of generality and to cover a wide variety of use cases, BigSR relies on the two main execution models adopted by major distributed execution frameworks: Bulk Synchronous Processing (BSP) and Record-at-A-Time (RAT). Accordingly, we implement BigSR on top of Apache Spark Streaming (BSP model) and Apache Flink (RAT model). In order to conclude on the impacts of BSP and RAT on E&S, we analyze the ability of the two models to support distributed stream reasoning and identify several types of use cases characterized by their levels of support. This classification allows for quantifying the E&S trade-off by assessing the scalability of each type of use case its level of expressiveness. Then, we conduct a series of experiments with 15 queries from 4 different datasets. Our experiments show that BigSR over both BSP and RAT generally scales up to high throughput beyond million-triples per second (with or without recursion), and RAT attains sub-millisecond delay for stateless query operators.\"","summary":"\"Several RSP systems have been implemented over the last few years. The most popular ones correspond to centralized engines, C-SPARQL @cite_31 , CQELS @cite_6 , ETALIS @cite_19 and SPARQL @math @cite_25 . Systems like CQELS-cloud @cite_16 and Strider @cite_11 tackle the scalability issue but distributing stream processing. Available RSP systems are equipped with their own syntax, which generally take the form of continuous versions of the standard SPARQL grammar. This limits the expressiveness on temporal logical operators, the combination or even nesting of window operators. Moreover, the support of recursion is also missing.\"","":""}
{"id":"2798208386","dialogue":"\"The trade-off between language expressiveness and system scalability (E&S) is a well-known problem in RDF stream reasoning. Higher expressiveness supports more complex reasoning logic, however, it may also hinder system scalability. Current research mainly focuses on logical frameworks suitable for stream reasoning as well as the implementation and the evaluation of prototype systems. These systems are normally developed in a centralized setting which suffer from inherent limited scalability, while an in-depth study of applying distributed solutions to cover E&S is still missing. In this paper, we aim to explore the feasibility of applying modern distributed computing frameworks to meet E&S all together. To do so, we first propose BigSR, a technical demonstrator that supports a positive fragment of the LARS framework. For the sake of generality and to cover a wide variety of use cases, BigSR relies on the two main execution models adopted by major distributed execution frameworks: Bulk Synchronous Processing (BSP) and Record-at-A-Time (RAT). Accordingly, we implement BigSR on top of Apache Spark Streaming (BSP model) and Apache Flink (RAT model). In order to conclude on the impacts of BSP and RAT on E&S, we analyze the ability of the two models to support distributed stream reasoning and identify several types of use cases characterized by their levels of support. This classification allows for quantifying the E&S trade-off by assessing the scalability of each type of use case its level of expressiveness. Then, we conduct a series of experiments with 15 queries from 4 different datasets. Our experiments show that BigSR over both BSP and RAT generally scales up to high throughput beyond million-triples per second (with or without recursion), and RAT attains sub-millisecond delay for stateless query operators.\"","summary":"\"Both StreamRule @cite_29 and its recent parallelized version StreamRule @math @cite_3 use a RSP engine for data stream pre-filtering and Clingo as the ASP solver. The expressiveness of BSP implementation in BigSR can fully cover StreamRule and StreamRule @math , since the implementation in these two reasoners stay on positive stratified Datalog program. Evaluation of StreamRule StreamRule @math showcases that the average throughput is around thousand-triples second with second-level delay. Comparatively, our BSP implementation has almost three orders of magnitude gains. Laser @cite_12 and Ticker @cite_7 are both stream processing systems based on the LARS framework but do not concentrate on scalability. Ticker concentrates incremental model maintenance and sacrifices performance by relying on an external ASP engine (Clingo). Laser also proposes an incremental model based on time interval annotations which can prevent unnecessary re-computations. Although Laser claims to represent a trade-off between expressiveness and data throughput, it cannot scale the way BigSR enables to. This is mainly due to Laser's inability to distribute stream processing.\"","":""}
{"id":"2798105912","dialogue":"\"Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a 'distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the 'distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at this https URL\"","summary":"\"One traditional approach for color enhancement is transferring the color of an example image to a given input image. It is originated from @cite_19 in which the global color distribution of an input image is warped to mimic an example style. There are many subsequent works to improve this technique @cite_0 . While this approach can provide expressive enhancement and diverse stylizations, the results highly depend on example images while providing proper exemplars is challenging. Recent works @cite_14 @cite_13 @cite_3 (semi-)automate exemplar selection by image retrieval methods. Liu al @cite_14 used a keyword-based image search to choose example images. Lee al @cite_13 learn a content-specific style ranking using a large photo collection and select the best exemplar images for color enhancement. For pixel-wise local enhancement, Hwang al @cite_3 find candidate images from a database then search local color enhancement operators.\"","":""}
{"id":"2798105912","dialogue":"\"Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a 'distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the 'distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at this https URL\"","summary":"\"Learning-based color enhancement is another dominant stream @cite_18 @cite_6 @cite_7 . Bychkovsky al @cite_18 present a number of input and retouched image pairs called MIT-Adobe FiveK, which is created by professional experts. They used this data to train a model for color and tone adjustment. Yan al @cite_7 propose a deep learning method to learn specific enhancement styles. Given the features of color and semantic context, a deep neural network as a non-linear mapping function is trained to produce the pixel color of specific styles.\"","":""}
{"id":"2963281296","dialogue":"\"Abstract Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21 and 75.64 on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.\"","summary":"\"As in the work of @cite_9 , one simple approach to exploit multiple visual features is to build an ensemble of distance functions, in which each distance function is learned using a single feature and the final distance is calculated from a weighted sum of these distance functions. However, the usage of predetermined weights is undesirable as highly discriminative features in one environment might become irrelevant in another one. In their work, a model to learn weights of these distance functions by optimizing the relative distance or by maximizing the average rank-k recognition rate is proposed. @cite_36 proposed a novel re-ranking method based on a fusion scheme that reweights an ensemble of distance metric outcomes according to their discriminative capacity. They particularly show that the fused distance perform largely better than any of the distances inferred by each feature separately.\"","":""}
{"id":"2963281296","dialogue":"\"Abstract Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21 and 75.64 on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.\"","summary":"\"To consider spatial information, a common usage in person re-identification is to divide the person image into few regions stripes and concatenate dense local features to implicitly encode the spatial layout of the person. @cite_5 proposed a model for person re-identification that combines spatial constraints and the polynomial feature map @cite_12 into a unified framework. They mention that enforcing the matching within corresponding regions can effectively reduce the risk of mismatching and become more robust to partial occlusions. In addition, their framework can benefit from the complementarity of global and local similarities.\"","":""}
{"id":"2963281296","dialogue":"\"Abstract Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21 and 75.64 on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.\"","summary":"\"The post-ranking method for person re-identification is a relatively unexplored area @cite_4 which has been attracting a lot of attention from the research community. Prates and Schwartz @cite_33 presented a Color-based Ranking Aggregation (CBRA) meth -od, which explores different feature representations to obtain complementary ranking lists, and combine them in order to improve person re-identification. In their work, the KISSME @cite_29 metric learning was adopt -ed and different strategies for ranking aggregation, based on the Stuart rank aggregation method @cite_20 , were proposed. Garc ' @cite_0 @cite_4 related that inspections on the ranked matches can be applied to refine the output in such a way that the correct match will have higher probability to be found in the first ranks. Hence, their work is founded on the idea that a ranking, achieved by any algorithm, contains valuable information which can be further exploited to improve the rank of the true match. To achieve such a goal, they propose an unsupervised post-ranking framework. Once the initial ranking is available, content and context sets are extracted. Then, these are exploited to remove the visual ambiguities and to obtain discriminant feature space which is finally exploited to compute the new ranking.\"","":""}
{"id":"2963281296","dialogue":"\"Abstract Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21 and 75.64 on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.\"","summary":"\"@cite_35 studied person re-identification with man -i -fold-based affinity learning. In their work, a novel affinity learning algorithm called Supervised Smoothed Manifold (SSM) is proposed, which can be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost identification accuracy.\"","":""}
{"id":"2963281296","dialogue":"\"Abstract Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21 and 75.64 on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.\"","summary":"\"In relation to domain adaptation in machine learning, @cite_17 proposed a schema called Mirror Representation to address the view-specific feature distortion problem in person re-identification. It embeds the view-specific feature transformation and enables alignment of the feature distributions across disjoint views for the same person. Zhang and collaborators @cite_6 argue that most existing approaches focus on learning a fixed distance metric for all instance pairs, while ignoring the individuality of each person. They formulate person re-identification as an imbalanced classification problem and learn a classifier specifically for each pedestrian such that the matching model is highly tuned to the individual appearance.\"","":""}
{"id":"2963281296","dialogue":"\"Abstract Person re-identification has received special attention by the human analysis community in the last few years. To address the challenges in this field, many researchers have proposed different strategies, which basically exploit either cross-view invariant features or cross-view robust metrics. In this work, we propose to exploit a post-ranking approach and combine different feature representations through ranking aggregation. Spatial information, which potentially benefits the person matching, is represented using a 2D body model, from which color and texture information are extracted and combined. We also consider background foreground information, automatically extracted via Deep Decompositional Network, and the usage of Convolutional Neural Network (CNN) features. To describe the matching between images we use the polynomial feature map, also taking into account local and global information. The Discriminant Context Information Analysis based post-ranking approach is used to improve initial ranking lists. Finally, the Stuart ranking aggregation method is employed to combine complementary ranking lists obtained from different feature representations. Experimental results demonstrated that we improve the state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21 and 75.64 on top-1 rank recognition rate, respectively, as well as obtaining competitive results on CUHK01 dataset.\"","summary":"\"Although a large number of existing approaches have exploited state-of-the-art visual features, advanced metric learning algorithms, post-ranking or ranking aggregation strategies, domain adaptation based models or even CNN based ones, state-of-the-art results on commonly evaluated person re-identification benchmarks is still far from the accuracy performance needed for most real-world surveillance applications @cite_9 .\"","":""}
{"id":"2963811979","dialogue":"\"In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition.\"","summary":"\"Activity recognition has been a popular research topic in computer vision @cite_18 @cite_24 @cite_28 @cite_19 @cite_16 . Hand-crafted features, such as dense trajectories @cite_19 gave promising results on many datasets. More recent works have focused on learning CNNs for activity recognition @cite_20 @cite_2 . Two-stream CNNs take spatial RGB frames and optical flow frames as input @cite_28 @cite_10 . 3D XYT convoltuional models have been trained to learn spatio-temporal features @cite_2 @cite_20 @cite_3 @cite_17 . To train these CNN models, large scale datasets such as Kinetics @cite_13 , THUMOS @cite_14 , and ActivityNet @cite_25 have been created.\"","":""}
{"id":"2963811979","dialogue":"\"In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition.\"","summary":"Many works have explored temporal feature aggregation for activity recognition. @cite_23 compared various pooling methods and found that LSTMs and max-pooling the entire video performed best. @cite_1 found that pooling intervals of different locations lengths was beneficial to activity recognition. @cite_26 found that learning important sub-event intervals and using those for classification improved performance.","":""}
{"id":"2963811979","dialogue":"\"In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition.\"","summary":"\"Recently, segment-based 3D CNNs have been used to capture spatio-temporal information simultaneously for activity detection @cite_8 @cite_27 @cite_21 . These approaches all rely on the 3D CNN to capture temporal dynamics, which usually only contain 16 frames. Some works have studied longer-term temporal structures @cite_20 @cite_24 @cite_23 @cite_22 , but it was generally done with a temporal pooling of local representations or (spatio-)temporal convolutions with larger fixed intervals. Recurrent neural networks (RNNs) also have been used to model activity transitions between frames @cite_11 @cite_12 @cite_5 .\"","":""}
{"id":"2797263747","dialogue":"\"We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms baseline approaches for grounding sounds into images. Several qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.\"","summary":"\"Sound source separation","":""}
{"id":"2797263747","dialogue":"\"We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms baseline approaches for grounding sounds into images. Several qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.\"","summary":"\"Our work builds off efforts to learn perceptual models that are self-supervised'' by leveraging natural contextual signals in images @cite_29 @cite_23 @cite_46 @cite_24 @cite_39 , videos @cite_32 @cite_48 @cite_44 @cite_36 @cite_38 @cite_2 , and even radio signals @cite_47 . These approaches utilize the power of supervised learning while not requiring manual annotations, instead deriving supervisory signals from the structure in natural data. Our model is similarly self-supervised, but uses self-supervision to learn to separate and ground sound in vision.\"","":""}
{"id":"2796428665","dialogue":"\"Generative Adversarial Networks (GANs) have been promising in the field of image generation, however, they have been hard to train for language generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them which causes high levels of instability in training GANs. Consequently, past work has resorted to pre-training with maximum-likelihood or training GANs without pre-training with a WGAN objective with a gradient penalty. In this study, we present a comparison of those approaches. Furthermore, we present the results of some experiments that indicate better training and convergence of Wasserstein GANs (WGANs) when a weaker regularization term is enforcing the Lipschitz constraint.\"","summary":"\"Sai et. al @cite_6 have introduced a simple baseline that addresses the discrete output space problem without relying on gradient estimators and shows that it is able to achieve state-of-the-art results on a Chinese poem generation dataset and presented quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.\"","":""}
{"id":"2796428665","dialogue":"\"Generative Adversarial Networks (GANs) have been promising in the field of image generation, however, they have been hard to train for language generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them which causes high levels of instability in training GANs. Consequently, past work has resorted to pre-training with maximum-likelihood or training GANs without pre-training with a WGAN objective with a gradient penalty. In this study, we present a comparison of those approaches. Furthermore, we present the results of some experiments that indicate better training and convergence of Wasserstein GANs (WGANs) when a weaker regularization term is enforcing the Lipschitz constraint.\"","summary":"\"Ofir et. al @cite_8 have shown that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. They empirically show that their approach vastly improves the quality of generated sequences compared to a convolutional baseline.\"","":""}
{"id":"2796428665","dialogue":"\"Generative Adversarial Networks (GANs) have been promising in the field of image generation, however, they have been hard to train for language generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them which causes high levels of instability in training GANs. Consequently, past work has resorted to pre-training with maximum-likelihood or training GANs without pre-training with a WGAN objective with a gradient penalty. In this study, we present a comparison of those approaches. Furthermore, we present the results of some experiments that indicate better training and convergence of Wasserstein GANs (WGANs) when a weaker regularization term is enforcing the Lipschitz constraint.\"","summary":"\"Henning et. al @cite_3 present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets. For stable training of Wasserstein GANs, they propose to use the following penalty term to enforce the Lipschitz constraint that appears in the objective function:\"","":""}
{"id":"2796152334","dialogue":"\"Multi-organ segmentation is a critical problem in medical image analysis due to its great value for computer-aided diagnosis, computer-aided surgery, and radiation therapy. Although fully-supervised segmentation methods can achieve good performance, they usually require a large amount of 3D data, such as CT scans, with voxel-wised annotations which are usually difficult, expensive, and slow to obtain. By contrast, large unannotated datasets of CT images are available. Inspired by the well-known semi-supervised learning framework co-training, we propose multi-planar co-training (MPCT), to generate more reliable pseudo-labels by enforcing consistency among multiple planes, i.e., saggital, coronal, and axial planes, of 3D unlabeled medical data, which play a vital role in our framework. Empirical results show that generating pseudo-labels by the multi-planar fusion rather than a single plane leads to a significant performance gain. We evaluate our approach on a new collected dataset and show that MPCT boosts the performance of a typical segmentation model, fully convolutional networks, by a large margin, when only a small set of labeled 3D data is available, i.e., 77.49 vs. 73.14 .\"","summary":"\"Early studies of abdominal organ segmentation focused on atlas-based methods @cite_34 @cite_10 @cite_20 . The frameworks are usually problematic because 1) they are not able to capture the large inter-subject variations of abdominal regions and 2) computational time is tightly dependent on the number of atlases. Recently, learning-based approaches with relatively large dataset have been introduced for multi-organ segmentation @cite_41 @cite_3 @cite_18 . Especially, deep Convolutional Neural Networks (CNNs) based methods have achieved a great success in the medical image segmentation @cite_21 @cite_5 @cite_19 @cite_32 @cite_2 @cite_14 @cite_48 in the last few years. Compared with multi-atlas-based approaches, CNNs based methods are generally more efficient and accurate. CNNs based methods for multi-organ segmentation can be divided into two major categories: 3D CNNs @cite_21 @cite_5 @cite_19 based and 2D CNNs @cite_32 @cite_2 @cite_14 @cite_48 based. 3D CNNs usually adopt the sliding-window strategy to avoid the problem, leading to high time complexity. Compared with 3D CNNs, 2D CNNs based algorithms can be directly end-to-end trained using 2D deep networks, which is less time-consuming.\"","":""}
{"id":"2796152334","dialogue":"\"Multi-organ segmentation is a critical problem in medical image analysis due to its great value for computer-aided diagnosis, computer-aided surgery, and radiation therapy. Although fully-supervised segmentation methods can achieve good performance, they usually require a large amount of 3D data, such as CT scans, with voxel-wised annotations which are usually difficult, expensive, and slow to obtain. By contrast, large unannotated datasets of CT images are available. Inspired by the well-known semi-supervised learning framework co-training, we propose multi-planar co-training (MPCT), to generate more reliable pseudo-labels by enforcing consistency among multiple planes, i.e., saggital, coronal, and axial planes, of 3D unlabeled medical data, which play a vital role in our framework. Empirical results show that generating pseudo-labels by the multi-planar fusion rather than a single plane leads to a significant performance gain. We evaluate our approach on a new collected dataset and show that MPCT boosts the performance of a typical segmentation model, fully convolutional networks, by a large margin, when only a small set of labeled 3D data is available, i.e., 77.49 vs. 73.14 .\"","summary":"\"The most commonly used techniques for semi-supervised learning include self-training @cite_50 @cite_47 , co-training @cite_49 , multi-view learning @cite_0 and graph-based methods @cite_53 @cite_52 .\"","":""}
{"id":"2796152334","dialogue":"\"Multi-organ segmentation is a critical problem in medical image analysis due to its great value for computer-aided diagnosis, computer-aided surgery, and radiation therapy. Although fully-supervised segmentation methods can achieve good performance, they usually require a large amount of 3D data, such as CT scans, with voxel-wised annotations which are usually difficult, expensive, and slow to obtain. By contrast, large unannotated datasets of CT images are available. Inspired by the well-known semi-supervised learning framework co-training, we propose multi-planar co-training (MPCT), to generate more reliable pseudo-labels by enforcing consistency among multiple planes, i.e., saggital, coronal, and axial planes, of 3D unlabeled medical data, which play a vital role in our framework. Empirical results show that generating pseudo-labels by the multi-planar fusion rather than a single plane leads to a significant performance gain. We evaluate our approach on a new collected dataset and show that MPCT boosts the performance of a typical segmentation model, fully convolutional networks, by a large margin, when only a small set of labeled 3D data is available, i.e., 77.49 vs. 73.14 .\"","summary":"\"In self-training, the classifier is iteratively re-trained using the training set augmented by adding the unlabeled data with their own predictions. The procedure repeated until some convergence criteria are satisfied. In such case, one can imagine that a classification mistake can reinforce itself. Self-training has achieved great performances in many computer vision problems @cite_50 @cite_47 and recently has been applied to deep learning based semi-supervised learning in the biomedical imaging domain @cite_13 .\"","":""}
{"id":"2796152334","dialogue":"\"Multi-organ segmentation is a critical problem in medical image analysis due to its great value for computer-aided diagnosis, computer-aided surgery, and radiation therapy. Although fully-supervised segmentation methods can achieve good performance, they usually require a large amount of 3D data, such as CT scans, with voxel-wised annotations which are usually difficult, expensive, and slow to obtain. By contrast, large unannotated datasets of CT images are available. Inspired by the well-known semi-supervised learning framework co-training, we propose multi-planar co-training (MPCT), to generate more reliable pseudo-labels by enforcing consistency among multiple planes, i.e., saggital, coronal, and axial planes, of 3D unlabeled medical data, which play a vital role in our framework. Empirical results show that generating pseudo-labels by the multi-planar fusion rather than a single plane leads to a significant performance gain. We evaluate our approach on a new collected dataset and show that MPCT boosts the performance of a typical segmentation model, fully convolutional networks, by a large margin, when only a small set of labeled 3D data is available, i.e., 77.49 vs. 73.14 .\"","summary":"\"Co-training @cite_49 assumes that (1) features can be split into two independent sets and (2) each sub-feature set is sufficient to train a good classifier. During the learning process, each classifier is retrained with the additional training examples given by the other classifier. Co-training utilizes multiple sets of independent features which describe the same data, and therefore tends to yield more accurate and robust results than self-training @cite_11 . Multi-view learning @cite_0 , in general, defines learning paradigms that utilize the agreement among different learners. Co-training is one of the earliest schemes for multi-view learning.\"","":""}
{"id":"2796152334","dialogue":"\"Multi-organ segmentation is a critical problem in medical image analysis due to its great value for computer-aided diagnosis, computer-aided surgery, and radiation therapy. Although fully-supervised segmentation methods can achieve good performance, they usually require a large amount of 3D data, such as CT scans, with voxel-wised annotations which are usually difficult, expensive, and slow to obtain. By contrast, large unannotated datasets of CT images are available. Inspired by the well-known semi-supervised learning framework co-training, we propose multi-planar co-training (MPCT), to generate more reliable pseudo-labels by enforcing consistency among multiple planes, i.e., saggital, coronal, and axial planes, of 3D unlabeled medical data, which play a vital role in our framework. Empirical results show that generating pseudo-labels by the multi-planar fusion rather than a single plane leads to a significant performance gain. We evaluate our approach on a new collected dataset and show that MPCT boosts the performance of a typical segmentation model, fully convolutional networks, by a large margin, when only a small set of labeled 3D data is available, i.e., 77.49 vs. 73.14 .\"","summary":"\"Graph-based semi-supervised methods define a graph where the nodes are labeled and unlabeled examples in the dataset, and edges reflect the similarity of examples. These methods have been widely adopted in non-deep-learning based semi-supervised learning algorithms in the biomedical imaging domain @cite_26 @cite_46 @cite_42 .\"","":""}
{"id":"2796457187","dialogue":"\"We consider two types of searching models, where the goal is to design an adaptive algorithm that locates an unknown vertex in a graph by repeatedly performing queries. In the vertex-query model, each query points to a vertex @math and the response either admits that @math is the target or provides a neighbor of @math on a shortest path from @math to the target. This model has been introduced for trees by Onak and Parys [FOCS 2006] and by Emamjomeh- [STOC 2016] for arbitrary graphs. In the edge-query model, each query chooses an edge and the response reveals which endpoint of the edge is closer to the target, breaking ties arbitrarily. Our goal is to analyze solutions to these problems assuming that some responses may be erroneous. We develop a scheme for tackling such noisy models with the following line of arguments: For each of the two models, we analyze a generic strategy that assumes a fixed number of lies and give a precise bound for its length via an amortized analysis. From this, we derive bounds for both a linearly bounded error rate, where the number of errors in @math queries is bounded by @math for some @math , and a probabilistic model in which each response is incorrect with some probability @math . The bounds for adversarial case turn out to be strong enough for non-adversarial scenarios as well. We obtain thus a much simpler strategy performing fewer vertex-queries than one by Emamjomeh- [STOC 2016]. For edge-queries, not studied before for general graphs, we obtain bounds that are tight up to @math factors in all error models. Applying our graph-theoretic results to the setting of edge-queries for paths, we obtain a number of improvements over existing bounds for searching in a sorted array in the presence of errors, including an exponential improvement for the prefix-bounded model in unbounded domains.\"","summary":"\"Regarding the problem of searching in graphs without errors, many papers have been devoted to trees, mainly because it is a structure that naturally generalizes paths, which represents the classical binary search (see e.g. @cite_3 for search in a path with non-uniform query times). This query model in case of trees is equivalent to several other problems, including vertex ranking @cite_4 or tree-depth @cite_19 . There exist linear-time algorithms for finding optimal query strategies @cite_13 @cite_38 . A lot of effort has been done to understand the complexity for trees with non-uniform query times. It turns out that the problem becomes hard for trees @cite_14 @cite_31 . Also refer the reader to works on a closely related query game with edge queries @cite_10 @cite_32 @cite_16 @cite_8 @cite_36 . For general graphs, a strategy that always queries a 1-median (the minimizer of the sum of distances over all vertices) has length at most @math @cite_26 .\"","":""}
{"id":"2795485067","dialogue":"\"Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS). However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue. In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE). This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner. Experiments using the VCTK and Blizzard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the expressions in its synthesized speech by incorporating global characteristics into the speech generating process.\"","summary":"@cite_18 @cite_14 pointed out that UESS can be divided into two parts: predicting expressive information from text; and synthesizing the speech with a particular expression. In this paper only the latter stage is considered for simplicity.","":""}
{"id":"2795487127","dialogue":"\"Network testing plays an important role in the iterative process of developing new communication protocols and algorithms. However, test environments have to keep up with the evolution of technology and require continuous update and redesign. In this article, we propose COINS, a framework that can be used by wireless technology developers to enable CI practices in their testbed infrastructure. As a proof-of-concept, we provide a reference architecture and implementation of COINS for controlled testing of multi-technology 5G MTC networks. The implementation upgrades an existing wireless experimentation testbed with new software and hardware functionalities. It blends web service technology and operating system virtualization technologies with emerging Internet of Things technologies enabling CI for wireless networks. Moreover, we also extend an existing qualitative methodology for comparing similar frameworks and identify and discuss open challenges for wider use of CI practices in wireless technology development.\"","summary":"\"While we did not find any other framework addressing wireless experimentation with CI support, we have identified three existing frameworks that are to some extent similar to COINS, i.e., the control and Management Framework (OMF) @cite_7 , the Network Implementation Testbed Laboratory (NITOS) @cite_2 and the Berlin Open Wireless Lab (BOWL) @cite_14 . We performed a feature-wise comparison of these frameworks and summarized it in Table ,.\"","":""}
{"id":"2795487127","dialogue":"\"Network testing plays an important role in the iterative process of developing new communication protocols and algorithms. However, test environments have to keep up with the evolution of technology and require continuous update and redesign. In this article, we propose COINS, a framework that can be used by wireless technology developers to enable CI practices in their testbed infrastructure. As a proof-of-concept, we provide a reference architecture and implementation of COINS for controlled testing of multi-technology 5G MTC networks. The implementation upgrades an existing wireless experimentation testbed with new software and hardware functionalities. It blends web service technology and operating system virtualization technologies with emerging Internet of Things technologies enabling CI for wireless networks. Moreover, we also extend an existing qualitative methodology for comparing similar frameworks and identify and discuss open challenges for wider use of CI practices in wireless technology development.\"","summary":"\"In the comparison, we considered an extensive list of features that can be used to compare the experimentation systems @cite_12 . The list of features, however, lacks the CI-specific properties that our work focuses on; therefore, we extended the list with four core CI concepts @cite_0 @cite_13 : Existence of a single source repository that contains everything needed for the completely automated build process. Support for completely automated build process executed on each commit. Self-tests included inside the repository, which run on each build. Fast build process so that each commit can be built and tested.\"","":""}
{"id":"2795412335","dialogue":"\"Concerns have reached the mainstream about how social media are affecting political outcomes. One trajectory for this is the exposure of politicians to online abuse. In this paper we use 1.4 million tweets from the months before the 2015 and 2017 UK general elections to explore the abuse directed at politicians. This collection allows us to look at abuse broken down by both party and gender and aimed at specific Members of Parliament. It also allows us to investigate the characteristics of those who send abuse and their topics of interest. Results show that in both absolute and proportional terms, abuse increased substantially in 2017 compared with 2015. Abusive replies are somewhat less directed at women and those not in the currently governing party. Those who send the abuse may be issue-focused, or they may repeatedly target an individual. In the latter category, accounts are more likely to be throwaway. Those sending abuse have a wide range of topical triggers, including borders and terrorism.\"","summary":"\"Whilst online fora have attracted much attention as a way of exploring political dynamics @cite_15 @cite_4 @cite_8 @cite_12 @cite_16 @cite_13 , and the effect of abuse and incivility in these contexts has been explored @cite_7 @cite_19 @cite_3 @cite_2 , little work exists regarding the abusive and intimidating ways people address politicians online; a trend that has worrying implications for democracy. collected tweets centred around candidates for the European Parliament election in 2014 from Spain, Germany, the United Kingdom and France posted in the month surrounding the election. They find that the extent of the abuse and harrassment a politician is subject to correlates with their engagement with the medium. Their analysis focuses on the way in which uncivil behaviour negatively impacts on the potential of the medium to increase interactivity and positively stimulate democracy. Stambolieva studies online abuse against female Members of Parliament (MPs) only; in studying male MPs as well, we are able to contrast the level of abuse they each receive. Furthermore, we contrast proportional with absolute figures, creating quite a different impression from the one she gives.\"","":""}
{"id":"2795412335","dialogue":"\"Concerns have reached the mainstream about how social media are affecting political outcomes. One trajectory for this is the exposure of politicians to online abuse. In this paper we use 1.4 million tweets from the months before the 2015 and 2017 UK general elections to explore the abuse directed at politicians. This collection allows us to look at abuse broken down by both party and gender and aimed at specific Members of Parliament. It also allows us to investigate the characteristics of those who send abuse and their topics of interest. Results show that in both absolute and proportional terms, abuse increased substantially in 2017 compared with 2015. Abusive replies are somewhat less directed at women and those not in the currently governing party. Those who send the abuse may be issue-focused, or they may repeatedly target an individual. In the latter category, accounts are more likely to be throwaway. Those sending abuse have a wide range of topical triggers, including borders and terrorism.\"","summary":"\"A larger body of work has looked at hatred on social media more generally @cite_17 @cite_18 @cite_5 @cite_10 . Williams and Burnap present work demonstrating the potential of Twitter for evidencing social models of online hate crime that could support prediction, as well as exploring how attitudes co-evolve with events to determine their impact @cite_6 @cite_0 . use natural language processing (NLP) to identify the groups targeted for hatred on Twitter and Whisper.\"","":""}
{"id":"2795412335","dialogue":"\"Concerns have reached the mainstream about how social media are affecting political outcomes. One trajectory for this is the exposure of politicians to online abuse. In this paper we use 1.4 million tweets from the months before the 2015 and 2017 UK general elections to explore the abuse directed at politicians. This collection allows us to look at abuse broken down by both party and gender and aimed at specific Members of Parliament. It also allows us to investigate the characteristics of those who send abuse and their topics of interest. Results show that in both absolute and proportional terms, abuse increased substantially in 2017 compared with 2015. Abusive replies are somewhat less directed at women and those not in the currently governing party. Those who send the abuse may be issue-focused, or they may repeatedly target an individual. In the latter category, accounts are more likely to be throwaway. Those sending abuse have a wide range of topical triggers, including borders and terrorism.\"","summary":"\"Work exists regarding accurately identifying abusive messages automatically @cite_0 @cite_9 @cite_14 @cite_20 @cite_1 . The work of has been described as the state of the art, with precision recall of 0.63 being reported as equivalent to human performance. report a human interannotator agreement of only 0.69 in annotating racial slurs, and report a human agreement of 0.8 with a Krippendorf's alpha of 0.25 on UK data, demonstrating that the limiting factor is the complexity of the task definition. Burnap and Williams particularly focus on hate speech with regards to protected characteristics such as race, disability and sexual orientation. Waseem and Hovy also focus on hate speech, and share a gold standard UK Tweet corpus. seek to identify the problem accounts rather than the problem material. Schmidt and Wiegand provide a review of prior work and methods.\"","":""}
{"id":"2796203807","dialogue":"\"Inverse dynamics control and feedforward linearization techniques are typically used to convert the complex nonlinear dynamics of robots to a set of decoupled double integrators, and then a standard, outer-loop controller can be used to calculate the commanded acceleration for the linearized system. However, these methods typically depend on having a very accurate system model, which is not available in practice. While this challenge has been addressed in the literature using different learning approaches, most of these approaches do not provide safety guarantees in terms of stability of the learning-based control system. In this paper, we provide a novel, learning-based control approach based on Gaussian processes (GPs) that ensures both stability of the closed-loop system and high-accuracy tracking. In our proposed approach, we use GPs to approximate the error between the commanded acceleration and the actual acceleration of the system, and then use the predicted mean and variance of the GP to calculate an upper bound on the uncertainty of the linearization. This uncertainty bound is then used in a robust, outer-loop controller to ensure stability of the overall system. Moreover, we show that using our proposed strategy, the tracking error converges to a ball with a radius that can be made arbitrarily small through appropriate control design. Furthermore, we verify the effectiveness of the proposed approach via simulations on (i) a 2 degree-of-freedom (DOF) planar manipulator, and (ii) a 6 DOF industrial manipulator using the Gazebo simulator.\"","summary":"\"The study of safe learning dates back to the beginning of this century @cite_17 . In @cite_15 and @cite_11 , Lyapunov-based reinforcement learning is used to allow a learning agent to safely switch between pre-computed baseline controllers. Then, in @cite_2 , risk-sensitive reinforcement learning is proposed, in which the expected return is heuristically weighted with the probability of reaching an error state. In several other papers, including @cite_19 , @cite_18 and @cite_3 , safe exploration methods are utilized to allow the learning modules to achieve a desired balance between ensuring safe operation and exploring new states for improved performance. In @cite_17 , a general framework is proposed for ensuring safety of learning-based control strategies for uncertain robotic systems. In this framework, robust reachability guarantees from control theory are combined with Bayesian analysis based on empirical observations. The result is a safety-preserving, supervisory controller of the learning module that allows the system to freely execute its learning policy almost everywhere, but imposes control actions to ensure safety at critical states. Despite its effectiveness for ensuring safety, the supervisory controller in this approach has no role in reducing tracking errors.\"","":""}
{"id":"2796203807","dialogue":"\"Inverse dynamics control and feedforward linearization techniques are typically used to convert the complex nonlinear dynamics of robots to a set of decoupled double integrators, and then a standard, outer-loop controller can be used to calculate the commanded acceleration for the linearized system. However, these methods typically depend on having a very accurate system model, which is not available in practice. While this challenge has been addressed in the literature using different learning approaches, most of these approaches do not provide safety guarantees in terms of stability of the learning-based control system. In this paper, we provide a novel, learning-based control approach based on Gaussian processes (GPs) that ensures both stability of the closed-loop system and high-accuracy tracking. In our proposed approach, we use GPs to approximate the error between the commanded acceleration and the actual acceleration of the system, and then use the predicted mean and variance of the GP to calculate an upper bound on the uncertainty of the linearization. This uncertainty bound is then used in a robust, outer-loop controller to ensure stability of the overall system. Moreover, we show that using our proposed strategy, the tracking error converges to a ball with a radius that can be made arbitrarily small through appropriate control design. Furthermore, we verify the effectiveness of the proposed approach via simulations on (i) a 2 degree-of-freedom (DOF) planar manipulator, and (ii) a 6 DOF industrial manipulator using the Gazebo simulator.\"","summary":"\"Focusing our attention on safe, learning-based inverse dynamics control, we refer to @cite_4 @cite_1 @cite_14 . In @cite_4 , a model reference adaptive control (MRAC) architecture based on Gaussian processes (GPs) is proposed, and stability of the overall control system is proved. While the approach in @cite_4 is based on adaptive control theory, our approach is based on robust control theory. In particular, in @cite_4 , the mean of the GP is used to exactly cancel the uncertainty vector, while in our approach, we use both the mean and variance of the GP to learn an upper bound on the uncertainty vector to be used in a robust, outer-loop controller. Hence, unlike @cite_4 , in our approach, the uncertainty of the learning module is not only incorporated in the stability analysis but also in the outer-loop controller design. Intuitively, the less certain our GPs are, the more robust the outer-loop controller should be for ensuring safety. When more data is collected and the GPs are more certain, the outer-loop controller can be less conservative for improved performance. While the results of @cite_4 are tested in simulations on a two-dimensional system, we test our results experimentally on a 6 DOF manipulator.\"","":""}
{"id":"2796203807","dialogue":"\"Inverse dynamics control and feedforward linearization techniques are typically used to convert the complex nonlinear dynamics of robots to a set of decoupled double integrators, and then a standard, outer-loop controller can be used to calculate the commanded acceleration for the linearized system. However, these methods typically depend on having a very accurate system model, which is not available in practice. While this challenge has been addressed in the literature using different learning approaches, most of these approaches do not provide safety guarantees in terms of stability of the learning-based control system. In this paper, we provide a novel, learning-based control approach based on Gaussian processes (GPs) that ensures both stability of the closed-loop system and high-accuracy tracking. In our proposed approach, we use GPs to approximate the error between the commanded acceleration and the actual acceleration of the system, and then use the predicted mean and variance of the GP to calculate an upper bound on the uncertainty of the linearization. This uncertainty bound is then used in a robust, outer-loop controller to ensure stability of the overall system. Moreover, we show that using our proposed strategy, the tracking error converges to a ball with a radius that can be made arbitrarily small through appropriate control design. Furthermore, we verify the effectiveness of the proposed approach via simulations on (i) a 2 degree-of-freedom (DOF) planar manipulator, and (ii) a 6 DOF industrial manipulator using the Gazebo simulator.\"","summary":"\"In @cite_1 @cite_14 , GPs are utilized to learn the errors in the output torques of the inverse dynamics model online. In @cite_1 , the GP learning is combined with a state-of-the-art gradient descent method for learning feedback terms online. The main idea behind this approach is that the gradient descent method would correct for fast perturbations, while the GP is responsible for correcting slow perturbations. This allows for exponential smoothing of the GP hyperparameters, which increases the robustness of the GP at the cost of having slower reactiveness. Nevertheless, @cite_1 does not provide a proof of the robust stability of the closed-loop system. @cite_14 , the variance of the GP prediction is utilized to adapt the parameters of an outer-loop PD controller online, and the uniform ultimate boundedness of the tracking error is proved under some assumptions on the structure of the PD controller (e.g., the gain matrix was assumed to be diagonal, which imposes a decentralized gain control scheme). The results of @cite_14 are verified via simulations on a 2 DOF manipulator.\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"Profiling plays a key role in managed runtimes, either for code optimization or memory management decisions @cite_29 @cite_37 @cite_35 @cite_25 @cite_27 @cite_20 @cite_10 . We focus on getting quality profiling information to drive object pretenuring. is, to the best of our knowledge, the first online profiler targeting the dynamic pretenuring of objects in Big Data applications running on HotSpot. This section compares our work with state-of-art systems, namely, off-line and online profilers that guide systems where small changes are needed in the heap organization and collection. It ends with a comparative analysis of systems that demand a more profound change, either to the application framework or the runtime itself, in some cases manipulating application-defined types and or organizing the heap in special purpose regions, and placing data directly in an off-heap space.\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"@cite_25 introduced an algorithm where an object lifetime is tracked based on timestamps assigned when the object lost an incoming reference, and when GC identifies it as an unreachable object. This is implemented in a tracing tool called Merlin that, when is analyzing a program, it can be up to 300 times slower compared to a non-profiled run. @cite_27 uses the same algorithm but adds new functionalities to his Elephant track tool in terms of precision and comprehensiveness of reference sources (i.e. weak reference). Another system, Resurrector @cite_20 , relaxes on precision to provide faster profiling but still introduces 3 to 40 times slowdown depending on the workload.\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"@cite_2 extends the profile-based pretenuring of Cheng's solution @cite_32 using the Merlin tracing tool @cite_25 . They have a two stage profiling. The first stage happens during the build of the JVM @. Profiling at this stage is used to improve the performance of JVM itself, since Jikes RVM @cite_37 is a meta-circular JVM. reports this is particularly useful for tight heaps (i.e. heaps that are just above the minimum size for a given application, reaching at most 150MB) and not suitable for heaps with Gigabytes of objects. The second stage is an application-specific process, based on the off-line profiling made with Merlin @cite_25 .\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"@cite_21 presents an headroom schema which drives pretenuring based on the space left on the heap before garbage collection is necessary. Although their solution brings advantages to collection times, they push much of the overhead to the mutator and also to the off-line process, which is not always possible or accurate. This approach makes the classification not only dependent on the application but also on the overall heap size. Finally, @cite_21 do not target large heaps or a modern garbage collector like G1.\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"NG2C @cite_12 extends G1 to support object pretenuring. However, it also needs offline profiling and programmer's help to identify the generation where a new object should be allocated. Thus, we can say that it uses an off-line profiler to establish a relation between allocation sites and object lifetimes, missing the opportunity to avoid inhomogeneous allocation behavior @cite_0 . @cite_7 extends the operation of the Immix garbage collector in Jikes RVM @cite_9 with a new programming interface between the application and the GC, in order to manage dominant data structures (i.e. a data structure holding most of the objects during the lifetime of the program) more efficiently.The main advantage comes from reducing the occurrence of highly entangled deep-shaped data structures lay-out in memory, thus improving performance of the parallel tracing stage.\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"needs the calling context to profile objects at relevant allocation sites. Ball and Laurus @cite_33 compute a unique number for each possible path of a control flow graph inside a procedure. The computation is done offline and added to the source code. This is not suited for because modern workloads have many possible paths inside each routine, and the technique can not capture the inter-procedure path needed for to distinguish allocation sites. Bond and McKinley @cite_5 also compute a single value, but at runtime, to determine the sequence of active stack frames in a inter-procedural way. However, they need to maintain non-commutativity to differentiate call orders, such as calling methods @math and @math . This is not a requirement for and so we can have a smaller impact on code instrumentation, with low impact on the throughput. uses an adaptive solution to store allocation contexts, with a low ratio of collisions (as shown in Table ) while using fewer bits to store information.\"","":""}
{"id":"2796346424","dialogue":"\"Low latency services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms (e.g., Lucene, Graphchi, Cassandra) which run on top of memory managed runtimes, such as the JVM. These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in memory fragmentation). This leads to long and frequent application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of middle to long-lived objects (which is the case for a wide spectrum of Big Data applications). Previous works try to reduce such application pauses by allocating objects off-heap or in special allocation regions generations, thus alleviating the pressure on memory management. However, all these solutions require a combination of programmer effort and knowledge, source code access, or off-line profiling, with clear negative impact on programmer productivity and or application performance. This paper presents ROLP, a runtime object lifetime profiling system. ROLP profiles application code at runtime in order to identify which allocation contexts create objects with middle to long lifetimes, given that such objects need to be handled differently (regarding short-lived ones). This profiling information greatly improves memory management decisions, leading to long tail latencies reduction of up to 51 for Lucene, 85 for GraphChi, and 60 for Cassandra, with negligible throughput and memory overhead. ROLP is implemented for the OpenJDK 8 HotSpot JVM and it does not require any programmer effort or source code access.\"","summary":"\"@cite_14 performs online profiling to optimize memory allocation operations; it performs a single allocation (improvement over allocation inlining) of a block big enough to fit multiple objects that are allocated in sequence. Objects need not have parent-child or sibling relationships among them because it is the control-flow relations between hot code flows and locations that are monitored, as we address in our work. So, while allocations may take place in different methods, equivalent GC behavior is ensured. However, while doing online profiling, it only addresses initial allocation and not the issue of pretenuring objects.\"","":""}
{"id":"2963140844","dialogue":"\"This paper proposes a 3D shape descriptor network","summary":"which is a deep convolutional energy-based model","":""}
{"id":"2963140844","dialogue":"\"This paper proposes a 3D shape descriptor network","summary":"which is a deep convolutional energy-based model","":""}
{"id":"2963140844","dialogue":"\"This paper proposes a 3D shape descriptor network","summary":"which is a deep convolutional energy-based model","":""}
{"id":"2796239628","dialogue":"\"Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video context. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100 (Meteor score increases from 4.82 to 9.65).\"","summary":"\"Analogous to region proposals in image domain, temporal action proposals are candidate temporal windows that possibly contain actions. Sparse-prop @cite_27 applies dictionary learning for generating class-independent proposals. S-CNN @cite_41 uses 3D convolutional neural networks (CNNs) @cite_13 to generate multi-scale segments (proposals). TURN TAP @cite_1 uses clip pyramid features in their model, and it predicts proposals and refines temporal boundaries jointly. DAPs @cite_23 first applies Long Short-Term Memory (LSTM) @cite_11 to encoding video content in a sliding window and then predicts proposals covered by the window. Built on @cite_23 , SST @cite_17 further takes long sequence training problem into consideration and generates proposals in a single pass. However, all these methods either fail to produce long proposals or do not exploit future context. In contrast, our model for temporal proposal tackles these two problems simultaneously.\"","":""}
{"id":"2796239628","dialogue":"\"Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video context. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100 (Meteor score increases from 4.82 to 9.65).\"","summary":"\"While aforementioned captioning methods generate only one sentence for an input video, video paragraph generation focuses on producing multiple semantics-fluent sentences. Rohrbach adapted statistical machine translation (SMT) @cite_16 to generate semantic consistent sentences with desired level of details @cite_5 . Yu proposed a hierarchical RNN to model both cross-sentence dependency and word dependency @cite_26 .\"","":""}
{"id":"2795143128","dialogue":"\"We train a network to generate mappings between training sets and classification policies (a 'classifier generator') by conditioning on the entire training set via an attentional mechanism. The network is directly optimized for test set performance on an training set of related tasks, which is then transferred to unseen 'test' tasks. We use this to optimize for performance in the low-data and unsupervised learning regimes, and obtain significantly better performance in the 10-50 datapoint regime than support vector classifiers, random forests, XGBoost, and k-nearest neighbors on a range of small datasets.\"","summary":"\"Our approach belongs to the class of model-based meta-learning methods called memory-augmented neural networks @cite_14 . We similarly use a memory containing the training data and maximize test set performance over the distribution of datasets, but rather than construct this memory one sample at a time through a Neural Turing Machine, we make use of a self-attention based architecture to process all of the training set and its internal relationships in parallel, in a strictly permutation-invariant fashion.\"","":""}
{"id":"2795143128","dialogue":"\"We train a network to generate mappings between training sets and classification policies (a 'classifier generator') by conditioning on the entire training set via an attentional mechanism. The network is directly optimized for test set performance on an training set of related tasks, which is then transferred to unseen 'test' tasks. We use this to optimize for performance in the low-data and unsupervised learning regimes, and obtain significantly better performance in the 10-50 datapoint regime than support vector classifiers, random forests, XGBoost, and k-nearest neighbors on a range of small datasets.\"","summary":"\"Metric-based meta-learning approaches have also been applied to the general problem of one-shot learning, with most of the results being in the image domain. This is a family of approaches which use a combination of an embedding transformation with interpretation of the embeddings in the context of an imposed metric space in order to achieve one-shot learning of similarity relationships in a shared domain (e.g. visual similarity). These approaches include matching networks @cite_31 , siamese networks @cite_40 , and FaceNet @cite_25 . The work Towards a Neural Statistician'' @cite_10 learns latent spaces which explain the statistics of the input (it extracts summary statistics from domain examples), which naturally produces a space that has good properties for metric-based one-shot learning approaches on datasets.\"","":""}
{"id":"2795143128","dialogue":"\"We train a network to generate mappings between training sets and classification policies (a 'classifier generator') by conditioning on the entire training set via an attentional mechanism. The network is directly optimized for test set performance on an training set of related tasks, which is then transferred to unseen 'test' tasks. We use this to optimize for performance in the low-data and unsupervised learning regimes, and obtain significantly better performance in the 10-50 datapoint regime than support vector classifiers, random forests, XGBoost, and k-nearest neighbors on a range of small datasets.\"","summary":"\"In optimization-based meta-learning, the process of gradient descent can be formulated as a recurrently applied differentiable transform which should be optimized to reduce the final loss at the end of a period of training @cite_20 . In MAML @cite_12 , this is cast as a form of transfer learning problem, and as such the parameters of the model used to solve the particular task are still the network parameters (rather than some latent representation of the task inferred directly from the training set), but for which the model as a whole has been adapted such that only a small number of gradient descent steps will be needed in order to obtain high performance. This sort of optimization-based approach has recently been explicitly applied to the problem of few-shot classification @cite_0 .\"","":""}
{"id":"2795143128","dialogue":"\"We train a network to generate mappings between training sets and classification policies (a 'classifier generator') by conditioning on the entire training set via an attentional mechanism. The network is directly optimized for test set performance on an training set of related tasks, which is then transferred to unseen 'test' tasks. We use this to optimize for performance in the low-data and unsupervised learning regimes, and obtain significantly better performance in the 10-50 datapoint regime than support vector classifiers, random forests, XGBoost, and k-nearest neighbors on a range of small datasets.\"","summary":"Attentive meta-learning @cite_39 is similar to our approach in that an attention mechanism is used to effectively make use of rich conditioning information which acts to specify the particular sub-task which must be solved in a given context.","":""}
{"id":"2963846885","dialogue":"\"Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem, in which instances of unseen (target) classes tend to be categorized as one of the seen (source) classes. So they yield poor performance after being deployed in the generalized ZSL settings. In this paper, we propose a straightforward yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate the bias problem. Our method follows the way of transductive learning, which assumes that both the labeled source images and unlabeled target images are available for training. In the semantic embedding space, the labeled source images are mapped to several fixed points specified by the source categories, and the unlabeled target images are forced to be mapped to other points specified by the target categories. Experiments conducted on AwA2, CUB and SUN datasets demonstrate that our method outperforms existing state-of-the-art approaches by a huge margin of 9.3 24.5 following generalized ZSL settings, and by a large margin of 0.2 16.2 following conventional ZSL settings.\"","summary":"\"ZSL relies on the semantic space to associate source and target classes. Various semantic spaces have been investigated, including attributes @cite_33 @cite_3 @cite_44 @cite_40 @cite_29 , word vector @cite_4 @cite_0 , text description @cite_23 @cite_13 and human gaze @cite_5 . The attribute has been shown to be an effective semantic space @cite_14 @cite_20 @cite_29 for ZSL. However, its superior performance is obtained at the cost of much more expensive human labor. As an alternative, the word vectors are gaining more attention recently @cite_15 @cite_39 since they are learned from the large text corpus in an unsupervised way. Albeit their popularity, the word vectors often suffer from visual-semantic discrepancy problem @cite_2 @cite_21 @cite_25 . In addition to the word vectors, human gaze @cite_5 is recently proposed to replace the attributes, as its annotation can be performed by non-experts without domain knowledge.\"","":""}
{"id":"2963915677","dialogue":"\"This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.\"","summary":"\"@math D face reconstruction was first introduced for recognition by Blanz and Vetter @cite_46 . They reconstructed @math D faces by fitting @math DMM to @math D face images, and used the obtained @math DMM parameters as features for face recognition. Their employed @math DMM fitting method is essentially an image-based analysis-by-synthesis approach, which does not consider the features unique to different individuals. This method was recently improved by @cite_25 via pooling the @math DMM parameters of the images of the same subject and using a CNN to regress the pooled parameters. They experimentally proved the improved discriminative power of their obtained @math DMM parameters.\"","":""}
{"id":"2963915677","dialogue":"\"This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.\"","summary":"\"Instead of using @math DMM parameters for recognition, @cite_14 proposed to recover pose and expression normalized @math D face shapes directly from @math D face landmarks via cascaded regressors and match the reconstructed @math D face shapes via the iterative closest point algorithm for face recognition. Other researchers @cite_39 @cite_37 utilized the reconstructed @math D face shapes for face alignment to assist extracting pose-robust features.\"","":""}
{"id":"2963915677","dialogue":"\"This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.\"","summary":"\"To summarize, . @cite_14 and @cite_25 , even though the identity of @math D face shapes in the training data is stressed, respectively, by pooling @math DMM parameters and by normalizing pose and expression, their methods of learning mapping from @math D images to @math D face shapes are in the sense of utilizing identity labels of the training data (see Fig. ).\"","":""}
{"id":"2963915677","dialogue":"\"This paper proposes an encoder-decoder network to disentangle shape features during 3D face reconstruction from single 2D images, such that the tasks of reconstructing accurate 3D face shapes and learning discriminative shape features for face recognition can be accomplished simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. To construct training data we develop a method for fitting 3D morphable model (3DMM) to multiple 2D images of a subject. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.\"","summary":"\"@cite_6 proposed to represent @math D face shapes by @math D volumetric coordinates, and train a CNN to directly regress the coordinates from the input @math D face image. Considering the high dimensionality of original @math D face point clouds, as a compromise, they employed @math D volumetric representations. In consequence, the @math D face shapes generated by their method are of low resolution, which are apparently not favorable for face recognition.\"","":""}
{"id":"2964051375","dialogue":"\"Conditional probabilistic graphical models provide a powerful framework for structured regression in spatio-temporal datasets with complex correlation patterns. However, in real-life applications a large fraction of observations is often missing, which can severely limit the representational power of these models. In this paper we propose a Marginalized Gaussian Conditional Random Fields (m-GCRF) structured regression model for dealing with missing labels in partially observed temporal attributed graphs. This method is aimed at learning with both labeled and unlabeled parts and effectively predicting future values in a graph. The method is even capable of learning from nodes for which the response variable is never observed in history, which poses problems for many state-of-the-art models that can handle missing data. The proposed model is characterized for various missingness mechanisms on 500 synthetic graphs. The benefits of the new method are also demonstrated on a challenging application for predicting precipitation based on partial observations of climate variables in a temporal graph that spans the entire continental US. We also show that the method can be useful for optimizing the costs of data collection in climate applications via active reduction of the number of weather stations to consider. In experiments on these real-world and synthetic datasets we show that the proposed model is consistently more accurate than alternative semi-supervised structured models, as well as models that either use imputation to deal with missing values or simply ignore them altogether.\"","summary":"\"One of the standard ways of handling missing values is imputing values based on some predictive model, and then applying the analysis on a fully observed dataset. To exploit the graph structure, previous studies have proposed imputation of missing values based on the exponential random graph model @cite_11 . The limitation of such an approach is that it is slow, as it requires Gibbs sampling, and so it cannot handle large graphs. Imputation of missing values can also be accomplished using matrix (or tensor) factorization methods. These methods can impute missing values with high accuracy even when large percentages (up to 95 , imputation-based methods use only point estimates of the missing values, effectively ignoring the prediction uncertainty when learning with imputed values. Techniques known as Multiple Imputation (MI) try to correct for this drawback, by sampling from the posterior distribution of missing values. On the other hand, these techniques can be less effective when a larger fraction of data is missing @cite_4 , and can be computationally very demanding.\"","":""}
{"id":"2964051375","dialogue":"\"Conditional probabilistic graphical models provide a powerful framework for structured regression in spatio-temporal datasets with complex correlation patterns. However, in real-life applications a large fraction of observations is often missing, which can severely limit the representational power of these models. In this paper we propose a Marginalized Gaussian Conditional Random Fields (m-GCRF) structured regression model for dealing with missing labels in partially observed temporal attributed graphs. This method is aimed at learning with both labeled and unlabeled parts and effectively predicting future values in a graph. The method is even capable of learning from nodes for which the response variable is never observed in history, which poses problems for many state-of-the-art models that can handle missing data. The proposed model is characterized for various missingness mechanisms on 500 synthetic graphs. The benefits of the new method are also demonstrated on a challenging application for predicting precipitation based on partial observations of climate variables in a temporal graph that spans the entire continental US. We also show that the method can be useful for optimizing the costs of data collection in climate applications via active reduction of the number of weather stations to consider. In experiments on these real-world and synthetic datasets we show that the proposed model is consistently more accurate than alternative semi-supervised structured models, as well as models that either use imputation to deal with missing values or simply ignore them altogether.\"","summary":"\"There also exist variants of the conditional graphical CRF models for regression (e.g. the CCRF @cite_19 , or GCRF @cite_18 models). However, these structured regression models are not designed to cope with unlabeled data, other than ignoring the portion of data with missing labels.\"","":""}
{"id":"2794616326","dialogue":"\"Inferring walls configuration of indoor environment could help robot \"\"understand\"\" the environment better. This allows the robot to execute a task that involves inter-room navigation","summary":"such as picking an object in the kitchen. In this paper","":""}
{"id":"2962949233","dialogue":"\"Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based on these observations, we propose a motion-appearance co-memory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.\"","summary":"\"Video QA is a relatively new task compared with image QA. Yu al @cite_19 adopted a semantic attention mechanism, which combines the detected concepts in videos with text encoding decoding to generate answers. Comparing with images, temporal domain is unique to videos. A temporal attention mechanism is leveraged to selectively attend to one or more periods of a video in @cite_33 @cite_7 @cite_48 . Besides temporal attention mechanism, Jang al @cite_33 and Xu al @cite_11 also utilized motion information along with appearance information in videos. Recently Na al @cite_0 and Kim al @cite_37 both introduced the memory mechanism to their models for video QA. However, their models @cite_0 @cite_35 both lack motion analysis and dynamic memory update mechanism.\"","":""}
{"id":"2962949233","dialogue":"\"Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based on these observations, we propose a motion-appearance co-memory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.\"","summary":"\"To answer the video-based questions correctly, temporal analysis of videos is necessary. Shou al @cite_2 presented a multi-stage Segment-CNN model to generate action proposals and localize actions in videos. Temporal Unit Regression Network (TURN) @cite_32 and Cascaded Boundary Regression (CBR) @cite_14 exploit the temporal boundary regression mechanism for proposal generation and action detection. Recently Gao al @cite_22 and Hendricks al @cite_46 proposed to localize activities by language queries, their methods involve of joint modeling of the videos and language queries, which also related to video QA.\"","":""}
{"id":"2890887549","dialogue":"\"Popular deep neural networks (DNNs) spend the majority of their execution time computing convolutions. The Winograd family of algorithms can greatly reduce the number of arithmetic operations required and is present in many DNN software frameworks. However","summary":"the performance gain is at the expense of a reduction in floating point (FP) numerical accuracy. In this paper","":""}
{"id":"2795079431","dialogue":"\"We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model","summary":"a novel and powerful method using facenet's Euclidean latent space to understand the images. As the name suggests","":""}
{"id":"2795079431","dialogue":"\"We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model","summary":"a novel and powerful method using facenet's Euclidean latent space to understand the images. As the name suggests","":""}
{"id":"2795079431","dialogue":"\"We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model","summary":"a novel and powerful method using facenet's Euclidean latent space to understand the images. As the name suggests","":""}
{"id":"2795262365","dialogue":"\"We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.\"","summary":"\"Image-based human pose estimation has many applications, for a comprehensive survey, see @cite_10 . Early approaches such as the histogram of oriented gradients (HOG) and deformable parts model (DPM) rely on hand-craft features and graphical models @cite_2 @cite_14 @cite_16 @cite_26 @cite_15 @cite_5 . These methods suffer from the limited representation capabilities and are not extensible to complex scenarios.\"","":""}
{"id":"2795262365","dialogue":"\"We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.\"","summary":"\"Recently, Wei @cite_27 used very deep sequential conv-deconv architecture with large receptive fields to directly perform pose matching on the heatmaps. They also enforced intermediate supervision between conv-deconv pairs to prevent gradient vanish, thus a very deep network became feasible, and the deeper network can learn the keypoints relationship with lager receptive field. The hourglass module proposed by Newell @cite_24 is an extension of Wei with the addition of residual connections between the conv-deconv sub-modules. The hourglass module can effectively capture and combine features across scales. Chu @cite_4 adopted stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. Yang @cite_7 designed a pyramid residual module (PRM) to enhance the deep CNN invariance across scales, by learning the convolutional filters on various feature scales.\"","":""}
{"id":"2795262365","dialogue":"\"We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.\"","summary":"\"State-of-the-art DNNs for pose estimation are still limited in the capability of modeling human body structural for effective keypoint matching. Existing methods rely on a brute-force approach by increasing network depth to implicitly enrich the keypoint relationship modeling capability. A major weakness in this regard is the ambiguities arising from the occlusions, clutter backgrounds, or multiple body parts in the scene. In the MPII pose benchmark @cite_11 , many methods @cite_22 @cite_27 @cite_24 @cite_4 @cite_7 rely on repeating their pose estimation pipeline multiple times in various scales, in order to improve performance by a small margin using averaging of results. This indicates the lack of an effective solution to handle scale and structural priors in the modeling.\"","":""}
{"id":"2795247881","dialogue":"\"Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase decrease the learning rate momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.\"","summary":"\"This work builds on earlier work by the author. In particular, cyclical learning rates were introduced by @cite_8 and later updated in @cite_1 . Section provides updated experiments on super-convergence . There is a discussion in the literature on modifying the batch size instead of the learning rate, such as discussed in @cite_10 .\"","":""}
{"id":"2795247881","dialogue":"\"Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase decrease the learning rate momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.\"","summary":"\"Several recent papers discuss the use of large learning rate and small batch size, such as @cite_9 @cite_0 @cite_2 . They demonstrate that the ratio of the learning rate over the batch size guides training. The recommendations in this report differs from those papers on the optimal setting of learning rates and batch sizes.\"","":""}
{"id":"2795247881","dialogue":"\"Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase decrease the learning rate momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.\"","summary":"There also exist approaches to learn optimal hyper-parameters by differentiating the gradient with respect to the hyper-parameters (for example see @cite_4 ). The approach in this report is simpler for the practitioner to perform. @PARASPLIT","":""}
{"id":"2794515184","dialogue":"\"Computer-generated graphics (CGs) are images generated by computer software. The rapid development of computer graphics technologies has made it easier to generate photorealistic computer graphics, and these graphics are quite difficult to distinguish from natural images (NIs) with the naked eye. In this paper, we propose a method based on sensor pattern noise (SPN) and deep learning to distinguish CGs from NIs. Before being fed into our convolutional neural network (CNN)-based model, these images—CGs and NIs—are clipped into image patches. Furthermore, three high-pass filters (HPFs) are used to remove low-frequency signals, which represent the image content. These filters are also used to reveal the residual signal as well as SPN introduced by the digital camera device. Different from the traditional methods of distinguishing CGs from NIs, the proposed method utilizes a five-layer CNN to classify the input image patches. Based on the classification results of the image patches, we deploy a majority vote scheme to obtain the classification results for the full-size images. The experiments have demonstrated that (1) the proposed method with three HPFs can achieve better results than that with only one HPF or no HPF and that (2) the proposed method with three HPFs achieves 100 accuracy, although the NIs undergo a JPEG compression with a quality factor of 75.\"","summary":"Gando et al @cite_18 presented a deep learning method based on a fine-tuned deep convolutional neural network. This method can automatically distinguish illustrations from photographs and achieve 96.8 Rahmouni et al @cite_20 presented a custom pooling layer to extract statistical features and a CNN framework to distinguish computer-generated graphics from real photographic images. A weighted voting scheme was used to aggregate the local estimates of class probabilities and predict the label of the whole picture. The best accuracy in @cite_20 is 93.2","":""}
{"id":"2794515184","dialogue":"\"Computer-generated graphics (CGs) are images generated by computer software. The rapid development of computer graphics technologies has made it easier to generate photorealistic computer graphics, and these graphics are quite difficult to distinguish from natural images (NIs) with the naked eye. In this paper, we propose a method based on sensor pattern noise (SPN) and deep learning to distinguish CGs from NIs. Before being fed into our convolutional neural network (CNN)-based model, these images—CGs and NIs—are clipped into image patches. Furthermore, three high-pass filters (HPFs) are used to remove low-frequency signals, which represent the image content. These filters are also used to reveal the residual signal as well as SPN introduced by the digital camera device. Different from the traditional methods of distinguishing CGs from NIs, the proposed method utilizes a five-layer CNN to classify the input image patches. Based on the classification results of the image patches, we deploy a majority vote scheme to obtain the classification results for the full-size images. The experiments have demonstrated that (1) the proposed method with three HPFs can achieve better results than that with only one HPF or no HPF and that (2) the proposed method with three HPFs achieves 100 accuracy, although the NIs undergo a JPEG compression with a quality factor of 75.\"","summary":"\"Different digital cameras introduce different noise to their output digital images. The main noise sources are due to the imperfection of CCD or CMOS sensors. It has been named as sensor pattern noise (SPN) and is used as a fingerprint to characterize an individual camera. In particular, SPN has been used in image forgery detection @cite_6 and source camera identification @cite_2 .\"","":""}
{"id":"2794515184","dialogue":"\"Computer-generated graphics (CGs) are images generated by computer software. The rapid development of computer graphics technologies has made it easier to generate photorealistic computer graphics, and these graphics are quite difficult to distinguish from natural images (NIs) with the naked eye. In this paper, we propose a method based on sensor pattern noise (SPN) and deep learning to distinguish CGs from NIs. Before being fed into our convolutional neural network (CNN)-based model, these images—CGs and NIs—are clipped into image patches. Furthermore, three high-pass filters (HPFs) are used to remove low-frequency signals, which represent the image content. These filters are also used to reveal the residual signal as well as SPN introduced by the digital camera device. Different from the traditional methods of distinguishing CGs from NIs, the proposed method utilizes a five-layer CNN to classify the input image patches. Based on the classification results of the image patches, we deploy a majority vote scheme to obtain the classification results for the full-size images. The experiments have demonstrated that (1) the proposed method with three HPFs can achieve better results than that with only one HPF or no HPF and that (2) the proposed method with three HPFs achieves 100 accuracy, although the NIs undergo a JPEG compression with a quality factor of 75.\"","summary":"\"Villalba et al @cite_24 presented a method for video source acquisition identification based on sensor noise extraction from video key frames. Photo response non-uniformity (PRNU) is the primary part of the sensor pattern noise in an image. @cite_24 , the PRNU is used to calculate the sensor pattern noise and characterize the fingerprints into feature vectors. Then, the feature vectors are extracted from the video key frames and trained by a SVM-based classifier.\"","":""}
{"id":"2950759545","dialogue":"\"Network embedding represents nodes in a continuous vector space and preserves structure information from the Network. Existing methods usually adopt a \"\"one-size-fits-all\"\" approach when concerning multi-scale structure information","summary":"such as first- and second-order proximity of nodes","":""}
{"id":"2793326011","dialogue":"\"Research on reinforcement learning has demonstrated promising results in manifold applications and domains. Still, efficiently learning effective robot behaviors is very difficult, due to unstructured scenarios, high uncertainties, and large state dimensionality (e.g. multi-agent systems or hyper-redundant robots). To alleviate this problem, we present DOP, a deep model-based reinforcement learning algorithm, which exploits action values to both (1) guide the exploration of the state space and (2) plan effective policies. Specifically, we exploit deep neural networks to learn Q-functions that are used to attack the curse of dimensionality during a Monte-Carlo tree search. Our algorithm, in fact, constructs upper confidence bounds on the learned value function to select actions optimistically. We implement and evaluate DOP on different scenarios: (1) a cooperative navigation problem, (2) a fetching task for a 7-DOF KUKA robot, and (3) a human-robot handover with a humanoid robot (both in simulation and real). The obtained results show the effectiveness of DOP in the chosen applications, where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance.\"","summary":"\"Recent trends in reinforcement learning have shown improved generalization capabilities, by exploiting deep learning techniques. For example, @cite_12 use a deep @math -network to learn directly from high-dimensional visual sensory inputs on Atari 2600 games. @cite_16 @cite_17 use deep value networks and policy networks to respectively evaluate board positions and select moves to achieve superhuman performance in Go. @cite_1 , instead, the authors execute multiple agents in parallel, on several instances of the same environment, to learn a variety of tasks using actor-critic with asynchronous gradient descent. Similar advancements have been shown in the robotics domain. For instance, @cite_13 represent policies through deep convolutional neural networks, and train them using a partially observed guided policy search method on real-world manipulation tasks. Moreover, @cite_2 use deep learning in simulation, and propose progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. Unfortunately, planning and learning with deep networks is computationally demanding (i.e., requires a huge number of heavy simulations). For this reason, @cite_10 introduce I2As to exploit outcome of virtual policy executions, in the Sokoban domain, learned with a deep network.\"","":""}
{"id":"2793326011","dialogue":"\"Research on reinforcement learning has demonstrated promising results in manifold applications and domains. Still, efficiently learning effective robot behaviors is very difficult, due to unstructured scenarios, high uncertainties, and large state dimensionality (e.g. multi-agent systems or hyper-redundant robots). To alleviate this problem, we present DOP, a deep model-based reinforcement learning algorithm, which exploits action values to both (1) guide the exploration of the state space and (2) plan effective policies. Specifically, we exploit deep neural networks to learn Q-functions that are used to attack the curse of dimensionality during a Monte-Carlo tree search. Our algorithm, in fact, constructs upper confidence bounds on the learned value function to select actions optimistically. We implement and evaluate DOP on different scenarios: (1) a cooperative navigation problem, (2) a fetching task for a 7-DOF KUKA robot, and (3) a human-robot handover with a humanoid robot (both in simulation and real). The obtained results show the effectiveness of DOP in the chosen applications, where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance.\"","summary":"\"In this paper, we address generalization at learning time by introducing , an iterative algorithm for policy generation that makes use of deep @math -networks to drive a focused exploration. relies on previous work @cite_21 and extends it with a different representation to obtain improved generalization capabilities over higher dimensional problems. In particular, we approximate the @math function using @math -learning with a deep convolutional neural network. Similar to @math -CP @cite_21 and TD-search @cite_6 , we aim at reducing the variance of value estimates during the search procedure by means of temporal difference learning. However, as in @cite_21 , extends TD-search by constructing upper confidence bounds on the value function, and by selecting optimistically with respect to those, instead of performing @math -greedy exploration. Thanks to the generalization capabilities of deep networks, improves over @math -CP both in terms of policy and exploration. Our representation, in fact, not only enables the algorithm to explore more informative portions of the search space @cite_20 @cite_11 , but also is able to generalize better among them with positive effects on the overall policy and search space expansion.\"","":""}
{"id":"2793025567","dialogue":"\"Recent years have witnessed many exciting achievements for object detection using deep learning techniques. Despite achieving significant progresses, most existing detectors are designed to detect objects with relatively low-quality prediction of locations, i.e., often trained with the threshold of Intersection over Union (IoU) set to 0.5 by default, which can yield low-quality or even noisy detections. It remains an open challenge for how to devise and train a high-quality detector that can achieve more precise localization (i.e., IoU @math 0.5) without sacrificing the detection performance. In this paper, we propose a novel single-shot detection framework of Bidirectional Pyramid Networks (BPN) towards high-quality object detection, which consists of two novel components: (i) a Bidirectional Feature Pyramid structure for more effective and robust feature representations; and (ii) a Cascade Anchor Refinement to gradually refine the quality of predesigned anchors for more effective training. Our experiments showed that the proposed BPN achieves the best performances among all the single-stage object detectors on both PASCAL VOC and MS COCO datasets, especially for high-quality detections.\"","summary":"\"In literature, most object detection studies were focused on the detection with relatively low localization quality, with a default IoU threshold of 0.5. There are only a few related studies for high-quality detection. LocNet @cite_1 learns a postprocessing network for location refinement, which however does not optimize the whole system end-to-end and is not designed for high-quality detection tasks. MultiPath Network @cite_4 proposed to learn multiple detection branches for different quality thresholds. However, it still suffers insufficient training samples and it is computationally slow due to the nature of two-stage detectors. Cascaded RCNN @cite_15 learns regressors in a cascaded way, which gradually increases qualified proposal numbers towards high-quality detection. However, it is still based on two-stage RCNN and its slow inference speed is a critical drawback, especially the feature re-extraction step is operated by time-consuming ROI Pooling or ROI Warping.\"","":""}
{"id":"2793025567","dialogue":"\"Recent years have witnessed many exciting achievements for object detection using deep learning techniques. Despite achieving significant progresses, most existing detectors are designed to detect objects with relatively low-quality prediction of locations, i.e., often trained with the threshold of Intersection over Union (IoU) set to 0.5 by default, which can yield low-quality or even noisy detections. It remains an open challenge for how to devise and train a high-quality detector that can achieve more precise localization (i.e., IoU @math 0.5) without sacrificing the detection performance. In this paper, we propose a novel single-shot detection framework of Bidirectional Pyramid Networks (BPN) towards high-quality object detection, which consists of two novel components: (i) a Bidirectional Feature Pyramid structure for more effective and robust feature representations; and (ii) a Cascade Anchor Refinement to gradually refine the quality of predesigned anchors for more effective training. Our experiments showed that the proposed BPN achieves the best performances among all the single-stage object detectors on both PASCAL VOC and MS COCO datasets, especially for high-quality detections.\"","summary":"\"Our work is also related to the studies for multi-scale feature fusion, which has been proved to be an effective and important structure for object detection with different scales. ION @cite_19 extracts region features from different layers by ROI Pooling operation; HyperNet @cite_8 directly concatenates features at different layers using deconvolution layers. FPN @cite_21 and DSSD @cite_17 fuses features of different scales with lateral connection in a bottom-up manneer, which effectively improve the detection of small objects. However, the vanilla feature pyramid @cite_21 only considers boosting shallow layer features with deep layer features, but ignores the fact that the instance information in shallow layer features can be helpful to deep semantic layer features. We overcome this limitation by the proposed Bidirectional Feature Pyramid structure.\"","":""}
{"id":"2792650790","dialogue":"In this note we study pseudo-multipliers associated to the harmonic oscillator (also called Hermite multipliers) belonging to Schatten classes on @math . We also investigate the spectral trace of these operators.","summary":"\"Now, we include some references on the subject. Sufficient conditions for the @math -nuclearity of spectral multipliers associated to the harmonic oscillator, but, in modulation spaces and Wiener amalgam spaces have been considered by J. Delgado, M. Ruzhansky and B. Wang in @cite_1 @cite_14 . The Properties of these multipliers in @math -spaces have been investigated in the references S. Bagchi, S. Thangavelu @cite_7 , J. Epperson @cite_2 , K. Stempak and J.L. Torrea @cite_3 @cite_16 @cite_11 , S. Thangavelu @cite_18 @cite_5 and references therein. Hermite expansions for distributions can be found in B. Simon @cite_21 . The @math -nuclearity and Grothendieck-Lidskii formulae for multipliers and other types of integral operators can be found in @cite_6 @cite_14 . On Hilbert spaces the class of @math -nuclear operators agrees with the Schatten-von Neumann class @math in this context operators with integral kernel on Lebesgue spaces and, in particular, operators with kernel acting of a special way with anharmonic oscillators of the form @math @math has been considered on Schatten classes on @math in J. Delgado and M. Ruzhansky @cite_23 . The proof of our results will be presented in the next section.\"","":""}
{"id":"2963810623","dialogue":"\"As applications grow in capability, they also grow in complexity. This complexity in turn gets pushed into modules and libraries. In addition, hardware configurations become increasingly elaborate, too. These two trends make understanding, debugging and analyzing the performance of applications more and more difficult.\"","summary":"\"Wrapping C C++ libraries is not new. SWIG @cite_0 , first released in 1996, generates wrappers for C C++ libraries so they can be called from other languages like Python, Go and Lua. SWIG does not provide library interception for extracting, e.g., performance data. Furthermore, it is not possible to create C++ or C wrappers for C C++ libraries. SWIG uses its own C C++ preprocessor and parser.\"","":""}
{"id":"2963810623","dialogue":"\"As applications grow in capability, they also grow in complexity. This complexity in turn gets pushed into modules and libraries. In addition, hardware configurations become increasingly elaborate, too. These two trends make understanding, debugging and analyzing the performance of applications more and more difficult.\"","summary":"One possible application for tools interfaces and library wrappers is to check for correct API usage. For example MUST @cite_2 uses MPI's profiling interface to ensure correct use and to detect possible deadlocks. The wrapping code is generated manually with a simple proprietary wrapper generator.","":""}
{"id":"2793803704","dialogue":"\"Infrastructure as a service clouds hide the complexity of maintaining the physical infrastructure with a slight disadvantage: they also hide their internal working details. Should users need knowledge about these details e.g., to increase the reliability or performance of their applications, they would need solutions to detect behavioural changes in the underlying system. Existing runtime solutions for such purposes offer limited capabilities as they are mostly restricted to revealing weekly or yearly behavioural periodicity in the infrastructure. This article proposes a technique for predicting generic background workload by means of simulations that are capable of providing additional knowledge of the underlying private cloud systems in order to support activities like cloud orchestration or workflow enactment. Our technique uses long-running scientific workflows and their behaviour discrepancies and tries to replicate these in a simulated cloud with known (trace-based) workloads. We argue that the better we can mimic the current discrepancies the better we can tell expected workloads in the near future on the real life cloud. We evaluated the proposed prediction approach with a biochemical application on both real and simulated cloud infrastructures. The proposed algorithm has shown to produce significantly ( 20 ) better workload predictions for the future of simulated clouds than random workload selection.\"","summary":"\"Concerning workload modelling, @cite_0 used data traces obtained from a data centre to characterise and predict workload on VMs. Their goal was to explore cross-VM workload correlations, and predict workload changes due to dependencies among applications running in different VMs -- while we approach the load prediction from the workflow enactment point of view.\"","":""}
{"id":"2793803704","dialogue":"\"Infrastructure as a service clouds hide the complexity of maintaining the physical infrastructure with a slight disadvantage: they also hide their internal working details. Should users need knowledge about these details e.g., to increase the reliability or performance of their applications, they would need solutions to detect behavioural changes in the underlying system. Existing runtime solutions for such purposes offer limited capabilities as they are mostly restricted to revealing weekly or yearly behavioural periodicity in the infrastructure. This article proposes a technique for predicting generic background workload by means of simulations that are capable of providing additional knowledge of the underlying private cloud systems in order to support activities like cloud orchestration or workflow enactment. Our technique uses long-running scientific workflows and their behaviour discrepancies and tries to replicate these in a simulated cloud with known (trace-based) workloads. We argue that the better we can mimic the current discrepancies the better we can tell expected workloads in the near future on the real life cloud. We evaluated the proposed prediction approach with a biochemical application on both real and simulated cloud infrastructures. The proposed algorithm has shown to produce significantly ( 20 ) better workload predictions for the future of simulated clouds than random workload selection.\"","summary":"\"@cite_9 developed CloudProphet to predict legacy application performance in clouds. This tool is able to trace the workload of an application running locally, and to replay the same workload in the cloud for further investigations and prediction. In contrast, our work presents a technique to identify load characteristics independent from the workflow ran on cloud resources.\"","":""}
{"id":"2793803704","dialogue":"\"Infrastructure as a service clouds hide the complexity of maintaining the physical infrastructure with a slight disadvantage: they also hide their internal working details. Should users need knowledge about these details e.g., to increase the reliability or performance of their applications, they would need solutions to detect behavioural changes in the underlying system. Existing runtime solutions for such purposes offer limited capabilities as they are mostly restricted to revealing weekly or yearly behavioural periodicity in the infrastructure. This article proposes a technique for predicting generic background workload by means of simulations that are capable of providing additional knowledge of the underlying private cloud systems in order to support activities like cloud orchestration or workflow enactment. Our technique uses long-running scientific workflows and their behaviour discrepancies and tries to replicate these in a simulated cloud with known (trace-based) workloads. We argue that the better we can mimic the current discrepancies the better we can tell expected workloads in the near future on the real life cloud. We evaluated the proposed prediction approach with a biochemical application on both real and simulated cloud infrastructures. The proposed algorithm has shown to produce significantly ( 20 ) better workload predictions for the future of simulated clouds than random workload selection.\"","summary":"\"@cite_11 also identified performance uncertainties of multi-tenant virtual machine instances over time in Cloud environments. They proposed a model called R-MOHEFT that considered uncertainty intervals for workflow activity processing times. They developed a three-objective (i.e., makespan, monetary cost, and robustness) optimisation for Cloud scheduling in a commercial setting. In contrast to this approach our goal is to identify patterns in earlier workloads to overcome the uncertainty, and apply simulations to predict future background load of the infrastructure.\"","":""}
{"id":"2793803704","dialogue":"\"Infrastructure as a service clouds hide the complexity of maintaining the physical infrastructure with a slight disadvantage: they also hide their internal working details. Should users need knowledge about these details e.g., to increase the reliability or performance of their applications, they would need solutions to detect behavioural changes in the underlying system. Existing runtime solutions for such purposes offer limited capabilities as they are mostly restricted to revealing weekly or yearly behavioural periodicity in the infrastructure. This article proposes a technique for predicting generic background workload by means of simulations that are capable of providing additional knowledge of the underlying private cloud systems in order to support activities like cloud orchestration or workflow enactment. Our technique uses long-running scientific workflows and their behaviour discrepancies and tries to replicate these in a simulated cloud with known (trace-based) workloads. We argue that the better we can mimic the current discrepancies the better we can tell expected workloads in the near future on the real life cloud. We evaluated the proposed prediction approach with a biochemical application on both real and simulated cloud infrastructures. The proposed algorithm has shown to produce significantly ( 20 ) better workload predictions for the future of simulated clouds than random workload selection.\"","summary":"\"@cite_15 offers cloud workload prediction based on autoregressive integrated moving average. They argue that proactive dynamic provisioning of resources could achieve good quality of service. Their model's accuracy is evaluated by predicting future workloads of real request traces to web servers. Additionally, @cite_3 developed a workload model for the CloudSim simulator using generalised extreme value lambda distributions. This model captures user behavioural patterns and supports the simulation of resource utilisation in clouds. They argue that user behaviour must be considered in workload modelling to reflect realistic conditions. Our approach share this view: we apply a runtime behaviour analysis to find a workflow enactment plan that best matches the infrastructure load including user activities.\"","":""}
{"id":"2793803704","dialogue":"\"Infrastructure as a service clouds hide the complexity of maintaining the physical infrastructure with a slight disadvantage: they also hide their internal working details. Should users need knowledge about these details e.g., to increase the reliability or performance of their applications, they would need solutions to detect behavioural changes in the underlying system. Existing runtime solutions for such purposes offer limited capabilities as they are mostly restricted to revealing weekly or yearly behavioural periodicity in the infrastructure. This article proposes a technique for predicting generic background workload by means of simulations that are capable of providing additional knowledge of the underlying private cloud systems in order to support activities like cloud orchestration or workflow enactment. Our technique uses long-running scientific workflows and their behaviour discrepancies and tries to replicate these in a simulated cloud with known (trace-based) workloads. We argue that the better we can mimic the current discrepancies the better we can tell expected workloads in the near future on the real life cloud. We evaluated the proposed prediction approach with a biochemical application on both real and simulated cloud infrastructures. The proposed algorithm has shown to produce significantly ( 20 ) better workload predictions for the future of simulated clouds than random workload selection.\"","summary":"\"@cite_4 used workload prediction based on identifying similar past occurrences of the current short-term workload history for efficient resource scaling. This approach is the closest to ours (albeit, we have a different focus support for on-line decision making in scientific workflow enactors etc.), as it uses real-world traces from clouds and grids. They examine historic data to identify similar usage patterns to a current window of records, and their algorithm predicts the system usage by extrapolating beyond the identified patterns. In contrast, our work's specific focus on scientific workflows allows the analysis and prediction of recently observed execution time discrepancies, by introducing simulations to the prediction and validation phases.\"","":""}
{"id":"2793308753","dialogue":"\"The positive-unlabeled (PU) classification is a common scenario in real-world applications such as healthcare","summary":"text classification","":""}
{"id":"2792862287","dialogue":"\"The advent of SDN has brought a plethora of new architectures and controller designs for many use-cases and scenarios. Existing SDN deployments focus on campus, datacenter and WAN networks. However, little research efforts have been devoted to the scenario of effectively controlling a full deployment of end-nodes (e.g. smartphones) that are transient and scattered across the Internet. In this paper, we present a rigorous analysis of the challenges associated with an SDN architecture for end-nodes, show that such challenges are not found in existing SDN scenarios, and provide practical design guidelines to address them. Then, and following these guidelines we present a reference architecture based on a decentralized, distributed and symmetric controller with a connectionless pull-oriented southbound and an intent-driven northbound. Finally, we measure a proof-of-concept deployment to assess the validity of the analysis as well as the architecture.\"","summary":"\"There have been other works that discussed the interactions of end-nodes and SDN. Nevertheless, and to the best of our knowledge, this paper is the first ever to carefully explore all implications of a full SDN deployment of end-nodes. For instance, the authors of @cite_14 enable SDN within a mobile device to aggregate all its available interfaces. However, and contrary to our work, they do not consider that end-nodes can be transient, scattered, with low traffic locality and very numerous. Similarly, the authors of @cite_7 extend OpenFlow to bring SDN to end-nodes and complement existing approaches in the field of Software-defined Radio. They -intentionally- kept out the scope the complete architecture to support SDN-aware end-nodes. Similarly, meSDN @cite_6 proposes a mobile extension for SDN to optimize wireless channel transmission on an existing SDN network. However, it does not consider devices connecting to legacy -non SDN- networks or transient devices that roam frequently.\"","":""}
{"id":"2794091644","dialogue":"\"Meaningful facial parts can convey key cues for both facial action unit detection and expression prediction. Textured 3D face scan can provide both detailed 3D geometric shape and 2D texture appearance cues of the face which are beneficial for Facial Expression Recognition (FER). However, accurate facial parts extraction as well as their fusion are challenging tasks. In this paper, a novel system for 3D FER is designed based on accurate facial parts extraction and deep feature fusion of facial parts. In particular, each textured 3D face scan is firstly represented as a 2D texture map and a depth map with one-to-one dense correspondence. Then, the facial parts of both texture map and depth map are extracted using a novel 4-stage process consists of facial landmark localization, facial rotation correction, facial resizing, facial parts bounding box extraction and post-processing procedures. Finally, deep fusion Convolutional Neural Networks (CNNs) features of all facial parts are learned from both texture maps and depth maps, respectively and nonlinear SVMs are used for expression prediction. Experiments are conducted on the BU-3DFE database, demonstrating the effectiveness of combing different facial parts, texture and depth cues and reporting the state-of-the-art results in comparison with all existing methods under the same setting.\"","summary":"\"Having small patches as proposed by @cite_15 lacks the ability to automatically localize and determine what part is in effect for an expression. It can also have trouble distinguishing patches when applied on faces of different gender and ethnicity. Having a learned detector for the face and parts as proposed by @cite_32 cannot guarantee the robustness as much as face detection, alignment and localization techniques do, especially when there are peculiar samples provided. Both proposals used the JAFFE and CK+ database, which are not very diverse when it comes to their subjects ethnicities. In their experiments, they adopt a cross-validation approach, in which they do not ensure a subject independent protocol. This can result in a high performance which can be inconsistent when a new subject is ever tested.\"","":""}
{"id":"2791374212","dialogue":"\"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community.\"","summary":"\"Two notable exceptions are the and datasets. @cite_2 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Also, the task is not exactly question answering, but identification of document passages containing the answer.\"","":""}
{"id":"2791374212","dialogue":"\"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community.\"","summary":"\"@cite_10 is a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from and . While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.\"","":""}
{"id":"2792210438","dialogue":"\"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However","summary":"it has thus far seen limited application to sequential data","":""}
{"id":"2792210438","dialogue":"\"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However","summary":"it has thus far seen limited application to sequential data","":""}
{"id":"2792210438","dialogue":"\"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However","summary":"it has thus far seen limited application to sequential data","":""}
{"id":"2792210438","dialogue":"\"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However","summary":"it has thus far seen limited application to sequential data","":""}
{"id":"2794297345","dialogue":"\"We consider ordered pairs @math where @math is a finite set of size @math and @math is some collection of @math -element subsets of @math such that every @math -element subset of @math is contained in exactly @math \"\"blocks\"\" @math for some fixed @math . We represent each block @math by a zero-one vector @math of length @math and explore the ideal @math of polynomials in @math variables with complex coefficients which vanish on the set @math . After setting up the basic theory","summary":"we investigate two parameters related to this ideal: @math is the smallest degree of a non-trivial polynomial in the ideal @math and @math is the smallest integer @math such that @math is generated by a set of polynomials of degree at most @math . We first prove the general bounds @math . Examining important families of examples","":""}
{"id":"2790469074","dialogue":"\"We consider the problem of near-optimal arm identification in the fixed confidence setting of the infinitely armed bandit problem when nothing is known about the arm reservoir distribution. We (1) introduce a PAC-like framework within which to derive and cast results; (2) derive a sample complexity lower bound for near-optimal arm identification; (3) propose an algorithm that identifies a nearly-optimal arm with high probability and derive an upper bound on its sample complexity which is within a log factor of our lower bound; and (4) discuss whether our log^2(1 delta) dependence is inescapable for \"\"two-phase\"\" (select arms first","summary":"identify the best later) algorithms in the infinite setting. This work permits the application of bandit models to a broader class of problems where fewer assumptions hold.\"","":""}
{"id":"2963599965","dialogue":"\"The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.\"","summary":"\"AI-based data analytics and deep neural network applications have become increasingly important in recent years. These applications lead to rapid development of software and hardware that efficiently express and support tensor operations, which are fundamental for deep neural network applications. TensorFlow is among the most popular open-source programming framework that uses a computational graph with tensor operations as nodes of the graph @cite_1 . Caffe, Torch and Microsoft CNTK are other popular programming frameworks for developing deep neural networks @cite_26 .\"","":""}
{"id":"2963599965","dialogue":"\"The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.\"","summary":"\"The seminal paper by Krizhevsky @cite_0 has established GPUs as the main workforce in training deep neural networks and triggered a renaissance of deep-learning applications. Besides NVIDIA Tensor Cores @cite_9 discussed in this paper, several companies are also employing and developing specialized hardware for high-performance inference. Microsoft deployed the Catapult system that uses FPGAs @cite_27 . Movidius developed the Myriad 2 Vision Processing Unit @cite_15 . Google designed and developed Tensor Processing Unit (TPU) specifically for inference workloads. The main engine of the TPU is a MAC matrix multiply unit containing 256 @math 256 MACs, each capable of performing 8-bit multiply-and-adds on signed or unsigned integers. In December 2017, Intel announced the release of the Neural Network Processor (NPP) @cite_24 , which implements a new memory architecture for tensor operations. NPP does not have standard caches and data movement is programmable by software. In addition, neuromorphic hardware, such as the IBM TrueNorth @cite_12 and SpiNNaker @cite_21 chips, mimics the functioning of spiking neural network. Although their original design purpose is to simulate the brain, they may also find usage in AI applications.\"","":""}
{"id":"2963973986","dialogue":"\"The Virtual Network Embedding Problem (VNEP) captures the essence of many resource allocation problems of today’s infrastructure providers, which offer their physical computation and networking resources to customers. Customers request resources in the form of Virtual Networks, i.e. as a directed graph which specifies computational requirements at the nodes and communication requirements on the edges. An embedding of a Virtual Network on the shared physical infrastructure is the joint mapping of (virtual) nodes to physical servers together with the mapping of (virtual) edges onto paths in the physical network connecting the respective servers.This work initiates the study of approximation algorithms for the VNEP. Concretely, we study the offline setting with admission control: given multiple request graphs the task is to embed the most profitable subset while not exceeding resource capacities. Our approximation is based on the randomized rounding of Linear Programming (LP) solutions. Interestingly, we uncover that the standard LP formulation exhibits an inherent structural deficit when considering general virtual networks: its solutions cannot be decomposed into valid embeddings. In turn, focusing on the class of cactus request graphs, we devise a novel LP formulation, whose solutions can be decomposed into convex combinations of valid embedding. Proving performance guarantees of our rounding scheme, we obtain the first approximation algorithm for the VNEP in the resource augmentation model.We propose two rounding heuristics and evaluate their performance in an extensive computational study, showing that these consistently yield good solutions (even without augmentations).\"","summary":"\"In the last decade, the VNEP has attracted much attention due to its many applications and the survey @cite_6 from 2013 already lists more than 80 different algorithms for its many variations @cite_6 . The VNEP is known to be @math -hard and inapproximable in general (unless @math ) @cite_18 . Based on the hardness of the VNEP, most works consider heuristics without any performance guarantee @cite_6 @cite_1 . Other works proposed exact methods as integer or constraint programming, coming at the cost of an exponential runtime @cite_20 @cite_2 @cite_13 .\"","":""}
{"id":"2963973986","dialogue":"\"The Virtual Network Embedding Problem (VNEP) captures the essence of many resource allocation problems of today’s infrastructure providers, which offer their physical computation and networking resources to customers. Customers request resources in the form of Virtual Networks, i.e. as a directed graph which specifies computational requirements at the nodes and communication requirements on the edges. An embedding of a Virtual Network on the shared physical infrastructure is the joint mapping of (virtual) nodes to physical servers together with the mapping of (virtual) edges onto paths in the physical network connecting the respective servers.This work initiates the study of approximation algorithms for the VNEP. Concretely, we study the offline setting with admission control: given multiple request graphs the task is to embed the most profitable subset while not exceeding resource capacities. Our approximation is based on the randomized rounding of Linear Programming (LP) solutions. Interestingly, we uncover that the standard LP formulation exhibits an inherent structural deficit when considering general virtual networks: its solutions cannot be decomposed into valid embeddings. In turn, focusing on the class of cactus request graphs, we devise a novel LP formulation, whose solutions can be decomposed into convex combinations of valid embedding. Proving performance guarantees of our rounding scheme, we obtain the first approximation algorithm for the VNEP in the resource augmentation model.We propose two rounding heuristics and evaluate their performance in an extensive computational study, showing that these consistently yield good solutions (even without augmentations).\"","summary":"\"A column generation approach was proposed by in @cite_2 to efficiently compute solutions to the VNEP by generating feasible mappings on-the-fly subject to a specific cost measure. In particular, compute feasible mappings by relying on heuristics and, if no feasible heuristical solution was found, rely on Mixed-Integer Programming to compute a cost optimal feasible embedding in non-polynomial time. We believe that our work can bridge the gap between the heuristic generation of feasible mappings and the optimal generation of feasible mappings as our approach can be used to compute mappings.\"","":""}
{"id":"2963973986","dialogue":"\"The Virtual Network Embedding Problem (VNEP) captures the essence of many resource allocation problems of today’s infrastructure providers, which offer their physical computation and networking resources to customers. Customers request resources in the form of Virtual Networks, i.e. as a directed graph which specifies computational requirements at the nodes and communication requirements on the edges. An embedding of a Virtual Network on the shared physical infrastructure is the joint mapping of (virtual) nodes to physical servers together with the mapping of (virtual) edges onto paths in the physical network connecting the respective servers.This work initiates the study of approximation algorithms for the VNEP. Concretely, we study the offline setting with admission control: given multiple request graphs the task is to embed the most profitable subset while not exceeding resource capacities. Our approximation is based on the randomized rounding of Linear Programming (LP) solutions. Interestingly, we uncover that the standard LP formulation exhibits an inherent structural deficit when considering general virtual networks: its solutions cannot be decomposed into valid embeddings. In turn, focusing on the class of cactus request graphs, we devise a novel LP formulation, whose solutions can be decomposed into convex combinations of valid embedding. Proving performance guarantees of our rounding scheme, we obtain the first approximation algorithm for the VNEP in the resource augmentation model.We propose two rounding heuristics and evaluate their performance in an extensive computational study, showing that these consistently yield good solutions (even without augmentations).\"","summary":"\"Acknowledging the hardness of the general VNEP and the diversity of applications, several subproblems of the VNEP have been studied recently by considering restricted graph classes for the virtual networks and the substrate graph. For example, virtual clusters with uniform demands are studied in @cite_0 @cite_8 , line requests are studied in @cite_9 @cite_7 @cite_15 and tree requests were studied in @cite_7 @cite_11 .\"","":""}
{"id":"2963973986","dialogue":"\"The Virtual Network Embedding Problem (VNEP) captures the essence of many resource allocation problems of today’s infrastructure providers, which offer their physical computation and networking resources to customers. Customers request resources in the form of Virtual Networks, i.e. as a directed graph which specifies computational requirements at the nodes and communication requirements on the edges. An embedding of a Virtual Network on the shared physical infrastructure is the joint mapping of (virtual) nodes to physical servers together with the mapping of (virtual) edges onto paths in the physical network connecting the respective servers.This work initiates the study of approximation algorithms for the VNEP. Concretely, we study the offline setting with admission control: given multiple request graphs the task is to embed the most profitable subset while not exceeding resource capacities. Our approximation is based on the randomized rounding of Linear Programming (LP) solutions. Interestingly, we uncover that the standard LP formulation exhibits an inherent structural deficit when considering general virtual networks: its solutions cannot be decomposed into valid embeddings. In turn, focusing on the class of cactus request graphs, we devise a novel LP formulation, whose solutions can be decomposed into convex combinations of valid embedding. Proving performance guarantees of our rounding scheme, we obtain the first approximation algorithm for the VNEP in the resource augmentation model.We propose two rounding heuristics and evaluate their performance in an extensive computational study, showing that these consistently yield good solutions (even without augmentations).\"","summary":"\"Considering approximation algorithms, employed randomized rounding in @cite_7 to obtain a constant approximation for embedding line requests on arbitrary substrate graphs under strong assumptions on the benefits and the capacities. In their interesting work, @cite_11 give an @math time @math -approximation algorithm for minimizing the load of embedding @math -depth trees based on a @math -node substrate. Their result is based on a strong LP relaxation inspired by the Sherali-Adams hierarchy.\"","":""}
{"id":"2963973986","dialogue":"\"The Virtual Network Embedding Problem (VNEP) captures the essence of many resource allocation problems of today’s infrastructure providers, which offer their physical computation and networking resources to customers. Customers request resources in the form of Virtual Networks, i.e. as a directed graph which specifies computational requirements at the nodes and communication requirements on the edges. An embedding of a Virtual Network on the shared physical infrastructure is the joint mapping of (virtual) nodes to physical servers together with the mapping of (virtual) edges onto paths in the physical network connecting the respective servers.This work initiates the study of approximation algorithms for the VNEP. Concretely, we study the offline setting with admission control: given multiple request graphs the task is to embed the most profitable subset while not exceeding resource capacities. Our approximation is based on the randomized rounding of Linear Programming (LP) solutions. Interestingly, we uncover that the standard LP formulation exhibits an inherent structural deficit when considering general virtual networks: its solutions cannot be decomposed into valid embeddings. In turn, focusing on the class of cactus request graphs, we devise a novel LP formulation, whose solutions can be decomposed into convex combinations of valid embedding. Proving performance guarantees of our rounding scheme, we obtain the first approximation algorithm for the VNEP in the resource augmentation model.We propose two rounding heuristics and evaluate their performance in an extensive computational study, showing that these consistently yield good solutions (even without augmentations).\"","summary":"\"In our preliminary technical report @cite_5 similar results were presented. The current work presents a significantly simpler LP formulation and also provides an extensive computational evaluation. Additionally, in our recent technical report @cite_10 , the approximation approach presented in this work is extended beyond cactus request graphs. However, approximating more general request graphs comes at the price of non-polynomial runtimes.\"","":""}
{"id":"2791544670","dialogue":"\"We propose a Standing Wave Decomposition (SWD) approximation to Gaussian Process regression (GP). GP involves a costly matrix inversion operation, which limits applicability to large data analysis. For an input space that can be approximated by a grid and when correlations among data are short-ranged, the kernel matrix inversion can be replaced by analytic diagonalization using the SWD. We show that this approach applies to uni- and multi-dimensional input data, extends to include longer-range correlations, and the grid can be in a latent space and used as inducing points. Through simulations, we show that our approximate method outperforms existing methods in predictive accuracy per unit time in the regime where data are plentiful. Our SWD-GP is recommended for regression analyses where there is a relatively large amount of data and or there are constraints on computation time.\"","summary":"\"Previous works have also taken advantage of structured data to increase the computational efficiency of GP. For input data that lie on a grid","":""}
{"id":"2791544670","dialogue":"\"We propose a Standing Wave Decomposition (SWD) approximation to Gaussian Process regression (GP). GP involves a costly matrix inversion operation, which limits applicability to large data analysis. For an input space that can be approximated by a grid and when correlations among data are short-ranged, the kernel matrix inversion can be replaced by analytic diagonalization using the SWD. We show that this approach applies to uni- and multi-dimensional input data, extends to include longer-range correlations, and the grid can be in a latent space and used as inducing points. Through simulations, we show that our approximate method outperforms existing methods in predictive accuracy per unit time in the regime where data are plentiful. Our SWD-GP is recommended for regression analyses where there is a relatively large amount of data and or there are constraints on computation time.\"","summary":"\"Variational approaches are another popular method for GP inference. Variational methods turn the inference of the predictive mean and variance into an optimization problem @cite_5 . Variational GP often provides more accurate prediction than FITC @cite_9 and has been made progressively faster through a series of development---from stochastic variational inference @cite_7 , to distributed variational inference @cite_29 , to asynchronous distributed variational inference @cite_12 ---to handle billions of input data. Similar to our work and @cite_24 , @cite_16 uses grid inducing points with stochastic variational inference, which allows added efficiencies in computation via the Kronecker and Khatri-Rao products.\"","":""}
{"id":"2793217730","dialogue":"\"Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.\"","summary":"\"requires learning similarity between visual and language modalities. Karpathy @cite_14 first align sentence fragments and image regions in a subspace, and later apply a bi-directional RNN for multimodal alignment in @cite_15 . Hu @cite_23 employ a 2-layer LSTM to rank proposals based on encoded query and visual features. Rohrbach @cite_0 employ a latent attention network conditioned on query which ranks proposals in weakly supervised scenario. Recently, Plummer @cite_28 augment the CCA model @cite_11 to leverage extensive linguistic cues in the phrases. Chen @cite_25 introduce regression mechanism in phrase grounding to improve proposals' quality. Xiao @cite_38 leverage query's language structural information to guide the learning of phrase grounding model in weakly supervised scenario. Chen @cite_12 apply reinforcement learning techniques to leverage context information. In this paper, we explore consistency in visual and language modalities and leverage complementary knowledge to further boost performance of weakly supervised grounding.\"","":""}
{"id":"2793217730","dialogue":"\"Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.\"","summary":"\"is a method aims at learning a model without heavy manual labeling work. It is widely used in different computer vision tasks. Crandall @cite_33 leverage the class labeling to learn a part-based spatial model without detailed annotation of object location and spatial relationship. Maxime @cite_18 propose to learn the interaction between human and objects purely from action labeling for still images. Recently, Prest @cite_6 apply a deep convolutional neural network and its score maps to address object localization with image level class labels. For phrase grounding task, Rohrbach @cite_0 propose to adopt an attention model which is optimized by learning to reconstruct query's information, and avoids human labeling for object locations for each query in the training set. Based on this, Xiao @cite_38 leverage a continuous attention map and explore detailed structural reconstruction of language modality. Inspired by the success of weakly supervised learning, we propose to apply another visual consistency to further boost performance.\"","":""}
{"id":"2793217730","dialogue":"\"Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.\"","summary":"\"is a technique widely used for tasks in different domains. Hinton @cite_2 propose to compress knowledge learned from one model into another one which is too computationally expensive to train. Inspired by this, Aytar @cite_3 apply visual knowledge to train a sound classification network. Owens @cite_36 use ambient sound information to train an object detection network. Lin . @cite_17 leverage knowledge learned in Visual Question Answering (VQA) task in image retrieval. Zhang @cite_8 apply knowledge learned in image captioning and VQA to train a network detecting visual relation in images. For phrase grounding, we propose to leverage knowledge learned from pre-trained deep neural network to filter out unrelated proposals for visual consistency.\"","":""}
{"id":"2793783553","dialogue":"\"Array codes have been widely employed in storage systems, such as Redundant Arrays of Inexpensive Disks (RAID). The row-diagonal parity (RDP) codes and EVENODD codes are two popular double-parity array codes. As the capacity of hard disks increases, better fault tolerance by using array codes with three or more parity disks is needed. Although many extensions of RDP codes and EVENODD codes have been proposed, the high decoding complexity is the main drawback of them. In this paper, we present a new construction for all families of EVENODD codes and RDP codes, and propose a unified form of them. Under this unified form, RDP codes can be treated as shortened codes of EVENODD codes. Moreover, an efficient decoding algorithm based on an LU factorization of Vandermonde matrix is proposed when the number of continuous surviving parity columns is no less than the number of erased information columns. The new decoding algorithm is faster than the existing algorithms when more than three information columns fail. The proposed efficient decoding algorithm is also applicable to other Vandermonde array codes. Thus the proposed MDS array code is practically very meaningful for storage systems that need higher reliability.\"","summary":"\"There are many follow-up studies on EVENODD codes @cite_4 and RDP codes @cite_19 along different directions, such as the extensions of fault tolerance @cite_21 @cite_1 @cite_16 , the improvement of repair problem @cite_24 @cite_17 @cite_8 @cite_6 and efficient decoding methods @cite_18 @cite_20 @cite_5 @cite_9 of their extensions.\"","":""}
{"id":"2793783553","dialogue":"\"Array codes have been widely employed in storage systems, such as Redundant Arrays of Inexpensive Disks (RAID). The row-diagonal parity (RDP) codes and EVENODD codes are two popular double-parity array codes. As the capacity of hard disks increases, better fault tolerance by using array codes with three or more parity disks is needed. Although many extensions of RDP codes and EVENODD codes have been proposed, the high decoding complexity is the main drawback of them. In this paper, we present a new construction for all families of EVENODD codes and RDP codes, and propose a unified form of them. Under this unified form, RDP codes can be treated as shortened codes of EVENODD codes. Moreover, an efficient decoding algorithm based on an LU factorization of Vandermonde matrix is proposed when the number of continuous surviving parity columns is no less than the number of erased information columns. The new decoding algorithm is faster than the existing algorithms when more than three information columns fail. The proposed efficient decoding algorithm is also applicable to other Vandermonde array codes. Thus the proposed MDS array code is practically very meaningful for storage systems that need higher reliability.\"","summary":"\"Huang and Xu @cite_18 extended the EVENODD codes to be STAR codes with three parity columns. The EVENODD codes were extended by Blaum, Bruck and Vardy @cite_21 @cite_1 for three or more parity columns, with the additional assumption that the multiplicative order of 2 mod @math is equal to @math . A sufficient condition for the extended EVENODD codes to be MDS with more than eight parity columns is given in @cite_26 . Goel and Corbett @cite_16 proposed the RTP codes that extend the RDP codes to tolerate three disk failures. Blaum @cite_2 generalized the RDP codes that can correct more than three column erasures and showed that the extended EVENODD codes and generalized RDP codes share the same MDS property condition. Blaum and Roth @cite_10 proposed Blaum-Roth codes, which are non-systematic MDS array codes constructed over a Vandermonde matrix. Some efficient systematic encoding methods for Blaum-Roth codes are given in @cite_10 @cite_23 @cite_13 . We call the existing MDS array codes in @cite_4 @cite_19 @cite_21 @cite_1 @cite_16 @cite_2 @cite_10 @cite_18 @cite_20 @cite_5 @cite_9 as Vandermonde MDS array codes, as their constructions are based on Vandermonde matrices.\"","":""}
{"id":"2790161686","dialogue":"\"In this work, we present a deep convolutional pyramid person matching network (PPMN) with specially designed Pyramid Matching Module to address the problem of person re-identification. The architecture takes a pair of RGB images as input, and outputs a similiarity value indicating whether the two input images represent the same person or not. Based on deep convolutional neural networks, our approach first learns the discriminative semantic representation with the semantic-component-aware features for persons and then employs the Pyramid Matching Module to match the common semantic-components of persons, which is robust to the variation of spatial scales and misalignment of locations posed by viewpoint changes. The above two processes are jointly optimized via a unified end-to-end deep learning scheme. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art approaches, especially on the rank-1 recognition rate.\"","summary":"\"In the literature, most existing efforts of person Re-ID are mainly carried in two aspects: the discriminative representation learning and the effective matching strategy learning. For image representation, a number of approaches pay attention to designing robust descriptors againist misalignments and variations. Early studies employ hand-crafted features including HSV color histogram @cite_1 , SIFT @cite_10 , LBP @cite_16 features or the combination of them. Recently, several deep convolutional architectures @cite_6 @cite_7 have been proposed for person Re-ID and have shown significant improvements over those with hand-crafted features.\"","":""}
{"id":"2790161686","dialogue":"\"In this work, we present a deep convolutional pyramid person matching network (PPMN) with specially designed Pyramid Matching Module to address the problem of person re-identification. The architecture takes a pair of RGB images as input, and outputs a similiarity value indicating whether the two input images represent the same person or not. Based on deep convolutional neural networks, our approach first learns the discriminative semantic representation with the semantic-component-aware features for persons and then employs the Pyramid Matching Module to match the common semantic-components of persons, which is robust to the variation of spatial scales and misalignment of locations posed by viewpoint changes. The above two processes are jointly optimized via a unified end-to-end deep learning scheme. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art approaches, especially on the rank-1 recognition rate.\"","summary":"\"For matching strategy, the essential idea behind metric learning is to find a mapping function from the feature space to the distance space so as to minimize the intra-personal variance while maximizing the inter-personal margin. Many approaches have been proposed based on this idea including LMNN @cite_14 and KISSME @cite_11 . Recently, some efforts jointly learn the representation and classifier in a unified deep architecture. For example, patch-based methods @cite_2 @cite_6 decompose images into patches and perform patchwise distance measurement to find the spatial relationship. Part-based methods @cite_3 divide one person into equal parts and jointly perform bodywise and partwise correspondence learning since the pedestrians keep upright in general. Different from all the above efforts which focus on feature distance measurement, our proposed method aims at learning the semantic correspondence of semantic-components based on the semantics-aware features and is robust to the variation and misalignment posed by viewpoint changes.\"","":""}
{"id":"2794278393","dialogue":"\"We propose a concept system termed distributed base station (DBS), which enables distributed transmit beamforming at large carrier wavelengths to achieve significant range extension and or increased downlink data rate, providing a low-cost infrastructure for applications such as rural broadband. We consider a frequency division duplexed (FDD) system, using feedback from the receiver to achieve the required phase coherence. At a given range, @math cooperating transmitters can achieve @math -fold increase in received power compared to that for a single transmitters, and feedback-based algorithms with near-ideal performance have been prototyped. In this paper, however, we identify and address key technical issues in translating such power gains into range extension via a DBS. First, to combat the drop in per-node SNR with extended range, we design a feedback-based adaptation strategy that is suitably robust to noise. Second, to utilize available system bandwidth, we extend narrowband adaptation algorithms to wideband channels through interpolation over OFDM subcarriers. Third, we observe that the feedback channel may become a bottleneck unless sophisticated distributed reception strategies are employed, but show that acceptable performance can still be obtained with standard uplink reception if channel time variations are slow enough. We quantify system performance compactly via outage capacity analyses.\"","summary":"\"A simple example of distributed beamforming with aggregated feedback is the one bit feedback algorithm @cite_4 @cite_3 , in which transmitters use small random phase perturbations to perform stochastic ascent on the received signal strength, based on the receiver feedback, broadcast to all transmitters, of one bit per iteration. This approach is simple and has formed the basis for several prototypes @cite_24 @cite_12 . The relatively slow convergence of the original algorithm (e.g., @math iterations to reach 75 knowledge of previous perturbations @cite_17 , or using knowledge of channel statistics at the receiver @cite_34 . However, the method is fundamentally mismatched to low per-node SNRs @cite_2 , roughly speaking, because noise masks the effect of small phase perturbations. This motivates our approach, in which the transmitters employ large phase perturbations during a training phase, rather than attempting to make small adjustments while beamforming.\"","":""}
{"id":"2794278393","dialogue":"\"We propose a concept system termed distributed base station (DBS), which enables distributed transmit beamforming at large carrier wavelengths to achieve significant range extension and or increased downlink data rate, providing a low-cost infrastructure for applications such as rural broadband. We consider a frequency division duplexed (FDD) system, using feedback from the receiver to achieve the required phase coherence. At a given range, @math cooperating transmitters can achieve @math -fold increase in received power compared to that for a single transmitters, and feedback-based algorithms with near-ideal performance have been prototyped. In this paper, however, we identify and address key technical issues in translating such power gains into range extension via a DBS. First, to combat the drop in per-node SNR with extended range, we design a feedback-based adaptation strategy that is suitably robust to noise. Second, to utilize available system bandwidth, we extend narrowband adaptation algorithms to wideband channels through interpolation over OFDM subcarriers. Third, we observe that the feedback channel may become a bottleneck unless sophisticated distributed reception strategies are employed, but show that acceptable performance can still be obtained with standard uplink reception if channel time variations are slow enough. We quantify system performance compactly via outage capacity analyses.\"","summary":"\"The one-bit feedback algorithm has been extended to wideband regimes in a prior paper by the authors @cite_26 , by adding an additional bit per subcarrier which enables enforcement of phase continuity across subcarriers. This is fundamentally different from the training-based approach in this paper, which enables explicit channel estimation on pilot subcarriers for each transmitter, and hence is amenable to standard interpolation across subcarriers.\"","":""}
{"id":"2794278393","dialogue":"\"We propose a concept system termed distributed base station (DBS), which enables distributed transmit beamforming at large carrier wavelengths to achieve significant range extension and or increased downlink data rate, providing a low-cost infrastructure for applications such as rural broadband. We consider a frequency division duplexed (FDD) system, using feedback from the receiver to achieve the required phase coherence. At a given range, @math cooperating transmitters can achieve @math -fold increase in received power compared to that for a single transmitters, and feedback-based algorithms with near-ideal performance have been prototyped. In this paper, however, we identify and address key technical issues in translating such power gains into range extension via a DBS. First, to combat the drop in per-node SNR with extended range, we design a feedback-based adaptation strategy that is suitably robust to noise. Second, to utilize available system bandwidth, we extend narrowband adaptation algorithms to wideband channels through interpolation over OFDM subcarriers. Third, we observe that the feedback channel may become a bottleneck unless sophisticated distributed reception strategies are employed, but show that acceptable performance can still be obtained with standard uplink reception if channel time variations are slow enough. We quantify system performance compactly via outage capacity analyses.\"","summary":"\"Once we commit to a training phase, one possible approach is for the transmitters to take turns transmitting in training slots, with the receiver sending (quantized) feedback corresponding to each slot. Such time-multiplexed training has been successfully prototyped @cite_19 , and has been studied with quantized feedback in @cite_8 . The algorithm is energy efficient, since only one node is active per iteration, but unlike the DOST scheme proposed in this paper, it does not utilize integration over time to combat noise, so that its performance suffers in the low per-node SNR regime, as we show in our numerical results. The time-multiplexed approach also does not scale well at the protocol level because of the dependence of the training frame structure on the number of transmitter nodes, and the coordination required between them to take turns.\"","":""}
{"id":"2794278393","dialogue":"\"We propose a concept system termed distributed base station (DBS), which enables distributed transmit beamforming at large carrier wavelengths to achieve significant range extension and or increased downlink data rate, providing a low-cost infrastructure for applications such as rural broadband. We consider a frequency division duplexed (FDD) system, using feedback from the receiver to achieve the required phase coherence. At a given range, @math cooperating transmitters can achieve @math -fold increase in received power compared to that for a single transmitters, and feedback-based algorithms with near-ideal performance have been prototyped. In this paper, however, we identify and address key technical issues in translating such power gains into range extension via a DBS. First, to combat the drop in per-node SNR with extended range, we design a feedback-based adaptation strategy that is suitably robust to noise. Second, to utilize available system bandwidth, we extend narrowband adaptation algorithms to wideband channels through interpolation over OFDM subcarriers. Third, we observe that the feedback channel may become a bottleneck unless sophisticated distributed reception strategies are employed, but show that acceptable performance can still be obtained with standard uplink reception if channel time variations are slow enough. We quantify system performance compactly via outage capacity analyses.\"","summary":"\"Our work, and the related work discussed above, falls into the category of all-wireless distributed beamforming with explicit feedback, which allows flexible deployment of DBS supporting FDD operation. It is worth mentioning recent work on all-wireless distributed beamforming based on channel reciprocity, that relies on tight pre-synchronization of the cooperating nodes to emulate a centralized array with a common time base @cite_1 @cite_6 @cite_21 . Finally, there is also significant recent work on distributed MIMO based on coordination of infrastructure nodes (WiFi access points or cellular base stations) via a fast wired backhaul @cite_31 @cite_15 @cite_22 @cite_32 . Our emphasis is to scale up the number of nodes without fast wired backhaul and achieve massive MIMO gains @cite_28 @cite_30 @cite_35 .\"","":""}
{"id":"2794278393","dialogue":"\"We propose a concept system termed distributed base station (DBS), which enables distributed transmit beamforming at large carrier wavelengths to achieve significant range extension and or increased downlink data rate, providing a low-cost infrastructure for applications such as rural broadband. We consider a frequency division duplexed (FDD) system, using feedback from the receiver to achieve the required phase coherence. At a given range, @math cooperating transmitters can achieve @math -fold increase in received power compared to that for a single transmitters, and feedback-based algorithms with near-ideal performance have been prototyped. In this paper, however, we identify and address key technical issues in translating such power gains into range extension via a DBS. First, to combat the drop in per-node SNR with extended range, we design a feedback-based adaptation strategy that is suitably robust to noise. Second, to utilize available system bandwidth, we extend narrowband adaptation algorithms to wideband channels through interpolation over OFDM subcarriers. Third, we observe that the feedback channel may become a bottleneck unless sophisticated distributed reception strategies are employed, but show that acceptable performance can still be obtained with standard uplink reception if channel time variations are slow enough. We quantify system performance compactly via outage capacity analyses.\"","summary":"\"The present paper builds on our prior conference paper @cite_5 , which introduced the DOST scheme, and also discussed extensions to wideband systems. However, it goes well beyond @cite_5 by presenting a DBS concept system built around DOST, including an OFDM system design roughly consistent with LTE, specific prescriptions for pilot design, and outage capacity analyses for downlink and uplink which compactly characterize system-level performance.\"","":""}
{"id":"2793035934","dialogue":"\"Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire NVIDIA DGX-1 to learn successful strategies in Atari games in single-digit minutes, using both synchronous and asynchronous algorithms.\"","summary":"\"Efforts to parallelize and accelerate deep RL algorithms have been underway for several years. Gorila @cite_14 parallelized DQN using distributed computing. It achieved significant although sub-linear speedups using hundreds of computing units as samplers or learners, with central parameter servers for managing parameter updates. This effort suffered in sample complexity relative to single-threaded DQN. More recently, @cite_3 showed that a distributed, prioritized replay buffer can support faster learning while using hundreds of CPU cores for simulation and a single GPU for training. The same work used increased batch sizes, with a brief study of the effect of learning rate.\"","":""}
{"id":"2793035934","dialogue":"\"Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire NVIDIA DGX-1 to learn successful strategies in Atari games in single-digit minutes, using both synchronous and asynchronous algorithms.\"","summary":"\"The policy gradient method A3C is itself a parallelized algorithm. In GA3C @cite_6 , a speedup over CPU-only A3C was achieved by using a GPU. It was employed asynchronously, with predictor'' and trainer'' threads queuing observations and rewards for batched inferences and training updates. GA3C induced a policy lag'' between generation and consumption of training data, worsening sample complexity. In independent work simultaneous to ours, @cite_11 extended policy gradient methods to a distributed setting, enabling an alternative approach to multi-GPU training called IMPALA. They introduced a more heavily modified algorithm, , to mitigate policy lag--which we avoid--and did not employ GPU inference. In PAAC @cite_15 , the authors explored the use of many simulators and increased batch sizes learning rates in (single-GPU) batched A2C--ideas central to our studies. Our contributions to actor-critic methods exceed this work in a number of ways, chiefly: improved sampling organization, tremendously enhanced scale and speed using multiple GPUs, and inclusion of asynchronous optimization.\"","":""}
{"id":"2791941932","dialogue":"\"Crafting adversarial examples has become an important technique to evaluate the robustness of deep neural networks (DNNs). However, most existing works focus on attacking the image classification problem since its input space is continuous and output space is finite. In this paper, we study the much more challenging problem of crafting adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs are discrete text strings and outputs have an almost infinite number of possibilities. To address the challenges caused by the discrete input space, we propose a projected gradient method combined with group lasso and gradient regularization. To handle the almost infinite output space, we design some novel loss functions to conduct non-overlapping attack and targeted keyword attack. We apply our algorithm to machine translation and text summarization tasks, and verify the effectiveness of the proposed algorithm: by changing less than 3 words, we can make seq2seq model to produce desired outputs with high success rates. On the other hand, we recognize that, compared with the well-evaluated CNN-based classifiers, seq2seq models are intrinsically more robust to adversarial attacks.\"","summary":"\"Although lots of works have been done to fool CNN-based models, only a few studies have examined RNN-based models, and the majority of them focus on simple text classification problems. first uses Fast Gradient Sign Method (FGSM) to conduct an attack on RNN LSTM-based classification problems. In order to generate text adversarial examples, @cite_16 proposes to use reinforcement learning to locate important words that could be deleted in sentiment classification. and generate adversarial sequences by inserting or replacing existing words with typos and synonyms. aims to attack sentiment classification models in a black-box setting. It develops some scoring functions to find the most important words to modify. These approaches differ from our work in that they study simple text classification problems while we focus on the more challenging seq2seq model with sequential outputs. Other than attacking text classifiers, aims to fool reading comprehension systems by adding misleading sentences, which has a different focus than ours. uses the generative adversarial network (GAN) to craft natural adversarial examples. However, it can only perform the untargeted attack and also suffers from high computational cost.\"","":""}
{"id":"2793598667","dialogue":"\"This work describes the development of a high-resolution tactile-sensing finger for robot grasping. This finger, inspired by previous GelSight sensing techniques, features an integration that is slimer, more robust, and with more homogenoeus output than previous vision-based tactile sensors. To achieve a compact integration, we redesign the optical path from illumination source to camera by combining light guides and an arrangement of mirror reflections. The optical path can be parametrized with geometric design variables and we describe the tradeoffs between the thickness of the finger, the depth of field of the camera, and the size of the tactile sensing pad. The sensor can sustain the wear from continuous use--and abuse--in grasping tasks by combining tougher materials for the compliant soft gel, a textured fabric skin, a structurally rigid body, and a calibration process that ensures homogeneous illumination and contrast of the tactile images during use. Finally, we evaluate the sensor's durability along four metrics that capture the signal quality during more than 3000 grasping experiments.\"","summary":"\"The GelSight sensor is a vision-based tactile sensor that measures the 2D texture and 3D topography of the contact surface. It utilizes a piece of elastomeric gel with an opaque coating as the sensing surface, and a webcam above the gel to capture contact deformation from changes in lighting contrast as reflected by the opaque coating. The gel is illuminated by color LEDs with inclined angles and different directions. The resulting colored shading can be used to reconstruct the 3D geometry of the gel deformation. The original, larger GelSight sensor @cite_10 @cite_13 was designed to measure the 3D topography of the contact surface with micrometer-level spatial resolution. Li . @cite_7 designed a cuboid fingertip version that could be integrated in a robot finger. Li's sensor has a @math cm @math sensing area, and can measure fine 2D texture and coarse 3D information. A new version of the GelSight sensor was more recently proposed by Dong . @cite_21 to improve 3D geometry measurements and standardize the fabrication process. A detailed review of different versions of GelSight sensors can be found in @cite_1 .\"","":""}
{"id":"2793598667","dialogue":"\"This work describes the development of a high-resolution tactile-sensing finger for robot grasping. This finger, inspired by previous GelSight sensing techniques, features an integration that is slimer, more robust, and with more homogenoeus output than previous vision-based tactile sensors. To achieve a compact integration, we redesign the optical path from illumination source to camera by combining light guides and an arrangement of mirror reflections. The optical path can be parametrized with geometric design variables and we describe the tradeoffs between the thickness of the finger, the depth of field of the camera, and the size of the tactile sensing pad. The sensor can sustain the wear from continuous use--and abuse--in grasping tasks by combining tougher materials for the compliant soft gel, a textured fabric skin, a structurally rigid body, and a calibration process that ensures homogeneous illumination and contrast of the tactile images during use. Finally, we evaluate the sensor's durability along four metrics that capture the signal quality during more than 3000 grasping experiments.\"","summary":"\"GelSight-like sensors with rich 2D and 3D information have been successfully applied in robotic manipulation. Li . @cite_7 used GelSight's localization capabilities to insert a USB connector, where the sensor used the texture of the characteristic USB logo to guide the insertion. Izatt . @cite_15 explored the use of the 3D point cloud measured by a GelSight sensor in a state estimation filter to find the pose of a grasped object in a peg-in-hole task. Dong . @cite_21 used the GelSight sensor to detect slip from variations in the 2D texture of the contact surface in a robot picking task. The 2D image structure of the output from a GelSight sensor makes it a good fit for deep learning architectures. GelSight sensors have also been used to estimate grasp quality @cite_3 .\"","":""}
{"id":"2948290658","dialogue":"\"Detecting scene types in a movie can be very useful for application such as video editing, ratings assignment, and personalization. We propose a system for detecting kissing scenes in a movie. This system consists of two components. The first component is a binary classifier that predicts a binary label (i.e. kissing or not) given a features exctracted from both the still frames and audio waves of a one-second segment. The second component aggregates the binary labels for contiguous non-overlapping segments into a set of kissing scenes. We experimented with a variety of 2D and 3D convolutional architectures such as ResNet, DesnseNet, and VGGish and developed a highly accurate kissing detector that achieves a validation F1 score of 0.95 on a diverse database of Hollywood films ranging many genres and spanning multiple decades. The code for this project is available at this http URL\"","summary":"\"Another approach is to use two stream networks. @cite_12 explicitly modeled the temporal featurs in a stack of optical flow vectors in parallel to a network for capturing the spatial context. These networks are trained separately and then combined using an SVM. The predicted output is averaged across the sampled frames. This approach suffers from the false label assignment problem. In other words, the label is applied to all the sampled frames, and the labeled action may not be represented in all frames.\"","":""}
{"id":"2948290658","dialogue":"\"Detecting scene types in a movie can be very useful for application such as video editing, ratings assignment, and personalization. We propose a system for detecting kissing scenes in a movie. This system consists of two components. The first component is a binary classifier that predicts a binary label (i.e. kissing or not) given a features exctracted from both the still frames and audio waves of a one-second segment. The second component aggregates the binary labels for contiguous non-overlapping segments into a set of kissing scenes. We experimented with a variety of 2D and 3D convolutional architectures such as ResNet, DesnseNet, and VGGish and developed a highly accurate kissing detector that achieves a validation F1 score of 0.95 on a diverse database of Hollywood films ranging many genres and spanning multiple decades. The code for this project is available at this http URL\"","summary":"\"More recently, 3D convolutional networks have shown ImageNet-level success for video classification. @cite_9 have shown that these models can retrace the effectiveness of 2D networks and can be as deep (e.g. 152 layers for ResNet). 3D convnets can be resource intensive for training and inference, however, they present an elegant framework for processing video data. @cite_4 extend the two stream work with 3D networks and leverage pre-trained 2D models by repeating the weights in the third dimension. Our approach is closest to this line of work and explores using parallel networks (2D and 3D) consuming image and audio features which is, to the best of our knowledge, a novel approach.\"","":""}
{"id":"2963606198","dialogue":"\"Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) make attention maps an explicit and natural component of the end-to-end training for the first time, (2) provide self-guidance directly on these maps by exploring supervision from the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on PASCAL VOC 2012 test and val. sets. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.\"","summary":"\"Identifying bias in datasets @cite_21 is another important usage of the network attention. @cite_5 analyses the location of attention maps of a trained model to find out the dataset bias, which helps them to build a better unbiased dataset. However, in practical applications, it is hard remove all the bias of the dataset and time-consuming to build a new dataset. How to garantee the generalization ability of the learned network is still challenging. Different from the existing methods, our model can fundamentally solve this problem by providing supervision directly on network's attention and guiding the network to focus on the areas critical to the task of interest, therefore is robust to dataset bias.\"","":""}
{"id":"2788627818","dialogue":"\"This paper introduces a novel anchor design to support anchor-based face detection for superior scale-invariant performance, especially on tiny faces. To achieve this, we explicitly address the problem that anchor-based detectors drop performance drastically on faces with tiny sizes, e.g. less than 16x16 pixels. In this paper, we investigate why this is the case. We discover that current anchor design cannot guarantee high overlaps between tiny faces and anchor boxes, which increases the difficulty of training. The new Expected Max Overlapping (EMO) score is proposed which can theoretically explain the low overlapping issue and inspire several effective strategies of new anchor design leading to higher face overlaps, including anchor stride reduction with new network architectures, extra shifted anchors, and stochastic face shifting. Comprehensive experiments show that our proposed method significantly outperforms the baseline anchor-based detector, while consistently achieving state-of-the-art results on challenging face detection datasets with competitive runtime speed.\"","summary":"\"However, the Wider Face dataset @cite_42 pushes the challenge to another level. In addition to heavy occlusion, extreme pose, and strong illumination, the ultra small sizes of faces in crowd images have become one of the most challenging problems in robust face detection. To solve this, CMS-RCNN @cite_35 incorporates body contextual information to help infer the location of faces. HR @cite_47 builds multi-level image pyramids for multi-scale training and testing which finds upscaled tiny faces. SFD @cite_22 addresses this with scale-equitable framework and new anchor matching strategy. In this paper, we introduce a novel anchor design for finding more tiny faces, leading to state-of-the-art detection performance.\"","":""}
{"id":"2788268216","dialogue":"\"Political speeches and debates play an important role in shaping the images of politicians, and the public often relies on media outlets to select bits of political communication from a large pool of utterances. It is an important research question to understand what factors impact this selection process. To quantitatively explore the selection process, we build a three-decade dataset of presidential debate transcripts and post-debate coverage. We first examine the effect of wording and propose a binary classification framework that controls for both the speaker and the debate situations. We find that crowdworkers can only achieve an accuracy of 60 in this task, indicating that media choices are not entirely obvious. Our classifiers outperform crowdworkers on average, mainly in primary debates. We also compare important factors from crowdworkers’ free responses with those from data-driven methods and find interesting differences. Few crowdworkers mentioned that “context matters”, whereas our data show that well-quoted sentences are more distinct from the previous utterance by the same speaker than less-quoted sentences. Finally, we examine the aggregate effect of media preferences towards different wordings to understand the extent of fragmentation among media outlets. By analyzing a bipartite graph built from quoting behavior in our data, we observe a decreasing trend in bipartisan coverage.\"","summary":"\"The effect of post-debate coverage on public opinion. Studies have shown that media choices about coverage can have serious consequences @cite_37 @cite_24 @cite_38 @cite_27 @cite_17 @cite_1 @cite_50 . For instance, show that in the 2004 U.S. election, citizens who only read the news coverage rated Kerry more negatively compared to those who watched the debate firsthand, because media outlets highlighted the moment of Kerry outing Cheney's lesbian daughter, although this moment did not catch much attention from the live audience. develop a mobile app to collect real-time feedback for presidential debates. discuss the media's critical tendency and its partisan consequences in the U.S.\"","":""}
{"id":"2788268216","dialogue":"\"Political speeches and debates play an important role in shaping the images of politicians, and the public often relies on media outlets to select bits of political communication from a large pool of utterances. It is an important research question to understand what factors impact this selection process. To quantitatively explore the selection process, we build a three-decade dataset of presidential debate transcripts and post-debate coverage. We first examine the effect of wording and propose a binary classification framework that controls for both the speaker and the debate situations. We find that crowdworkers can only achieve an accuracy of 60 in this task, indicating that media choices are not entirely obvious. Our classifiers outperform crowdworkers on average, mainly in primary debates. We also compare important factors from crowdworkers’ free responses with those from data-driven methods and find interesting differences. Few crowdworkers mentioned that “context matters”, whereas our data show that well-quoted sentences are more distinct from the previous utterance by the same speaker than less-quoted sentences. Finally, we examine the aggregate effect of media preferences towards different wordings to understand the extent of fragmentation among media outlets. By analyzing a bipartite graph built from quoting behavior in our data, we observe a decreasing trend in bipartisan coverage.\"","summary":"\"Influences between the media and politicians. Although our work focuses on media selection of highlights, politicians often behave based on their beliefs about media preferences, which suggests complex dynamics between the media and politicians @cite_33 @cite_43 @cite_28 @cite_22 @cite_19 . For instance, discuss politicians' increasing adaptation to different news values and formats in the presence of media abundance. Also relevant is research on the influence of politicians on the media, including agenda-setting, rhetorical positioning, and framing @cite_3 @cite_7 @cite_4 @cite_13 @cite_5 .\"","":""}
{"id":"2788268216","dialogue":"\"Political speeches and debates play an important role in shaping the images of politicians, and the public often relies on media outlets to select bits of political communication from a large pool of utterances. It is an important research question to understand what factors impact this selection process. To quantitatively explore the selection process, we build a three-decade dataset of presidential debate transcripts and post-debate coverage. We first examine the effect of wording and propose a binary classification framework that controls for both the speaker and the debate situations. We find that crowdworkers can only achieve an accuracy of 60 in this task, indicating that media choices are not entirely obvious. Our classifiers outperform crowdworkers on average, mainly in primary debates. We also compare important factors from crowdworkers’ free responses with those from data-driven methods and find interesting differences. Few crowdworkers mentioned that “context matters”, whereas our data show that well-quoted sentences are more distinct from the previous utterance by the same speaker than less-quoted sentences. Finally, we examine the aggregate effect of media preferences towards different wordings to understand the extent of fragmentation among media outlets. By analyzing a bipartite graph built from quoting behavior in our data, we observe a decreasing trend in bipartisan coverage.\"","summary":"\"Power dynamics in debates and other types of coverage. Studies have shown that language use and topic control in debates can reflect influence between candidates and indicate power dynamics @cite_30 @cite_23 @cite_25 . More recently, social media have also become an important channel to monitor public opinion on debates in real time @cite_0 @cite_8 and potentially change news media coverage.\"","":""}
{"id":"2788989367","dialogue":"\"Evolution Strategies (ES) have recently been demonstrated to be a viable alternative to reinforcement learning (RL) algorithms on a set of challenging deep RL problems, including Atari games and MuJoCo humanoid locomotion benchmarks. While the ES algorithms in that work belonged to the specialized class of natural evolution strategies (which resemble approximate gradient RL algorithms, such as REINFORCE), we demonstrate that even a very basic canonical ES algorithm can achieve the same or even better performance. This success of a basic ES algorithm suggests that the state-of-the-art can be advanced further by integrating the many advances made in the field of ES in the last decades. We also demonstrate qualitatively that ES algorithms have very different performance characteristics than traditional RL algorithms: on some games, they learn to exploit the environment and perform much better while on others they can get stuck in suboptimal local minima. Combining their strengths with those of traditional RL algorithms is therefore likely to lead to new advances in the state of the art.\"","summary":"\"Evolutionary strategies, such as the Covariance Matrix Adaptation Evolution Strategy (CMA-ES @cite_8 ), are commonly used as a baseline approach in reinforcement learning tasks @cite_9 @cite_5 @cite_22 @cite_27 . Here, we only discuss the most recent related works, which also followed up on the work by ; in particular, we discuss three related arXiv preprints that scientists at Uber released in the last two months about work concurrent to ours.\"","":""}
{"id":"2604367072","dialogue":"\"Vehicle safety depends on (a) the range of identified hazards and (b) the operational situations for which mitigations of these hazards are acceptably decreasing risk. Moreover, with an increasing degree of autonomy, risk ownership is likely to increase for vendors towards regulatory certification. Hence, highly automated vehicles have to be equipped with verified controllers capable of reliably identifying and mitigating hazards in all possible operational situations. To this end, available methods for the design and verification of automated vehicle controllers have to be supported by models for hazard analysis and mitigation.\"","summary":"\"G \"\"udemann and Ortmeier @cite_7 present a language for probabilistic system modeling for safety analysis. Formalized as","":""}
{"id":"2604367072","dialogue":"\"Vehicle safety depends on (a) the range of identified hazards and (b) the operational situations for which mitigations of these hazards are acceptably decreasing risk. Moreover, with an increasing degree of autonomy, risk ownership is likely to increase for vendors towards regulatory certification. Hence, highly automated vehicles have to be equipped with verified controllers capable of reliably identifying and mitigating hazards in all possible operational situations. To this end, available methods for the design and verification of automated vehicle controllers have to be supported by models for hazard analysis and mitigation.\"","summary":"\"@cite_12 present an algorithm for finding permissive robot action plans optimal to safety and performance. They employ (helpful in regarding uncertainty and robot limitations) to model robot behavior, and two abstractions from this model to capture a system's modes and hazards. Our framework uses three layers of abstraction ( @math , @math , @math ), operational situations to capture control modes, and a structure to capture hazards. While they directly encode hazard severity for plan selection, our framework allows the planner to calculate the risk priority based on a causal event tree towards mishaps. As opposed to complete behavioral planning, our approach focuses the construction of mitigation planning models. For example, for system faults we can plan mitigations by using adaptation mechanisms of a given control system architecture.\"","":""}
{"id":"2604367072","dialogue":"\"Vehicle safety depends on (a) the range of identified hazards and (b) the operational situations for which mitigations of these hazards are acceptably decreasing risk. Moreover, with an increasing degree of autonomy, risk ownership is likely to increase for vendors towards regulatory certification. Hence, highly automated vehicles have to be equipped with verified controllers capable of reliably identifying and mitigating hazards in all possible operational situations. To this end, available methods for the design and verification of automated vehicle controllers have to be supported by models for hazard analysis and mitigation.\"","summary":"\"Jha and Raman @cite_10 discuss the synthesis of vehicle trajectories from probabilistic temporal logic assertions. Synthesized trajectories take into account perception uncertainty through approximation of sensed obstacles by combining Gaussian polytopes. In a similar context, Rizaldi and Althoff @cite_6 formalize safe driving policies to derive safe control strategies implementing worst-case braking scenarios in autonomous driving. They apply a hybrid-trace-based formalization of physics required for model checking of recorded @cite_6 and planned @cite_3 strategies. @cite_10 @cite_6 @cite_3 discuss low-level control for a specific class of driving scenarios, whereas our approach provides for (i) the investigation and combination of many related operational situations, thus, forming a more comprehensive perspective of driving safety, (ii) regarding various kinds of hazards that might play a role in high- and low-level control beyond safe and optimal trajectory planning and collision avoidance.\"","":""}
{"id":"2604367072","dialogue":"\"Vehicle safety depends on (a) the range of identified hazards and (b) the operational situations for which mitigations of these hazards are acceptably decreasing risk. Moreover, with an increasing degree of autonomy, risk ownership is likely to increase for vendors towards regulatory certification. Hence, highly automated vehicles have to be equipped with verified controllers capable of reliably identifying and mitigating hazards in all possible operational situations. To this end, available methods for the design and verification of automated vehicle controllers have to be supported by models for hazard analysis and mitigation.\"","summary":"\"Wei et al. @cite_0 describe an autonomous driving platform, capable of bringing vehicles to a safe state and stop, activating a fail-operational mode on critical failure, and a limp-home mode on less critical failure. These are mitigation strategies we can assess in our framework. Their work elaborates on designing a specific class of architectures. Additionally, we provide an approach to systematically evaluate risks and, consequently, derive an architecture design.\"","":""}
{"id":"2604367072","dialogue":"\"Vehicle safety depends on (a) the range of identified hazards and (b) the operational situations for which mitigations of these hazards are acceptably decreasing risk. Moreover, with an increasing degree of autonomy, risk ownership is likely to increase for vendors towards regulatory certification. Hence, highly automated vehicles have to be equipped with verified controllers capable of reliably identifying and mitigating hazards in all possible operational situations. To this end, available methods for the design and verification of automated vehicle controllers have to be supported by models for hazard analysis and mitigation.\"","summary":"\"Babin et al. @cite_13 propose a system reconfiguration approach developed with the Event-B method in a correct-by-construction fashion using a behavior pattern similar to our approach (particularly, fig:statemodel ). Reconfiguration as one way to faults is discussed in this work. Wardzi ' n ski @cite_1 discusses hazard identification and mitigation for autonomous vehicles by predetermined risk assessment ( with safety barriers) and dynamic risk assessment. For both, he provides argumentation patterns for creating AV safety cases. In addition to his work, the abstraction and the method we propose covers both paradigms in one framework. We provide formal notions of all core concepts.\"","":""}
{"id":"2788904152","dialogue":"\"In this paper, we introduce the use of a personalized Gaussian Process model (pGP) to predict per-patient changes in ADAS-Cog13 -- a significant predictor of Alzheimer's Disease (AD) in the cognitive domain -- using data from each patient's previous visits, and testing on future (held-out) data. We start by learning a population-level model using multi-modal data from previously seen patients using a base Gaussian Process (GP) regression. The personalized GP (pGP) is formed by adapting the base GP sequentially over time to a new (target) patient using domain adaptive GPs. We extend this personalized approach to predict the values of ADAS-Cog13 over the future 6, 12, 18, and 24 months. We compare this approach to a GP model trained only on past data of the target patients (tGP), as well as to a new approach that combines pGP with tGP. We find that the new approach, combining pGP with tGP, leads to large improvements in accurately forecasting future ADAS-Cog13 scores.\"","summary":"\"However, most of these works attempted forecasting of changes in subjects' CS, which deals with a limited number of future outcomes (i.e., either binary or a three-class). By contrast, we aim to forecast ADAS-Cog13, defined on a more fine-grained scale (85 levels), which poses a more challenging machine learning problem. While in our recent work @cite_21 , we investigated forecasting of ADAS-Cog13 along with other scores (CDSRB, CS, and MMSE), we did so for one step ahead (6 months). In this work, we attempt forecasting of up to 24 months ahead by focusing on ADAS-Cog13.\"","":""}
{"id":"2788187279","dialogue":"\"In this paper, we propose to study the problem of COURT VIEW GENeration from the fact description in a criminal case. The task aims to improve the interpretability of charge prediction systems and help automatic legal document generation. We formulate this task as a text-to-text natural language generation (NLG) problem. Sequenceto-sequence model has achieved cutting-edge performances in many NLG tasks. However, due to the non-distinctions of fact descriptions, it is hard for Seq2Seq model to generate charge-discriminative court views. In this work, we explore charge labels to tackle this issue. We propose a label-conditioned Seq2Seq model with attention for this problem, to decode court views conditioned on encoded charge labels. Experimental results show the effectiveness of our method.\"","summary":"\"Our work is firstly related to previous studies on legal assistant systems. Previous work considers the task of charge prediction as a text classification problem @cite_27 @cite_29 @cite_43 @cite_24 . Recently, investigate deep learning methods for this task. Besides, there are also works on identifying applicable articles for a given case @cite_17 @cite_43 @cite_22 , answering legal questions as a consulting system @cite_44 @cite_19 and searching relevant cases for a given query @cite_42 @cite_32 . As a legal assistant system, @math can benefit automatic legal document generation by generating court views from fact descriptions obtained from the last phase, through legal professionals or other technics like information extraction @cite_30 from raw documents in a case, if we generate legal documents step by step. Our work is also related to recent studies on model interpretation @cite_37 @cite_10 @cite_6 . Recently, much work has paid attention to giving textual explanations for classifications. generate visual explanations for image classification. propose to learn to select most supportive snippets from raw texts for text classification. @math @math @math can improve the interpretability of charge prediction systems by generating textual court views when predict the charges.\"","":""}
{"id":"2788187279","dialogue":"\"In this paper, we propose to study the problem of COURT VIEW GENeration from the fact description in a criminal case. The task aims to improve the interpretability of charge prediction systems and help automatic legal document generation. We formulate this task as a text-to-text natural language generation (NLG) problem. Sequenceto-sequence model has achieved cutting-edge performances in many NLG tasks. However, due to the non-distinctions of fact descriptions, it is hard for Seq2Seq model to generate charge-discriminative court views. In this work, we explore charge labels to tackle this issue. We propose a label-conditioned Seq2Seq model with attention for this problem, to decode court views conditioned on encoded charge labels. Experimental results show the effectiveness of our method.\"","summary":"\"Our label-conditioned Seq2Seq model steams from widely used encoder-decoder paradigm @cite_34 which has been widely used in machine translation @cite_39 @cite_20 , summarization @cite_38 @cite_40 @cite_18 @cite_0 , semantic parsing @cite_41 and paraphrase @cite_9 or other NLG problems such as product review generation @cite_23 and code generation @cite_1 @cite_14 . propose to encode image labels for visual-language models to generate justification texts for image classification. We also introduce charge labels into Seq2Seq model to improve the charge-discriminations of generated rationales. Widely used attention mechanism @cite_28 @cite_36 is applied to generate fact details more accurately.\"","":""}
{"id":"2788242942","dialogue":"\"We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a densely connected autoencoder architecture capable of learning a joint representation of both local graph structure and available external node features for the multi-task learning of link prediction and node classification. To the best of our knowledge, this is the first architecture that can be efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification. We provide comprehensive empirical evaluation of our models on a range of challenging benchmark graph-structured datasets, and demonstrate significant improvement in accuracy over related methods for graph representation learning. Code implementation is available at this https URL\"","summary":"\"The field of graph representation learning is seeing a resurgence of research interest in recent years, driven in part by the latest advances in deep learning. The aim is to learn a mapping that encodes the input graph into low-dimensional feature embeddings while preserving its original global structure. @cite_14 succinctly articulate the diverse set of previously proposed approaches for graph representation learning, or graph embedding, as belonging within a unified encoder-decoder framework. In this section, we summarize three classes of encoder-decoder models most related to our work: matrix factorization (MF), autoencoders, and graph convolutional networks (GCNs).\"","":""}
{"id":"2788242942","dialogue":"\"We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a densely connected autoencoder architecture capable of learning a joint representation of both local graph structure and available external node features for the multi-task learning of link prediction and node classification. To the best of our knowledge, this is the first architecture that can be efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification. We provide comprehensive empirical evaluation of our models on a range of challenging benchmark graph-structured datasets, and demonstrate significant improvement in accuracy over related methods for graph representation learning. Code implementation is available at this https URL\"","summary":"\"Our work is inspired by recent successful applications of autoencoder architectures for collaborative filtering that outperform popular matrix factorization methods @cite_18 @cite_21 @cite_0 , and is related to Structural Deep Network Embedding (SDNE) @cite_28 for link prediction. Similar to SDNE, our models rely on the autoencoder to learn non-linear node embeddings from local graph neighborhoods. However, our models have several important distinctions: 1) we leverage extensive parameter sharing between the encoder and decoder parts to enhance representation learning; 2) our @math LoNGAE model can optionally concatenate side node features to the adjacency matrix for improved link prediction performance; and 3) the @math LoNGAE model can be trained end-to-end in a single stage for multi-task learning of link prediction and semi-supervised node classification. On the other hand, training SDNE requires multiple steps that are difficult to jointly optimize: i) pre-training via a deep belief network; and ii) utilizing a separate downstream classifier on top of node embeddings for LPNC.\"","":""}
{"id":"2788959741","dialogue":"\"Modern low-latency anonymity systems, no matter whether constructed as an overlay or implemented at the network layer, offer limited security guarantees against traffic analysis. On the other hand, high-latency anonymity systems offer strong security guarantees at the cost of computational overhead and long delays, which are excessive for interactive applications. We propose TARANET, an anonymity system that implements protection against traffic analysis at the network layer, and limits the incurred latency and overhead. In TARANET's setup phase, traffic analysis is thwarted by mixing. In the data transmission phase, end hosts and ASes coordinate to shape traffic into constant-rate transmission using packet splitting. Our prototype implementation shows that TARANET can forward anonymous traffic at over 50 Gbps using commodity hardware.\"","summary":"\"Recent research @cite_50 @cite_43 @cite_70 proposes that incorporate anonymous communication as a service of network infrastructures in the Internet and next generation network architectures @cite_39 @cite_55 @cite_69 . The basic assumption of a network-layer anonymity system is that Autonomous Systems (AS) can conduct efficient cryptographic operations when forwarding packets to conceal forwarding information. Additionally, a network-layer anonymity system uses direct forwarding paths rather than reroute packets through overlay networks as in Tor @cite_14 . This processing would be done on (software) routers, for instance, but more abstractedly the term is used to refer to the device or set of devices dedicated to the anonymity system within an AS.\"","":""}
{"id":"2788959741","dialogue":"\"Modern low-latency anonymity systems, no matter whether constructed as an overlay or implemented at the network layer, offer limited security guarantees against traffic analysis. On the other hand, high-latency anonymity systems offer strong security guarantees at the cost of computational overhead and long delays, which are excessive for interactive applications. We propose TARANET, an anonymity system that implements protection against traffic analysis at the network layer, and limits the incurred latency and overhead. In TARANET's setup phase, traffic analysis is thwarted by mixing. In the data transmission phase, end hosts and ASes coordinate to shape traffic into constant-rate transmission using packet splitting. Our prototype implementation shows that TARANET can forward anonymous traffic at over 50 Gbps using commodity hardware.\"","summary":"\"The first class of network-layer anonymity protocols proposed is the so-called system, which consists of two proposals, @cite_50 and @cite_43 . These systems defend against topological attacks by encrypting forwarding information in packet headers. However, in both schemes, packets stay unchanged from hop to hop, thus enabling bit-pattern correlation of packets at distinct compromised nodes.\"","":""}
{"id":"2788235277","dialogue":"\"Deep neural networks (DNNs) have shown phenomenal success in a wide range of applications. However, recent studies have discovered that they are vulnerable to Adversarial Examples, i.e., original samples with added subtle perturbations. Such perturbations are often too small and imperceptible to humans, yet they can easily fool the neural networks. Few defense techniques against adversarial examples have been proposed, but they require modifying the target model or prior knowledge of adversarial examples generation methods. Likewise, their performance remarkably drops upon encountering adversarial example types not used during the training stage. In this paper, we propose a new framework that can be used to enhance DNNs' robustness by detecting adversarial examples. In particular, we employ the decision layer of independently trained models as features for posterior detection. The proposed framework doesn't require any prior knowledge of adversarial examples generation techniques, and can be directly augmented with unmodified off-the-shelf models. Experiments on the standard MNIST and CIFAR10 datasets show that it generalizes well across not only different adversarial examples generation methods but also various additive perturbations. Specifically, distinct binary classifiers trained on top of our proposed features can achieve a high detection rate (>90 ) in a set of white-box attacks and maintain this performance when tested against unseen attacks.\"","summary":"\"Adversarial attacks (samples with adversarially-crafted small perturbations) have recently emerged as a significant threat to the deep learning techniques, thereby this finding may hinder the large scale adoption of DNNs-based systems in practical applications. There exists a number of methods to generate adversarial samples or adversarial examples (AEs) mainly based on the gradient of networks @cite_26 or solving optimization problems @cite_34 , and so on. Very few works have attempted to give scientific reasoning for the vulnerability phenomena of DNNs to AEs. For instance, authors in @cite_34 gave preliminary explanation that since the input space is densely populated by low-probability adversarial pockets, thus each point in the space is closer to many AEs. These AEs points can be easily doctored to attain the desired model outcome. In turn, Goodfellow @cite_26 argued that linear nature of DNNs-based classifiers is the main source of vulnerability. While, the work in @cite_27 discussed boundary tilting'' view and argued that usually AEs lie in regions where the decision boundary is close to the manifold of training data.\"","":""}
{"id":"2788235277","dialogue":"\"Deep neural networks (DNNs) have shown phenomenal success in a wide range of applications. However, recent studies have discovered that they are vulnerable to Adversarial Examples, i.e., original samples with added subtle perturbations. Such perturbations are often too small and imperceptible to humans, yet they can easily fool the neural networks. Few defense techniques against adversarial examples have been proposed, but they require modifying the target model or prior knowledge of adversarial examples generation methods. Likewise, their performance remarkably drops upon encountering adversarial example types not used during the training stage. In this paper, we propose a new framework that can be used to enhance DNNs' robustness by detecting adversarial examples. In particular, we employ the decision layer of independently trained models as features for posterior detection. The proposed framework doesn't require any prior knowledge of adversarial examples generation techniques, and can be directly augmented with unmodified off-the-shelf models. Experiments on the standard MNIST and CIFAR10 datasets show that it generalizes well across not only different adversarial examples generation methods but also various additive perturbations. Specifically, distinct binary classifiers trained on top of our proposed features can achieve a high detection rate (>90 ) in a set of white-box attacks and maintain this performance when tested against unseen attacks.\"","summary":"\"The proposed defenses for mitigating AEs can be grouped into two categories. The first category techniques try either improving the robustness of DNNs or suppressing the success rates of attacks. For instance, @cite_26 , which is training the system with AEs to augment the regularization and loss functions and making the system more resilient. The other technique is @cite_33 in which additional DNNs with softmax are trained to obstruct the deep learning system from fitting too tightly to the data. However, it has been demonstrated that defensive distillation method can be easily circumvented with a minimal modified attack @cite_24 . Other approaches are or @cite_17 , i.e., removing the adversarial noise from the input samples before feeding them to neural networks, and @cite_25 , i.e., modifying the traditional neural network architectures, e.g., adding extra specific robust layers and functions.\"","":""}
{"id":"2789228953","dialogue":"\"This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m s.\"","summary":"Object tracking is one of the fundamental problems in computer vision and has been extensively studied @cite_25 and applied in many different tasks.","":""}
{"id":"2789228953","dialogue":"\"This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m s.\"","summary":"\"The tracker @cite_14 first transforms the image into an appropriate feature space, and uses a classifier as well as a motion model to determine the presence of an object in a frame, which is referred to as tracking by detection.\"","":""}
{"id":"2789228953","dialogue":"\"This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m s.\"","summary":"\"More recent methods like @cite_34 employ convolutional neural networks to learn motion and appearance of objects. The feature maps of higher convolutional layers provide robust and accurate appearance representations, but lack spatial resolution. Lower layers on the other hand provide higher spatial resolution and less refined appearance representations. This hierarchical structure is used in @cite_35 by inferring responses of correlation filters on each corresponding layer pair.\"","":""}
{"id":"2789228953","dialogue":"\"This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that light-weight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m s.\"","summary":"\"In @cite_38 a semi-supervised approach is described that provides a fusion between using sparse ground truth data from a LiDAR sensor and stereo view synthesis, estimating one image in a stereo pair from the other, to infer dense depth maps. Others @cite_15 @cite_26 in turn rely solely on stereo as a supervision signal, which comes with the benefit of easily available or obtainable data.\"","":""}
{"id":"2786178913","dialogue":"\"It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.\"","summary":"\"As a consequence, the input of Parseval networks can be recovered if but only if the built-in non-linearities are invertible as well, which is typically not the case. @cite_4 derive conditions under which pooling representations are, but our method directly overcomes this issue. The Scattering transform is an example of predefined deep representation, approximately invariant to translations, that can be reconstructed when the degree of invariance specified is small. Yet, it requires a gradient descent optimization and no guarantee of convergences are known. In summary, the references make clear that invertibility requires special care in designing the architecture or special care in designing the optimization procedure. In this paper, we introduce a network, that overcomes these issues and has an exact inverse by construction.\"","":""}
{"id":"2786178913","dialogue":"\"It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.\"","summary":"\"Our main inspiration for this work is the recent reversible residual network (RevNet), introduced in @cite_7 . RevNets are in turn closely related to NICE and Real-NVP architectures , which make use of constrained Jacobian determinants for generative modeling. All these architectures are similar to the lifting scheme and Feistel cipher diagrams , as we will show. RevNets illustrate how to build invertible ResNet-type blocks that avoid storing intermediate activations necessary for the backward pass. However, RevNets still employ multiple non-invertible operators like max-pooling and downsampling operators as part of the network. As such, RevNets are not invertible by construction. In this paper, we show how to build an invertible type of RevNet architecture that performs competitively with RevNets on Imagenet, which we call @math -RevNet for invertible RevNet.\"","":""}
{"id":"2952007329","dialogue":"\"This paper proposes a new architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures in deep learning models. ATPL is a new architecture to bridge this gap by exploiting Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, aiming to integrate deep learning with explicit language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via TPR-based deep neural network; 2) employing attention modules to compute TPR; and 3) integration of TPR with typical deep learning architectures including Long Short-Term Memory (LSTM) and Feedforward Neural Network (FFNN). The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. This ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a sentence. Experimental results demonstrate the effectiveness of the proposed approach.\"","summary":"\"Our proposed image captioning system follows a great deal of recent caption-generation literature in exploiting end-to-end deep learning with a CNN image-analysis front end producing a distributed representation that is then used to drive a natural-language generation process, typically using RNNs @cite_13 @cite_20 @cite_11 . Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks @cite_9 @cite_2 @cite_25 @cite_14 . Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves --- filler role structure.\"","":""}
{"id":"2952007329","dialogue":"\"This paper proposes a new architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures in deep learning models. ATPL is a new architecture to bridge this gap by exploiting Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, aiming to integrate deep learning with explicit language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via TPR-based deep neural network; 2) employing attention modules to compute TPR; and 3) integration of TPR with typical deep learning architectures including Long Short-Term Memory (LSTM) and Feedforward Neural Network (FFNN). The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. This ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a sentence. Experimental results demonstrate the effectiveness of the proposed approach.\"","summary":"\"Methods for automatic constituency parsing of a sentence, our third task, include methods based on probabilistic context-free grammars (CFGs) @cite_15 , the shift-reduce method @cite_7 , sequence-to-sequence LSTMs @cite_8 . Our constituency parser is similar to the sequence-to-sequence LSTMs @cite_8 since both use LSTM neural networks to design a constituency parser. Different from @cite_8 , our constituency parser uses TPR and unbinding role vectors to extract features that contain grammatical information.\"","":""}
{"id":"2788538338","dialogue":"\"Let @math and @math be sets of @math red and @math blue points in the plane, respectively, with @math . Let @math be a perfect matching between points from @math and @math , using @math straight line segments to match the points, that is, each point is an endpoint of exactly one line segment, and each line segment has one red and one blue endpoint. We forbid line segments to cross. Denote the length of a longest line segment in @math with @math , which we also call the [ ] value, bn of @math . We aim to find a matching under given constraints that minimizes @math . Any such matching is called a [ ] bottleneck matching of @math .\"","summary":"\"In @cite_0 , Chang, Tang and Lee gave an @math -time algorithm for computing a bottleneck matching of a point set, but allowing crossings. This result was extended by Efrat and Katz in @cite_9 to higher-dimensional Euclidean spaces.\"","":""}
{"id":"2788538338","dialogue":"\"Let @math and @math be sets of @math red and @math blue points in the plane, respectively, with @math . Let @math be a perfect matching between points from @math and @math , using @math straight line segments to match the points, that is, each point is an endpoint of exactly one line segment, and each line segment has one red and one blue endpoint. We forbid line segments to cross. Denote the length of a longest line segment in @math with @math , which we also call the [ ] value, bn of @math . We aim to find a matching under given constraints that minimizes @math . Any such matching is called a [ ] bottleneck matching of @math .\"","summary":"\"A variant of the bichromatic case is the so-called bicolored (or multicolored, when there are arbitrary many colors) case, where only the points of the same color are allowed to be matched. Abu-Affash, Bhore and Carmi in @cite_12 examined bicolored matchings that minimize the number of crossings between edges matching different color sets. They presented an algorithm to compute a bottleneck matching of points in convex position among all matchings that have no crossings of this kind.\"","":""}
{"id":"2963063929","dialogue":"\"Embedded devices in the Internet of Things (IoT) face a wide variety of security challenges. For example, software attackers perform code injection and code-reuse attacks on their remote interfaces, and physical access to IoT devices allows to tamper with code in memory, steal confidential Intellectual Property (IP), or mount fault attacks to manipulate a CPU's control flow. In this work, we present Sponge-based Control Flow Protection (SCFP). SCFP is a stateful, sponge-based scheme to ensure the confidentiality of software IP and its authentic execution on IoT devices. At compile time, SCFP encrypts and authenticates software with instruction-level granularity. During execution, an SCFP hardware extension between the CPU's fetch and decode stage continuously decrypts and authenticates instructions. Sponge-based authenticated encryption in SCFP yields fine-grained control-flow integrity and thus prevents code-reuse, code-injection, and fault attacks on the code and the control flow. In addition, SCFP withstands any modification of software in memory. For evaluation, we extended a RISC-V core with SCFP and fabricated a real System on Chip (SoC). The average overhead in code size and execution time of SCFP on this design is 19.8 and 9.1 , respectively, and thus meets the requirements of embedded IoT devices.\"","summary":"\". Numerous cfi schemes have been introduced in the last 30 years. However, techniques providing fine-grained cfi and code integrity, as required to detect physical attacks, are quite rare @cite_16 .\"","":""}
{"id":"2963063929","dialogue":"\"Embedded devices in the Internet of Things (IoT) face a wide variety of security challenges. For example, software attackers perform code injection and code-reuse attacks on their remote interfaces, and physical access to IoT devices allows to tamper with code in memory, steal confidential Intellectual Property (IP), or mount fault attacks to manipulate a CPU's control flow. In this work, we present Sponge-based Control Flow Protection (SCFP). SCFP is a stateful, sponge-based scheme to ensure the confidentiality of software IP and its authentic execution on IoT devices. At compile time, SCFP encrypts and authenticates software with instruction-level granularity. During execution, an SCFP hardware extension between the CPU's fetch and decode stage continuously decrypts and authenticates instructions. Sponge-based authenticated encryption in SCFP yields fine-grained control-flow integrity and thus prevents code-reuse, code-injection, and fault attacks on the code and the control flow. In addition, SCFP withstands any modification of software in memory. For evaluation, we extended a RISC-V core with SCFP and fabricated a real System on Chip (SoC). The average overhead in code size and execution time of SCFP on this design is 19.8 and 9.1 , respectively, and thus meets the requirements of embedded IoT devices.\"","summary":"\"Another common approach to enforce cfi is to augment the processor with hardware monitors @cite_7 @cite_40 @cite_4 . Typically, these monitors continuously check that the processor behaves as expected and raise an alert when an error is observed. The disadvantage of this approach is that deciding between correct and incorrect behavior (1-bit of information) effectively introduces a single point of failure for the error detection. As a result, implementing a reliable monitor becomes a challenge on its own. Additionally, these techniques can only provide integrity authenticity and do not offer confidentiality.\"","":""}
{"id":"2963063929","dialogue":"\"Embedded devices in the Internet of Things (IoT) face a wide variety of security challenges. For example, software attackers perform code injection and code-reuse attacks on their remote interfaces, and physical access to IoT devices allows to tamper with code in memory, steal confidential Intellectual Property (IP), or mount fault attacks to manipulate a CPU's control flow. In this work, we present Sponge-based Control Flow Protection (SCFP). SCFP is a stateful, sponge-based scheme to ensure the confidentiality of software IP and its authentic execution on IoT devices. At compile time, SCFP encrypts and authenticates software with instruction-level granularity. During execution, an SCFP hardware extension between the CPU's fetch and decode stage continuously decrypts and authenticates instructions. Sponge-based authenticated encryption in SCFP yields fine-grained control-flow integrity and thus prevents code-reuse, code-injection, and fault attacks on the code and the control flow. In addition, SCFP withstands any modification of software in memory. For evaluation, we extended a RISC-V core with SCFP and fabricated a real System on Chip (SoC). The average overhead in code size and execution time of SCFP on this design is 19.8 and 9.1 , respectively, and thus meets the requirements of embedded IoT devices.\"","summary":"\". tee typically also provide confidentiality and authenticity for code. However, tee operate on a completely different level of granularity than scfp . The authenticated encryption in Intel SGX @cite_19 , for example, only ensures that the code and data in memory is protected against tampering. Physical faults in caches or on processor internal buses, on the other hand, are still possible. Also, SGX does not prevent common software attack techniques like code-reuse attacks within enclaves. Therefore, additional cfi schemes are needed to reach similar properties as scfp for code.\"","":""}
{"id":"2787244584","dialogue":"\"Vectorizing hand-drawn sketches is a challenging task, which is of paramount importance for creating CAD vectorized versions for the fashion and creative workflows. This paper proposes a complete framework that automatically transforms noisy and complex hand-drawn sketches with different stroke types in a precise, reliable and highly-simplified vectorized model. The proposed framework includes a novel line extraction algorithm based on a multi-resolution application of Pearson's cross correlation and a new unbiased thinning algorithm that can get rid of scribbles and variable-width strokes to obtain clean 1-pixel lines. Other contributions include variants of pruning, merging and edge linking procedures to post-process the obtained paths. Finally, a modification of the original Schneider's vectorization algorithm is designed to obtain fewer control points in the resulting Bezier splines. All the proposed steps of the framework have been extensively tested and compared with state-of-the-art algorithms, showing (both qualitatively and quantitatively) its outperformance.\"","summary":"\"This section reports the most relevant previous works on sketch vectorization. @cite_21 proposed a line enhancement method, based on Gabor and Kalman filters. It can be used to enhance lines for subsequent vectorization. However, this approach fails to correctly extract all the drawing components when the image is noisy or presents parallel strokes, resulting, for instance, in gaps in the final vectorized result or strokes incorrectly merged. Moreover, experiments are conducted with quite simple images.\"","":""}
{"id":"2787244584","dialogue":"\"Vectorizing hand-drawn sketches is a challenging task, which is of paramount importance for creating CAD vectorized versions for the fashion and creative workflows. This paper proposes a complete framework that automatically transforms noisy and complex hand-drawn sketches with different stroke types in a precise, reliable and highly-simplified vectorized model. The proposed framework includes a novel line extraction algorithm based on a multi-resolution application of Pearson's cross correlation and a new unbiased thinning algorithm that can get rid of scribbles and variable-width strokes to obtain clean 1-pixel lines. Other contributions include variants of pruning, merging and edge linking procedures to post-process the obtained paths. Finally, a modification of the original Schneider's vectorization algorithm is designed to obtain fewer control points in the resulting Bezier splines. All the proposed steps of the framework have been extensively tested and compared with state-of-the-art algorithms, showing (both qualitatively and quantitatively) its outperformance.\"","summary":"\"@cite_11 reported a first proposal of a framework transforming raw images to full vectorized representations. However, the binarization'' step is not considered at all, by presenting directly the skeleton processing and vectorization steps. In addition to this limitation, this paper also bases the vectorization to the simple fitting of straight lines and circular arcs (instead of using Bezier interpolation), which represents a too simplified and limited representation of the resulting path.\"","":""}
{"id":"2787244584","dialogue":"\"Vectorizing hand-drawn sketches is a challenging task, which is of paramount importance for creating CAD vectorized versions for the fashion and creative workflows. This paper proposes a complete framework that automatically transforms noisy and complex hand-drawn sketches with different stroke types in a precise, reliable and highly-simplified vectorized model. The proposed framework includes a novel line extraction algorithm based on a multi-resolution application of Pearson's cross correlation and a new unbiased thinning algorithm that can get rid of scribbles and variable-width strokes to obtain clean 1-pixel lines. Other contributions include variants of pruning, merging and edge linking procedures to post-process the obtained paths. Finally, a modification of the original Schneider's vectorization algorithm is designed to obtain fewer control points in the resulting Bezier splines. All the proposed steps of the framework have been extensively tested and compared with state-of-the-art algorithms, showing (both qualitatively and quantitatively) its outperformance.\"","summary":"\"@cite_2 provided a more complete study of the whole vectorization process. They provide a neat derivation-based algorithm to estimate accurate centerlines for sketches. They also provide a good insight of the problem of correct junction selection. Unfortunately, they work under the assumption of somewhat clean'' lines, that does not hold in many real case scenarios, such as those we are aiming at.\"","":""}
{"id":"2787244584","dialogue":"\"Vectorizing hand-drawn sketches is a challenging task, which is of paramount importance for creating CAD vectorized versions for the fashion and creative workflows. This paper proposes a complete framework that automatically transforms noisy and complex hand-drawn sketches with different stroke types in a precise, reliable and highly-simplified vectorized model. The proposed framework includes a novel line extraction algorithm based on a multi-resolution application of Pearson's cross correlation and a new unbiased thinning algorithm that can get rid of scribbles and variable-width strokes to obtain clean 1-pixel lines. Other contributions include variants of pruning, merging and edge linking procedures to post-process the obtained paths. Finally, a modification of the original Schneider's vectorization algorithm is designed to obtain fewer control points in the resulting Bezier splines. All the proposed steps of the framework have been extensively tested and compared with state-of-the-art algorithms, showing (both qualitatively and quantitatively) its outperformance.\"","summary":"\"Another recent work, @cite_8 , provided a good proposal for a vectorization system. For the line extraction part they rely on Vector Fields, which give high quality results with clean images, but fail in presence of noise and fuzzy lines. Still, they dedicated a lot of attention to correctly disambiguate junctions and parallel strokes.\"","":""}
{"id":"2787244584","dialogue":"\"Vectorizing hand-drawn sketches is a challenging task, which is of paramount importance for creating CAD vectorized versions for the fashion and creative workflows. This paper proposes a complete framework that automatically transforms noisy and complex hand-drawn sketches with different stroke types in a precise, reliable and highly-simplified vectorized model. The proposed framework includes a novel line extraction algorithm based on a multi-resolution application of Pearson's cross correlation and a new unbiased thinning algorithm that can get rid of scribbles and variable-width strokes to obtain clean 1-pixel lines. Other contributions include variants of pruning, merging and edge linking procedures to post-process the obtained paths. Finally, a modification of the original Schneider's vectorization algorithm is designed to obtain fewer control points in the resulting Bezier splines. All the proposed steps of the framework have been extensively tested and compared with state-of-the-art algorithms, showing (both qualitatively and quantitatively) its outperformance.\"","summary":"\"The sketch vectorization field also partially overlaps with the so-called Coherence Enhancing'' field. @cite_3 estimated Tangent Vector Fields from images, and used them in order to clean or simplify the input. They do that by averaging a pixel value with its corresponding neighbors along the Vector Fields. This could be integrated as a useful preprocessing step in our system, or could be used as a standalone tool if the objective is just to obtain a simplified representation of the input image.\"","":""}
{"id":"2789056124","dialogue":"\"We present a framework for learning efficient holistic representation for handwritten word images. The proposed method uses a deep convolutional neural network with traditional classification loss. The major strengths of our work lie in: (i) the efficient usage of synthetic data to pre-train a deep network, (ii) an adapted version of ResNet-34 architecture with region of interest pooling (referred as HWNet v2) which learns discriminative features with variable sized word images, and (iii) realistic augmentation of training data with multiple scales and elastic distortion which mimics the natural process of handwriting. We further investigate the process of fine-tuning at various layers to reduce the domain gap between synthetic and real domain and also analyze the in-variances learned at different layers using recent visualization techniques proposed in literature. Our representation leads to state of the art word spotting performance on standard handwritten datasets and historical manuscripts in different languages with minimal representation size. On the challenging IAM dataset, our method is first to report an mAP above 0.90 for word spotting with a representation size of just 32 dimensions. Further more, we also present results on printed document datasets in English and Indic scripts which validates the generic nature of the proposed framework for learning word image representation.\"","summary":"\"Learning holistic representation for word images was popularized under the name of word spotting which was first coined by in @cite_74 for indexing handwritten word images. Initial attempts @cite_74 @cite_8 @cite_57 @cite_69 , mostly focused on variable length representations of word images by considering it as a temporal sequence. Most of these methods used profile features @cite_19 @cite_52 which are computed at each column of the word image and are summarized using various pixel level statistics. @cite_18 , used profile features namely vertical profile, upper & lower word profile, and background to ink transitions. @cite_19 , used profile features along with moments based features. @cite_8 , used a combination of profile, structural, and transfer domain (Discrete Fourier Transform) features for word image representation. Dynamic Time Warping () based algorithms were found to be useful for matching variable length representations and is quite popular in speech @cite_41 @cite_24 and other sequence matching problems. @cite_25 @cite_28 , profile features were combined with shape based structural features for a partial matching scheme using . Although these features are fast to compute, it's susceptible to noise and common degradations present in documents and thereby limiting to a reasonable quality of printed documents.\"","":""}
{"id":"2789056124","dialogue":"\"We present a framework for learning efficient holistic representation for handwritten word images. The proposed method uses a deep convolutional neural network with traditional classification loss. The major strengths of our work lie in: (i) the efficient usage of synthetic data to pre-train a deep network, (ii) an adapted version of ResNet-34 architecture with region of interest pooling (referred as HWNet v2) which learns discriminative features with variable sized word images, and (iii) realistic augmentation of training data with multiple scales and elastic distortion which mimics the natural process of handwriting. We further investigate the process of fine-tuning at various layers to reduce the domain gap between synthetic and real domain and also analyze the in-variances learned at different layers using recent visualization techniques proposed in literature. Our representation leads to state of the art word spotting performance on standard handwritten datasets and historical manuscripts in different languages with minimal representation size. On the challenging IAM dataset, our method is first to report an mAP above 0.90 for word spotting with a representation size of just 32 dimensions. Further more, we also present results on printed document datasets in English and Indic scripts which validates the generic nature of the proposed framework for learning word image representation.\"","summary":"\"With the popularity of the local gradient features such as @cite_33 , @cite_20 which describes a patch using histograms of edge orientations computed from a gradient image, the features are less susceptible to stray pixels and variations in brightness and contrast. Methods such as @cite_69 @cite_57 adapted local gradient features for word spotting where @cite_69 used a continuous algorithm for partial word matching from the line images and @cite_57 used Hidden Markov Model () based classification method. Most of the features discussed above are not robust to different fonts, writing styles and required careful image pre-processing techniques such as binarization, slant and skew correction which remain hard for handwritten and historical documents. Moreover, the methods such as and based scheme of matching variable length representations do not scale to large datasets due to the higher time complexity. Hence, the later methods appreciated more on fixed length representations built on top of highly engineered features proposed in computer vision.\"","":""}
{"id":"2743557796","dialogue":"\"This paper introduces a new family of models of intensional Martin-L \"\"of type theory. We use constructive ordered algebra in toposes. Identity types in the models are given by a notion of Moore path. By considering a particular gros topos","summary":"we show that there is such a model that is non-truncated","":""}
{"id":"2743557796","dialogue":"\"This paper introduces a new family of models of intensional Martin-L \"\"of type theory. We use constructive ordered algebra in toposes. Identity types in the models are given by a notion of Moore path. By considering a particular gros topos","summary":"we show that there is such a model that is non-truncated","":""}
{"id":"2743557796","dialogue":"\"This paper introduces a new family of models of intensional Martin-L \"\"of type theory. We use constructive ordered algebra in toposes. Identity types in the models are given by a notion of Moore path. By considering a particular gros topos","summary":"we show that there is such a model that is non-truncated","":""}
{"id":"2787462651","dialogue":"\"To build a satisfying chatbot that has the ability of managing a goal-oriented multi-turn dialogue, accurate modeling of human conversation is crucial. In this paper we concentrate on the task of response selection for multi-turn human-computer conversation with a given context. Previous approaches show weakness in capturing information of rare keywords that appear in either or both context and correct response, and struggle with long input sequences. We propose Cross Convolution Network (CCN) and Multi Frequency word embedding to address both problems. We train several models using the Ubuntu Dialogue dataset which is the largest freely available multi-turn based dialogue corpus. We further build an ensemble model by averaging predictions of multiple models. We achieve a new state-of-the-art on this dataset with considerable improvements compared to previous best results.\"","summary":"\"The problem of next response selection in multi-turn conversation is more general than a traditional question answering (QA) problem @cite_9 @cite_19 . The prediction is made based on the entire conversation context which does not necessarily include a question. In single turn response selection, the model ignores the entire context and only leverages the last utterance to select response @cite_4 @cite_3 @cite_25 . Since an utterance can change the topic or negate affirm the previous utterances, it is of paramount importance that models for response selection in multi-turn conversation have a certain understanding of the entire context. Moreover, next response selection system is a supervised dialogue system since it incorporates explicit signals specifying whether the provided response is correct or not @cite_23 . This system is of interest because it admits a natural evaluation metric, namely the recall and precision measures (See for a detailed explanation.). We consider Ubuntu Dialogue Corpus @cite_1 to evaluate our retrieval-based model since the dataset is the most relevant public dataset to supervised dialogue systems @cite_23 .\"","":""}
{"id":"2787462651","dialogue":"\"To build a satisfying chatbot that has the ability of managing a goal-oriented multi-turn dialogue, accurate modeling of human conversation is crucial. In this paper we concentrate on the task of response selection for multi-turn human-computer conversation with a given context. Previous approaches show weakness in capturing information of rare keywords that appear in either or both context and correct response, and struggle with long input sequences. We propose Cross Convolution Network (CCN) and Multi Frequency word embedding to address both problems. We train several models using the Ubuntu Dialogue dataset which is the largest freely available multi-turn based dialogue corpus. We further build an ensemble model by averaging predictions of multiple models. We achieve a new state-of-the-art on this dataset with considerable improvements compared to previous best results.\"","summary":"\"The original paper that introduced the Ubuntu Dialogue dataset have implemented a TF-IDF model in addition to neural network models with vanilla RNN and LSTM @cite_1 . Later, evaluated the performances of various LSTMs, Bi-LSTMs and CNNs (Convolutional Neural Networks @cite_11 ) on the dataset and created an ensemble by averaging predictions of multiple models. An RNN-CNN model combined with attention vectors is implemented by . Further, Multi-view Response Selection @cite_13 proposed an RNN-CNN model which integrates information from both word sequence view and utterance sequence view. A deep learning model incorporating background knowledge to enhance the sequence semantic modeling ability of LSTM is implemented in that achieved the state-of-the-art result.\"","":""}
{"id":"2786634640","dialogue":"\"Over the past few years","summary":"a number of new \"\"fringe\"\" communities","":""}
{"id":"2786634640","dialogue":"\"Over the past few years","summary":"a number of new \"\"fringe\"\" communities","":""}
{"id":"2786634640","dialogue":"\"Over the past few years","summary":"a number of new \"\"fringe\"\" communities","":""}
{"id":"2785761199","dialogue":"\"Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at this https URL.\"","summary":"\"define additive functions over embeddings. In many translational approaches, the embedding for each entity @math is a single vector @math and the embedding for each relation @math is a vector @math and two matrices @math and @math . The dissimilarity function for a triple @math is defined as @math (i.e. encouraging @math ) where @math represents norm @math of vector @math . Translational approaches having this dissimilarity function usually differ on the restrictions they impose on @math and @math . In TransE @cite_39 , @math , @math . In TransR @cite_32 , @math . In STransE @cite_4 , no restrictions are imposed on the matrices. FTransE @cite_13 , slightly changes the dissimilarity function defining it as @math for a value of @math that minimizes the norm for each triple. In the rest of the paper, we let represent the FTransE model where no restrictions are imposed over @math and @math .\"","":""}
{"id":"2785761199","dialogue":"\"Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at this https URL.\"","summary":"\"generally use a neural network that learns how the head, relation, and tail embeddings interact. E-MLP @cite_15 considers the embeddings for each entity @math to be a vector @math , and for each relation @math to be a matrix @math and a vector @math . To make a prediction about a triple @math , E-MLP feeds @math into a two-layer neural network whose weights for the first layer are the matrix @math and for the second layer are @math . ER-MLP @cite_2 , considers the embeddings for both entities and relations to be single vectors and feeds @math into a two layer neural network. In @cite_16 , once the entity vectors are provided by the convolutional neural network and the relation vector is provided by the long-short time memory network, for each triple the vectors are concatenated similar to ER-MLP and are fed into a four-layer neural network. Neural tensor network (NTN) @cite_15 combines E-MLP with several bilinear parts (see Subsection for a definition of bilinear models).\"","":""}
{"id":"2785525924","dialogue":"\"Many applications require categorization of text documents using predefined categories. The main approach to performing text categorization is learning from labeled examples. For many tasks, it may be difficult to find examples in one language but easy in others. The problem of learning from examples in one or more languages and classifying (categorizing) in another is called cross-lingual learning. In this work, we present a novel approach that solves the general cross-lingual text categorization problem. Our method generates, for each training document, a set of language-independent features. Using these features for training yields a language-independent classifier. At the classification stage, we generate language-independent features for the unlabeled document, and apply the classifier on the new representation. To build the feature generator, we utilize a hierarchical language-independent ontology, where each concept has a set of support documents for each language involved. In the preprocessing stage, we use the support documents to build a set of language-independent feature generators, one for each language. The collection of these generators is used to map any document into the language-independent feature space. Our methodology works on the most general cross-lingual text categorization problems, being able to learn from any mix of languages and classify documents in any other language. We also present a method for exploiting the hierarchical structure of the ontology to create virtual supporting documents for languages that do not have them. We tested our method, using Wikipedia as our ontology, on the most commonly used test collections in cross-lingual text categorization, and found that it outperforms existing methods.\"","summary":"\"Some work circumvents the main problem of CLTC by using a black-box machine translation tool, such as Google Translate and others, to translate whole documents @cite_11 @cite_21 . The translated labeled documents are then used, perhaps with additional labeled documents that were originally in the target language, to learn a target-language classifier. Some works assume the availability of unlabeled target language documents, and use the tool to translate them into the source language @cite_0 , thus transforming the problem into a semi-supervised learning task.\"","":""}
{"id":"2785525924","dialogue":"\"Many applications require categorization of text documents using predefined categories. The main approach to performing text categorization is learning from labeled examples. For many tasks, it may be difficult to find examples in one language but easy in others. The problem of learning from examples in one or more languages and classifying (categorizing) in another is called cross-lingual learning. In this work, we present a novel approach that solves the general cross-lingual text categorization problem. Our method generates, for each training document, a set of language-independent features. Using these features for training yields a language-independent classifier. At the classification stage, we generate language-independent features for the unlabeled document, and apply the classifier on the new representation. To build the feature generator, we utilize a hierarchical language-independent ontology, where each concept has a set of support documents for each language involved. In the preprocessing stage, we use the support documents to build a set of language-independent feature generators, one for each language. The collection of these generators is used to map any document into the language-independent feature space. Our methodology works on the most general cross-lingual text categorization problems, being able to learn from any mix of languages and classify documents in any other language. We also present a method for exploiting the hierarchical structure of the ontology to create virtual supporting documents for languages that do not have them. We tested our method, using Wikipedia as our ontology, on the most commonly used test collections in cross-lingual text categorization, and found that it outperforms existing methods.\"","summary":"\"The mapping from the source and target languages to the intermediate representation is usually learned from some external source. For Wikipedia concepts, usually the source is the text of the articles @cite_9 @cite_18 @cite_10 @cite_4 . Topic-based features and corresponding features are typically inferred from comparable corpora @cite_12 . Mapping from each language to these language-independent features is commonly learned either from the original labeled training set @cite_14 @cite_3 @cite_15 , or from independent (unlabeled) corpora in the required language @cite_13 @cite_6 .\"","":""}
{"id":"2786958491","dialogue":"\"In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67 absolute accuracy on CIFAR-10 and gains 0.24 absolute accuracy on SVHN. Our model's performance is a substantial improvement compared to the current state of the art solution.\"","summary":"\"@cite_5 , authors train FearNet's long-term memory store with a process called intrinsic replay @cite_7 . This process is similar to our recursive training method, where pseudo-items are generated by the decoder and then rehearsed with the new items. Our generative model is isolated from the classification network, whereas FearNet combines these models. This means that when the long-term system is being trained, the classification loss and reconstruction loss are minimised concurrently. A further difference is that the reconstruction loss is minimised across each layer of the autoencoder. This limitation means that the function of neurons in intermediate layers of the network are being constrained.\"","":""}
{"id":"2786958491","dialogue":"\"In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67 absolute accuracy on CIFAR-10 and gains 0.24 absolute accuracy on SVHN. Our model's performance is a substantial improvement compared to the current state of the art solution.\"","summary":"\"FearNet was shown to counteract CF on supposedly complex tasks such as CIFAR-100. However, FearNet was never trained on the CIFAR-100's raw images but rather the output of the first 49 weight layers (including the mean pooling layer) in ResNet-50 @cite_11 that had been pre-trained on ImageNet. This means that the classification is done by a much smaller multi-layer perceptron and the majority of the work has been pre-trained into the ResNet architecture. Subsequently, the autoencoder is not learning to reproduce CIFAR-100 images but rather the items' output from the ResNet architecture. Although this is a more difficult task than training on MNIST, this method is incomparable to training a convolutional neural network to classify the task from raw input.\"","":""}
{"id":"2786958491","dialogue":"\"In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67 absolute accuracy on CIFAR-10 and gains 0.24 absolute accuracy on SVHN. Our model's performance is a substantial improvement compared to the current state of the art solution.\"","summary":"\"A recent review of methods for overcoming CF concluded that current algorithms do not solve CF @cite_14 . They also found that EWC performed the best for learning multiple tasks and thus, we compare pseudo-recursal to EWC. The main contributions of our paper are; we show that pseudo-recursal can be used to overcome the CF problem in DNNs, sequentially learning CIFAR-10, SVHN and MNIST and we demonstrate that pseudo-rehearsal can be applied recursively to a separate classification and generative model. This architecture also satisfies the previously mentioned criteria. Although we limit ourselves to image classification in this paper, our techniques are applicable to other problems.\"","":""}
{"id":"2787105875","dialogue":"\"Ontology learning (OL) is the process of automatically generating an ontological knowledge base from a plain text document. In this paper, we propose a new ontology learning approach and tool, called DLOL, which generates a knowledge base in the description logic (DL) SHOQ(D) from a collection of factual non-negative IS-A sentences in English. We provide extensive experimental results on the accuracy of DLOL, giving experimental comparisons to three state-of-the-art existing OL tools, namely Text2Onto, FRED, and LExO. Here, we use the standard OL accuracy measure, called lexical accuracy, and a novel OL accuracy measure, called instance-based inference model. In our experimental results, DLOL turns out to be about 21 and 46 , respectively, better than the best of the other three approaches.\"","summary":"\"There has been significant literature over the last decade on the problem of Ontology Learning (OL). Most of these works can be categorized into two approaches as discussed earlier: (i) , and (ii) . Light-weight OL from text documents is arguably the most widely used approach in the field of OL @cite_35 . It can be further divided into three general approaches: (i) , (ii) , and (iii) . It is to be noted that the general disadvantage of light-weight OL of not being able to generate definitional T-Box (and corresponding A-Box), as discussed in the introduction section, is inherent in all the three approaches. This is where @math parts away significantly from light-weight OL approaches. We first discuss light-weight OL in the following three sub-sections for providing a contrasting perspective, and then conclude the section with a discussion on formal OL.\"","":""}
{"id":"2787105875","dialogue":"\"Ontology learning (OL) is the process of automatically generating an ontological knowledge base from a plain text document. In this paper, we propose a new ontology learning approach and tool, called DLOL, which generates a knowledge base in the description logic (DL) SHOQ(D) from a collection of factual non-negative IS-A sentences in English. We provide extensive experimental results on the accuracy of DLOL, giving experimental comparisons to three state-of-the-art existing OL tools, namely Text2Onto, FRED, and LExO. Here, we use the standard OL accuracy measure, called lexical accuracy, and a novel OL accuracy measure, called instance-based inference model. In our experimental results, DLOL turns out to be about 21 and 46 , respectively, better than the best of the other three approaches.\"","summary":"\"Research on light-weight OL heavily draws models and methodologies from the area of statistical NLP with the assumption of . The central objective is to model a similarity measure for concept comparison, thereby creating semantic spaces derived from two alternative models: (i) vector-space models (VSM), and (ii) probabilistic models. In VSM approaches, concept similarity is computed using distance (or similarity) measures in high-dimensional concept vector space. Such computation can be based on: (i) (such as cosine similarity @cite_42 , Jaccard similarity @cite_78 ), (ii) based (such as term-document based matrix models like LSA @cite_74 or term-context terms based matrix models like HAL @cite_79 ). Similar concepts are then clustered into taxonomies using variations of known hierarchical clustering algorithms. An early work on OL as an end-to-end application was proposed in @cite_51 @cite_65 where noun terms were organized into hypernymic trees using bottom-up hierarchical clustering algorithm. In @cite_68 a clustering based algorithm called CBC was proposed for generating concept lists of similar instances with sense discrimination. However, such lists were unlabeled and hence, not useful in applications such as question-answering as observed in @cite_85 .\"","":""}
{"id":"2787428865","dialogue":"\"Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers expose the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to 10 programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.\"","summary":"\"Doppio @cite_41 and Whalesong @cite_46 implement bytecode interpreters in the browser that do not use the JavaScript stack. Therefore, they can suspend and resume execution. However, since these are bytecode interpreters for other platforms (JVM and Racket, respectively), existing compilers and libraries would have to change significantly to use them. Browsix @cite_15 acts as an operating system'' for processes in Web Workers. Therefore, it inherits Web Workers' restrictions: workers cannot share JavaScript values and cannot interact with the Web page. It also does not provide deep stacks. allows code to run in the main browser thread, enabling access to the DOM and allowing execution control for IDEs.\"","":""}
{"id":"2787428865","dialogue":"\"Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers expose the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to 10 programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.\"","summary":"\"Pivot @cite_59 isolates untrusted JavaScript into an iframe and rewrites the program to use generators, which allows blocking I O between the iframe and the outside world. is not an isolation framework and implements blocking without generators or ( sec:callcc ).\"","":""}
{"id":"2787428865","dialogue":"\"Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers expose the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to 10 programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.\"","summary":"\"Many past projects have investigated implementing continuations in other platforms that do not natively support them, from C to .NET @cite_13 @cite_47 @cite_81 @cite_25 @cite_32 @cite_83 @cite_18 . They use a variety of strategies ranging from with trampolines, to C's |setjmp| and |longjmp| to effectively provide tail calls. These systems do not provide 's other features ( intro ).\"","":""}
{"id":"2964349548","dialogue":"\"AbstractWe solve the Milne, constant-source, and albedo problems for isotropic scattering in a two-dimensional “Flatland” half-space via the Wiener–Hopf method. The Flatland H-function is derived a...\"","summary":"\"Infinite media problems have been well studied in Flatland as well as spaces of general dimension @cite_20 @cite_4 @cite_15 @cite_13 @cite_14 @cite_18 @cite_26 @cite_5 @cite_6 and for beams @cite_23 . Some exact solutions have been presented for bounded @cite_25 and layered @cite_24 media, and the singular eigenfunctions for Flatland have been derived @cite_3 . However, to the best of the authors' knowledge, solutions to the classic Milne and albedo problems for the half-space and the @math -function have not been presented. @cite_19 , using an asymptotic analysis, have presented solutions to the classic Milne and albedo problems in two dimensions. The solutions make use of the Flatland equivalent of Chandrasekhar's @math -function, which is left as the solution to an integral equation. We present a complementary derivation of the Milne and albedo problem solutions using the Wiener-Hopf technique. In addition, we present new solutions and benchmark values for @math and provide some Monte Carlo comparisons for the albedo problem.\"","":""}
{"id":"2964349548","dialogue":"\"AbstractWe solve the Milne, constant-source, and albedo problems for isotropic scattering in a two-dimensional “Flatland” half-space via the Wiener–Hopf method. The Flatland H-function is derived a...\"","summary":"\"In the study of energy-dependent neutron transport in three-dimensional volumes with plane symmetry, @cite_0 presented a general family of solutions to the Milne problem using the method of singular eigenfunctions. When their variable- @math factor @math takes on the specific quantity (in our notation) @math , their energy-dependent 3D solution becomes equivalent to our monoenergetic Flatland solution. Thus, the Flatland @math -function has, if only inadvertently, been presented long ago.\"","":""}
{"id":"2963504418","dialogue":"\"Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. We propose a new method to estimate piecewise affine motion fields directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient 1D piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost, which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation.\"","summary":"\"The differences between existing methods concern mainly the solver for the image-partitioning problem. In a continuous setting, following the approach of Mumford and Shah @cite_37 , the problem has been addressed with an implicit level-set representation of the partitioning curve in @cite_29 @cite_19 @cite_48 . A primal-dual optimization strategy was used in @cite_8 . In a discrete setting, iterated conditional modes and high confidence first approaches were exploited in @cite_38 @cite_20 @cite_55 . Graph-cuts methods have also been used in @cite_41 , and more recently in @cite_36 . Layered models, introduced in @cite_51 , involve a similar optimization problem but add a depth information between the different regions, from which occlusions can be derived. This model has been revitalized in @cite_21 @cite_15 @cite_30 @cite_23 .\"","":""}
{"id":"2963504418","dialogue":"\"Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. We propose a new method to estimate piecewise affine motion fields directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient 1D piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost, which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation.\"","summary":"\"The importance of initialization when optimizing ) with an alternating scheme is illustrated in @cite_15 @cite_8 , where the optimization is initialized through advanced motion estimation methods @cite_13 @cite_3 . In @cite_34 , an alternating direction method of multipliers (ADMM) approach is used to solve ) without intermediate segmentation steps. However, the underlying model is piecewise-constant and not rich enough in most practical scenarios; it is initialized by a block matching algorithm.\"","":""}
{"id":"2963504418","dialogue":"\"Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. We propose a new method to estimate piecewise affine motion fields directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient 1D piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost, which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation.\"","summary":"\"Most of the computational effort is spent on the image-partitioning problem. The earliest works retain at most five regions to make the problem tractable . More recently, the layered approach handles a larger number of regions but requires several hours of computation, and the primal dual approach can take up to one hour despite a GPU implementation. The method proposed in @cite_36 achieves around fifteen minutes for @math image, with a graph cut minimization approach.\"","":""}
{"id":"2963504418","dialogue":"\"Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. We propose a new method to estimate piecewise affine motion fields directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient 1D piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost, which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation.\"","summary":"\"Beyond solving ), other techniques can be involved to improve the results. They include the handling of occlusions @cite_44 @cite_20 @cite_8 , label cost terms to limit the number of regions @cite_8 @cite_36 , edge-driven models to fit image boundaries @cite_19 , deviations from the parametric models to estimate more complex deformations @cite_21 , smoothness of the parameters of neighboring regions @cite_36 , or post-processing refinements with a variational optimization of TV-based models @cite_36 . Yet other methods rely on similar principles but incorporate additional information obtained from their applicative context, like epipolar constraints @cite_27 @cite_7 , temporal consistency @cite_27 , or semantic information about the type of moving objects in the scene @cite_23 .\"","":""}
{"id":"2963504418","dialogue":"\"Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. We propose a new method to estimate piecewise affine motion fields directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient 1D piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost, which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation.\"","summary":"\"Extensions of TV to second order derivatives result in approximately piecewise-affine solutions @cite_5 @cite_22 . However, the @math norm does not delineate moving objects as sharply as the Mumford-Shah model ). In this line, the over-parametrized approach @cite_11 @cite_6 , which models a spatially varying parameter field with TV regularization, also shows this undesirable effect.\"","":""}
{"id":"2591681106","dialogue":"\"With progress in enabling autonomous cars to drive safely on the road, it is time to start asking how they should be driving. A common answer is that they should be adopting their users' driving style. This makes the assumption that users want their autonomous cars to drive like they drive - aggressive drivers want aggressive cars, defensive drivers want defensive cars. In this paper, we put that assumption to the test. We find that users tend to prefer a significantly more defensive driving style than their own. Interestingly, they prefer the style they think is their own, even though their actual driving style tends to be more aggressive. We also find that preferences do depend on the specific driving scenario, opening the door for new ways of learning driving style preference.\"","summary":"\"The typical behavioral patterns of a driver are usually referred to by the term . This includes the choice of driving speed, headway, overtaking of other vehicles, or the tendency to commit traffic violations @cite_16 .\"","":""}
{"id":"2591681106","dialogue":"\"With progress in enabling autonomous cars to drive safely on the road, it is time to start asking how they should be driving. A common answer is that they should be adopting their users' driving style. This makes the assumption that users want their autonomous cars to drive like they drive - aggressive drivers want aggressive cars, defensive drivers want defensive cars. In this paper, we put that assumption to the test. We find that users tend to prefer a significantly more defensive driving style than their own. Interestingly, they prefer the style they think is their own, even though their actual driving style tends to be more aggressive. We also find that preferences do depend on the specific driving scenario, opening the door for new ways of learning driving style preference.\"","summary":"\"Defensiveness-aggressiveness is the most commonly used metric for defining driving style. Prior work refers to drivers as aggressive assertive versus defensive @cite_20 ; or mild versus moderate versus aggressive @cite_4 . In the Multidimensional Driving Style Inventory (MDSI), Taubman-Ben-Ari identified four broad driving styles: (1) reckless and careless driving, characterized by, for example, higher speed; (2) anxious driving; (3) angry and hostile driving, characterized by more use of the horn and flash functionality; and (4) patient and careful driving @cite_2 . Similarly, Huysduynen categorized driving style as angry driving, anxious driving, dissociative driving, distress-reduction driving and careful driving style @cite_16 . Horswill provided a valuable distinction between skill and style in the context of driving behaviors @cite_11 . Hong @cite_7 differentiated styles in terms of defensiveness, as well as by propensity for violation of rules. Scherer defined driving style in terms of comfort @cite_23 . Lee @cite_17 analyzed lane changes as a function of its severity (degree to which the vehicle in the destination lane was cut off), urgency (how soon the lane change was needed), and type classification for the full population of 8,667 lane changes.\"","":""}
{"id":"2787472298","dialogue":"\"Text summarization condenses a text to a shorter version while retaining the important informations. Abstractive summarization is a recent development that generates new phrases, rather than simply copying or rephrasing sentences within the original text. Recently neural sequence-to-sequence models have achieved good results in the field of abstractive summarization, which opens new possibilities and applications for industrial purposes. However, most practitioners observe that these models still use large parts of the original text in the output summaries, making them often similar to extractive frameworks. To address this drawback, we first introduce a new metric to measure how much of a summary is extracted from the input text. Secondly, we present a novel method, that relies on a diversity factor in computing the neural network loss, to improve the diversity of the summaries generated by any neural abstractive model implementing beam search. Finally, we show that this method not only makes the system less extractive, but also improves the overall rouge score of state-of-the-art methods by at least 2 points.\"","summary":"\"DBS has been used in multiple topics, such as dialogue response generation @cite_16 , machine translation @cite_5 , but also abstractive summarization @cite_5 . However, DBS on its own contributes only marginally (+0.25 @math -score) to the performance of abstractive summarization. This is why we combine it with a candidate selection algorithm used in multiple fields, Maximal Marginal Relevance (MMR) @cite_21 @cite_24 . MMR is an algorithm that balances relevance and diversity in multiple set-based information retrieval tasks.\"","":""}
{"id":"2787711783","dialogue":"\"Existing text generation methods tend to produce repeated and \"\"boring\"\" expressions. To tackle this problem","summary":"we propose a new text generation model","":""}
{"id":"2787728049","dialogue":"\"The well-known @math -disjoint path problem ( @math -DPP) asks for pairwise vertex-disjoint paths between @math specified pairs of vertices @math in a given graph","summary":"if they exist. The decision version of the shortest @math -DPP asks for the length of the shortest (in terms of total length) such paths. Similarly the search and counting versions ask for one such and the number of such shortest set of paths","":""}
{"id":"2787154142","dialogue":"\"Deep neural networks (DNNs) have achieved exceptional performances in many tasks, particularly, in supervised classification tasks. However, achievements with supervised classification tasks are based on large datasets with well-separated classes. Typically, real-world applications involve wild datasets that include similar classes; thus, evaluating similarities between classes and understanding relations among classes are important. To address this issue, a similarity metric, ClassSim, based on the misclassification ratios of trained DNNs is proposed herein. We conducted image recognition experiments to demonstrate that the proposed method provides better similarities compared with existing methods and is useful for classification problems. Source code including all experimental results is available at this https URL\"","summary":"\"Many methods to compute similarity between images have been proposed. Recently, DNNs have been used to extract image features to compute similarities @cite_13 @cite_14 . For example, DNNs based similarities have been applied to image retrieval @cite_10 , person reidentification @cite_8 , facial recognition @cite_7 , and visual similarity for product design @cite_0 .\"","":""}
{"id":"2787154142","dialogue":"\"Deep neural networks (DNNs) have achieved exceptional performances in many tasks, particularly, in supervised classification tasks. However, achievements with supervised classification tasks are based on large datasets with well-separated classes. Typically, real-world applications involve wild datasets that include similar classes; thus, evaluating similarities between classes and understanding relations among classes are important. To address this issue, a similarity metric, ClassSim, based on the misclassification ratios of trained DNNs is proposed herein. We conducted image recognition experiments to demonstrate that the proposed method provides better similarities compared with existing methods and is useful for classification problems. Source code including all experimental results is available at this https URL\"","summary":"\"However, a method to estimate the similarities between classes has been proposed @cite_3 @cite_4 . In that method, images are divided into patches, and features are extracted from each patch using traditional methods, such as RGB color moment. In addition, to compute the distance between classes, we must assume that the images are generated from Gaussian mixture models (GMMs). Note that the number of GMM components must be determined manually relative to the number of target classes. In addition, the distance between classes expresses an inverse relation with similarities; they are not normalized, and their absolute values are meaningless. Here, two distances are involved, i.e., parametric distance (PD), which is the quadratic distance of the means and variances of a GMM, and an approximation of KL divergence. These two methods return similar results. Here, strong assumptions and simplifications were used to treat inter-class similarities realistically.\"","":""}
{"id":"2787154142","dialogue":"\"Deep neural networks (DNNs) have achieved exceptional performances in many tasks, particularly, in supervised classification tasks. However, achievements with supervised classification tasks are based on large datasets with well-separated classes. Typically, real-world applications involve wild datasets that include similar classes; thus, evaluating similarities between classes and understanding relations among classes are important. To address this issue, a similarity metric, ClassSim, based on the misclassification ratios of trained DNNs is proposed herein. We conducted image recognition experiments to demonstrate that the proposed method provides better similarities compared with existing methods and is useful for classification problems. Source code including all experimental results is available at this https URL\"","summary":"\"Open set classification problems @cite_19 are inherent and difficult in real-world applications. Thus, few studies have addressed such problems.\"","":""}
{"id":"2787154142","dialogue":"\"Deep neural networks (DNNs) have achieved exceptional performances in many tasks, particularly, in supervised classification tasks. However, achievements with supervised classification tasks are based on large datasets with well-separated classes. Typically, real-world applications involve wild datasets that include similar classes; thus, evaluating similarities between classes and understanding relations among classes are important. To address this issue, a similarity metric, ClassSim, based on the misclassification ratios of trained DNNs is proposed herein. We conducted image recognition experiments to demonstrate that the proposed method provides better similarities compared with existing methods and is useful for classification problems. Source code including all experimental results is available at this https URL\"","summary":"\"However, a solution that employs features extracted using a DNN and meta-recognition has been proposed @cite_1 . This solution is useful to eliminate dissimilar unknown unknowns and is, in particular, effective for fooling images.\"","":""}
{"id":"2964083189","dialogue":"\"We present a framework in Isabelle for verifying asymptotic time complexity of imperative programs. We build upon an extension of Imperative HOL and its separation logic to include running time. Our framework is able to handle advanced techniques for time complexity analysis, such as the use of the Akra–Bazzi theorem and amortized analysis. Various automation is built and incorporated into the auto2 prover to reason about separation logic with time credits, and to derive asymptotic behaviour of functions. As case studies, we verify the asymptotic time complexity (in addition to functional correctness) of imperative algorithms and data structures such as median of medians selection, Karatsuba’s algorithm, and splay trees.\"","summary":"\"They present several examples including binary search, the Bellman--Ford algorithm and union-find. We provide several advanced examples including those involving the Akra--Bazzi method. We also demonstrate that verification of amortized analysis of functional programs @cite_8 can be converted to verification of imperative programs with little additional effort.\"","":""}
{"id":"2964083189","dialogue":"\"We present a framework in Isabelle for verifying asymptotic time complexity of imperative programs. We build upon an extension of Imperative HOL and its separation logic to include running time. Our framework is able to handle advanced techniques for time complexity analysis, such as the use of the Akra–Bazzi theorem and amortized analysis. Various automation is built and incorporated into the auto2 prover to reason about separation logic with time credits, and to derive asymptotic behaviour of functions. As case studies, we verify the asymptotic time complexity (in addition to functional correctness) of imperative algorithms and data structures such as median of medians selection, Karatsuba’s algorithm, and splay trees.\"","summary":"\"@cite_18 present TiML, a functional programming language which can be annotated by invariants and specifically also with time complexity annotations in types. The type checker extracts verification conditions from these programs, which are handled by an SMT solver. They also make the observation that annotational burden can be lowered by not providing a closed form for a time bound, but only specifying its asymptotic behaviour. For recursive functions, the generated VCs include a recurrence (e.g. @math ) and one is left to show that there exists a solution for @math which is additionally in some asymptotic bound, e.g. @math . By employing an recurrence solver based on heuristic pattern matching they make use of the Master Theorem in order to discharge such VCs. In that manner they are able to verify the asymptotic complexity of merge sort. Additionally they can handle amortized complexity, giving Dynamic Arrays and Functional Queues as examples. Several parts of their work rely on non-verified components, including the use of SMT solvers and the pattern matching for recurrence relations. In contrast, our work is verified throughout by Isabelle's kernel.\"","":""}
{"id":"2964083189","dialogue":"\"We present a framework in Isabelle for verifying asymptotic time complexity of imperative programs. We build upon an extension of Imperative HOL and its separation logic to include running time. Our framework is able to handle advanced techniques for time complexity analysis, such as the use of the Akra–Bazzi theorem and amortized analysis. Various automation is built and incorporated into the auto2 prover to reason about separation logic with time credits, and to derive asymptotic behaviour of functions. As case studies, we verify the asymptotic time complexity (in addition to functional correctness) of imperative algorithms and data structures such as median of medians selection, Karatsuba’s algorithm, and splay trees.\"","summary":"\"On the other end of the scale we want to mention Automatic Amortized Resource Analysis (AARA). Possibly the first example of a resource analysis logic based on potentials is due to Hofmann and Jost @cite_19 . They pioneer the use of potentials coded into the type system in order to automatically extract bounds in the runtime of functional programs. successfully developed this idea further @cite_2 @cite_10 . @cite_5 @cite_6 extend this work to imperative programs and automatically solve extracted inequalities by efficient off-the-shelf LP-solvers. While the potentials involved are restricted to a specific shape, the analysis performs well and at the same time generates Coq proof objects certifying their resulting bounds.\"","":""}
{"id":"2786092739","dialogue":"\"In this paper, we propose a simple and effective geometric model fitting method to fit and segment multi-structure data even in the presence of severe outliers. We cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs. Specifically, a hypergraph is first constructed, where the vertices represent model hypotheses and the hyperedges denote data points. The hypergraph involves higher-order similarities (instead of pairwise similarities used on a simple graph), and it can characterize complex relationships between model hypotheses and data points. In addition, we develop a hypergraph reduction technique to remove “insignificant” vertices while retaining as many “significant” vertices as possible in the hypergraph. Based on the simplified hypergraph, we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time. Finally, the proposed mode-seeking algorithm detects modes according to two key elements, i.e., the weighting scores of vertices and the similarity analysis between vertices. Overall, the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously. Experimental results demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both synthetic data and real images.\"","summary":"\"Recently, some hypergraph based methods, e.g., @cite_15 @cite_6 @cite_32 @cite_29 , have been proposed for robust model fitting due to its effectiveness . For example, Liu and Yan @cite_6 proposed the random consensus graph (RCG) to fit multiple structures in data. @cite_29 proposed to use large hyperedges for face clustering and motion segmentation.\"","":""}
{"id":"2786092739","dialogue":"\"In this paper, we propose a simple and effective geometric model fitting method to fit and segment multi-structure data even in the presence of severe outliers. We cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs. Specifically, a hypergraph is first constructed, where the vertices represent model hypotheses and the hyperedges denote data points. The hypergraph involves higher-order similarities (instead of pairwise similarities used on a simple graph), and it can characterize complex relationships between model hypotheses and data points. In addition, we develop a hypergraph reduction technique to remove “insignificant” vertices while retaining as many “significant” vertices as possible in the hypergraph. Based on the simplified hypergraph, we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time. Finally, the proposed mode-seeking algorithm detects modes according to two key elements, i.e., the weighting scores of vertices and the similarity analysis between vertices. Overall, the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously. Experimental results demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both synthetic data and real images.\"","summary":"\"Compared with the hypergraph constructed in the previous methods (e.g., @cite_15 @cite_6 @cite_32 @cite_29 ), where a hyperedge is constrained to connect with a fixed number of vertices, the hyperedge of hypergraphs constructed in this paper can connect with a varying number of vertices (that is we construct non-uniform hypergraphs as those in @cite_0 ) . In addition, the vertices of the hypergraph constructed in the previous methods (e.g., @cite_15 @cite_6 @cite_32 @cite_29 ) represent data points, while the vertices of the hypergraph constructed in this paper denote model hypotheses. Therefore, we can directly deal with the model fitting problem in the parameter space.\"","":""}
{"id":"2786092739","dialogue":"\"In this paper, we propose a simple and effective geometric model fitting method to fit and segment multi-structure data even in the presence of severe outliers. We cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs. Specifically, a hypergraph is first constructed, where the vertices represent model hypotheses and the hyperedges denote data points. The hypergraph involves higher-order similarities (instead of pairwise similarities used on a simple graph), and it can characterize complex relationships between model hypotheses and data points. In addition, we develop a hypergraph reduction technique to remove “insignificant” vertices while retaining as many “significant” vertices as possible in the hypergraph. Based on the simplified hypergraph, we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time. Finally, the proposed mode-seeking algorithm detects modes according to two key elements, i.e., the weighting scores of vertices and the similarity analysis between vertices. Overall, the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously. Experimental results demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both synthetic data and real images.\"","summary":"\"In addition to the above-mentioned robust fitting methods, there are several other related fitting methods, such as KF @cite_11 , J-linkage @cite_19 , T-linkage @cite_24 , SCAMS @cite_4 , PM @cite_27 , PEARL @cite_1 , AKSWH @cite_14 , HS @cite_18 , RELRT @cite_31 and GMD @cite_33 . KF, J-linkage, T-linkage and SCAMS directly deal with data points for model fitting but they are sensitive to unbalanced data distributions that are quite common in practical applications . In addition, these methods have difficulties in dealing with the data points near the intersection of model instances. The computational costs of J-linkage and T-linkage are high due to the use of the agglomerative clustering procedure. The other robust fitting methods also have some problems. For example, PM requires the input of the number of model instances in data; PEARL is sensitive to the initial generated hypotheses; AKSWH may remove some good model hypotheses corresponding to the correct model instances involving a small number of data points, during the procedure of selecting significant hypotheses; HS encounters the computational complexity problem due to the expansion and dropping strategy used; both RELRT and GMD only work for single-structure data.\"","":""}
{"id":"2966695781","dialogue":"\"Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights in the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest practical ways to harness it to create more robust and compact representations.\"","summary":"\"There is a long line of research revolving around the use of randomly drawn features in machine learning. Extreme Learning Machines show the utility of keeping some layer of a neural net fixed - but this is usually done only for one or two layers, and not within layers @cite_38 or across multiple (more than two) layers. @cite_33 has shown how picking random features has merits over matching kernels to the data. @cite_28 have analytically shown useful properties of random nets with Gaussian weights. As mentioned in the work of @cite_11 , many of the theoretical works on deep neural networks assume specific conditions which are not known to hold in practice; we show empirically what happens when weights are selected randomly (and fixed) throughout various layer of the network and within layers.\"","":""}
{"id":"2966695781","dialogue":"\"Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights in the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest practical ways to harness it to create more robust and compact representations.\"","summary":"\"A very recent result is that of @cite_6 , showing - quite surprisingly - that using a fixed, Hadamard matrix @cite_9 for a final classification layer does not hinder the performance of a classifier. In contrast, we do not impose any constraints on the values of any of the fixed weights (except drawing them from the same distribution as that of the learned ones), and evaluate the effect of fixing many different subsets of weights throughout the network.\"","":""}
{"id":"2966695781","dialogue":"\"Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights in the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest practical ways to harness it to create more robust and compact representations.\"","summary":"\"Many works attempt to learn a compact representation by pruning unimportant filters: for example, compressing the network after learning @cite_17 @cite_37 @cite_5 @cite_19 ; performing tensor-decompositions on the filter representations @cite_30 or regularizing their structure to have a sparse representation @cite_7 ; and designing networks which are compact to begin with, either by architectural changes @cite_27 @cite_14 , or by learningdiscrete weights , @cite_34 @cite_22 .\"","":""}
{"id":"2787145221","dialogue":"\"Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.\"","summary":"\": Studies have focused on simplifying the temporal aspect of dynamics @cite_15 @cite_33 @cite_34 in order to model co-occurrences of information across the modalities. In these models, each modality is summarized in a representation by collapsing the time dimension, such as averaging the modality information through time @cite_4 . While these models are successful in understanding co-occurrences, the lack of temporal modeling is a major flaw as these models cannot deal with multiple contradictory evidences, eg. if a smile and frown happen together in an utterance. Furthermore, these approaches cannot accurately model long sequences since the representation over long periods of time become less informative.\"","":""}
{"id":"2787145221","dialogue":"\"Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.\"","summary":"\": Approaches have used multimodal input feature concatenation instead of modeling and dynamics explicitly. In other words, these approaches rely on generic models (such as Support Vector Machines or deep neural networks) to learn both and dynamics without any specific model design. This concatenation technique is known as early fusion @cite_19 @cite_24 . Often, these early fusion approaches remove the time factor as well @cite_2 @cite_8 . We additionally compare to a stronger recurrent baseline that uses early fusion while maintaining the factor of time. A shortcoming of these models is the lack of detailed modeling for dynamics, which in turn affects the modeling of dynamics, as well as causing overfitting on input data @cite_0 .\"","":""}
{"id":"2787145221","dialogue":"\"Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.\"","summary":": Extensions of Hidden Markov Models @cite_22 and Hidden Conditional Random Fields @cite_1 @cite_17 have been proposed for learning from multiple different views (modalities) @cite_7 @cite_29 . Extensions of LSTMs have also been proposed in a multi-view setting @cite_9 .","":""}
{"id":"2787484216","dialogue":"\"A binary string transmitted via a memoryless i.i.d. deletion channel is received as a subsequence of the original input. From this, one obtains a posterior distribution on the channel input, corresponding to a set of candidate supersequences weighted by the number of times the received subsequence can be embedded in them. In a previous work it is conjectured on the basis of experimental data that the entropy of the posterior is minimized and maximized by the constant and the alternating strings, respectively. In this work, we present an algorithm for counting the number of subsequence embeddings using a run-length encoding of strings. We then describe two different ways of clustering the space of supersequences and prove that their cardinality depends only on the length of the received subsequence and its Hamming weight, but not its exact form. Then, we consider supersequences that contain a single embedding of a fixed subsequence, referred to as singletons, and provide a closed form expression for enumerating them using the same run-length encoding. We prove an analogous result for the minimization and maximization of the number of singletons, by the alternating and the uniform strings, respectively. Next, we prove the original minimal entropy conjecture for the special cases of single and double deletions using similar clustering techniques and the same run-length encoding, which allow us to characterize the distribution of the number of subsequence embeddings in the space of compatible supersequences to demonstrate the effect of an entropy decreasing operation.\"","summary":"\"Studies involving subsequences and supersequences encompass a wide variety of problems that arise in various contexts such as formal languages, coding theory, computer intrusion detection and DNA sequencing to name a few. Despite their prevalence in such a wide range of disciplines, they remain largely unexplored and still present a considerable wealth of unanswered questions. In the realm of stringology and formal languages, the problem of determining the number of distinct subsequences obtainable from a fixed number of deletions, and closely related problems, have been studied extensively in @cite_26 @cite_24 @cite_5 @cite_15 . Perhaps it is worth noting that the same entropy minimizing and maximizing strings conjectured in @cite_22 and characterized in the present work, have been shown to lead to the minimum and maximum number of distinct subsequences, respectively. The problems of finding shortest common supersequences (SCS) and longest common subsequences (LCS) represent two well-known NP-hard problems @cite_6 @cite_9 @cite_0 that involve subproblems similar to our work. Finally, devising efficient algorithms for subsequence combinatorics based on dynamic programming for counting the number of occurrences of a subsequence in DNA sequencing is yet another important and closely related line of research @cite_3 @cite_19 .\"","":""}
{"id":"2787484216","dialogue":"\"A binary string transmitted via a memoryless i.i.d. deletion channel is received as a subsequence of the original input. From this, one obtains a posterior distribution on the channel input, corresponding to a set of candidate supersequences weighted by the number of times the received subsequence can be embedded in them. In a previous work it is conjectured on the basis of experimental data that the entropy of the posterior is minimized and maximized by the constant and the alternating strings, respectively. In this work, we present an algorithm for counting the number of subsequence embeddings using a run-length encoding of strings. We then describe two different ways of clustering the space of supersequences and prove that their cardinality depends only on the length of the received subsequence and its Hamming weight, but not its exact form. Then, we consider supersequences that contain a single embedding of a fixed subsequence, referred to as singletons, and provide a closed form expression for enumerating them using the same run-length encoding. We prove an analogous result for the minimization and maximization of the number of singletons, by the alternating and the uniform strings, respectively. Next, we prove the original minimal entropy conjecture for the special cases of single and double deletions using similar clustering techniques and the same run-length encoding, which allow us to characterize the distribution of the number of subsequence embeddings in the space of compatible supersequences to demonstrate the effect of an entropy decreasing operation.\"","summary":"\"In coding theory, and more specifically in the context of insertion and deletions channels, similar long-standing problems have been studied extensively, and yet many problems still remain elusive. This includes designing optimal coding schemes and determining the capacity of deletion channels, both of which incorporate the same underlying combinatorial problem addressed in the present work. Considering a finite number of insertions and deletions for designing correcting codes for synchronization errors @cite_16 @cite_1 @cite_17 and reconstructing the original string from a fixed subsequence @cite_8 represent two specific and related research areas. More recent work on the characterization of the number of subsequences obtained via the deletion channel @cite_2 @cite_18 @cite_10 , e.g., in terms of the number of runs in a string, shows great overlap with our work. A graph-theoretic approach for deletion correcting codes, closely related to our clustering analysis, including an alternative proof for the Hamming weight clustering given in Theorem based on a different approach, is given in @cite_14 .\"","":""}
{"id":"2787484216","dialogue":"\"A binary string transmitted via a memoryless i.i.d. deletion channel is received as a subsequence of the original input. From this, one obtains a posterior distribution on the channel input, corresponding to a set of candidate supersequences weighted by the number of times the received subsequence can be embedded in them. In a previous work it is conjectured on the basis of experimental data that the entropy of the posterior is minimized and maximized by the constant and the alternating strings, respectively. In this work, we present an algorithm for counting the number of subsequence embeddings using a run-length encoding of strings. We then describe two different ways of clustering the space of supersequences and prove that their cardinality depends only on the length of the received subsequence and its Hamming weight, but not its exact form. Then, we consider supersequences that contain a single embedding of a fixed subsequence, referred to as singletons, and provide a closed form expression for enumerating them using the same run-length encoding. We prove an analogous result for the minimization and maximization of the number of singletons, by the alternating and the uniform strings, respectively. Next, we prove the original minimal entropy conjecture for the special cases of single and double deletions using similar clustering techniques and the same run-length encoding, which allow us to characterize the distribution of the number of subsequence embeddings in the space of compatible supersequences to demonstrate the effect of an entropy decreasing operation.\"","summary":"An important body of research in this area is dedicated to deriving tight bounds on the capacity of deletion channels @cite_12 @cite_7 @cite_20 @cite_11 and developing bounding techniques @cite_4 .","":""}
{"id":"2787484216","dialogue":"\"A binary string transmitted via a memoryless i.i.d. deletion channel is received as a subsequence of the original input. From this, one obtains a posterior distribution on the channel input, corresponding to a set of candidate supersequences weighted by the number of times the received subsequence can be embedded in them. In a previous work it is conjectured on the basis of experimental data that the entropy of the posterior is minimized and maximized by the constant and the alternating strings, respectively. In this work, we present an algorithm for counting the number of subsequence embeddings using a run-length encoding of strings. We then describe two different ways of clustering the space of supersequences and prove that their cardinality depends only on the length of the received subsequence and its Hamming weight, but not its exact form. Then, we consider supersequences that contain a single embedding of a fixed subsequence, referred to as singletons, and provide a closed form expression for enumerating them using the same run-length encoding. We prove an analogous result for the minimization and maximization of the number of singletons, by the alternating and the uniform strings, respectively. Next, we prove the original minimal entropy conjecture for the special cases of single and double deletions using similar clustering techniques and the same run-length encoding, which allow us to characterize the distribution of the number of subsequence embeddings in the space of compatible supersequences to demonstrate the effect of an entropy decreasing operation.\"","summary":"\"Perhaps rather surprisingly, the problem of determining the number of occurrences of a fixed subsequence in random sequences has not received the same amount and level of attention from the various communities. The state-of-the-art in the finite-length regime remains rather limited in scope. More precisely, the distribution of the number of occurrences constitutes a central problem in coding theory, with a maximum likelihood decoding argument, which represents the holy grail in the study of deletion channels. A comprehensive survey, which among other things, outlines the significance of figuring out this particular distribution is given by Mitzenmacher in @cite_13 .\"","":""}
{"id":"2964204643","dialogue":"\"Recently, feature selection has become an increasingly important area of research due to the surge in high-dimensional datasets in all areas of modern life. A plethora of feature selection algorithms have been proposed, but it is difficult to truly analyse the quality of a given algorithm. Ideally, an algorithm would be evaluated by measuring how well it removes known bad features. Acquiring datasets with such features is inherently difficult, and so a common technique is to add synthetic bad features to an existing dataset. While adding noisy features is an easy task, it is very difficult to automatically add complex, redundant features. This work proposes one of the first approaches to generating redundant features, using a novel genetic programming approach. Initial experiments show that our proposed method can automatically create difficult, redundant features which have the potential to be used for creating high-quality feature selection benchmark datasets.\"","summary":"\"A variety of tree-based GP approaches to FC have been proposed, including for problems such as classification and clustering @cite_12 @cite_10 . Most work uses a representation where a single GP tree produces a single constructed feature, as the output of the tree. The input to the tree is generally the set of features, and an optional random value input. This representation has been extended so that multiple features may be constructed in a single GP individual, commonly using a multi-tree representation @cite_0 @cite_10 . Other representations have also been proposed @cite_4 , including using multiple sub-trees as a set of constructed features @cite_2 @cite_12 , using specially-tailored node designs @cite_9 , cooperative co-evolutionary GP @cite_8 , and even by performing multiple GP runs (each producing a single constructed feature) @cite_5 . These works share similarity with this paper in that they perform a transformation of the original feature space, but they do so in order to improve the performance of a data mining task, rather than to perform feature creation.\"","":""}
{"id":"2963561466","dialogue":"\"In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information – in our case the swimming style of an athlete – and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16 more correct body joint detections compared to the baseline architecture.\"","summary":"\"Computer vision has been adopted for various applications in the sports domain. Prominent tasks include sports type @cite_17 and activity recognition @cite_4 @cite_19 , tracking athletes and other objects of interest in videos @cite_8 @cite_24 and human pose estimation @cite_20 @cite_31 . @cite_23 offer an overview of a wide range of application.\"","":""}
{"id":"2963561466","dialogue":"\"In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information – in our case the swimming style of an athlete – and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16 more correct body joint detections compared to the baseline architecture.\"","summary":"\"For performance analysis of individual athletes, @cite_15 propose a method to facilitate speed and stride length estimation for runners based on hand-held camera recordings. Specific to an aquatic environment, @cite_7 describe how swimmers can be tracked when filmed by a moving camera above the water surface. @cite_2 discuss the identification of characteristic poses of swimmers. @cite_32 present a CNN approach for automatic stroke-rate estimation in swimmer videos.\"","":""}
{"id":"2963561466","dialogue":"\"In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information – in our case the swimming style of an athlete – and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16 more correct body joint detections compared to the baseline architecture.\"","summary":"\"The traditional approach to human pose estimation are pictorial structures, where the human body is modeled as a collection of interacting parts @cite_11 @cite_29 @cite_14 @cite_18 @cite_27 . The model describes the appearance of individual parts and the relationship between interacting parts in a probabilistic fashion. The goal is to find the most probable part configuration given an input image.\"","":""}
{"id":"2963561466","dialogue":"\"In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information – in our case the swimming style of an athlete – and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16 more correct body joint detections compared to the baseline architecture.\"","summary":"\"Recent literature focuses on methods using CNNs to overcome the drawbacks of hand-crafted image features and limited part interactions present in classical approaches. The currently best results on popular human pose estimation benchmarks like the Leeds Sports Pose (LSP) @cite_14 and MPII Human Pose @cite_16 datasets all apply CNNs. @cite_5 describe an architecture that directly re -gres -ses the image coordinates of body joints. Subsequent publications regress confidence maps that indicate the likelihood of all possible joint locations in an image @cite_6 @cite_1 @cite_0 @cite_31 . This spatial encoding of the learning objective seems to be more natural to CNNs compared to the direct regression of image coordinates. Another common design are architectures performing iterative refinement @cite_13 @cite_12 @cite_28 . After producing an initial pose estimate it is progressively refined in the deeper layers of the network. There are also proposals to use classical part-based models to refine the pose estimates from CNN-based methods, either as a separate post-processing step @cite_10 or by mapping the domain-specific part interactions into the neural network itself for an end-to-end trainable architecture @cite_30 .\"","":""}
{"id":"2963561466","dialogue":"\"In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information – in our case the swimming style of an athlete – and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16 more correct body joint detections compared to the baseline architecture.\"","summary":"\"While most publications focus on human pose estimation on single 2D images, we are additionally interested in human pose estimation on videos. @cite_20 @cite_33 use pictorial structures to model humans in videos. They extend the spatial interactions between body parts by temporal dependencies that describe the change of body part configurations over time. Flowing Conv-Nets @cite_1 combine a CNN for human pose estimation on single images with a second CNN for the optical flow in videos that enables an estimate of the movement of body parts. In @cite_21 , optical flow and both spatial and temporal part interactions are used jointly in a single network architecture. @cite_9 describe a recurrent neural network (RNN) architecture applied to sequential video frames. In our approach we avoid the computational expensive extraction of optical flow and the data-intensive training of RNNs due to limited video material.\"","":""}
{"id":"2785170342","dialogue":"\"Common quality metrics of graph drawing have been about the readability criteria, such as small number of edge crossings, small drawing area and small total edge length. Bold graph drawing considers more realistic drawings consisting of vertices as disks of some radius and edges as rectangles of some width. However, the relationship that links these readability criteria with the rendering criteria in node-link diagrams has still not been well-established. This paper introduces a model, so-called INKA (Ink-Active), that encapsulates mathematically the relationship between all common drawing factors. Consequently, we investigate our INKA model on several common drawing algorithms and real-world graphs.\"","summary":"\"Criteria for good' graph visualization have been investigated extensively @cite_5 . Graph drawing algorithms over the years typically take into account one or more aesthetic criteria for better of the drawing. These aesthetic criteria include, for example, enumerate minimizing the number of edge crossings @cite_14 ; minimizing the total area @cite_4 ; edge lengths should be short but not too short @cite_0 . Amongst these aesthetics, small number of edge crossings is one of the most common criterion from previous user studies @cite_1 . Besides, the amount of ink and minimum total edge length has been used in many layout algorithms; for example, @cite_22 @cite_6 @cite_7 @cite_23 . Achieving small total area is another common approach @cite_4 . Overall, improving multiple aesthetics can produce better graph drawings @cite_16 .\"","":""}
{"id":"2785170342","dialogue":"\"Common quality metrics of graph drawing have been about the readability criteria, such as small number of edge crossings, small drawing area and small total edge length. Bold graph drawing considers more realistic drawings consisting of vertices as disks of some radius and edges as rectangles of some width. However, the relationship that links these readability criteria with the rendering criteria in node-link diagrams has still not been well-established. This paper introduces a model, so-called INKA (Ink-Active), that encapsulates mathematically the relationship between all common drawing factors. Consequently, we investigate our INKA model on several common drawing algorithms and real-world graphs.\"","summary":"\"A number of graph drawing algorithms have aimed for a minimum total edge length, or more precisely, a minimum amount of ink. This criterion has been studied @cite_22 @cite_6 @cite_7 @cite_23 .\"","":""}
{"id":"2787744110","dialogue":"\"Either by ensuring the continuing availability of information","summary":"or by deliberately caching content that might get deleted or removed","":""}
{"id":"2787744110","dialogue":"\"Either by ensuring the continuing availability of information","summary":"or by deliberately caching content that might get deleted or removed","":""}
{"id":"2787744110","dialogue":"\"Either by ensuring the continuing availability of information","summary":"or by deliberately caching content that might get deleted or removed","":""}
{"id":"2963172156","dialogue":"\"In this paper, we propose a learning-based method to compose a video-story from a group of video clips that describe an activity or experience. We learn the coherence between video clips from real videos via the Recurrent Neural Network (RNN) that jointly incorporates the spatial-temporal semantics and motion dynamics to generate smooth and relevant compositions. We further rearrange the results generated by the RNN to make the overall video-story compatible with the storyline structure via a submodular ranking optimization process. Experimental results on the video-story dataset show that the proposed algorithm outperforms the state-of-the-art approach.\"","summary":"\"Video Summarization. As introduced in , although having different goals, the technical aspects of video summarization are quit similar and can be sufficiently utilized by video composition. Many video summarization approaches have been proposed via different image-based feature representations and optimization methods, either through low-level feature such as optical flow @cite_31 and image differences @cite_12 , or high-level representations, including object trackers @cite_19 and importance scores @cite_16 . On the other hand, subshot-based methods represent summarizations via spatio-temporal features @cite_25 . Numerous supervised approaches select the subshots to represent the videos based on submodular function @cite_18 and exemplas @cite_1 . All these methods require ground truths for training.\"","":""}
{"id":"2963172156","dialogue":"\"In this paper, we propose a learning-based method to compose a video-story from a group of video clips that describe an activity or experience. We learn the coherence between video clips from real videos via the Recurrent Neural Network (RNN) that jointly incorporates the spatial-temporal semantics and motion dynamics to generate smooth and relevant compositions. We further rearrange the results generated by the RNN to make the overall video-story compatible with the storyline structure via a submodular ranking optimization process. Experimental results on the video-story dataset show that the proposed algorithm outperforms the state-of-the-art approach.\"","summary":"\"However, the labeling of the ground truth for either video summarization or video caption is too subjective and difficult as a consistent limitation to the above methods. In contrast, our model is learned in an unsupervised manner, making the framework more flexible to utilize large amount of data to improve the performance. Story Composition. The story composition methods typically focus on identifying the temporal alignment of the image sets (photo albums). @cite_8 use static and dynamic features to find the temporal order of the image sequence. @cite_15 learn the pairwise transition to construct the storyline graphs. Recently, an unsupervised method proposed by @cite_13 use a skipping Recurrent Neural Network to learn long-term correlations.\"","":""}
{"id":"2786399470","dialogue":"\"Automatic image captioning has recently approached human-level performance due to the latest advances in computer vision and natural language understanding. However, most of the current models can only generate plain factual descriptions about the content of a given image. However, for human beings, image caption writing is quite flexible and diverse, where additional language dimensions, such as emotion, humor and language styles, are often incorporated to produce diverse, emotional, or appealing captions. In particular, we are interested in generating sentiment-conveying image descriptions, which has received little attention. The main challenge is how to effectively inject sentiments into the generated captions without altering the semantic matching between the visual content and the generated descriptions. In this work, we propose two different models, which employ different schemes for injecting sentiments into image captions. Compared with the few existing approaches, the proposed models are much simpler and yet more effective. The experimental results show that our model outperform the state-of-the-art models in generating sentimental (i.e., sentiment-bearing) image captions. In addition, we can also easily manipulate the model by assigning different sentiments to the testing image to generate captions with the corresponding sentiments.\"","summary":"\"Recent studies on image captioning have been focusing on the application of deep neural networks since the release of MS-COCO Image Captioning Challenge http: mscoco.org dataset #captions-challenge2015 . The authors from @cite_14 @cite_20 first apply deep learning to predicting words and phrases from the given images. The captions are then generated by another language model, which composes the candidate words into a sentence.\"","":""}
{"id":"2786399470","dialogue":"\"Automatic image captioning has recently approached human-level performance due to the latest advances in computer vision and natural language understanding. However, most of the current models can only generate plain factual descriptions about the content of a given image. However, for human beings, image caption writing is quite flexible and diverse, where additional language dimensions, such as emotion, humor and language styles, are often incorporated to produce diverse, emotional, or appealing captions. In particular, we are interested in generating sentiment-conveying image descriptions, which has received little attention. The main challenge is how to effectively inject sentiments into the generated captions without altering the semantic matching between the visual content and the generated descriptions. In this work, we propose two different models, which employ different schemes for injecting sentiments into image captions. Compared with the few existing approaches, the proposed models are much simpler and yet more effective. The experimental results show that our model outperform the state-of-the-art models in generating sentimental (i.e., sentiment-bearing) image captions. In addition, we can also easily manipulate the model by assigning different sentiments to the testing image to generate captions with the corresponding sentiments.\"","summary":"\"Meanwhile, most of recent publications have been using an encoder-decoder framework @cite_7 @cite_12 for decoding the encoded images into a sentence. The work in @cite_24 proposed a CNN-RNN framework, which is simple but very effective. Their model ranked first in the 2015 MS-COCO Image Captioning Challenge. Both @cite_27 and @cite_1 employed the multimodal RNN for learning the semantic mapping between images and words, where the encoded image is supplied at each step of the RNN for learning the multimodal layer. More recently, attention model @cite_4 @cite_29 @cite_32 @cite_28 @cite_22 , which tries to learn the alignment between source language and target language in machine translation, has widely been adopted for building better image captioning systems.\"","":""}
{"id":"2786399470","dialogue":"\"Automatic image captioning has recently approached human-level performance due to the latest advances in computer vision and natural language understanding. However, most of the current models can only generate plain factual descriptions about the content of a given image. However, for human beings, image caption writing is quite flexible and diverse, where additional language dimensions, such as emotion, humor and language styles, are often incorporated to produce diverse, emotional, or appealing captions. In particular, we are interested in generating sentiment-conveying image descriptions, which has received little attention. The main challenge is how to effectively inject sentiments into the generated captions without altering the semantic matching between the visual content and the generated descriptions. In this work, we propose two different models, which employ different schemes for injecting sentiments into image captions. Compared with the few existing approaches, the proposed models are much simpler and yet more effective. The experimental results show that our model outperform the state-of-the-art models in generating sentimental (i.e., sentiment-bearing) image captions. In addition, we can also easily manipulate the model by assigning different sentiments to the testing image to generate captions with the corresponding sentiments.\"","summary":"\"In summary, generating different styles of captions requires us to not only bridge the semantic meanings between image and text, but also build a language generative model that can understand the differences between different language styles. This makes it more difficult for building sentimental captioning systems. There have been several preliminary studies @cite_11 @cite_31 @cite_19 . However, @cite_11 proposed a model that cannot be trained in an end-to-end fashion. There were only some preliminary example results in @cite_19 . On the other hand, @cite_31 tried to learn the model from large scale weakly and noisily supervised data, where additional care needs to be taken to reduce the noises. In contrast, our proposed approach is simple to train, yet it significantly outperforms the state-of-the-art.\"","":""}
{"id":"2787781581","dialogue":"\"Migrants' assimilation is a major challenge for European societies, in part because of the sudden surge of refugees in recent years and in part because of long-term demographic trends. In this paper, we use Facebook's data for advertisers to study the levels of assimilation of Arabic-speaking migrants in Germany, as seen through the interests they express online. Our results indicate a gradient of assimilation along demographic lines, language spoken and country of origin. Given the difficulty to collect timely migration data, in particular for traits related to cultural assimilation, the methods that we develop and the results that we provide open new lines of research that computational social scientists are well-positioned to address.\"","summary":"Understanding migrant integration and the effectiveness of policy measures to favor assimilation is a longstanding challenge. A wide range of aspects such as civic integration policies' @cite_7 or multiculturalism @cite_5 have been analyzed. These studies developed evaluation metrics based on concepts such as political trust @cite_11 as well as lack of electoral participation or composite measures of civic integration @cite_5 . For a review of empirical and theoretical challenges see @cite_6 (2005).","":""}
{"id":"2787781581","dialogue":"\"Migrants' assimilation is a major challenge for European societies, in part because of the sudden surge of refugees in recent years and in part because of long-term demographic trends. In this paper, we use Facebook's data for advertisers to study the levels of assimilation of Arabic-speaking migrants in Germany, as seen through the interests they express online. Our results indicate a gradient of assimilation along demographic lines, language spoken and country of origin. Given the difficulty to collect timely migration data, in particular for traits related to cultural assimilation, the methods that we develop and the results that we provide open new lines of research that computational social scientists are well-positioned to address.\"","summary":"\"New information, like Web and social media data, are a main source of innovation in the context of migration studies. Research in this area has focused on using online data to improve estimates of migration flows and stocks. After Zagheni and Weber used geo-located Yahoo! e-mail data to estimate international migration flows @cite_12 , several platforms have been used to understand the network structure of migration, including Facebook @cite_3 and Google+ @cite_1 . Geo-located Twitter data has proved useful for studying the relationship between internal and international migration @cite_2 , as well as short-term mobility versus long-term migration @cite_9 @cite_4 . LinkedIn data has provided insights into global patterns of migration for professionals @cite_10 .\"","":""}
{"id":"2786721402","dialogue":"\"Arterial spin labeling perfusion MRI is a noninvasive technique for measuring quantitative cerebral blood flow (CBF), but the measurement is subject to a low signal-to-noise-ratio(SNR). Various post-processing methods have been proposed to denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is an emerging technique that can learn the most representative signal from data without prior modeling which can be highly complex and analytically indescribable. The purpose of this study was to assess whether the record breaking performance of DL can be translated into ASL MRI denoising. We used convolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL) to inherently consider the inter-voxel correlations. To better guide DL-ASL training, we incorporated prior knowledge about ASL MRI: the structural similarity between ASL CBF map and grey matter probability map. A relatively large sample data were used to train the model which was subsequently applied to a new set of data for testing. Experimental results showed that DL-ASL achieved state-of-the-art denoising performance for ASL MRI as compared to current routine methods in terms of higher SNR, keeping CBF quantification quality while shorten the acquisition time by 75 , and automatic partial volume correction.\"","summary":"\"Imaging denoising is a classic low-level vision problem which have been widely studied in past decades. The image prior modeling often play a central role in image denoising. Traditional methods that used image prior knowledge as regularization techniques, scuh as nonlocal self-similarity models @cite_8 @cite_17 @cite_15 @cite_11 , Markov Random Field (MRF) @cite_9 @cite_12 @cite_19 and spares models @cite_0 @cite_13 , have shown very promising performance. However, in traditional denoising methods, image prior knowledge are explicitly pre-defined, which are often limited in capturing the full characteristics of image structure and limited in blind image denoising.\"","":""}
{"id":"2786721402","dialogue":"\"Arterial spin labeling perfusion MRI is a noninvasive technique for measuring quantitative cerebral blood flow (CBF), but the measurement is subject to a low signal-to-noise-ratio(SNR). Various post-processing methods have been proposed to denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is an emerging technique that can learn the most representative signal from data without prior modeling which can be highly complex and analytically indescribable. The purpose of this study was to assess whether the record breaking performance of DL can be translated into ASL MRI denoising. We used convolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL) to inherently consider the inter-voxel correlations. To better guide DL-ASL training, we incorporated prior knowledge about ASL MRI: the structural similarity between ASL CBF map and grey matter probability map. A relatively large sample data were used to train the model which was subsequently applied to a new set of data for testing. Experimental results showed that DL-ASL achieved state-of-the-art denoising performance for ASL MRI as compared to current routine methods in terms of higher SNR, keeping CBF quantification quality while shorten the acquisition time by 75 , and automatic partial volume correction.\"","summary":"\"Jain and Seung @cite_1 demonstrated that convolutional neural networks (CNNs) can be used for image denoising and claimed that CNNs have achieved comparable or even superior performance than the MRF methods. @cite_21 proposed to incorporate residual learning and batch normalization learning strategies into very deep CNN for denoising. @cite_23 proposed to use skip-layer connection to symmetrically link convolutional and deconvolutional layers, which is able to train even deeper CNN architecture for denoising. Peng and Fang @cite_7 proposed a wider CNN network which has relatively fewer layers but has larger size and number of filters in each layer. They claimed that for low-level vision tasks, the depth of the network is not the key, while the width of the architecture is more important. They state that for denoising tasks, deep learning denoising models learn prior pixel distribution information from original image and then use the learned filter banks to restore degrade images. Thus, the more concentrated convolutions to capture the prior image distribution from noisy images, the better the denoising performances.\"","":""}
{"id":"2786721402","dialogue":"\"Arterial spin labeling perfusion MRI is a noninvasive technique for measuring quantitative cerebral blood flow (CBF), but the measurement is subject to a low signal-to-noise-ratio(SNR). Various post-processing methods have been proposed to denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is an emerging technique that can learn the most representative signal from data without prior modeling which can be highly complex and analytically indescribable. The purpose of this study was to assess whether the record breaking performance of DL can be translated into ASL MRI denoising. We used convolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL) to inherently consider the inter-voxel correlations. To better guide DL-ASL training, we incorporated prior knowledge about ASL MRI: the structural similarity between ASL CBF map and grey matter probability map. A relatively large sample data were used to train the model which was subsequently applied to a new set of data for testing. Experimental results showed that DL-ASL achieved state-of-the-art denoising performance for ASL MRI as compared to current routine methods in terms of higher SNR, keeping CBF quantification quality while shorten the acquisition time by 75 , and automatic partial volume correction.\"","summary":"\"Residual learning is a technique to solve the gradient vanish problem @cite_3 . As the the number of layers increases, the training accuracy of CNN begins to decrease due to gradient vanishing in lower layers. By constructing residual units (i.e., identity shortcuts or skip connections) between a few layers, residual network learns a residual mapping which is much easier to train and prevent gradient vanish. With residual learning strategy, training extremely deep CNN become possible. @cite_3 shows improved performance when using residual learning for image classification and object detection.\"","":""}
{"id":"2786721402","dialogue":"\"Arterial spin labeling perfusion MRI is a noninvasive technique for measuring quantitative cerebral blood flow (CBF), but the measurement is subject to a low signal-to-noise-ratio(SNR). Various post-processing methods have been proposed to denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is an emerging technique that can learn the most representative signal from data without prior modeling which can be highly complex and analytically indescribable. The purpose of this study was to assess whether the record breaking performance of DL can be translated into ASL MRI denoising. We used convolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL) to inherently consider the inter-voxel correlations. To better guide DL-ASL training, we incorporated prior knowledge about ASL MRI: the structural similarity between ASL CBF map and grey matter probability map. A relatively large sample data were used to train the model which was subsequently applied to a new set of data for testing. Experimental results showed that DL-ASL achieved state-of-the-art denoising performance for ASL MRI as compared to current routine methods in terms of higher SNR, keeping CBF quantification quality while shorten the acquisition time by 75 , and automatic partial volume correction.\"","summary":"\"There are several studies that incorporate residual learning for denoising tasks @cite_23 @cite_21 @cite_7 . In @cite_23 , they used Skip shortcuts to connect from convolutional feature maps to their corresponding deconvolutional feature maps every a few layers, which help ease back-propagation and reuse details. In @cite_21 , proposed DnCNN to using a mapping directly from an input observation to the corresponding reference observation.\"","":""}
{"id":"2786140833","dialogue":"\"Efficient, reliable trapping of execution in a program at the desired location is a hot area of research for security professionals. The progression of debuggers and malware is akin to a game of cat and mouse - each are constantly in a state of trying to thwart one another. At the core of most efficient debuggers today is a combination of virtual machines and traditional binary modification breakpoints (int3). In this paper, we present a design for Virtual Breakpoints, a modification to the x86 MMU which brings breakpoint management into hardware alongside page tables. We demonstrate the fundamental abstraction failures of current trapping methods, and rebuild the mechanism from the ground up. Our design delivers fast, reliable trapping without the pitfalls of binary modification.\"","summary":"\"Overshadow implemented a shadow page table containing multiple mappings (encrypted and unencrypted) of a guest's physical memory, and actively tracks the identity'' of the guest process attempting to read a page. When the accessing process does not have the correct identity, an encrypted page is presented (on read) or the machine is terminated (on write). When the accessing process does have the correct identity, an unencrypted page is presented with the permissions originally granted to the page. Unfortunately, its dependence on shadow page tables means it's likely to be insufficiently efficient @cite_0 for modern operating systems and heavy load systems.\"","":""}
{"id":"2786140833","dialogue":"\"Efficient, reliable trapping of execution in a program at the desired location is a hot area of research for security professionals. The progression of debuggers and malware is akin to a game of cat and mouse - each are constantly in a state of trying to thwart one another. At the core of most efficient debuggers today is a combination of virtual machines and traditional binary modification breakpoints (int3). In this paper, we present a design for Virtual Breakpoints, a modification to the x86 MMU which brings breakpoint management into hardware alongside page tables. We demonstrate the fundamental abstraction failures of current trapping methods, and rebuild the mechanism from the ground up. Our design delivers fast, reliable trapping without the pitfalls of binary modification.\"","summary":"\"Finally, Spider @cite_9 comes the closest to implementing a solution that maximizes efficiency, while retaining the flexibility of traditional binary modification breakpoints. Similar to VAMPiRE, it leverages virtual page permissions to determine what view'' of memory should be provided to the processor. On read write, a sanitized'' view of memory (sans breakpoints) is provided to hardware to prevent detection. On execute, the modified page (containing int3 instructions) is provided directly to the processor.\"","":""}
{"id":"2079218873","dialogue":"\"We present an efficient and scalable algorithm for segmenting 3D RGBD point clouds by combining depth, color, and temporal information using a multistage, hierarchical graph-based approach. Our algorithm processes a moving window over several point clouds to group similar regions over a graph, resulting in an initial over-segmentation. These regions are then merged to yield a dendrogram using agglomerative clustering via a minimum spanning tree algorithm. Bipartite graph matching at a given level of the hierarchical tree yields the final segmentation of the point clouds by maintaining region identities over arbitrarily long periods of time. We show that a multistage segmentation with depth then color yields better results than a linear combination of depth and color. Due to its incremental processing, our algorithm can process videos of any length and in a streaming pipeline. The algorithm's ability to produce robust, efficient segmentation is demonstrated with numerous experimental results on challenging sequences from our own as well as public RGBD data sets.\"","summary":"\"One way to organize related work is use the five types of algorithms for super-voxel video segmentation analyzed by Xu and Corso @cite_0 . (A) Paris and Durand @cite_1 propose a method that achieves hierarchical segmentation in videos using topological persistence using the classic mode-seeking meanshift algorithm interpreted under Morse theory as a topological decomposition of the feature space. (B) @cite_17 use , in which the Nystrom approximation is applied to solve the normalized cut problem, for spatiotemporal grouping. (C) @cite_19 is a variant of optimizing the normalized cut that computes a hierarchy of sequentially coarser segments by an algebraic multigrid solver. (D) is an adaptation of the Felzenszwalb and Huttenlocher image segmentation algorithm @cite_3 to video segmentation by building the graph in the spatiotemporal volume where voxels (volumetric pixels) are nodes connected to 26 neighbors. (E) is an algorithm for video segmentation proposed in @cite_21 that iteratively builds a tree structure of region graphs, starting from over-segmented spatiotemporal volumes obtained using the method illustrated above. The regions are described by LAB histograms of the voxel members, the edge weights are defined by the @math distance, and the regions are merged using the same technique as in @cite_3 .\"","":""}
{"id":"2785757655","dialogue":"\"We consider the problem of retrieving objects from image data and learning to classify them into meaningful semantic categories with minimal supervision. To that end, we propose a fully differentiable unsupervised deep clustering approach to learn semantic classes in an end-to-end fashion without individual class labeling using only unlabeled object proposals. The key contributions of our work are 1) a kmeans clustering objective where the clusters are learned as parameters of the network and are represented as memory units, and 2) simultaneously building a feature representation, or embedding, while learning to cluster it. This approach shows promising results on two popular computer vision datasets: on CIFAR10 for clustering objects, and on the more complex and challenging Cityscapes dataset for semantically discovering classes which visually correspond to cars, people, and bicycles. Currently, the only supervision provided is segmentation objectness masks, but this method can be extended to use an unsupervised objectness-based object generation mechanism which will make the approach completely unsupervised.\"","summary":"\"Unsupervised learning ( @cite_20 ) and unsupervised deep learning ( @cite_16 , @cite_18 , @cite_1 ) are central topics to Machine Learning. Unsupervised deep learning has been shown to improve results on classification tasks per @cite_5 , especially given small datasets and complicated high dimensional data such as video. This has been explored by many representations including sequence to sequence learning and textual representations ( @cite_9 , @cite_17 ).\"","":""}
{"id":"2785757655","dialogue":"\"We consider the problem of retrieving objects from image data and learning to classify them into meaningful semantic categories with minimal supervision. To that end, we propose a fully differentiable unsupervised deep clustering approach to learn semantic classes in an end-to-end fashion without individual class labeling using only unlabeled object proposals. The key contributions of our work are 1) a kmeans clustering objective where the clusters are learned as parameters of the network and are represented as memory units, and 2) simultaneously building a feature representation, or embedding, while learning to cluster it. This approach shows promising results on two popular computer vision datasets: on CIFAR10 for clustering objects, and on the more complex and challenging Cityscapes dataset for semantically discovering classes which visually correspond to cars, people, and bicycles. Currently, the only supervision provided is segmentation objectness masks, but this method can be extended to use an unsupervised objectness-based object generation mechanism which will make the approach completely unsupervised.\"","summary":"Our work focuses on unsupervised deep learning for discovering visual object categories. This has also been shown to improve results such as in @cite_0 . Unsupervised discovery of visual objects has been a large topic of interest in computer vision ( @cite_6 @cite_19 @cite_13 @cite_11 @cite_15 @cite_22 ).","":""}
{"id":"2785757655","dialogue":"\"We consider the problem of retrieving objects from image data and learning to classify them into meaningful semantic categories with minimal supervision. To that end, we propose a fully differentiable unsupervised deep clustering approach to learn semantic classes in an end-to-end fashion without individual class labeling using only unlabeled object proposals. The key contributions of our work are 1) a kmeans clustering objective where the clusters are learned as parameters of the network and are represented as memory units, and 2) simultaneously building a feature representation, or embedding, while learning to cluster it. This approach shows promising results on two popular computer vision datasets: on CIFAR10 for clustering objects, and on the more complex and challenging Cityscapes dataset for semantically discovering classes which visually correspond to cars, people, and bicycles. Currently, the only supervision provided is segmentation objectness masks, but this method can be extended to use an unsupervised objectness-based object generation mechanism which will make the approach completely unsupervised.\"","summary":"\"Building specialized, deep embeddings to help computer vision tasks is also a popular approach such as in @cite_8 . Transfer learning from supervised tasks has proven to be very successful. Further, @cite_8 propose learning the lower dimensional embedding through unsupervised learning and show improved performance when transfered to other supervised tasks.\"","":""}
{"id":"2785757655","dialogue":"\"We consider the problem of retrieving objects from image data and learning to classify them into meaningful semantic categories with minimal supervision. To that end, we propose a fully differentiable unsupervised deep clustering approach to learn semantic classes in an end-to-end fashion without individual class labeling using only unlabeled object proposals. The key contributions of our work are 1) a kmeans clustering objective where the clusters are learned as parameters of the network and are represented as memory units, and 2) simultaneously building a feature representation, or embedding, while learning to cluster it. This approach shows promising results on two popular computer vision datasets: on CIFAR10 for clustering objects, and on the more complex and challenging Cityscapes dataset for semantically discovering classes which visually correspond to cars, people, and bicycles. Currently, the only supervision provided is segmentation objectness masks, but this method can be extended to use an unsupervised objectness-based object generation mechanism which will make the approach completely unsupervised.\"","summary":"\"Despite the popularity of building different embeddings, there is little work investigating the use of clustering to modify the embedding in an end-to-end deep learning framework. @cite_23 investigate a differentiable version of the kmeans algorithm and examine its convergence properties. Our work focuses on learnable feature representations (instead of fixed ones as in @cite_23 ) and introduces memory units for the task.\"","":""}
{"id":"2785227457","dialogue":"\"In this paper, we attempt to employ convolutional recurrent neural networks for weather temperature estimation using only image data. We study ambient temperature estimation based on deep neural networks in two scenarios a) estimating temperature of a single outdoor image, and b) predicting temperature of the last image in an image sequence. In the first scenario, visual features are extracted by a convolutional neural network trained on a large-scale image dataset. We demonstrate that promising performance can be obtained, and analyze how volume of training data influences performance. In the second scenario, we consider the temporal evolution of visual appearance, and construct a recurrent neural network to predict the temperature of the last image in a given image sequence. We obtain better prediction accuracy compared to the state-of-the-art models. Further, we investigate how performance varies when information is extracted from different scene regions, and when images are captured in different daytime hours. Our approach further reinforces the idea of using only visual information for cost efficient weather prediction in the future.\"","summary":"\"In addition to weather condition classification, more weather properties have also been investigated. Jacobs and his colleagues @cite_11 initiated a project for collecting outdoor scene images captured by static webcams over a long period of time. The collected images form the Archive of Many Outdoor Scene (AMOS) dataset @cite_11 . Based on the AMOS dataset, they proposed that webcams installed across the earth can be viewed as image sensors and can enable us to understand weather patterns and variations over time @cite_0 . More specifically, they adopted principal component analysis and canonical correlation analysis to predict wind velocity and vapor pressure from a sequence of images.\"","":""}
{"id":"2785227457","dialogue":"\"In this paper, we attempt to employ convolutional recurrent neural networks for weather temperature estimation using only image data. We study ambient temperature estimation based on deep neural networks in two scenarios a) estimating temperature of a single outdoor image, and b) predicting temperature of the last image in an image sequence. In the first scenario, visual features are extracted by a convolutional neural network trained on a large-scale image dataset. We demonstrate that promising performance can be obtained, and analyze how volume of training data influences performance. In the second scenario, we consider the temporal evolution of visual appearance, and construct a recurrent neural network to predict the temperature of the last image in a given image sequence. We obtain better prediction accuracy compared to the state-of-the-art models. Further, we investigate how performance varies when information is extracted from different scene regions, and when images are captured in different daytime hours. Our approach further reinforces the idea of using only visual information for cost efficient weather prediction in the future.\"","summary":"\"Recently, @cite_1 estimated scene attributes like lighting, weather conditions, and seasons for images captured by webcams based on a set of regressors. @cite_4 studied the correlation between pixel intensity camera motion and temperature and found a moderate correlation. With this observation, a regression model considering pixel intensity was constructed to predict temperature. Following the discussion in @cite_4 , @cite_13 showed that, with appropriate fine tuning, deep features can be promising for temperature prediction. @cite_2 proposed a selective comparison learning scheme and showed that the state-of-the-art temperature prediction performance can be obtained by a CNN-based approach.\"","":""}
{"id":"2785227457","dialogue":"\"In this paper, we attempt to employ convolutional recurrent neural networks for weather temperature estimation using only image data. We study ambient temperature estimation based on deep neural networks in two scenarios a) estimating temperature of a single outdoor image, and b) predicting temperature of the last image in an image sequence. In the first scenario, visual features are extracted by a convolutional neural network trained on a large-scale image dataset. We demonstrate that promising performance can be obtained, and analyze how volume of training data influences performance. In the second scenario, we consider the temporal evolution of visual appearance, and construct a recurrent neural network to predict the temperature of the last image in a given image sequence. We obtain better prediction accuracy compared to the state-of-the-art models. Further, we investigate how performance varies when information is extracted from different scene regions, and when images are captured in different daytime hours. Our approach further reinforces the idea of using only visual information for cost efficient weather prediction in the future.\"","summary":"\"In this work, we aim at predicting temperature from a single image, as well as forecasting the temperature of the last image in a given image sequence. Deep learning approaches will be developed to consider temporal evolution of visual appearance, and promising performance will be shown. Compared with @cite_4 , @cite_13 , and @cite_2 , we particularly advocate the importance of modeling temporal evolution with designed deep networks.\"","":""}
{"id":"2784871935","dialogue":"\"Unraveling the nature of the communication model that governs which two individuals in a swarm interact with each other is an important line of inquiry in the collective behavior sciences. A number of models have been proposed in the biological swarm literature, with the leading models being the metric, topological, and visual models. The hypothesis evaluated in this manuscript is whether the choice of a communication model impacts the performance of a tasked artificial swarm. The biological models are used to design coordination algorithms for a simulated swarm, which are evaluated over a range of six swarm robotics tasks. Each task has an associated set of performance metrics that are used to evaluate how the communication models fare against each other. The general findings demonstrate that the communication model significantly affects the swarm's performance for individual tasks, and this result implies that the communication model-task pairing is an important consideration when designing artificial swarms. Further analysis of each tasks' performance metrics reveal instances in which pairwise considerations of model and one of the various experimental factors becomes relevant. The reported research demonstrates that the artificial swarm's task performance can be increased through the careful selection of a communications model.\"","summary":"\"The metric model is one of the earliest models developed to represent range-limited communication between biological swarm agents @cite_27 @cite_19 @cite_42 . This model is used as a benchmark for comparison testing relatively newer communication models @cite_48 @cite_21 @cite_28 . The model is widely-used in the field of multi-robot systems, as well, due to its ability to capture sensor range constraints @cite_58 @cite_36 @cite_54 . However, a field study of European starlings, Sturnus vulgaris , indicates that the swarm uses a topological, rather than a range limited model @cite_21 . Specifically, starlings coordinate with their nearest six to seven neighbors (topological distance). An artificial swarm, in response to a simulated predator, decomposes into fewer groups, and produces more cohesive swarms, when using the topological model compared to the metric model @cite_21 . Strandburg-Peshkin @cite_48 introduced the visual model, and show that it best predicts how golden shiners, Notemigonus crysoleucas , behave in response to stimuli. The model's low clustering makes it fundamentally different from the metric and topological models, from a network-theoretic perspective.\"","":""}
{"id":"2783404025","dialogue":"\"Modern software development depends on APIs to reuse code and increase productivity. As most software systems, these libraries and frameworks also evolve, which may break existing clients. However, the main reasons to introduce breaking changes in APIs are unclear. Therefore, in this paper, we report the results of an almost 4-month long field study with the developers of 400 popular Java libraries and frameworks. We configured an infrastructure to observe all changes in these libraries and to detect breaking changes shortly after their introduction in the code. After identifying breaking changes, we asked the developers to explain the reasons behind their decision to change the APIs. During the study, we identified 59 breaking changes, confirmed by the developers of 19 projects. By analyzing the developers' answers, we report that breaking changes are mostly motivated by the need to implement new features, by the desire to make the APIs simpler and with fewer elements, and to improve maintainability. We conclude by providing suggestions to language designers, tool builders, software engineering researchers and API developers.\"","summary":"\"In another related study @cite_14 , we investigate breaking changes in 317 real-world Java libraries, including 9K releases and 260K client applications. We show that 15 compatibility with previous versions and that the frequency of breaking changes increases over time. Using data from the BOA ultra-large dataset @cite_22 , we report that less than 3 Dig and Johnson @cite_27 studied API changes in five frameworks and libraries (Eclipse, Mortgage, Struts, Log4J, and JHotDraw). They report that more than 80 changes in these systems were due to refactorings. By contrast, using a large dataset of 400 popular Java libraries and frameworks, we also found that BCs are usually related to refactorings, but at a lower rate (47\"","":""}
{"id":"2779140735","dialogue":"\"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly","summary":"there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable","":""}
{"id":"2776039898","dialogue":"\"Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics.\"","summary":"\"NER for social media content is however difficult, leading to much work, including general approaches @cite_35 , topic-specific approaches @cite_2 , adapting from known genres @cite_34 ; these are driven by and evaluated in multiple recent shared tasks @cite_6 @cite_37 . The task is generally cast as a domain adaptation problem from newswire data, integrating the two kinds of data for training @cite_14 or including a lexical normalisation step @cite_18 to shift text to territory more familiar to existing models and methods. Major challenges are that NEs mentioned in tweets change over time @cite_7 , and that diversity of context makes NER more difficult @cite_15 . This paper addresses NER without using large amounts of labelled in-domain data, in order to track entity propagation at scale.\"","":""}
{"id":"2776039898","dialogue":"\"Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics.\"","summary":"\"Studies of information diffusion have largely concentrated on signals of diffusion such as tracking URLs, hashtags, quotes @cite_4 , and adoption behaviour (e.g. group signups); however to the best of our knowledge such studies have yet to focus on how entities diffuse. We now focus on key pieces of work that are closely-aligned to the study of entity-diffusion in the context of social networks -- should the reader wish to know more about information diffusion models, and in greater detail, then please refer to 's @cite_5 comprehensive survey of such models.\"","":""}
{"id":"2776039898","dialogue":"\"Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics.\"","summary":"\"The study of information adoption and sharing was undertaken by @cite_23 who conducted a large-scale randomised controlled trial to examine the effects of on information diffusion, using the Facebook platform. The authors were able to assign Facebook users with a @math probability to a group, and the remainder to a group and then information (i.e. status posts) posted within the latter's group. found that users who were to information (i.e. those in the feed group) from their friends are more likely to share it on -- implying that such exposure has an influential effect.\"","":""}
{"id":"2776039898","dialogue":"\"Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics.\"","summary":"\"Prior work on Reddit has examined the site's evolution since launch, seeing it evolve from a bulletin-like page to a large community site with many segragated and unique sub-communities that reinforce a general perception of the overall community @cite_22 . This observation supports the use of Reddit as a study venue for information diffusion, finding that communities are large, well-defined, and cohesive. Later work covers the mapping of popular content @cite_26 and of network structure @cite_13 , though not the diffusion of information through those networks.\"","":""}
{"id":"2776039898","dialogue":"\"Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics.\"","summary":"\"@cite_27 predicted adoption probabilities in social networks by controlling for potential confounding, unobservable variables -- proposing a modification of expectation-maximisation to induce a Naive bayes predictive model. The authors found that social influence alone is insufficient to recover the diffusion process, and thus external factors -- that are latent -- must be countered for within any predictive model -- this was in the context of predicting the adoption of social ties. The adoption of information within a social network and its propagation was studied by @cite_3 by considering the role of temporal dynamics. The authors found that the probability of diffusion between users ( on Chinese microblogging platform Sina Weibo) reduces as a function of time from the last interaction between the users, thereby suggesting that have a strong effect in diffusion. We build time into our adaptation of 's @cite_39 general threshold diffusion model -- by comparing static and discrete time versions of adoption probabilities.\"","":""}
{"id":"2777719142","dialogue":"\"In this paper we introduce the core results of the project on software development for social robotics systems. The usability of maintenance and control features is crucial for many kinds of systems, but in the case of social robotics we also have to take into account that (1) the humanoid robot physically interacts with humans, (2) the conversation with children might have different requirements in comparison to the conversation with adults. The results of our work were implement for the humanoid PAL REEM robot, but their core ideas can be applied for other types of humanoid robots. We developed a web-based solution that supports the management of robot-guided tours, provides recommendations for the users as well as allows for a visual analysis of the data on previous tours.\"","summary":"\"@cite_26 presented the concept of Social Robot Architecture, which integrates the key elements of agenthood and robotics in a coherent and systematic manner. The ethical and social implications of robotics were discussed by in @cite_25 . @cite_17 examined social-psychology concepts to apply them to the human-robot interaction. @cite_7 presented a case study where they analysed the effects of robot features (human-likeness and gender) and user characteristics on the human-robot interaction acceptance and psychological anthropomorphism. @cite_8 analysed the effects of gesture on the perception of psychological anthropomorphism, by conducting a case study using the Honda humanoid robot. @cite_19 conducted a cross-cultural study on generation of culture dependent facial expressions of humanoid robot. @cite_4 discussed the use of observational studies of human-robot social interaction in open human-inhabited environments. Klein and Cook @cite_20 analysed and compared the findings in the UK and Germany on robot-therapy with emotional robots as a treatment approach for people with cognitive impairments.\"","":""}
{"id":"2777719142","dialogue":"\"In this paper we introduce the core results of the project on software development for social robotics systems. The usability of maintenance and control features is crucial for many kinds of systems, but in the case of social robotics we also have to take into account that (1) the humanoid robot physically interacts with humans, (2) the conversation with children might have different requirements in comparison to the conversation with adults. The results of our work were implement for the humanoid PAL REEM robot, but their core ideas can be applied for other types of humanoid robots. We developed a web-based solution that supports the management of robot-guided tours, provides recommendations for the users as well as allows for a visual analysis of the data on previous tours.\"","summary":"There were also a number of surveys and literature reviews on the related topics. A survey on social robots for long-term interaction was presented in @cite_2 . A systematic review on application of social robotics in the Autism Spectrum Disorders treatment was presented in @cite_6 . @cite_1 presented a survey on the roles and benefits of social robots in the therapy of children with autism.","":""}
{"id":"2777719142","dialogue":"\"In this paper we introduce the core results of the project on software development for social robotics systems. The usability of maintenance and control features is crucial for many kinds of systems, but in the case of social robotics we also have to take into account that (1) the humanoid robot physically interacts with humans, (2) the conversation with children might have different requirements in comparison to the conversation with adults. The results of our work were implement for the humanoid PAL REEM robot, but their core ideas can be applied for other types of humanoid robots. We developed a web-based solution that supports the management of robot-guided tours, provides recommendations for the users as well as allows for a visual analysis of the data on previous tours.\"","summary":"@cite_12 introduced a design framework enabling the development of social robotics applications by cross-disciplinary teams of programmers and interaction designers.","":""}
{"id":"2778718264","dialogue":"\"The encoder-decoder model is widely used in natural language generation tasks. However, the model sometimes suffers from repeated redundant generation, misses important phrases, and includes irrelevant entities. Toward solving these problems we propose a novel source-side token prediction module. Our method jointly estimates the probability distributions over source and target vocabularies to capture a correspondence between source and target tokens. The experiments show that the proposed model outperforms the current state-of-the-art method in the headline generation task. Additionally, we show that our method has an ability to learn a reasonable token-wise correspondence without knowing any true alignments.\"","summary":"\"Apart from the , some studies proposed methods to improve the performance of the headline generation task. incorporated AMR @cite_7 into the encoder to use the syntactic and semantic information of the source. also encoded additional information of the source such as TF-IDF, part-of-speech tags and named entities. modeled the typical structure of a headline, such as Who Action What'' with a variational auto-encoder. These approach improved the performance of the headline generation but it is unclear whether they can reduce the .\"","":""}
{"id":"2962876782","dialogue":"\"The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.\"","summary":"\"Since objects describe smooth trajectories in image space, as a function of viewing angle, it has long been known that such trajectories span a 3D manifold in image space, parameterized by the viewing angle. Hence, many of the manifold modeling methods proposed in the literature @cite_33 @cite_39 @cite_8 could, in principle, be used to develop trajectory transfer algorithms. However, many of these methods are transductive, , they do not produce a function that can make predictions for images outside of the training set, and do not leverage recent advances in deep learning. While deep learning could be used to explicitly model pose manifolds, it is difficult to rely on CNNs pre-trained on ImageNet for this purpose. This is because these networks attempt to collapse the manifold into a space where class discrimination is linear. On the other hand, the feature trajectories in response to pose variability are readily available. These trajectories are also much easier to model. For example, if the CNN is successful in mapping the pose manifold of a given object into a single point, , exhibits total pose invariance for that object, the problem is already solved and trajectory leaning is trivial for that object.\"","":""}
{"id":"2962876782","dialogue":"\"The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.\"","summary":"\".5ex One of the main goals of trajectory transfer is to fatten'' a feature space, by augmenting a dataset with feature responses of unseen object poses. In this sense, the problem is related to extensive recent literature on GANs @cite_14 , which have been successfully used to generate images, image-to-image translations @cite_21 , inpainting @cite_3 or style-transfer @cite_10 . While our work uses an encoder-decoder architecture, which is fairly common in the GAN-based image generation literature, we aim for a different goal of generating CNN feature responses. This prevents access to a dataset of real'' feature responses across the pose manifold, since these are generally unknown. While an ImageNet CNN could be used to produce some features, the problem that we are trying to solve is exactly the fact that ImageNet CNNs do not effectively model the pose manifold. Hence, the GAN formalism of learning to match a real'' distribution is not easily applicable to trajectory transfer.\"","":""}
{"id":"2962876782","dialogue":"\"The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.\"","summary":"\"Instead, trajectory transfer is more closely related to the topic of transfer learning, where, now, there is extensive work on problems such as zero-shot @cite_26 @cite_38 @cite_13 or @math -shot @cite_7 @cite_37 @cite_16 @cite_31 learning. However, these methods tend to be of general purpose. In some cases, they exploit generic semantic properties, such as attributes or affordances @cite_38 @cite_13 , in others they simply rely on generic machine learning for domain adaptation @cite_26 , transfer learning @cite_37 or, more recently, meta-learning @cite_16 @cite_41 @cite_31 . None of these methods exploits specific properties of the pose manifold, such as the parametrizations of Figure . The introduction of networks that enforce such parameterizations is a form of regularization that improves on the transfer performance of generic procedures. This was shown on the AGA work @cite_32 and is confirmed by our results, which show even larger gains over very recent generic methods, such as feature hallucination proposed in @cite_24 .\"","":""}
{"id":"2962876782","dialogue":"\"The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.\"","summary":"\".5ex Finally, trajectory transfer is of interest for problems involving multi-view recognition. Due to the increased cost of multi-view imaging, these problems frequently include some degree of learning from computer generated images. This is, for example, an established practice in the shape recognition literature, where synthetic image datasets @cite_12 @cite_2 are routinely used. The emergence of these artificial datasets has enabled a rich literature in shape recognition methods @cite_5 @cite_19 @cite_6 @cite_4 @cite_11 @cite_28 and already produced some interesting conclusions. For example, while many representations have been proposed, there is some evidence that the problem could be solved as one of multi-view recognition, using simple multi-view extensions of current CNNs @cite_0 . It is not clear, however, how these methods or conclusions generalize to real world images. Our results show that feature trajectory transfer models, such as FATTEN, learned on synthetic datasets, such as ModelNet @cite_23 , can be successfully transferred to real image datasets, such as SUN-RGBD @cite_34 .\"","":""}
{"id":"2783270640","dialogue":"\"The framework of Integral Quadratic Constraints (IQC) introduced by (2014) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to semi-definite programming (SDP). In particular, this technique was applied to Nesterov's accelerated method (NAM). For quadratic functions, this SDP was explicitly solved leading to a new bound on the convergence rate of NAM, and for arbitrary strongly convex functions it was shown numerically that IQC can improve bounds from Nesterov (2004). Unfortunately, an explicit analytic solution to the SDP was not provided. In this paper, we provide such an analytical solution, obtaining a new general and explicit upper bound on the convergence rate of NAM, which we further optimize over its parameters. To the best of our knowledge, this is the best, and explicit, upper bound on the convergence rate of NAM for strongly convex functions.\"","summary":"\"Several recent works have revisited NAM and computed bounds on its convergence rate based on different techniques. In addition to @cite_6 , the following works are relevant. @cite_14 views NAM as a linear coupling between GD and Mirror Descent, and, for @math , re-derives the previously known bound @math @cite_12 , with the choice @math and @math , which is different from Nesterov's bound. This rate is not of the type that we consider in this paper.\"","":""}
{"id":"2783270640","dialogue":"\"The framework of Integral Quadratic Constraints (IQC) introduced by (2014) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to semi-definite programming (SDP). In particular, this technique was applied to Nesterov's accelerated method (NAM). For quadratic functions, this SDP was explicitly solved leading to a new bound on the convergence rate of NAM, and for arbitrary strongly convex functions it was shown numerically that IQC can improve bounds from Nesterov (2004). Unfortunately, an explicit analytic solution to the SDP was not provided. In this paper, we provide such an analytical solution, obtaining a new general and explicit upper bound on the convergence rate of NAM, which we further optimize over its parameters. To the best of our knowledge, this is the best, and explicit, upper bound on the convergence rate of NAM for strongly convex functions.\"","summary":"\"The work of @cite_3 views (an adaptive version of) NAM with @math as the discretization of the second-order ODE @math . For @math and @math they obtain @math If @math , this leads to @math . Unfortunately, @cite_3 show that their framework is incapable of providing linear convergence rates in general, which we know to hold for @math .\"","":""}
{"id":"2783270640","dialogue":"\"The framework of Integral Quadratic Constraints (IQC) introduced by (2014) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to semi-definite programming (SDP). In particular, this technique was applied to Nesterov's accelerated method (NAM). For quadratic functions, this SDP was explicitly solved leading to a new bound on the convergence rate of NAM, and for arbitrary strongly convex functions it was shown numerically that IQC can improve bounds from Nesterov (2004). Unfortunately, an explicit analytic solution to the SDP was not provided. In this paper, we provide such an analytical solution, obtaining a new general and explicit upper bound on the convergence rate of NAM, which we further optimize over its parameters. To the best of our knowledge, this is the best, and explicit, upper bound on the convergence rate of NAM for strongly convex functions.\"","summary":"\"The work of @cite_4 focuses only on a convex quadratic function @math , and obtain @math for @math and @math , basically re-deriving .\"","":""}
{"id":"2783270640","dialogue":"\"The framework of Integral Quadratic Constraints (IQC) introduced by (2014) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to semi-definite programming (SDP). In particular, this technique was applied to Nesterov's accelerated method (NAM). For quadratic functions, this SDP was explicitly solved leading to a new bound on the convergence rate of NAM, and for arbitrary strongly convex functions it was shown numerically that IQC can improve bounds from Nesterov (2004). Unfortunately, an explicit analytic solution to the SDP was not provided. In this paper, we provide such an analytical solution, obtaining a new general and explicit upper bound on the convergence rate of NAM, which we further optimize over its parameters. To the best of our knowledge, this is the best, and explicit, upper bound on the convergence rate of NAM for strongly convex functions.\"","summary":"\"Finally, @cite_7 gives a possible geometric interpretation of why NAM accelerates convergence. For their NAM-type method, the result @math is obtained, basically re-deriving .\"","":""}
{"id":"2783002080","dialogue":"\"Traditional radio systems are strictly co-designed on the lower levels of the OSI stack for compatibility and efficiency. Although this has enabled the success of radio communications, it has also introduced lengthy standardization processes and imposed static allocation of the radio spectrum. Various initiatives have been undertaken by the research community to tackle the problem of artificial spectrum scarcity by both making frequency allocation more dynamic and building flexible radios to replace the static ones. There is reason to believe that just as computer vision and control have been overhauled by the introduction of machine learning, wireless communication can also be improved by utilizing similar techniques to increase the flexibility of wireless networks. In this work, we pose the problem of discovering low-level wireless communication schemes ex-nihilo between two agents in a fully decentralized fashion as a reinforcement learning problem. Our proposed approach uses policy gradients to learn an optimal bi-directional communication scheme and shows surprisingly sophisticated and intelligent learning behavior. We present the results of extensive experiments and an analysis of the fidelity of our approach.\"","summary":"\"Since Widrow's early work, research into the use of neural architectures and machine learning techniques to address problems in communications has taken off in many directions. Ibnkahla's comprehensive review of the intersection of communications and machine learning @cite_9 is a testament to the impressive efforts made in this area of research. His survey lists various learning-based approaches to adaptive equalization, nonlinear channel modeling, coding, error correcting codes, spread spectrum applications, network planning, modulation detection and many more. Due to the vastness of this field, we refer the reader to the review. Research in making radio agents adaptive to achieve cooperative goals began in 1999, when Mitola introduced the concept of the cognitive radio @cite_13 . His proposal was for cognitive radios to use model-based reasoning to achieve competency in radio related tasks using both supervised and unsupervised learning. A review of cognitive radio work in years after that is in @cite_15 .\"","":""}
{"id":"2783002080","dialogue":"\"Traditional radio systems are strictly co-designed on the lower levels of the OSI stack for compatibility and efficiency. Although this has enabled the success of radio communications, it has also introduced lengthy standardization processes and imposed static allocation of the radio spectrum. Various initiatives have been undertaken by the research community to tackle the problem of artificial spectrum scarcity by both making frequency allocation more dynamic and building flexible radios to replace the static ones. There is reason to believe that just as computer vision and control have been overhauled by the introduction of machine learning, wireless communication can also be improved by utilizing similar techniques to increase the flexibility of wireless networks. In this work, we pose the problem of discovering low-level wireless communication schemes ex-nihilo between two agents in a fully decentralized fashion as a reinforcement learning problem. Our proposed approach uses policy gradients to learn an optimal bi-directional communication scheme and shows surprisingly sophisticated and intelligent learning behavior. We present the results of extensive experiments and an analysis of the fidelity of our approach.\"","summary":"\"Beyond the applications to game-playing and control tasks, some researchers have begun to investigate the use of reinforcement learning in various communication-based cooperative multi-agent tasks. @cite_11 applied variants of deep Q-networks (DQN) without experience replay to prisoner's games with multiple agents. The considered problems require the agents to communicate over a very simple noiseless channel of small bandwidth and develop a collaborative strategy. Mordatch and Abbeel @cite_17 show how a grounded, compositional language can emerge when agents have to communicate their intentions to each other in order to maximize their reward. Finally, Abadi and Andersen @cite_4 study the problem of learning encryption between agents with the help of an adversary.\"","":""}
{"id":"2964205637","dialogue":"\"The reflections caused by common semi-reflectors, such as glass windows, can impact the performance of computer vision algorithms. State-of-the-art methods can remove reflections on synthetic data and in controlled scenarios. However, they are based on strong assumptions and do not generalize well to real-world images. Contrary to a common misconception, real-world images are challenging even when polarization information is used. We present a deep learning approach to separate the reflected and the transmitted components of the recorded irradiance, which explicitly uses the polarization properties of light. To train it, we introduce an accurate synthetic data generation pipeline, which simulates realistic reflections, including those generated by curved and non-ideal surfaces, non-static scenes, and high-dynamic-range scenes.\"","summary":"\"can leverage gradient information to solve the problem. Levin and Weiss, for instance, require manual input to separate gradients of the reflection and the transmission @cite_14 . Methods that are fully automated can distinguish the gradients of the reflected and transmitted images by leveraging the defocus blur @cite_13 : reflections can be blurry because the subject behind the semi-reflector is much closer than the reflected image @cite_16 , or because the camera is focused at infinity and the reflected objects are close to the surface @cite_1 . Moreover, for the case of double-pane or thick windows, the reflection can appear doubled'' @cite_24 , and this can be used to separate it from the transmitted image @cite_32 . While these methods show impressive results, their assumptions are stringent and do not generalize well to real-world cases, causing them to fail on common cases.\"","":""}
{"id":"2964205637","dialogue":"\"The reflections caused by common semi-reflectors, such as glass windows, can impact the performance of computer vision algorithms. State-of-the-art methods can remove reflections on synthetic data and in controlled scenarios. However, they are based on strong assumptions and do not generalize well to real-world images. Contrary to a common misconception, real-world images are challenging even when polarization information is used. We present a deep learning approach to separate the reflected and the transmitted components of the recorded irradiance, which explicitly uses the polarization properties of light. To train it, we introduce an accurate synthetic data generation pipeline, which simulates realistic reflections, including those generated by curved and non-ideal surfaces, non-static scenes, and high-dynamic-range scenes.\"","summary":"\"can also be used to remove reflections. Several methods propose different ways to estimate the relative motion of the reflected and transmitted image, which can be used to separate them @cite_25 @cite_12 @cite_29 @cite_28 @cite_27 . It is important to note that these methods assume static scenes---the motion is the apparent motion of the reflected layer relative to the transmitted layer, not scene motion. Other than that, these methods make assumptions that are less stringent than those made by single-image methods. Nonetheless, these algorithms work well when reflected and transmitted scenes are shallow in terms of depth, so that their velocity can be assumed uniform. For the case of spatially and temporally varying mixes, Kaftory and Zeevi propose to use sparse component analysis instead @cite_15 .\"","":""}
{"id":"2964205637","dialogue":"\"The reflections caused by common semi-reflectors, such as glass windows, can impact the performance of computer vision algorithms. State-of-the-art methods can remove reflections on synthetic data and in controlled scenarios. However, they are based on strong assumptions and do not generalize well to real-world images. Contrary to a common misconception, real-world images are challenging even when polarization information is used. We present a deep learning approach to separate the reflected and the transmitted components of the recorded irradiance, which explicitly uses the polarization properties of light. To train it, we introduce an accurate synthetic data generation pipeline, which simulates realistic reflections, including those generated by curved and non-ideal surfaces, non-static scenes, and high-dynamic-range scenes.\"","summary":"\"offer a third venue to tackle this problem. Assuming that images taken at different polarization angles offer independent measurements of the same scene, reflection and transmission can be separated using independent component analysis @cite_31 @cite_3 @cite_18 . An additional prior that can be leveraged is given by double reflections, when the semi-reflective surface generates them @cite_24 . Under ideal conditions, and leveraging polarization information, a solution can also be found in closed form @cite_26 @cite_4 . In our experiments, we found that most of the pictures captured in unconstrained settings break even the well-founded assumptions used by these papers, as shown in Figure .\"","":""}
{"id":"2773153974","dialogue":"\"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls.\"","summary":"\"In a broad sense we address anomaly detection in sequential data @cite_49 while focusing on intrusion detection in cyber-physical systems. Intrusion @cite_2 refers to possible security breaches in (cyber-)systems, namely malicious activity or policy violations. It covers both intrusions , i.e. attacks from the outside, and misuse, i.e. attacks from within the system. An intrusion detection system (IDS) is thus a device that monitors a system for detecting potential intrusions. The IDS will be referred to as NIDS if the detection takes place on a network and HIDS if it takes place on a host of a network. Furthermore, we distinguish i) signature-based IDS approaches, that detects attacks by looking for predefined specific patterns, such as byte sequences in network packets, or known malicious sequences of instructions used by malware, to ii) anomaly-based intrusion detection systems that were primarily introduced to detect unknown attacks (zero-day attacks).\"","":""}
{"id":"2773153974","dialogue":"\"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls.\"","summary":"\"In this work we exclusively address host intrusion detection system (HIDS) through semi-supervised anomaly-based approaches. Since Forrest's pioneering work @cite_37 most of HIDS (at least in the UNIX LINUX sphere) use system call sequencesas their primary source of information. Generally, sequences of system calls are represented as sequences of integer symbols, for which the order of occurrence of the symbol is of crucial importance. Numerous work and surveys have been published in the area of anomaly detection in the scope of intrusion detection, see @cite_11 @cite_25 , @cite_19 for recent studies. If we reduce the area of interest to anomaly detection in sequential data, four avenues for handling symbolic sequences are mainly followed:\"","":""}
{"id":"2773153974","dialogue":"\"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls.\"","summary":"\"@cite_40 are quite popular since a fixed size window enables a wide range of statistical, knowledge-based and machine learning techniques to be applied in a straightforward manner. A fixed-size window is first defined, and then it progressively slides along the tested sequence. Each window (basically a fixed-size subsequence) is in general represented by a feature vector. Then, models such as the one class Support Vector Machine @cite_17 , Multi Layer Perceptron and Convolutional Neural Networks @cite_47 are used to provide a score for deciding whether an anomaly is present or not.\"","":""}
{"id":"2773153974","dialogue":"\"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls.\"","summary":"\"@cite_0 process each sequence as a whole and a pair-wise sequence kernel (string kernel) is used to provide the sequence space with a similarity measure. The @math -Near-Neighbor rule or any of the so-called kernel machine methods can then be applied to model the 'normal' clusters and isolate the 'anomalies'. These approaches find their roots in text processing @cite_44 (Levenshtein's distance) or in Bioinformatics @cite_7 @cite_16 (Smith and Watermans), @cite_23 (Needleman-Wunsch), @cite_5 (BLAST) and Longest Common Subsequence (LCS) or Longest Similar Subsequence @cite_15 @cite_48 . Such methods do not seem to outperform window-based approaches and are in general much more costly in term of algorithmic complexity than state of the art methods.\"","":""}
{"id":"2773153974","dialogue":"\"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls.\"","summary":"\", essentially Hidden Markov Models (HMM) @cite_50 @cite_32 @cite_12 , Conditional random Fields (CRF) @cite_45 @cite_36 or Recurrent Neural Networks (RNN, LSTM, etc.) @cite_13 @cite_38 have been used with apparent success on various intrusion detection tasks, such as payload analysis or Network Layer Intrusion Detection or HIDS. However, the choice of parameters such as the order of the Markovian dependency, number of hidden variables, etc., is often the result of a compromise to avoid over-fitting, and long-term dependency is not necessarily easily modeled.\"","":""}
{"id":"2773153974","dialogue":"\"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls.\"","summary":"\"have been proposed initially to extract very simple n-gram features to enhance a vector space model similar to the one used in text mining @cite_4 , @cite_1 . Recently, a much ambitious model has been proposed that proposes to enact phrases and sentences, hence a language, from sequences of system calls @cite_6 . Nevertheless, these approaches suffer from the combinatorics explosion. When simple @math -grams models are used (with @math lower than @math or @math ) the size of the vector space model is very high (several millions of dimension) and in general the lack of available data to train the model limits its accuracy. In the case of approach @cite_6 , the combinatorics is much higher with an estimated feature space dimension of @math which makes this model intractable for common hardware.\"","":""}
{"id":"2783067984","dialogue":"\"This paper considers the exploitation of unmanned aerial vehicles (UAVs) in wireless networking, with which communication-enabled robots operate as flying wireless relays to provide connectivity or a capacity boost to a ground user. We focus on the particular problem of (automatic) UAV positioning, which greatly affects the end-to-end throughput performance. While existing methods rely on propagation distance minimiza- tion and statistical models for the presence or absence of a line-of-sight (LOS), we propose an approach capable of leveraging local topological information so as to offer better performance guarantees. The proposed method allows to strike a trade-off between minimizing distance path loss and discovering (near) LOS opportunities at locations away from the base station (BS)-user axis. Furthermore, the algorithm is shown to find the global optimal UAV position, although it only requires a local exploration of a signal strength map and the length of search trajectory is only linear to the geographical scale. Hence, it lends itself to online implementation. Significant throughput gains are found when compared to other positioning approaches based on LOS statistical models.\"","summary":"\"One essential issue for positioning the UAV is to predict the air-to-ground channel, since a precise UAV-user channel knowledge is usually not available before the UAV is sent to a target position. As a way to circumvent this problem, some prior works simply assumed LOS propagation from the UAV to the ground user, and hence the channel gain is merely an explicit continuous function of the UAV position. Using the LOS model, @cite_25 @cite_19 @cite_24 @cite_14 focused on UAV navigation problems, and @cite_5 studied the mimo channel to jointly optimize the UAV position and the orientation of a ula mounted at the UAV for the best spatial channel to multiple users. However, LOS models overestimate the channel gain in urban scenarios, especially in low altitude UAV applications, because there the air-to-ground signal is likely blocked by obstacles surrounding the user, and such a user-side shadowing effect significantly dominates the relay performance.\"","":""}
{"id":"2783139164","dialogue":"\"We present the Moments in Time Dataset","summary":"a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people","":""}
{"id":"2783139164","dialogue":"\"We present the Moments in Time Dataset","summary":"a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people","":""}
{"id":"2783893534","dialogue":"\"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21 on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.\"","summary":"\"Traditionally, image-based search methods drew their inspiration from textual retrieval systems @cite_25 . By using @math -means clustering method in the space of local feature descriptors such as SIFT @cite_19 , they are able to mimic textual word entities with the so-called visual words . Once the mapping from image salient keypoints to visually representative words was established, typical textual retrieval methods such as Bag-of-Words @cite_5 could be used. Video Google @cite_3 was one of the first visual search engines that relied on this concept. Several extensions of this concept were proposed, e.g. spatial verification @cite_9 that checks for geometrical correctness of initial query or fine-grained image search @cite_33 that accounts for semantic attributes of visual words.\"","":""}
{"id":"2783893534","dialogue":"\"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21 on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.\"","summary":"\"Successful applications of deep learning techniques in other computer vision applications have motivated researchers to apply those methods also to visual search. Although preliminary results did not seem promising due to the lack of robustness to cropping, scaling and image clutter @cite_10 , later works proved potential of those methods in the domain of image-based retrieval @cite_27 . Many other deep architectures such as Siamese networks were also proposed, and proved successful when applied to content-based image retrieval @cite_8 .\"","":""}
{"id":"2783893534","dialogue":"\"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21 on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.\"","summary":"\"Comparing the style similarity of two objects or scenes is one of the challenges that have to be addressed when training a machine learning model for interior design or fashion retrieval application. This problem is far from being solved mainly due to the lack of a clear metric defining how to measure style similarity. Various approaches have been proposed for defining style similarity metric. Some of them focus on evaluating similarity between shapes based on their structures @cite_28 @cite_30 and measuring the differences between scales and orientations of bounding boxes. Other approaches propose the structure-transcending style similarity that accounts for element similarity @cite_18 . In this work, we follow @cite_2 , and define style as We enforce this definition by including context information that groups different objects together (in terms of clothing items in an outfit or furniture in a room picture in interior design catalog). This allows us to a take data-driven approach that measures style similarity without using hand-crafted features and predefined styles.\"","":""}
{"id":"2783893534","dialogue":"\"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21 on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.\"","summary":"\"There has been a significant number of works published in the domain of fashion item retrieval or recommendation due to the potential of their application in highly profitable e-commerce business. Some of them focused on the notion of fashionability, e.g @cite_0 rated a user's photo in terms of how fashionable it is and provided fashion recommendations that would increase overall outfit score. Others focused on fashion items retrieval from online database when presented with user photos taken 'in the wild' usually with phone cameras @cite_16 . Finally, there is ongoing research in terms of clothing cosegmentation @cite_11 @cite_35 that is an important preprocessing step for better item retrieval results.\"","":""}
{"id":"2783893534","dialogue":"\"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21 on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.\"","summary":"\"@cite_6 present an encoder-decoder pipeline that learns a joint multimodal embedding (VSE) from images and a text","":""}
{"id":"2783893534","dialogue":"\"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21 on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.\"","summary":"\"Xintong @cite_4 train bi-LSTM model to predict next item in the outfit generation. Moreover","":""}
{"id":"2783668304","dialogue":"\"In large storage systems, files are often coded across several servers to improve reliability and retrieval speed. We consider a system of @math servers storing files using a Maximum Distance Separable code (cf. li2016mean ). Specifically, each file is stored in equally sized pieces across @math servers such that any @math pieces can reconstruct the original file. File requests are routed using the Batch Sampling routing scheme. I.e. when a request for a file is received, a centralized dispatcher routes the job into the @math -shortest queues among the @math for which the corresponding server contains a piece of the file being requested. We study the long time behavior of this class of load balancing mechanisms. In particular, it is shown that the ODE system that describes the mean field limit of the occupancy measure process has a unique fixed point which is stable. This fixed point corresponds to a distribution on @math of queue lengths with tails that decay super-exponentially. Upper and lower bounds on the decay rate are provided. Finally, we show that the unique invariant measure of the Markov occupancy measure process converges to the Dirac measure concentrated at the unique fixed point of the ODE system, establishing the interchangeability of the @math and @math limits.\"","summary":"\"Load balancing mechanisms similar to the type considered here have been studied in many works. Specifically, the Join-the-Shortest-Queue (JSQ), Join-the-Idle-Queue (JIQ), and Power-of- @math (also known as the supermarket model) routing schemes have garnered quite a bit of attention (see @cite_6 @cite_5 @cite_4 @cite_8 @cite_13 @cite_7 @cite_12 @cite_10 @cite_9 and references therein). The papers @cite_6 @cite_2 were the first to study the tail behavior of the fixed point of the ODE system associated with the Power-of- @math routing scheme, showing that in steady-state the fraction of queues with lengths exceeding @math decay super-exponentially in @math , a large improvement over the exponential rate for the setting where jobs are routed to servers uniformly at random. Later works on a similar theme include @cite_4 @cite_3 @cite_9 . In all these works, the authors study fluid and diffusion approximations for various types of load balancing mechanisms. In each case, stable fixed points are identified for the LLN limit and the interchangeability of limit property is established.\"","":""}
{"id":"2783645201","dialogue":"\"Recovering the latent photorealistic faces from their artistic portraits aids human perception and facial analysis. However, recovering photorealistic faces from stylized portraits while preserving identity is challenging because the fine details of real faces can be distorted or lost in stylized images. In this paper, we present a new Identity-preserving Face Recovery from Portraits (IFRP) method to recover latent photorealistic faces from unaligned stylized portraits. Our IFRP method consists of two components: Style Removal Network (SRN) and Discriminative Network (DN). The SRN is designed to transfer feature maps of stylized images to the feature maps of the corresponding photorealistic faces. By embedding spatial transformer networks into the SRN, our method can compensate for misalignments of stylized faces automatically and output aligned realistic face images. The DN is used to enforce recovered face images to be similar to authentic faces. To ensure the identity preservation, we promote the recovered and ground-truth faces to share similar visual features via a distance measure which compares features of recovered and ground-truth faces extracted from a pre-trained VGG network. Our approach is evaluated on a large-scale synthesized dataset of real and stylized face pairs and outperforms the state-of-the-art methods. In addition, we demonstrate that our method can also recover photorealistic faces from unseen stylized portraits (unavailable in training) as well as original paintings.\"","summary":"\"There exist many generative models for the problem of image generation @cite_5 @cite_36 @cite_5 @cite_49 @cite_3 @cite_6 @cite_46 . Among them, GANs are conceptually closely related to our problem as they employ an adversarial loss that forces the generated images to be as photorealistic as the ground-truth images.\"","":""}